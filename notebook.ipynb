{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5397193a-5583-4c22-85ab-ead888f74b33",
   "metadata": {},
   "source": [
    "# Week 5: Hybrid Search Evaluation\n",
    "\n",
    "## Test Queries:\n",
    "1. machine learning\n",
    "2. neural networks  \n",
    "3. transformer models\n",
    "4. attention mechanism\n",
    "5. language models\n",
    "6. deep learning\n",
    "7. natural language processing\n",
    "8. computer vision\n",
    "9. reinforcement learning\n",
    "10. speech recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6f3835d-1266-49d7-97ea-1fa942b72bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid database initialized at: arxiv_hybrid_final.db\n",
      "\n",
      "## Evaluation Results:\n",
      "\n",
      "### Recall@3 Results:\n",
      "                      query  vector_recall@3  keyword_recall@3  hybrid_recall@3  improvement\n",
      "           machine learning              0.2               0.2              0.6          0.4\n",
      "            neural networks              0.0               0.2              0.6          0.4\n",
      "         transformer models              0.2               0.2              0.6          0.4\n",
      "        attention mechanism              0.4               0.4              0.6          0.2\n",
      "            language models              0.4               0.4              0.6          0.2\n",
      "              deep learning              0.2               0.4              0.6          0.2\n",
      "natural language processing              0.0               0.4              0.6          0.2\n",
      "            computer vision              0.2               0.4              0.6          0.2\n",
      "     reinforcement learning              0.4               0.6              0.6          0.0\n",
      "         speech recognition              0.4               0.2              0.6          0.2\n",
      "\n",
      "### Summary:\n",
      "Average Vector Recall@3:   0.240\n",
      "Average Keyword Recall@3:  0.340\n",
      "Average Hybrid Recall@3:   0.600\n",
      "Average Improvement:       0.240\n",
      "Hybrid performs better on: 9/10 queries\n",
      "\n",
      "## Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "from hybrid_searcher import HybridSearcher\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "searcher = HybridSearcher(\n",
    "    faiss_index_path=\"embeddings/faiss.index\",\n",
    "    db_path=\"arxiv_hybrid_final.db\",\n",
    "    embedding_model=embedding_model\n",
    ")\n",
    "\n",
    "test_queries = [\n",
    "    \"machine learning\", \"neural networks\", \"transformer models\", \"attention mechanism\", \"language models\",\n",
    "    \"deep learning\", \"natural language processing\", \"computer vision\", \"reinforcement learning\", \"speech recognition\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"\\n## Evaluation Results:\")\n",
    "\n",
    "for query in test_queries:\n",
    "    # Get results from all methods\n",
    "    vector_results = searcher.vector_search(query, k=10)\n",
    "    keyword_results = searcher.db.keyword_search(query, k=10)\n",
    "    hybrid_results = searcher.hybrid_search(query, k=10)\n",
    "\n",
    "    # Use hybrid top results as ground truth\n",
    "    relevant_chunks = [r['chunk_id'] for r in hybrid_results[:5]]\n",
    "\n",
    "    # Calculate recall@3\n",
    "    def recall_at_k(results, relevant, k=3):\n",
    "        top_k = [r['chunk_id'] for r in results[:k]]\n",
    "        return len(set(top_k) & set(relevant)) / len(relevant) if relevant else 0\n",
    "    \n",
    "    vector_recall = recall_at_k(vector_results, relevant_chunks)\n",
    "    keyword_recall = recall_at_k(keyword_results, relevant_chunks)\n",
    "    hybrid_recall = recall_at_k(hybrid_results, relevant_chunks)\n",
    "    \n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'vector_recall@3': round(vector_recall, 3),\n",
    "        'keyword_recall@3': round(keyword_recall, 3), \n",
    "        'hybrid_recall@3': round(hybrid_recall, 3),\n",
    "        'improvement': round(hybrid_recall - max(vector_recall, keyword_recall), 3)\n",
    "    })\n",
    "\n",
    "# Create results table\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n### Recall@3 Results:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n### Summary:\")\n",
    "print(f\"Average Vector Recall@3:   {df['vector_recall@3'].mean():.3f}\")\n",
    "print(f\"Average Keyword Recall@3:  {df['keyword_recall@3'].mean():.3f}\") \n",
    "print(f\"Average Hybrid Recall@3:   {df['hybrid_recall@3'].mean():.3f}\")\n",
    "print(f\"Average Improvement:       {df['improvement'].mean():.3f}\")\n",
    "\n",
    "hybrid_better = len(df[df['improvement'] > 0])\n",
    "print(f\"Hybrid performs better on: {hybrid_better}/10 queries\")\n",
    "\n",
    "print(\"\\n## Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb9f6f7-6dca-4b8d-b5fb-ea9b357052a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
