{"id": "1904.07695v1_chunk_0", "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1 Short Text Topic Modeling Techniques, Applications, and Performance: A Survey Jipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu, Fellow, IEEE, Abstract—Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the ﬁrst comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a uniﬁed interface, benchmark datasets, to facilitate the expansion of new methods in this research ﬁeld. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm. Index Terms—Topic modeling, Short text, Sparseness, Short text topic modeling. ! 1 INTRODUCTION Short texts have become an important information source including news headlines, status updates, web page snippets, tweets, question/answer pairs, etc. Short text analysis has been attracting increasing attention in recent years due to the ubiquity of short text in the real-world [1]–[3]. Effective and efﬁcient models infer the latent topics from short texts, which can help discover the latent semantic structures that occur in a collection of documents. Short text topic modeling algorithms are always applied into many tasks such as topic detection [4], classiﬁcation [5], comment summarization [6], user interest proﬁling [7]. Traditional topic modeling algorithms such as probabilistic latent semantic analysis (PLSA) [8] and latent Dirichlet allocation (LDA) [9] are widely adopted for discovering latent semantic structure from text corpus without requiring any prior annotations or labeling of the documents. In these algorithms, each document may be viewed as a mixture of various topics and each topic is characterized by a distribution over all the words. Statistical techniques (e.g., Variational methods and Gibbs sampling) are then employed to infer the latent topic distribution of each document and the word distribution of each topic using higher- order word co-occurrence patterns [10]. These algorithms and their variants have had a major impact on numerous applied ﬁelds in modeling text collections news articles, research papers, and blogs [11]–[13]. However, traditional topic models experience large performance degradation over short texts due to the lack of word co-occurrence information in each short text [1], [14]. Therefore, short text topic modeling has already attracted much • J. Qiang, Z. Qian, Y. Li and Y. Yuan are with the Department of", "meta": {"paper_id": "1904.07695v1", "chunk_index": 0, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_1", "text": "[11]–[13]. However, traditional topic models experience large performance degradation over short texts due to the lack of word co-occurrence information in each short text [1], [14]. Therefore, short text topic modeling has already attracted much • J. Qiang, Z. Qian, Y. Li and Y. Yuan are with the Department of Computer Science, Yangzhou, Jiangsu, P. R. China, 225127. E-mail: {jpqiang,liyun, yhyuan}@yzu.edu.cn • X. Wu is with Department of Computer Science, University of Louisiana at Lafayette, Louisiana, USA. E-mail: xwu@louisiana.edu.edu attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. Earlier works [15], [16] still used traditional topic models for short texts, but exploited external knowledge or metadata to bring in additional useful word co-occurrences across short texts, and therefore may boost the performance of topic models. For example, Phan et al. [16] ﬁrst learned latent topics from Wikipedia, and then inferred topics from short texts. Weng et al. [7] and Mehrotra et al. [17] aggregated tweets for pseudo-document using hashtags and the same user respectively. The problem lies in that auxiliary information or metadata is not always available or just too costly for deployment. These studies suggest that topic models speciﬁcally designed for general short texts are imperative. This survey will provide a taxonomy that captures the existing short text topic modeling algorithms and their application domains. News aggregation websites often rely on news headlines to cluster different source news about the same event. In Table 1, we show an event about artiﬁcial intelligence reported on March 1, 2018. As presented, all these short texts were reported about the same event. From these short texts, we can found these following characteristics. (1) Obviously, each short text lacks enough word co-occurrence information. (2) Due to a few words in each text, most texts are probably generated by only one topic (e.g, text 1, text2, text 3). (3) Statistical information of words among texts cannot fully capture words that are semantically related but rarely co-occur. For example, President Trump of text 1 and White House of text 2 are highly semantically related, and AI is short for Artiﬁcal Intelligence. (4) The single-topic assumption may be too strong for some short texts. For example, text 3 is probably associated with a small number of topics (e.g., one to three topics). Considering these characteristics, existing short text topic modeling algorithms were proposed by trying to solve one or two of these characteristics. Here, we divide the short text topic modeling algorithms basically into the following three major arXiv:1904.07695v1 [cs.IR] 13 Apr 2019 JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 2 TABLE 1 An event about artiﬁcial intelligence was reported by different news media on March 1, 2018. Number Media Headline 1 Lawfare President Trump’s Executive Order on Artiﬁcial Intelligence 2 Nextgov White Houses Race to Maintain AI Dominance Misses Opportunity 3 Forbes Artiﬁcial Intelligence Regulation may be Impossible 4 CognitiveWorld Pop Culture, AI and Ethics categories. (1) Dirichlet multinomial mixture (DMM) based methods: A simple and effective model, Dirichlet Multinomial Mixture", "meta": {"paper_id": "1904.07695v1", "chunk_index": 1, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_2", "text": "Lawfare President Trump’s Executive Order on Artiﬁcial Intelligence 2 Nextgov White Houses Race to Maintain AI Dominance Misses Opportunity 3 Forbes Artiﬁcial Intelligence Regulation may be Impossible 4 CognitiveWorld Pop Culture, AI and Ethics categories. (1) Dirichlet multinomial mixture (DMM) based methods: A simple and effective model, Dirichlet Multinomial Mixture model, has been adopted to infer latent topics in short texts [18], [19]. DMM follows the simple assumption that each text is sam- pled from only one latent topic. Considering the characteristics (1) and (2) in short texts, this assumption is reasonable and suitable for short texts compared to the complex assumption adopted by LDA that each text is modeled over a set of topics [20], [21]. Nigam et al. [22] proposed an EM-based algorithm for Dirichlet Multi- nomial Mixture (DMM) model. Except for the basic expectation maximization (EM), a number of inference methods have been used to estimate the parameters including variation inference and Gibbs sampling. For example, Yu et al. [23] proposed the DMAFP model based on variational inference algorithm [24]. Yin et al. [18] proposed a collapsed Gibbs sampling algorithm for DMM. Other variations based on DMM [25]–[27] were proposed for improving the performance. The above models based on DMM ignore the characteristic (3). Therefore, many models by incorporating word embeddings into DMM were proposed [28], [29], because word embeddings learned from millions of external documents contain semantic information of words [30]. Not only word co-occurrence words belong to one topic, but words with high similarity have high probability belonging to one topic, which can effectively solve the data sparsity issue. To highlight the characteristic (4), a Poisson-based DMM model (PDMM) was proposed that allows each short text is sampled by a limited number of topics [31]. Accordingly, Li et al. [31] proposed a new model by directly extending the PDMM model using word embeddings. (2) Global word co-occurrences based methods: Consider- ing the characteristic (1), some models try to use the rich global word co-occurrence patterns for inferring latent topics [14], [32]. Due to the adequacy of global word co-occurrences, the sparsity of short texts is mitigated for these models. According to the utilizing strategies of global word co-occurrences, this type of models can be divided into two types. 1) The ﬁrst type directly uses the global word co-occurrences to infer latent topics. Biterm topic modeling (BTM) [14] posits that the two words in a biterm share the same topic drawn from a mixture of topics over the whole corpus. Some models extend the Biterm Topic Modeling (BTM) by incorporating the burstiness of biterms as prior knowledge [21] or distinguishing background words from topical words [33]. 2) The second type ﬁrst constructs word co-occurrence network using global word co-occurrences and then infers latent topics from this network, where each word correspond to one node and the weight of each edge stands for the empirical co-occurrence probability of the connected two words [32], [34]. (3) Self-aggregation based methods: Self-aggregation based methods are proposed to perform topic modeling and text self- aggregation during topic inference simultaneously. Short texts are merged", "meta": {"paper_id": "1904.07695v1", "chunk_index": 2, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_3", "text": "each word correspond to one node and the weight of each edge stands for the empirical co-occurrence probability of the connected two words [32], [34]. (3) Self-aggregation based methods: Self-aggregation based methods are proposed to perform topic modeling and text self- aggregation during topic inference simultaneously. Short texts are merged into long pseudo-documents before topic inference that can help improve word co-occurrence information. Different from the aforementioned aggregation strategies [7], [17], this type of methods SATM [20] and PTM [35] posit that each short text is sampled from a long pseudo-document unobserved in current text collection, and infer latent topics from long pseudo-documents, without depending on auxiliary information or metadata. Consid- ering the characteristic (3), Qiang et al. [36] and Bicalho et al. [37] merged short texts into long pseudo-documents using word embeddings. 1.1 Our contributions This survey has the following three-pronged contribution: (1) We propose a taxonomy of algorithms for short text topic modeling and explain their differences. We deﬁne three different tasks, i.e., application domains of short text topic modeling tech- niques. We illustrate the evolution of the topic, the challenges it faces, and future possible research directions. (2) To facilitate the expansion of new methods in this ﬁeld, we develop the ﬁrst comprehensive open-source JAVA library, called STTM, which not only includes all short text topic mod- eling algorithms discussed in this survey with a uniform easy- to-use programming interface but also includes a great num- ber of designed modules for the evaluation and application of short text topic modeling algorithms. STTM is open-sourced at https://github.com/qiang2100/STTM. (3) We ﬁnally provide a detailed analysis of short text topic modeling techniques and discuss their performance on various applications. For each model, we analyze their results through comprehensive comparative evaluation on some common datasets. 1.2 Organization of the survey The rest of this survey is organized as follows. In Section 2, we introduce the task of short text topic modeling. Section 3 proposes a taxonomy of short text topic modeling algorithms and provides a description of representative approaches in each category. The list of applications for which researchers have used the short text topic modeling algorithms is provided in Section 4. Section 5 presents our Java library for short text topic modeling algorithms. In the next two sections, we describe the experimental setup (Section 6) and evaluate the discussed models (Section 7). Finally, we draw our conclusions and discuss potential future research directions in Section 8. 2 DEFINITIONS In this section, we formally deﬁne the problem of short text topic modeling. Given a short text corpus D of N documents, with a vocabu- lary W of size V , and K pre-deﬁned latent topics. One document d is represented as (wd,1, wd,2, ..., wd,nd) in D including nd words. A topic φ in a given collection D is deﬁned as a multino- mial distribution over the vocabulary W, i.e., {p(w|φ)}w∈W . JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 3 TABLE 2 The notations of symbols used in the paper D, N Documents and number of documents in the", "meta": {"paper_id": "1904.07695v1", "chunk_index": 3, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_4", "text": "in a given collection D is deﬁned as a multino- mial distribution over the vocabulary W, i.e., {p(w|φ)}w∈W . JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 3 TABLE 2 The notations of symbols used in the paper D, N Documents and number of documents in the corpus W, V The vocabulary and number of words in the vocabulary K Number of pre-deﬁned latent topics l Average length of each document in D nk Number of words associated with topic k mk Number of documents associated with topic k nw k Number of word w associated with topic k in −→ d nd Number of words in document d nw d Number of word w in document d nk d Number of word associated with topic k in document d nw k,d Number of word w associated with topic k in document d P Long pseudo-document set generated by models φ Topic distribution θ Document-topic distribution z Topic indicator U Number of dimensions in word embeddings ζ Time cost of considering GPU model ς Maximum number of topics allowable in a short text c Size of sliding window The topic representation of a document d, θd, is deﬁned as a multinomial distribution over K topics, i.e., {p(φk|θd)}k=1,...,K. The general task of topic modeling aims to ﬁnd K salient topics φk=1,...,K from D and to ﬁnd the topic representation of each document θd=1,...,N. Most classical probabilistic topic models adopt the Dirichlet prior for both the topics and the topic representation of documents, which are ﬁrst used in LDA [9], which is φk ∼Dirichlet(β) and θd ∼Dirichlet(α). In practice, the Dirichlet prior smooths the topic mixture in individual documents and the word distribution of each topic, which alleviates the overﬁtting problem of probabilistic latent semantic analysis (PLSA) [8], especially when the number of topics and the size of vocabulary increase. Therefore, all of existing short text topic modeling algorithms adopt Dirichlet distribution as prior distribution. Given a short text corpus D with a vocabulary of size V , and the predeﬁned number of topics K, the major tasks of short text topic modeling can be deﬁned as to: (1). Learn the word representation of topics φ; (2). Learn the sparse topic representation of documents θ. All the notations used in this paper are summarized in Table 2. 3 ALGORITHMIC APPROACHES: A TAXONOMY In the past decade, there has been much work to discover latent topics from short texts using traditional topic modeling algorithms by incorporating external knowledge or metadata. More recently, researchers focused on proposing new short text topic modeling algorithms. In the following, we present historical context about the research progress in this domain, then propose a taxonomy of short text topic modeling techniques including: (1) Dirichlet Multinomial Mixture (DMM) based methods, (2) Global word co-occurrence based methods, and (3) Self-aggregation based methods. 3.1 Short Text Topic Modeling Research Context and Evolution Traditional topic modeling algorithms such as probabilistic latent semantic analysis (PLSA) [8] and latent Dirichlet allocation (LDA) [9] are widely adopted for discovering latent semantic structure from text", "meta": {"paper_id": "1904.07695v1", "chunk_index": 4, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_5", "text": "based methods, (2) Global word co-occurrence based methods, and (3) Self-aggregation based methods. 3.1 Short Text Topic Modeling Research Context and Evolution Traditional topic modeling algorithms such as probabilistic latent semantic analysis (PLSA) [8] and latent Dirichlet allocation (LDA) [9] are widely adopted for discovering latent semantic structure from text corpus by capturing word co-occurrence pat- tern at the document level. Hence, more word co-occurrences would bring in more reliable and better topic inference. Due to the lack of word co-occurrence information in each short text, traditional topic models have a large performance degradation over short texts. Earlier works focus on exploiting external knowledge to help enhance the topic inference of short texts. For example, Phan et al. [16] adopted the learned latent topics from Wikipedia to help infer the topic structure of short texts. Similarly, Jin et al. [15] searched auxiliary long texts for short texts to infer latent topics of short texts for clustering. A large regular text corpus of high quality is required by these models, which bring in big limitation for these models. Since 2010, research on topic discovery from short texts has been shifted to merging short texts into long pseudo-documents using different aggregation strategies before adopting traditional topic modeling to infer the latent topics. For example, Weng et al. [7] merge all tweets of one user into a pseudo-document before using LDA. Other information includes hashtags, timestamps, and named entities have been tread as metadata to merging short texts [17], [19], [38]. However, helpful metadata may not be accessible in any domains, e.g., news headlines and search snippets. These studies suggest that topic models speciﬁcally designed for general short texts are crucial. This survey will provide a taxonomy that captures the existing strategies and these application domains. 3.2 A Taxonomy of Short Text Topic Modeling Methods We propose a taxonomy of short text topic modeling ap- proaches. We categorize topic modeling approaches into three broad categories: (1) Dirichlet Multinomial Mixture (DMM) based, (2) Global Word co-occurrences based, and (3) Self- aggregation based. Below we describe the characteristics of each of these categories and present a summary of some representative methods for each category (cf. Table 3). 3.3 Dirichlet Multinomial Mixture based Methods Dirichlet Multinomial Mixture model (DMM) was ﬁrst pro- posed by Nigam et al. [22] based on the assumption that each document is sampled by only one topic. The assumption is more ﬁt for short texts than the assumption that each text is generated by multiple topics. Therefore, many models for short texts were proposed based on this simple assumption [19], [23], [25]. Yin et al. [18] proposed a DMM model based on collapse Gibbs sampling. Zhao et al. [19] proposed a Twitter-LDA model by assuming that one tweet is generated from one topic. Recently, more work incorporates word embeddings into DMM [29], [31]. 3.3.1 GSDMM DMM respectively chooses Dirichlet distribution for topic- word distribution φ and document-topic distribution θ as prior distribution with parameter α and β. DMM samples a topic zd for the document d by Multinomial distribution θ, and then generates all", "meta": {"paper_id": "1904.07695v1", "chunk_index": 5, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_6", "text": "more work incorporates word embeddings into DMM [29], [31]. 3.3.1 GSDMM DMM respectively chooses Dirichlet distribution for topic- word distribution φ and document-topic distribution θ as prior distribution with parameter α and β. DMM samples a topic zd for the document d by Multinomial distribution θ, and then generates all words in the document d from topic zd by Multinomial distribution φzd. The graphical model of DMM is shown in Figure 1. The generative process for DMM is described as follows: (1). Sample a topic proportion θ ∼Dirichlet(α). (2). For each topic k ∈{1, ..., K}: Draw a topic-word distribution θk ∼Dirichlet(β). (3). For each document d ∈D: (b)Sample a topic zd ∼Multinomial(θ). JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 4 TABLE 3 List of short text topic modeling approaches Category Year Published Authors Method Time Complexity of One Iteration DMM 2014 KDD [18] J. Yin & et al. GSDMM O(KNl) 2015 TACL [28] D. Nguyen & et al. LF-DMM O(O(2KNl + KV U)) 2016 SIGIR [29] C. Li & et al. GPU-DMM O(KNl + Nlζ + KV ) 2017 TOIS [31] C. Li & et al. GPU-PDMM O(Nl Pς−1 i=1 Ci K + Nlζ + KV ) Global word 2013 WWW [14] X. Chen & et al. BTM O(KNlc) co-occurrences 2016 KAIS [32] Y. Zuo & et al. WNTM O(KNlc(c −1)) Self-aggregation 2015 IJCAI X. Quan & et al. SATM O(NlPK) 2016 KDD Y. Zuo & et al. PTM O(Nl(P + K)) (c)For each word w ∈{wd,1, ..., wd,nd}: Sample a word w ∼Multinomial(φzd). Fig. 1. Graphical Model of GSDMM. Gibbs sampling algorithm for Dirichlet Multinomial Mixture model is denoted as GSDMM, which is based on the assumption that each text is sampled by a single topic [18]. Here, for better representation of latent topics, we represent a topic with the topic feature (CF) vector, which essentially is a big document combined with its documents. The TF vector of a topic k is deﬁned as a tuple {nw k (w ∈ W), mk, nk}, where nw k is the number of word w in topic k, mk is the number of documents in topic k, and nk is the number of words in topic k. The topic feature (TF) presents important addible and deletable properties, as described next. (1) Addible Property. A document d can be efﬁciently added to topic k by updating its TF vector as follows. nw k = nw k + nw d for each word w in d mk = mk + 1 ; nk = nk + nd (2) Deletable Property. A document d can be efﬁciently deleted from topic k by updating its TF vector as follows. nw k = nw k −nw d for each word w in d mk = mk −1 ; nk = nk −nd The hidden multinomial variable (zd) for document d is sampled based on collapsed Gibbs sampling, conditioned on a complete assignment of all other hidden variables. GSDMM uses the following conditional probability distribution to infer its topic, p(zd = k|Z¬d, D) ∝ mk,¬d", "meta": {"paper_id": "1904.07695v1", "chunk_index": 6, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_7", "text": "= mk −1 ; nk = nk −nd The hidden multinomial variable (zd) for document d is sampled based on collapsed Gibbs sampling, conditioned on a complete assignment of all other hidden variables. GSDMM uses the following conditional probability distribution to infer its topic, p(zd = k|Z¬d, D) ∝ mk,¬d + α N −1 + Kα Q w∈d Qnw d j=1(nw k,¬d + β + j −1) Qnd i=1(nk,¬d + V β + i −1) (1) where Z represents all topics of all documents, the subscript ¬d means document d is removed from its current topic feature (TF) vector, which is useful for the update learning process of GSDMM. For each document, we ﬁrst delete it from its current TF vector with the deletable property. Then, we reassign the document to a topic according to the probability of the document belonging to each of the K topics using Equation 1. After obtaining the topic of the document, we add it from its new TF vector with the addible property. Finally, the posterior distribution of each word belonging to each topic is calculated as the follows, φw k = nw k + β nk + V β (2) 3.3.2 LF-DMM The graphical model of LF-DMM is shown in Figure 2. Based on the assumption that each text is sampled by a single topic, LF-DMM generates the words by Dirichlet multinomial model or latent feature model. Given two latent-feature vectors τ associated with topic k and ω associated with word w, latent feature model generates a word w using softmax function by the formula, σ(w | τkωT ) = e(τk·ωw) Σw′∈W e(τk·ωw′) (3) where ω is pre-trained word vectors of all words W, and ωw is the word vector of word w. Fig. 2. Graphical Model of LF-DMM. For each word w of document d, a binary indicator vari- able Sd,w is sampled from a Bernoulli distribution to determine whether Dirichlet multinomial model or latent feature model will be used to generate w. The generative process is described as follows: (1). Sample a topic proportion θ ∼Dirichlet(α). (2). For each topic k ∈{1, ..., K}: (a) Draw a topic-word distribution θk ∼Dirichlet(β). (3). For each document d ∈D: (a) Sample a topic zd ∼Multinomial(θ). (b) For each word w ∈{wd,1, ..., wd,nd}: (i) Sample a binary indicator variable Sd,w ∈ Bernoulli(λ) (ii) Sample a word w ∼ (1 − sw)Multinomial(φzd) + sw(σ(τzdωT )). JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 5 Here, the hyper-parameter λ is the probability of a word being generated by latent feature model, and Sd,w indicates whether Dirichlet multinomial model or latent feature model is applied to word w of document d. The topic feature (TF) in LF-DMM is similar with GSDMM, so we do not present the addible and deletable properties for LF-DMM. Based on collapsed Gibbs sampling, LF-DMM uses the fol- lowing conditional probability distribution to infer the topic of the document d, p(zd = k|Z¬d, D, τ, ω) ∝(mk,¬d + α) Y w∈d ((1 −λ) nw k,¬d + β nk,¬d + V β", "meta": {"paper_id": "1904.07695v1", "chunk_index": 7, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_8", "text": "present the addible and deletable properties for LF-DMM. Based on collapsed Gibbs sampling, LF-DMM uses the fol- lowing conditional probability distribution to infer the topic of the document d, p(zd = k|Z¬d, D, τ, ω) ∝(mk,¬d + α) Y w∈d ((1 −λ) nw k,¬d + β nk,¬d + V β + λσ(w|τkωT ))nw d (4) where nw d is the number of word w in document d. The binary indicator variable Sd,w for word w in document d conditional on zd = k is inferred using the following distribution, p(Sd,w = s|zd = k) ∝    (1 −λ) n wi k,¬d+β nk,¬d+V β for s = 0, λσ(wi|τkωT ) for s = 1. (5) where the subscript ¬d means document d is removed from its current topic feature (TF) vector. After each iteration, LF-DMM estimates the topic vectors using the following optimization function, Lk = − X w∈W F w k (τk · ωw −log( X w′∈W eτk·ωw′)) + µ||τk||2 2 (6) where F w k is the number of times word w generated from topic k by latent feature model. LF-DMM adopted L-BFGS 1 [40] to ﬁnd the topic vector τk that minimizes Lk. 3.3.3 GPU-DMM Based on DMM model, GPU-DMM [29] promotes the seman- tically related words under the same topic during the sampling process by the generalized P´olya urn (GPU) model [41]. When a ball of a particular color is sampled, a certain number of balls of similar colors are put back along with the original ball and a new ball of that color. In this case, sampling a word w in topic k not only increases the probability of w itself under topic k, but also increases the probability of the semantically similar words of word w under topic k. Given pre-trained word embeddings, the semantic similarity between two words wi and wj is denoted by cos(wi, wj) that are measured by cosine similarity. For all word pairs in vocab- ulary, if the semantic similarity score is higher that a prede- ﬁned threshold ϵ, the word pair is saved into a matric M, i.e., M = {(wi, wj)|cos(wi, wj) > ϵ}. Then, the promotion matrix A with respect to each word pair is deﬁned below, Awi,wj =      1 wi = wj µ wj ∈Mwi and wj ̸= wi 0 otherwisex (7) where Mwi is the row in M corresponding to word wi and µ is the pre-deﬁned promotion weight. 1. LF-DMM used the implementation of the Mallet toolkit [39] GPU-DMM and DMM share the same generative process and graphical representation but differ in the topic inference process that they use. Different from DMM and LF-DMM, GPU-DMM ﬁrst samples a topic for a document, and then only reinforces only the semantically similar words if and only if a word has strong ties with the sampled topic. Therefore, a nonparametric probabilistic sampling process for word w in document d is as follows: Sd,w ∼Bernoulli(λw,zd) (8) λw,zd = p(z|w) pmax(z′|w) (9) pmax(z′|w) = max k p(z = k|w) p(z = k|w) = p(z =", "meta": {"paper_id": "1904.07695v1", "chunk_index": 8, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_9", "text": "words if and only if a word has strong ties with the sampled topic. Therefore, a nonparametric probabilistic sampling process for word w in document d is as follows: Sd,w ∼Bernoulli(λw,zd) (8) λw,zd = p(z|w) pmax(z′|w) (9) pmax(z′|w) = max k p(z = k|w) p(z = k|w) = p(z = k)p(w|z = k) PK i=1 p(z = i)p(w|z = i) (10) where Sd,w indicates whether GPU is applied to word w of document d given topic zd. We can see that GPU model is more likely to be applied to w if word w is highly relate to topic zd. The Topic feature vector of a topic k in GPU-DMM is deﬁned as a tuple {enw k (w ∈W), mk, enk}. TF makes the same changes with GSDMM when no GPU is applied, namely Sd,w = 0. Under Sd,w = 1, the addible and deletable properties of topic feature (TF) in GPU-DMM are described below. (1) Addible Property. A document d will be added into topic k by updating its TF vector as follows, enk = enk + nwi d · Awi,wj for each word wj ∈Mwi enwj k = enwj k + nw d · Awi,wj for each word wj ∈Mwi mk = mk + 1 (2) Deletable Property. A document d will be deleted from topic k by updating its TF vector as follows, enk = enk −nwi d · Awi,wj for each word wj ∈Mwi enwj k = enwj k −nw d · Awi,wj for each word wj ∈Mwi mk = mk −1 Accordingly, based on Gibbs sampling, the conditional dis- tribution to infer the topic for each document in Equation 1 is rewritten as follows: p(zd = k|Z¬d, D) ∝ mk,¬d + α N −1 + Kα× Q w∈d Qnw d j=1(enw k,¬d + β + j −1) Qnd i=1(enk,¬d + V β + i −1) (11) During each iteration, GPU-PDMM ﬁrst delete it from its current TF vector with the deletable property. After obtaining the topic of the document, GPU-DMM ﬁrst updates Sd,w for GPU using Equation 8, and then updates TF vector for each word using the addible property. Finally, the posterior distribution in Equation 2 for GPU-DMM is rewritten as follows: φw k = enw k + β enk + V β (12) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 6 3.3.4 GPU-PDMM Considering the single-topic assumption may be too strong for some short text corpus, Li et al. [31] ﬁrst proposed Poisson-based Dirichlet Multinomial Mixture model (PDMM) that allows each document can be generated by one or more (but not too many) topics. Then PDMM can be extended as GPU-PDMM model by incorporating generalized P´olya urn (GPU) model during the sampling process. In GPU-PDMM, each document is generated by td (0 < td ≤ ς) topics, where ς is the maximum number of topics allowable in a document. GPU-PDMM uses Poisson distribution to model td. The graphical model of GPU-PDMM is shown in Figure 3. The generative process of GPU-PDMM is described as follows. (1). Sample a topic proportion θ", "meta": {"paper_id": "1904.07695v1", "chunk_index": 9, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_10", "text": "(0 < td ≤ ς) topics, where ς is the maximum number of topics allowable in a document. GPU-PDMM uses Poisson distribution to model td. The graphical model of GPU-PDMM is shown in Figure 3. The generative process of GPU-PDMM is described as follows. (1). Sample a topic proportion θ ∼Dirichlet(α). (2). For each topic k ∈{1, ..., K}: (a) Draw a topic-word distribution θk ∼Dirichlet(β). (3). For each document d ∈D: (a) Sample a topic number td ∼Poisson(λ). (b) Sample td distinct topics Zd ∼Multinomial(θ). (c) For each word w ∈{wd,1, ..., wd,nd}: (i) Uniformly sample a topic zd,w ∼Zd. (ii) Sample a word w ∼Multinomial(φzd,w). Here td is sampled using Poisson distribution with parameter λ, and Zd is the topic set for document d. Fig. 3. Graphical Model of GPU-PDMM. The topic feature (TF) vector of a topic k in GPU-PDMM is deﬁned as a tuple enk, enw k (w ∈W), ck, dk, nk,d, nw k,d}, where ck is the number of words associated with topic k and dk represents the word set in topic k. The addible and deletable properties of topic feature (TF) in GPU-PDMM are described below. The enk and enw k of TF in GPU- PDMM makes the same changes with GPU-DMM. Here, we only describe other variables in TF. (1) Addible Property. Suppose that word w in document d will be added to topic k, TF feature is updates as follows, ck = ck + 1 ; dk = dk + w nk,d = nk,d + 1 ; nw k,d = nw k,d + 1 (2) Deletable Property. Suppose that word d will be deleted from topic k. TF feature is updated as follows, ck = ck −1 ; dk = dk −w nk,d = nk,d −1 ; nw k,d = nw k,d −1 The Gibbs sampling process of GPU-PDMM is similar to GPU-DMM, it updates the topic for word w in document d using the following equation, p(zd,w = k|z¬(d,w), Zd, D) ∝1 td × enw k,¬(d,w) + β PV w enw k,¬(d,w) + V β (13) Conditioned on all zd,w in document d, GPU-PDMM samples each possible Zd as follows, p(Zd|Z¬d, D) ∝λtd tnd d × Q k∈Zd(ck,¬d + α) Qtd−1 i=0 (PK k ck,¬d + Kα −i) × Y k∈Zd Q w∈dk Qnw k,d i=0 (enk,¬d + nw k,d) −i + β Qnk,d−1 i=0 (PV w enw k,¬d + nk,d −i + V β) . (14) During each iteration, for each document d, GPU-PDMM ﬁrst updates TF vector using Deletable Property and the topic for each word w in d using Equation 13. Then GPU-PDMM samples each possible Zd using Equation 14. Finally, GPU-PDMM sets all the values of zd,w based on the updates Zd, updates Sd,w for GPU using Equation 8, and then updates TF vector for each word using the addible property. Here, due to the computational costs involved in sampling Zd, GPU-PDMM only samples the more relevant topics for each document. Speciﬁcally, GPU-PDMM infers the topic probability p(z|d) of each document d using the follows, p(z = k|d) ∝", "meta": {"paper_id": "1904.07695v1", "chunk_index": 10, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_11", "text": "and then updates TF vector for each word using the addible property. Here, due to the computational costs involved in sampling Zd, GPU-PDMM only samples the more relevant topics for each document. Speciﬁcally, GPU-PDMM infers the topic probability p(z|d) of each document d using the follows, p(z = k|d) ∝ X w∈d p(z = k|w)p(w|d) where p(w|d) = nw d nd . GPU-PDMM only chooses the top M topics for document d based on the probability p(z|d) to generate Zd, where ς < M ≤K. The topic-word distribution can be calculated by Equation 12. 3.4 Global Word Co-occurrences based Methods The closer the two words, the more relevance the two words. Utilizing this idea, global word co-occurrences based methods learn the latent topics from the global word co-occurrences ob- tained from the original corpus. This type of methods needs to set sliding window for extracting word co-occurrences. In general, if the average length of each document is larger than 10, they use sliding window and set the size of the sliding window as 10, else they can directly take each document as a sliding window. 3.4.1 BTM BTM [14] ﬁrst generate biterms from the corpus D, where any two words in a document is treated as a biterm. Sup- pose that the corpus contains nb biterms B = {bi}nB i=1, where bi = (wi,1, wi,2). BTM infers topics over the biterms B. The generative process of BTM is described as follows, and its graph- ical model is shown in Figure 4. Fig. 4. Graphical Model of BTM. (1). Draw θ ∼Dirichlet(α). (2). For each topic k ∈[1, K] (a) draw φk ∼Dirichlet(β). JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 7 (3). For each biterm bi ∈B (a) draw zi ∼Multinomial(θ), (b) draw wi,1, wi,2 ∼Multinomial(φzi). The TF vector of a topic k in BTM is deﬁned as a tuple {nk(w ∈W), nk}. The addible and deletable properties of topic feature (TF) in BTM are described below. (1) Addible Property. A biterm bi can be efﬁciently added to topic k by updating its TF vector as follows. nwi,1 k = nwi,1 k + 1 ; nwi,2 k = nwi,2 k + 1 ; nk = nk + 1 (2) Deletable Property. A biterm bi can be efﬁciently deleted from topic k by unpdating its TF vector as follows. nwi,1 k = nwi,1 k −1 ; nwi,2 k = nwi,2 k −1 ; nk = nk −1 Using the technique of collapsed Gibbs sampling, BTM sam- ples the topic zi of biterm bi using the following conditional distribution, p(zi = k|Z¬i, B) ∝(nk,−i + α)× (nwi,1 k,¬i + β)(nwi,2 k,¬i + β) (nk,¬i + V β + 1)(nk,¬i + V β) (15) where Z¬i denotes the topics for all biterms except the current biterm bi, and nk is the number of biterms assigned to topic k. For each biterm, we ﬁrst delete it from its current TF vector with the deletable property. Then, we reassign the biterm to a topic using Equation 4. Accordingly, we update the new TF vector", "meta": {"paper_id": "1904.07695v1", "chunk_index": 11, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_12", "text": "except the current biterm bi, and nk is the number of biterms assigned to topic k. For each biterm, we ﬁrst delete it from its current TF vector with the deletable property. Then, we reassign the biterm to a topic using Equation 4. Accordingly, we update the new TF vector with the addible property. After ﬁnishing the iterations, BTM estimates φ and θ as follows, φw k = nw k + β nk + V β , (16) θk d = nb d X i=1 p(zi = k) (17) where θk d is the probability of topic k in document d, and nb d is the number of biterms in document d. 3.4.2 WNTM WNTM [32] uses global word co-occurrence to construct word co-occurrence network, and learns the distribution over topics for each word from word co-occurrence network using LDA. WNTM ﬁrst set the size of a sliding window, and the window is moving word by word. Suppose window size is set as 10 in the original paper, if one document has 15 words, it will have 16 windows in this document. As WNTM scanning word by word in one window, two distinct words in the window are regarded as co-occurrence. WNTM construct undirected word co-occurrence network, where each node of the word co-occurrence network represents one word and the weight of each edge is the number of co-occurrence of the two connected words. We can see that the number of nodes is the number of vocabulary V . Then, WNTM generates one pseudo-document l for each vertex v which is consisted of the adjacent vertices of this vertex in word network. The occur times of this adjacent vertex in l is determined by the weight of the edge. The number of words in l is the degree of the vertex v and the number of pseudo-documents P is the number of vertices. After obtaining pseudo-documents P, WNTM adopts LDA to learn latent topics from pseudo-documents. Therefore, the topic feature (TF) in LF-DMM is same with LDA. For each word w in l, WNTM infers its topic using the following conditional distribution, p(zl,w = k | Z¬(l,w), P, α, β) ∝(nk l,¬(l,w)+α) nw k,¬(l,w) + β nk,¬(l,w) + V β (18) where nk l is the number of topic k belonging to pseudo-document l, and ¬(l, w) means word w is removed from its pseudo- document l. Because each pseudo-document is each word’s adjacent word-list, the document-topic distribution learned from pseudo- document is the topic-word distribution in WNTM. Suppose pseudo-document l is generated from word w, the topic-word distribution of w is calculated using the following Equation, φw k = nk l + α nl + Kα (19) where nl is the number of words in l. Given topic-word distribution, the document-word distribution θd can be calculated as, θk d = nd X i=1 φwd,i k p(wd,i|d) p(wd,i|d) = nwd,i d nd where nwd,i d is the number of word wd,i in document d. 3.5 Self-aggregation based Methods Self-aggregation based methods alleviate the problem of sparseness by merging short texts into long", "meta": {"paper_id": "1904.07695v1", "chunk_index": 12, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_13", "text": "distribution θd can be calculated as, θk d = nd X i=1 φwd,i k p(wd,i|d) p(wd,i|d) = nwd,i d nd where nwd,i d is the number of word wd,i in document d. 3.5 Self-aggregation based Methods Self-aggregation based methods alleviate the problem of sparseness by merging short texts into long pseudo-documents P before inferring the latent topics [15], [17], [36]. The previous self-aggregation based methods ﬁrst merged short texts, and then applied topic models. Recently, SATM and PTM simultaneously integrate clustering and topic modeling in one iteration. In general, the number of pseudo-documents |P| is signiﬁcantly less than the number of short texts, namely |P|≪N. 3.5.1 SATM Self-aggregation based topic modeling (SATM) [20] supposes that each short text is sampled from an unobserved long pseudo- document, and infers latent topics from pseudo-documents using standard topic modeling. The Gibbs sampling process in SATM can be described in two indispensable steps. The ﬁrst step calculates the probability of the occurrence of a pseudo-document l in P conditioned on short document d in short corpus, which is estimated using the mixture of unigrams model [22], w˙d,i p(l|d) = p(l) QV i=1( n wd,i l nl )n wd,i d P|P | m=1 p(m) QV i=1( n wd,i m nm )n wd,i d (20) JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 8 where p(l) = nl N represents the probability of pseudo-document pl, nwd,i l is the number of word wd,i in pseudo-document pl, and nl is the number of words in pl. The second step estimates draws a pair of pseudo-document label ld,w and topic label zd,w jointly for word w in document d, which is similar with standard topic modeling (author-topic modeling) [42]. The addible and deletable properties of pseudo-document and topic feature (PTF) in SATM are described below. (1) Addible Property. A word w can be efﬁciently added into pseudo-document l and topic k by updating its TPF vector as follows. nw l = nw l + 1 ; nk l = nk l + 1 ; nl = nl + 1 nw k = nw k + 1 ; nk = nk + 1 (2) Deletable Property. A word w can be efﬁciently deleted from pseudo-document l and topic k by updating its PTF vector as follows. nw l = nw l −1 ; nk l = nk l −1 ; nl = nl −1 nw k = nw k −1 ; nk = nk −1 The pair of pseudo-document label ld,w and topic label zd,w jointly for word w in document d can be calculated by, p(ld,w = l,zd,w = k|Z¬(d,w), P¬(d,w)) ∝ p(l|d) × nk l,¬(d,w) + α nl,¬(d,w) + Kα · nw k,¬(d,w) + β nk,¬(d,w) + V β (21) where nk l is the number of words in pseudo-document l belonging to topic k. After ﬁnishing the iterations, SATM estimates φ and θ as follows, φw k = nw k + β nk + V β , (22) θk d = nd Y i=1 φwd,i k (23) 3.5.2 PTM The pseudo-document-based topic modeling (PTM) [35]", "meta": {"paper_id": "1904.07695v1", "chunk_index": 13, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_14", "text": "of words in pseudo-document l belonging to topic k. After ﬁnishing the iterations, SATM estimates φ and θ as follows, φw k = nw k + β nk + V β , (22) θk d = nd Y i=1 φwd,i k (23) 3.5.2 PTM The pseudo-document-based topic modeling (PTM) [35] sup- poses that each short text is sampled from one long pseudo- document pl, and then infers the latent topics from long pseudo- documents P. A multinomial distribution ϕ is used to model the distribution of short texts over pseudo-documents. The graphical model of PTM is shown in Figure 5. The generative process of PTM is described as follows, Fig. 5. Graphical Model of PTM. (1). Sample ϕ ∼Dir(λ) (2). For each topic k ∈[1, K] (a) draw φk ∼Dirichlet(β). (3). For each pseudo-document l (a) sample θl ∼Dir(α) (4). For each document d ∈D: (a) Sample a pseudo-document l ∼Multinomial(ϕ). (b) For each word w ∈{wd,1, ..., wd,nd} in d: (i) Sample a topic z ∼Multinomial(θl). (ii) Sample a word w ∼Multinomial(φz). The addible and deletable properties of pseudo-document and topic feature (PTF) in PTM are described below. (1) Addible Property. A document d can be efﬁciently added into pseudo-document l by updating its PTF vector as follows. nk l = nk l + 1 for zd,w = k in d ml = ml + 1 ; nl = nl + nd (2) Deletable Property. A document d can be efﬁciently deleted from pseudo-document l by updating its PF vector as follows. nk l = nk l −1 for zd,w = k in d ml = ml −1 ; nl = nl −nd Integrating out θ, φ and ϕ, the pseudo-document assignment l for short text d based on collapsed Gibbs sampling can be estimated as follows, p(ld = l|−−→ P¬d, D) ∝ ml,¬d N −1 + λ|P| Q k∈d Qnk d j=1(nk l,¬d + α + j −1) Qnd i=1(nl,¬d + Kα + i −1) (24) where ml is the number of short texts associated with pseudo- document l, nk l is the number of words associated with topic k in pseudo-document l. After obtaining the pseudo-document for each short text, PTM samples the topic assignment for each word w in document d. That is, p(zd,w = k|Z¬(d,w), D) ∝(nk l + α) nw k + β nk + V β (25) where nk l is the number of words associated with topic k in pseudo-document l. The document-word distribution θd can be calculated as, θk d = nk d + α nd + Kα (26) 4 APPLICATIONS With the emerging of social media, topic models have been used for social media content analysis, such as content charac- terizing and recommendation, text classiﬁcation, event tracking. However, although the corpus is composed of short texts, some previous work directly applied traditional topic models for topic discovery, since no speciﬁc short text topic models were proposed at that time. Therefore, it brings a new chance for short text topic modeling to improve the performance of these tasks. JOURNAL OF LATEX CLASS FILES,", "meta": {"paper_id": "1904.07695v1", "chunk_index": 14, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_15", "text": "composed of short texts, some previous work directly applied traditional topic models for topic discovery, since no speciﬁc short text topic models were proposed at that time. Therefore, it brings a new chance for short text topic modeling to improve the performance of these tasks. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 9 4.1 Content characterizing and recommendation Microblogging sites are used as publishing platforms to create and consume content from sets of users with overlapping and disparate interests, which results in many contents are useless for users. These work [19], [43] have been devoted to content analysis of Twitter. Ramage et al. [43] used topic models to discover latent topics from the tweets that can be roughly categorized into four types: substance topics about events and ideas, social topics recognizing language used toward a social end, status topics denoting personal updates, and style topics that embody broader trends in language usage. Next, they characterize selected Twitter users along these learned dimensions for providing interpretable summaries or characterizations of users tweet streams. Zhao et al. [19] performed content analysis on tweets using topic modeling to discover the difference between Twitter and traditional medium. Content analysis is crucial for content recommendation for mi- croblogging users [44], [45]. Phelan et al. [46] identiﬁed emerging topics of interest from Twitter information using topic modeling, and recommended news by matching emerging topics and recent news coverage in an RSS feed. Chen et al. [47] also studied content recommendation based on Twitter for better capture users’ attention by exploring three separate dimensions in designing such a recommender: content sources, topic interest models for users, and social voting. 4.2 Text Classiﬁcation Topic models for text classiﬁcation are mainly from the follow- ing two aspects. The ﬁrst one is topics discovered from external large-scale data corpora are added into short text documents as external features. For example, Phan et al. [16] built a classiﬁer on both a set of labeled training data and a set of latent topics discovered from a large-scale data collection. Chen et al. [48] inte- grated multi-granularity hidden topics discovered from short texts and produced discriminative features for short text classiﬁcation. Vo et al. [49] explored more external large-scale data collections which contain not only Wikipedia but also LNCS and DBLP for discovering latent topics. The other one is that topic models are used to obtain a low dimensional representation of each text, and then classify text using classiﬁcation methods [50], [51]. Compared with traditional statistical methods, the representation using topic models can get a compact, dense and lower dimensional vector in which each dimension of the vector usually represents a speciﬁc semantic meaning (e.g., a topic) [22]. Dai et al. [50] used the topic information from training data to extend representation for short text. Recent topic modeling methods on text representation have explicitly evaluated their models on this task. They showed that a low dimensional representation for each text sufﬁces to capture the semantic information. 4.3 Event Tracking Nowadays, a large volume of text data is generated from the social", "meta": {"paper_id": "1904.07695v1", "chunk_index": 15, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_16", "text": "representation for short text. Recent topic modeling methods on text representation have explicitly evaluated their models on this task. They showed that a low dimensional representation for each text sufﬁces to capture the semantic information. 4.3 Event Tracking Nowadays, a large volume of text data is generated from the social communities, such as blogs, tweets, and comments. The important task of event tracking is to observe and track the popular events or topics that evolve over time [52]–[54]. Lin et al. [52] proposed a novel topic modeling that models the popularity of events over time, taking into consideration the burstiness of user interest, information diffusion in the network structure, and the evolution of latent topics. Lau et al. [55] designed a novel topic modeling for event detecting, whose model has an in-built update mechanism based on time slices by implementing a dynamic vocabulary. 5 A JAVA LIBRARY FOR SHORT TEXT TOPIC MODELING We released an open-source Java library, STTM (Short Text Topic Modeling) 2, which is the ﬁrst comprehensive open-source library, which not only includes the state-of-the-art algorithms with a uniform easy-to-use programming interface but also in- cludes a great number of designed modules for the evaluation and application of short text topic modeling algorithms. The design of STTM follows three basic principles. (1) Preferring integration of existing algorithms rather than implementing them. If the original implementations are open, we always attempt to integrate the original codes rather than implement them. The work that we have done is to consolidate the input/output ﬁle formats and package these different approaches into some newly designed java classes with a uniform easy-to-use member functions. (2) Including traditional topic modeling algorithms for long texts. The classical topic modeling algorithm (LDA [9] and its variation LF-LDA [28]) are integrated, which is easy for users to the comparison of long text topic modeling algorithms and short text topic modeling algorithms. (3) Extendibility. Because short text topic modeling is an emerging research ﬁeld, many topics have not been studied yet. For incorporating future work easily, we try to make the class structures as extendable as possible when designing the core modules of STTM. Figure 6 shows the hierarchical architecture of STTM. STTM supports the entire knowledge discovery procedure including analysis, inference, evaluation, application for classiﬁcation and clustering. In the data layer, STTM is able to read a text ﬁle, in which each line represents one document. Here, a document is a sequence of words/tokens separated by whitespace characters. If we need to evaluate the algorithm, we also need to read a gold ﬁle, in which each line is the class label of one document. STTM provides implementations of DMM [18], LF-DMM [28], GPU- DMM [29], GPU-PDMM [31], BTM [14], WNTM [32], SATM [20], and PTM [35]. For each model, we not only provide how to train a model on existing corpus but also give how to infer topics on a new/unseen corpus using a pre-trained topic model. In addition, STTM presents three aspects of how to evaluate the performance of the algorithms (i.e., topic coherence, clustering, and classiﬁcation). For", "meta": {"paper_id": "1904.07695v1", "chunk_index": 16, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_17", "text": "we not only provide how to train a model on existing corpus but also give how to infer topics on a new/unseen corpus using a pre-trained topic model. In addition, STTM presents three aspects of how to evaluate the performance of the algorithms (i.e., topic coherence, clustering, and classiﬁcation). For topic coherence, we use the point-wise mutual information (PMI) to measure the coherence of topics [35]. Short text topic modeling algorithms are widely used for clustering and classiﬁcation. In the clustering module, STTM provides two measures (NMI and Purity) [21]. Based on the latent semantic representations learned by short text topic modeling, accuracy metric is chosen in classiﬁcations [14]. 6 EXPERIMENTAL SETUP In this section, we specify the parameter setting of the in- troduced short text topic models, dataset and evaluation metrics we used. All of these are implemented in our library STTM. The experiments were performed on a Ubuntu 18.04(bionic) system with 6 cores, Intel Xeon E5645 CPU and 12288 KB cache. For all models in comparison, we used the recommended setting by the authors and set the number of iterations as 2,000 2. https://github.com/qiang2100/STTM JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 10 Fig. 6. The architecture of STTM unless explicitly speciﬁed elsewhere. The word embeddings of LF-DMM, GPU-DMM, and GPU-PDMM are trained by Glove [56]. In this paper, we used the pre-trained word embeddings ”glove.6B.200d.txt”, where the dimension of the vector is 200. 6.1 Parameter Setting LDA: LDA is the most popular and classic topic modeling. We choose it as a baseline to the comparison. The hyper-parameters of LDA are set as α = 0.05 and β = 0.01 that are proved in the paper (BTM). The authors tuned parameters via grid search on the smallest collection to get the best performance. GSDMM: We set k = 300, α = 0.1 and β = 0.1 declared in the paper(GSDMM). LF-DMM: We set λ = 0.6, α = 0.1 and β = 0.01 shown in their paper. We set the iterations for baseline models as 1,500 and ran the further iterations 500 times. GPU-DMM: We use the hyper-parameter settings provided by the authors, α = 50/k, β = 0.01. The number of iterations is 1,000 in the paper. GPU-PDMM: All the settings are same as the model GPU- DMM. We set λ = 1.5, ς = 1.5, and M=10 declared in the original paper. BTM: The parameters α = 50/K and β = 0.01 are used, the model gets optimal performance. Each document is treated as one window. WNTM: We set α = 0.1 and β = 0.1 used in the original paper. The window size is set as 10 words. SATM: We set the number of pseudo-numbers as 300, and the hyper-parameters α = 50/k, β = 0.1. The number of iterations is set as 1,000. PTM: The hyper-parameters are α = 0.1 and β = 0.01. We also set the number of pseudo-document as 1000. 6.2 Datasets To show the effects and differences of the above nine models, we select the following six datasets to", "meta": {"paper_id": "1904.07695v1", "chunk_index": 17, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_18", "text": "= 0.1. The number of iterations is set as 1,000. PTM: The hyper-parameters are α = 0.1 and β = 0.01. We also set the number of pseudo-document as 1000. 6.2 Datasets To show the effects and differences of the above nine models, we select the following six datasets to verify the models. After preprocessing these datasets, we present the key information of the datasets that are summarized in Table 4, where K corresponds to the number of topics per dataset, N represents the number of documents in each dataset, Len shows the average length and maximum length of each document, and V indicates the size of the vocabulary. TABLE 4 The basic information of the corpus Dataset K N Len V SearchSnippets 8 12,295 14.4/37 5,547 StackOverﬂow 20 16,407 5.03/17 2,638 Biomedicine 20 19,448 7.44/28 4498 Tweet 89 2,472 8.55/20 5,096 GoogleNews 152 11,109 6.23/14 8,110 PascalFlickr 20 4,834 5.37/19 3,431 SearchSnippets: Given the predeﬁned phrases of 8 different domains, this dataset was chosen from the results of web search transaction. The 8 domains are Business, Computers, Culture- Arts, Education-Science, Engineering, Health, Politics-Society, and Sports, respectively. StackOverﬂow: The dataset is released on Kaggle.com. The raw dataset contains 3,370,528 samples from July 31st, 2012 to August 14, 2012. Here, the dataset randomly selects 20,000 question titles from 20 different tags. Biomedicine: Biomedicine makes use of the challenge data delivered on BioASQ’s ofﬁcial website. Tweet: In the 2011 and 2012 microblog tracks at Text RE- trieval Conference (TREC), there are 109 queries for using. After JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 11 removing the queries with none highly-relevant tweets, Tweet dataset includes 89 clusters and totally 2,472 tweets. GoogleNews: In the Google news site, the news articles are divided into clusters automatically. GoolgeNews dataset is downloaded from Google news site on November 27, 2013, and crawled the titles and snippets of 11,109 news articles belonging to 152 clusters. PascalFlickr: PascalFlickr dataset are a set of captions [57], which is used as evaluation for short text clustering [58]. 6.3 Evaluation Metrics It is still an open problem about how to evaluate short text topic models. A lot of metrics have been proposed for measuring the coherence of topics in texts [59], [60]. Although some metrics tend to be reasonable for long texts, they can be problematic for short texts [20]. Most conventional metrics (e.g., perplexity) try to estimate the likelihood of held-out testing data based on parameters inferred from training data. However, this likelihood is not necessarily a good indicator of the quality of the extracted topics [61]. To provide a good evaluation, we evaluate all models from many aspects using different metrics, Classiﬁcation Evaluation: Each document can bed repre- sented using document-topic distribution p(z|d). Therefore, we can evaluate the performance of topic modeling using text classi- ﬁcation. Here, we choose accuracy as a metric for classiﬁcation. Higher accuracy means the learned topics are more discriminative and representative. We use a linear kernel Support Vector Machine (SVM) classiﬁer in LIBLINEAR 3 with the default parameter set- tings. The accuracy", "meta": {"paper_id": "1904.07695v1", "chunk_index": 18, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_19", "text": "the performance of topic modeling using text classi- ﬁcation. Here, we choose accuracy as a metric for classiﬁcation. Higher accuracy means the learned topics are more discriminative and representative. We use a linear kernel Support Vector Machine (SVM) classiﬁer in LIBLINEAR 3 with the default parameter set- tings. The accuracy of classiﬁcation is computed through ﬁvefold cross-validation on all datasets. Cluster Evaluation (Purity and NMI): By choosing the maximum of topic probability for each document, we can get the cluster label for each text. Then, we can compare the cluster label and the golden label using metric Purity and NMI [18], [24]. Topic Coherence: Computing topic coherence, additional dataset (Wikipedia) as a single meta-document is needed to score word pairs using term co-occurrence in the paper (Automatic Evaluation of Topic Coherence). Here, we calculate the point-wise mutual information (PMI) of each word pair, estimated from the entire corpus of over one million English Wikipedia articles [31]. Using a sliding window of 10 words to identify co-occurrence, we computed the PMI of all a given word pair. The Wikipedia corpus can be downloaded here 4. Then, we can transfer the dataset from HTML to text using the code in the STTM package. Finally, due to the large size, we only choose 1,000,000 sentences from it. 7 EXPERIMENTS AND ANALYSIS In this section, we conduct experiments to evaluate the perfor- mance of the nine models. We run each model 20 times on each dataset and report the mean and standard deviation. 7.1 Classiﬁcation Accuracy Classiﬁcation accuracy is used to evaluate document-topic distribution. We represent each document with its document-topic distribution and employ text classiﬁcation method to assess. For DMM based methods, we use p(zd = k) to represent each document. For other models, we adopt the giving equation θk d. 3. https://liblinear.bwaldvogel.de/ 4. https://dumps.wikimedia.org/enwiki/ The classiﬁcation accuracy on six datasets using nine models is shown in Figure 7. We observe that although the performance of methods is dataset dependent, DMM based methods which utilize word embeddings outperform others, especially on Tweet and GoogleNews datasets. This is because GoogleNews and Tweet are general (not domain-speciﬁc) datasets and word embeddings used in this paper are trained in general datasets. If we try to use these models (LF-DMM, GPU-DMM, and GPU-PDMM) on domain-speciﬁc datasets, we can further improve the performance by re-training word embeddings on domain-speciﬁc datasets. We also observe that self-aggregation based methods are unable to achieve high accuracy, especially the SATM method. The performance of self-aggregation based methods is affected by generating long pseudo-documents. Without any auxiliary infor- mation or metadata, the error of this step of generating pseudo- documents will be ampliﬁed in the next step. In conclusion, these models based on the simple assumption (BTM and GSDMM) always outperform than LDA in all datasets, which indicate that two words or all words in one document are very likely to from one topic. Here we can see that the performance of other models (LF-DMM, GPU-DMM, GPU-PDMM, WNTM) are highly data set dependent. For example, WNTM achieves good performance on Tweet, GoogleNews and StackOverﬂow, but", "meta": {"paper_id": "1904.07695v1", "chunk_index": 19, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_20", "text": "datasets, which indicate that two words or all words in one document are very likely to from one topic. Here we can see that the performance of other models (LF-DMM, GPU-DMM, GPU-PDMM, WNTM) are highly data set dependent. For example, WNTM achieves good performance on Tweet, GoogleNews and StackOverﬂow, but performs poorly on other data sets. GPU-PDMM achieves the best performance on all data sets, except SearchSnippets. 7.2 Clustering Another important application of short text topic modeling is short text clustering. For each document, we choose the maximum value from its topic distribution as the cluster label. We report the mean value of each modeling on all datasets in the last column. The best results for each dataset using each metric are highlighted in bold. Table 5 illustrates the results using cluster metrics. We can see that all models outperform long text topic modeling (LDA), except SATM. Here, similar to the conclusions in classiﬁcation, we can observe that the performance of approaches is highly data set dependent. WNTM achieves the best performance on several datasets but performs poorly on PascalFlickr. GPU-PDMM performs very well on all datasets except SearchSnippets. For self-aggregation based methods, PTM performs better than SATM. For global word-occurrences based methods, two methods are very different from each other. WNTM performs better than BTM on Tweet and StackOverﬂow, and BTM achieves good performance on GoogleNews and PascalFlickr. For DMM based methods, GSDMM without incorporating word embeddings outperforms other methods on Biomedicine and SearchSnippets. 7.3 Topic Coherence Topic coherence is used to evaluate the quality of topic-word distribution. Here, we only choose the top 10 words for each topic based on the word probability. The results are shown in Figure 8. DMM based methods achieve the best performance on all datasets. LF-DMM has the best performance on four datasets (Biomedicine, GoogleNews, SearchSnippets, and Tweet), GPU- DMM has the best performance on StackOverﬂow, and GPU- PDMM achieves the best on PascalFlickr. It means that incor- porating word embeddings into DMM can help to alleviate the sparseness. Two methods based on global word co-occurrences perform very well and achieve a similar result on each dataset, JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 12 (a). Biomedicine (b). GoogleNews (c). PascalFlickr (d). SearchSnippets (e). StackOverﬂow (f). Tweet Fig. 7. Average classiﬁcation accuracy of all models on six datasets. which indicates that the adequacy of global word co-occurrences can mitigate the sparsity of short texts. Similar to the above results using other metrics, self-aggregation based methods perform very poorly. We also present the qualitative evaluation of latent topics. Here, we choose SearchSnippets dataset as an example, since it only contains eight topics that are Health, Politics-Society (politics), Engineering (engine.), Culture-Arts (culture), Sports, Computers, Business, and Education-Science (education). Table 6 shows the eight topics learned by the nine models. Each topic is visualized by the top ten words. Words that are noisy and lack of representativeness are highlighted in bold. From Table 6, we observe that LF-DMM can achieve a similar conclusion with topic coherence, which can learn more coherent topics with fewer", "meta": {"paper_id": "1904.07695v1", "chunk_index": 20, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_21", "text": "topics learned by the nine models. Each topic is visualized by the top ten words. Words that are noisy and lack of representativeness are highlighted in bold. From Table 6, we observe that LF-DMM can achieve a similar conclusion with topic coherence, which can learn more coherent topics with fewer noisy and meaningless words. GPU-DMM and GPU-PDMM can not discriminate the topic ’Engineering’. SATM remains the worst method in all short text topic models, which cannot discriminate three topics ’Engineering’, ’Politics-Society’ can ’Culture. Except LDA, PTM, WNTM, and SATM, other models can identify at least seven topics from all eight topics. 7.4 Inﬂuence of the number of iterations In this subsection, we try to investigate the inﬂuence of the number of iterations to the performance of all models using NMI metric. Since all models have converged when the number of iterations reaches 2000, we vary the number of iterations from 2 to 2024. The results are shown in Figure 9. We can see that models based DMM can converge fast to the optimal solutions and almost get stable within 30 iterations. Models based global word co- occurrences get stable within 60 iterations. Models based self- aggregation has the slowest convergence speed and the lowest iterative performance. 7.5 Efﬁciency In this part, we compare the efﬁciency of various short text topic models. Here, we choose the largest dataset ”Biomedicine” from all datasets to do the experiments. The average runtime of the initiation and per iteration for each model are reported in Table 7. Among all models evaluated, LDA and DMM are the most efﬁcient methods as expected. GPU-DMM is slightly slower than DMM and LDA, due to a similar Gibbs sampling process with GSDMM. LF-DMM and GPU-PDMM take much more time than GPU-DMM, because GPU-PDMM spends more time for the computational costs involved in sampling Zd and LF-DMM need much time for optimizing the topic vectors. We can see that GPU-PDMM is the slowest modeling compared with other models. Global word co-occurrences based methods are much slower than GSDMM, LDA and GPU-DMM and faster than the rest models. This is expected since they extend the number of words by extracting word co-occurrences. For self-aggregation based methods, the time is affected by the number of pseudo-documents. PTM is much faster than SATM but much slower than global word co-occurrences based methods. In addition, the models by incorporating word embeddings (GPU-DMM, LF-DMM, and GPU-PDMM) have the slowest time for the initiation due to the computational cost for the similarity between words. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 13 TABLE 5 Purity and NMI value of all models on six datasets. Model Biome dicine Google News Pascal Flickr Search Snippets Stack Overﬂow Tweet Mean Value LDA Purity 0.456 ± 0.011 0.793 ± 0.005 0.376 ± 0.013 0.740 ± 0.029 0.562 ± 0.013 0.821 ± 0.006 0.625 ± 0.013 NMI 0.356 ± 0.004 0.825 ± 0.002 0.321 ± 0.006 0.517 ± 0.025 0.425 ± 0.006 0.805 ± 0.004 0.542 ± 0.008 GSDMM Purity 0.494 ± 0.011 0.754 ± 0.014 0.360 ± 0.012", "meta": {"paper_id": "1904.07695v1", "chunk_index": 21, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_22", "text": "± 0.005 0.376 ± 0.013 0.740 ± 0.029 0.562 ± 0.013 0.821 ± 0.006 0.625 ± 0.013 NMI 0.356 ± 0.004 0.825 ± 0.002 0.321 ± 0.006 0.517 ± 0.025 0.425 ± 0.006 0.805 ± 0.004 0.542 ± 0.008 GSDMM Purity 0.494 ± 0.011 0.754 ± 0.014 0.360 ± 0.012 0.801 ± 0.024 0.713 ± 0.002 0.785 ± 0.011 0.650 ± 0.013 NMI 0.396 ± 0.006 0.851 ± 0.004 0.317 ± 0.005 0.608 ± 0.023 0.593 ± 0.002 0.801 ± 0.007 0.590 ± 0.001 LF-DMM Purity 0.421 ± 0.019 0.828 ± 0.009 0.381 ± 0.009 0.762 ± 0.042 0.518 ± 0.0217 0.856 ± 0.009 0.630 ± 0.018 NMI 0.348 ± 0.005 0.875 ± 0.005 0.365 ± 0.007 0.579 ± 0.026 0.443 ± 0.007 0.843 ± 0.006 0.578 ± 0.009 GPU-DMM Purity 0.433 ± 0.008 0.818 ± 0.005 0.395 ± 0.010 0.751 ± 0.035 0.511 ± 0.013 0.830 ± 0.006 0.623 ± 0.013 NMI 0.366 ± 0.006 0.852 ± 0.002 0.370 ± 0.004 0.561 ± 0.026 0.429 ± 0.003 0.810 ± 0.006 0.565 ± 0.008 GPU-PDMM Purity 0.481 ± 0.011 0.860 ± 0.002 0.368 ± 0.008 0.537 ± 0.030 0.702 ± 0.032 0.869 ± 0.005 0.636 ± 0.015 NMI 0.381 ± 0.005 0.871 ± 0.001 0.322 ± 0.003 0.341 ± 0.014 0.607 ± 0.013 0.830 ± 0.003 0.559 ± 0.007 BTM Purity 0.458 ± 0.012 0.849 ± 0.005 0.392 ± 0.011 0.765 ± 0.032 0.537 ± 0.019 0.814 ± 0.008 0.636 ± 0.014 NMI 0.380 ± 0.004 0.875 ± 0.003 0.368 ± 0.006 0.566 ± 0.027 0.456 ± 0.008 0.808 ± 0.005 0.575 ± 0.009 WNTM Purity 0.472 ± 0.009 0.837 ± 0.007 0.324 ± 0.005 0.712 ± 0.016 0.750 ± 0.026 0.856 ± 0.012 0.658 ± 0.013 NMI 0.369 ± 0.004 0.876 ± 0.004 0.295 ± 0.003 0.464 ± 0.011 0.659 ± 0.006 0.850 ± 0.009 0.585 ± 0.006 SATM Purity 0.384 ± 0.007 0.654 ± 0.008 0.237 ± 0.059 0.459 ± 0.055 0.505 ± 0.019 0.392 ± 0.011 0.438 ± 0.027 NMI 0.27 ± 0.001 0.76 ± 0.005 0.186 ± 0.049 0.205 ± 0.036 0.366 ± 0.011 0.507 ± 0.006 0.382 ± 0.018 PTM Purity 0.425 ± 0.012 0.807 ± 0.010 0.359 ± 0.012 0.674 ± 0.057 0.481 ± 0.034 0.839 ± 0.007 0.597 ± 0.022 NMI 0.353 ± 0.003 0.866 ± 0.005 0.336 ± 0.010 0.457 ± 0.045 0.442 ± 0.016 0.846 ± 0.006 0.550 ± 0.014 TABLE 7 The average runtime of initiation and per iteration of each model on Biomedicine (in milliseconds). Model Initiation time Per iteration time LDA 77 41.50 GSDMM 46 48.15 LF-DMM 3329 2243.03 GPU-DMM 13610 53.83 GPU-PDMM 13037 9685.44 BTM 320 160.43 WNTM 192 220.12 SATM 41 2015.03 PTM 126 818.32 8 CONCLUSION AND FUTURE WORK The review of short text topic modeling (STTM) techniques covered three broad categories of methods: DMM based, global word co-occurrences based, and self-aggregation based. We stud- ied the structure and properties preserved by various topic model- ing algorithms and characterized the challenges faced by short text topic modeling techniques in general as well as each category of approaches.", "meta": {"paper_id": "1904.07695v1", "chunk_index": 22, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_23", "text": "techniques covered three broad categories of methods: DMM based, global word co-occurrences based, and self-aggregation based. We stud- ied the structure and properties preserved by various topic model- ing algorithms and characterized the challenges faced by short text topic modeling techniques in general as well as each category of approaches. We presented various applications of STTM including content characterizing and recommendation, text classiﬁcation, and event tracking. We provided an open-source Java library, named STTM, which is consisted of short text topic modeling approaches surveyed and evaluation tasks including classiﬁcation, clustering, and topic coherence. Finally, we evaluated the surveyed approaches to these evaluation tasks using six publicly available real datasets and compared their strengths and weaknesses. Short text topic modeling is an emerging ﬁeld in machine learning, and there are many promising research directions: (1) Visualization: as shown in the survey, we display topics by listing the most frequent words of each topic (see Figure 6). This new ways of labeling the topics may be more reasonable by either choosing different words or displaying the chosen words differ- ently [62], [63]. How to display a document using topic models is also a difﬁcult problem? For each document, topic modeling provides useful information about the structure of the document. Binding with topic labels, this structure can help to identify the most interesting parts of the document. (2) Evaluation: Useful evaluation metrics for topic modeling algorithms have never been solved [10]. Topic coherence cannot distinguish the differences between topics. In addition, existing metrics only evaluate one part of topic modeling algorithms. One open direction for topic modeling is to develop new evaluation metrics that match how the methods are used. (3) Model checking. From the experimental results on this paper, each method has different performance on different datasets. When dealing with a new corpus or a new task, we cannot decide which topic modeling algorithms should I use. How can I decide which of the many modeling assumptions are suitable for my goals? New computational answers to these questions would be a signiﬁcant contribution to topic modeling. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 14 TABLE 6 The top ten words of each topic by each model on SearchSnippets dataset. LDA GSDMM LF-DMM Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) health wikipedia car music health political car movie health culture motor ﬁlm information encyclopedia engine movie information culture engine com cancer party engine music cancer wiki electrical com cancer culture com art disease democratic wheels art gov political com news gov democracy electrical ﬁlm healthy war electronics com medical culture products ﬁlm medical wikipedia wikipedia fashion medical political electric fashion news democracy digital movies news party system motor drug democracy cars movie research system home yahoo healthy war wheels wikipedia treatment congress models books disease party motor art disease republic olympic books physical presidential phone arts healthy republic energy video nutrition information digital arts food communist graduation rock nutrition philosophy calorie arts hiv government", "meta": {"paper_id": "1904.07695v1", "chunk_index": 23, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_24", "text": "news party system motor drug democracy cars movie research system home yahoo healthy war wheels wikipedia treatment congress models books disease party motor art disease republic olympic books physical presidential phone arts healthy republic energy video nutrition information digital arts food communist graduation rock nutrition philosophy calorie arts hiv government trade movies care philosophy fashion band Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 (sport) (computer) (business) (education) (sport) (computer) (business) (education) (sport) (computer) (business) (education) com computer gov edu news computer business research sports computer business research news business business research music software market edu football intel ﬁnancial edu sports web information science com web news science games software bank graduate football software school theory sports programming information theory news device economic resources games com trade journal football wikipedia stock information league linux trade science amazon news edu theoretical games memory gov school com digital news university game market research physics movie com com university hockey network market school soccer stock home computer game intel ﬁnance journal game hardware services faculty world internet law information wikipedia internet services physics soccer web law center tennis programming economic university tennis data home computer golf computers stock national GPU-DMM GPU-PDMM BTM Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) health political theory movie health wikipedia com culture health political car movie information culture theoretical music gov encyclopedia news art information wikipedia engine music cancer democracy physics com cancer wiki information american gov culture intel com medical wikipedia wikipedia ﬁlm medical political home history cancer democracy electrical amazon gov party edu news disease system online car medical encyclopedia com ﬁlm news system information movies healthy democracy world arts news party digital news healthy information science wikipedia nutrition party web imdb research wiki motor movies disease government research art physical government music museum disease system wheels books nutrition news amazon fashion hiv gov index income healthy government products art hiv gov com amazon diet war amazon literature nutrition war automatic video Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 (sport) (computer) (business) (education) (sport) (computer) (business) (education) (sport) (computer) (business) (education) sports computer business research movie computer business research sports computer business edu news web market edu sports software trade edu news software news research football software news science games programming management science football web market science com programming trade school football systems economic theory com programming information theory games com stock journal yahoo memory law school games com trade information soccer wikipedia information university game engine international university game internet stock university game memory com computer video intel gov journal soccer memory services journal wikipedia intel services information soccer design products theoretical match data home school tennis linux ﬁnance department ﬁlm electrical jobs physics world wikipedia gov theoretical world digital home graduate movies security bank department tennis linux ﬁnance physics WNTM SATM PTM Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 (health) (politics) (engine.) (culture) (health) (politics)", "meta": {"paper_id": "1904.07695v1", "chunk_index": 24, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_25", "text": "theoretical match data home school tennis linux ﬁnance department ﬁlm electrical jobs physics world wikipedia gov theoretical world digital home graduate movies security bank department tennis linux ﬁnance physics WNTM SATM PTM Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 Topic1 Topic2 Topic3 Topic4 (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) (health) (politics) (engine.) (culture) health political music movie health wikipedia culture movie health wikipedia amazon music information wikipedia engine com information research amazon news information encyclopedia theory movie cancer culture car amazon news trade wikipedia com cancer wiki com com hiv encyclopedia rock ﬁlm research information democracy ﬁlm medical political theoretical news healthy system com art gov wiki books wikipedia gov democracy physics ﬁlm nutrition democracy motor books medical gov com movies news system books movies disease party reviews fashion party international political reviews disease party car video medical government pop online home journal history online healthy war engine reviews news war band movies disease programming edu digital nutrition government models intel diet world wikipedia video healthy business encyclopedia articles hiv house electrical imdb Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 Topic5 Topic6 Topic7 Topic8 (sport) (computer) (business) (education) (sport) (computer) (business) (education) (sport) (computer) (business) (education) sports computer market research sports system business edu news computer business research football web business science games com news science sports edu news science news programming trade edu com web com research com software information edu games intel stock school news computer market school football web market journal com memory news theory football information yahoo information games school services art soccer internet information information game car stock university game research com resources game systems jobs journal world wikipedia internet program soccer programming trade culture tennis com ﬁnance university soccer memory services fashion culture university stock information match data home theoretical online device ﬁnancial department world information home directory league wikipedia tax physics wikipedia engine information home tennis systems gov library JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 15 (a). Biomedicine (b). GoogleNews (c). PascalFlickr (d). SearchSnippets (e). StackOverﬂow (f). Tweet Fig. 8. Topic Coherence of all models on six datasets. ACKNOWLEDGEMENT This research is partially supported by the National Natural Science Foundation of China under grant 61703362, and theNatu- ral Science Foundation of Jiangsu Province of China under grant BK20170513. REFERENCES [1] T. Lin, W. Tian, Q. Mei, C. Hong, The dual-sparse topic model: mining focused topics and focused terms in short text, in: International Conference on World Wide Web, 2014. [2] J. Qiang, P. Chen, W. Ding, T. Wang, F. Xie, X. Wu, Topic discovery from heterogeneous texts, in: Tools with Artiﬁcial Intelligence (ICTAI), 2016 IEEE 28th International Conference on, IEEE, 2016, pp. 196–203. [3] T. Shi, K. Kang, J. Choo, C. K. Reddy, Short-text topic modeling via non-negative matrix factorization enriched with local word-context correlations, in: Proceedings of the 2018 World Wide Web Conference on World Wide Web, International World Wide Web Conferences Steering Committee, 2018, pp. 1105–1114. [4] X. Wang, C. Zhai, X. Hu, R. Sproat, Mining correlated bursty topic patterns from coordinated text streams,", "meta": {"paper_id": "1904.07695v1", "chunk_index": 25, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_26", "text": "via non-negative matrix factorization enriched with local word-context correlations, in: Proceedings of the 2018 World Wide Web Conference on World Wide Web, International World Wide Web Conferences Steering Committee, 2018, pp. 1105–1114. [4] X. Wang, C. Zhai, X. Hu, R. Sproat, Mining correlated bursty topic patterns from coordinated text streams, in: Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, 2007, pp. 784–793. [5] B. Sriram, D. Fuhry, E. Demir, H. Ferhatosmanoglu, M. Demirbas, Short text classiﬁcation in twitter to improve information ﬁltering, in: Proceed- ings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, 2010, pp. 841–842. [6] Z. Ma, A. Sun, Q. Yuan, G. Cong, Topic-driven reader comments summarization, in: Proceedings of the 21st ACM international conference on Information and knowledge management, 2012, pp. 265–274. [7] J. Weng, E.-P. Lim, J. Jiang, Q. He, Twitterrank: ﬁnding topic-sensitive inﬂuential twitterers, in: Proceedings of the third ACM international conference on Web search and data mining, 2010, pp. 261–270. [8] T. Hofmann, Probabilistic latent semantic indexing, in: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, ACM, 1999, pp. 50–57. [9] D. M. Blei, A. Y. Ng, M. I. Jordan, Latent dirichlet allocation, the Journal of machine Learning research 3 (2003) 993–1022. [10] D. M. Blei, Probabilistic topic models, Communications of the ACM 55 (4) (2012) 77–84. [11] M. D. Hoffman, D. M. Blei, F. Bach, Online learning for latent dirichlet allocation, in: International Conference on Neural Information Process- ing Systems, 2010. [12] P. Xie, E. P. Xing, Integrating document clustering and topic modeling, UAI. [13] P. Xie, D. Yang, E. P. Xing, Incorporating word correlation knowledge into topic modeling, in: NACACL, 2015. [14] X. Cheng, X. Yan, Y. Lan, J. Guo, Btm: Topic modeling over short texts, Knowledge and Data Engineering, IEEE Transactions on 26 (12) (2014) 2928–2941. [15] O. Jin, N. N. Liu, K. Zhao, Y. Yu, Q. Yang, Transferring topical knowl- edge from auxiliary long texts for short text clustering, in: Proceedings of the 20th ACM international conference on Information and knowledge management, ACM, 2011, pp. 775–784. [16] X.-H. Phan, L.-M. Nguyen, S. Horiguchi, Learning to classify short and sparse text & web with hidden topics from large-scale data collections, in: Proceedings of the 17th international conference on World Wide Web, ACM, 2008, pp. 91–100. [17] R. Mehrotra, S. Sanner, W. Buntine, L. Xie, Improving lda topic models for microblogs via tweet pooling and automatic labeling, in: Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval, ACM, 2013, pp. 889–892. [18] J. Yin, J. Wang, A dirichlet multinomial mixture model-based approach for short text clustering, in: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, 2014, pp. 233–242. [19] W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, X. Li, Com- paring twitter and traditional media using topic models, in: Advances in Information Retrieval, 2011, pp. 338–349. [20] X. Quan, C. Kit, Y. Ge, S. J. Pan, Short", "meta": {"paper_id": "1904.07695v1", "chunk_index": 26, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_27", "text": "and data mining, 2014, pp. 233–242. [19] W. X. Zhao, J. Jiang, J. Weng, J. He, E.-P. Lim, H. Yan, X. Li, Com- paring twitter and traditional media using topic models, in: Advances in Information Retrieval, 2011, pp. 338–349. [20] X. Quan, C. Kit, Y. Ge, S. J. Pan, Short and sparse text topic modeling via self-aggregation, in: Proceedings of the 24th International Conference on Artiﬁcial Intelligence, 2015, pp. 2270–2276. [21] X. Yan, J. Guo, Y. Lan, J. Xu, X. Cheng, A probabilistic model for bursty topic discovery in microblogs., in: AAAI, 2015, pp. 353–359. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 16 (a). Biomedicine (b). GoogleNews (c). PascalFlickr (d). SearchSnippets (e). StackOverﬂow (f). Tweet Fig. 9. NMI values with different number of iterations on every corpora. [22] K. Nigam, A. K. McCallum, S. Thrun, T. Mitchell, Text classiﬁcation from labeled and unlabeled documents using em, Machine learning 39 (2- 3) (2000) 103–134. [23] G. Yu, R. Huang, Z. Wang, Document clustering via dirichlet process mixture model with feature selection, in: SIGKDD, ACM, 2010, pp. 763–772. [24] R. Huang, G. Yu, Z. Wang, J. Zhang, L. Shi, Dirichlet process mixture model for document clustering with feature partition, Knowledge and Data Engineering, IEEE Transactions on 25 (8) (2013) 1748–1759. [25] J. Qiang, Y. Li, Y. Yuan, X. Wu, Short text clustering based on pitman-yor process mixture model, Applied Intelligence 48 (7) (2018) 1802–1812. [26] J. Yin, J. Wang, A text clustering algorithm using an online clustering scheme for initialization, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2016, pp. 1995–2004. [27] J. Yin, D. Chao, Z. Liu, W. Zhang, X. Yu, J. Wang, Model-based clustering of short text streams, in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2018, pp. 2634–2642. [28] D. Q. Nguyen, R. Billingsley, L. Du, M. Johnson, Improving topic models with latent feature word representations, Transactions of the Association for Computational Linguistics 3 (2015) 299–313. [29] C. Li, H. Wang, Z. Zhang, A. Sun, Z. Ma, Topic modeling for short texts with auxiliary word embeddings, in: Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, ACM, 2016, pp. 165–174. [30] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, J. Dean, Distributed representations of words and phrases and their compositionality, in: Advances in neural information processing systems, 2013, pp. 3111– 3119. [31] C. Li, Y. Duan, H. Wang, Z. Zhang, A. Sun, Z. Ma, Enhancing topic mod- eling for short texts with auxiliary word embeddings, ACM Transactions on Information Systems (TOIS) 36 (2) (2017) 11. [32] Y. Zuo, J. Zhao, K. Xu, Word network topic model: a simple but general solution for short and imbalanced texts, Knowledge and Information Systems 48 (2) (2016) 379–398. [33] W. Chen, J. Wang, Y. Zhang, H. Yan, X. Li, User based aggregation for biterm topic model, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language", "meta": {"paper_id": "1904.07695v1", "chunk_index": 27, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_28", "text": "imbalanced texts, Knowledge and Information Systems 48 (2) (2016) 379–398. [33] W. Chen, J. Wang, Y. Zhang, H. Yan, X. Li, User based aggregation for biterm topic model, in: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Vol. 2, 2015, pp. 489–494. [34] W. Wang, H. Zhou, K. He, J. E. Hopcroft, Learning latent topics from the word co-occurrence network, National Conference of Theoretical Computer Science (2017) 18–30. [35] Y. Zuo, J. Wu, H. Zhang, H. Lin, F. Wang, K. Xu, H. Xiong, Topic modeling of short texts: A pseudo-document view, in: Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, ACM, 2016, pp. 2105–2114. [36] J. Qiang, P. Chen, T. Wang, X. Wu, Topic modeling over short texts by incorporating word embeddings, in: Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, Springer, 2017, pp. 363–374. [37] P. V. Bicalho, M. Pita, G. Pedrosa, A. Lacerda, G. L. Pappa, A general framework to expand short text for topic modeling, Information Sciences 393 (2017) 66–81. [38] L. Hong, B. D. Davison, Empirical study of topic modeling in twitter, in: Proceedings of the ﬁrst workshop on social media analytics, ACM, 2010, pp. 80–88. [39] A. K. McCallum, Mallet: A machine learning for language toolkit. [40] D. C. Liu, J. Nocedal, On the limited memory bfgs method for large scale optimization, Mathematical programming 45 (1-3) (1989) 503–528. [41] H. Mahmoud, P´olya urn models, Chapman and Hall/CRC, 2008. [42] M. Rosen-Zvi, T. L. Grifﬁths, M. Steyvers, P. Smyth, The author-topic model for authors and documents, uncertainty in artiﬁcial intelligence (2004) 487–494. JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 17 [43] D. Ramage, S. Dumais, D. Liebling, Characterizing microblogs with topic models, in: Fourth International AAAI Conference on Weblogs and Social Media, 2010. [44] I. Guy, Social recommender systems, in: Recommender systems hand- book, Springer, 2015, pp. 511–543. [45] X. Qian, H. Feng, G. Zhao, T. Mei, Personalized recommendation com- bining user interest and social circle, IEEE transactions on knowledge and data engineering 26 (7) (2014) 1763–1777. [46] O. Phelan, K. McCarthy, B. Smyth, Using twitter to recommend real- time topical news, in: Proceedings of the third ACM conference on Recommender systems, ACM, 2009, pp. 385–388. [47] J. Chen, R. Nairn, L. Nelson, M. Bernstein, E. Chi, Short and tweet: experiments on recommending content from information streams, in: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, ACM, 2010, pp. 1185–1194. [48] M. Chen, X. Jin, D. Shen, Short text classiﬁcation improved by learning multi-granularity topics, in: IJCAI, 2011, pp. 1776–1781. [49] D.-T. Vo, C.-Y. Ock, Learning to classify short text from scientiﬁc documents using topic models with various types of knowledge, Expert Systems with Applications 42 (3) (2015) 1684–1698. [50] Z. Dai, A. Sun, X.-Y. Liu, Crest: Cluster-based representation enrichment for short text classiﬁcation, in: Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, Springer, 2013, pp. 256–267. [51] A. H. Razavi, D. Inkpen, Text representation using multi-level", "meta": {"paper_id": "1904.07695v1", "chunk_index": 28, "filename": "1904.07695v1.pdf", "chunk_length": 512}}
{"id": "1904.07695v1_chunk_29", "text": "various types of knowledge, Expert Systems with Applications 42 (3) (2015) 1684–1698. [50] Z. Dai, A. Sun, X.-Y. Liu, Crest: Cluster-based representation enrichment for short text classiﬁcation, in: Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining, Springer, 2013, pp. 256–267. [51] A. H. Razavi, D. Inkpen, Text representation using multi-level latent dirichlet allocation, in: Canadian Conference on Artiﬁcial Intelligence, Springer, 2014, pp. 215–226. [52] C. X. Lin, B. Zhao, Q. Mei, J. Han, Pet: a statistical model for popular events tracking in social communities, in: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2010, pp. 929–938. [53] C. C. Aggarwal, K. Subbian, Event detection in social streams, in: Proceedings of the 2012 SIAM international conference on data mining, SIAM, 2012, pp. 624–635. [54] A. Ritter, O. Etzioni, S. Clark, et al., Open domain event extraction from twitter, in: Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2012, pp. 1104–1112. [55] J. H. Lau, N. Collier, T. Baldwin, On-line trend analysis with topic models:\\# twitter trends detection topic model online, Proceedings of COLING 2012 (2012) 1519–1534. [56] J. Pennington, R. Socher, C. Manning, Glove: Global vectors for word representation, in: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), 2014, pp. 1532–1543. [57] C. Rashtchian, P. Young, M. Hodosh, J. Hockenmaier, Collecting im- age annotations using amazon’s mechanical turk, in: Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, Association for Computational Linguistics, 2010, pp. 139–147. [58] C. Finegan-Dollak, R. Coke, R. Zhang, X. Ye, D. Radev, Effects of creativity and cluster tightness on short text clustering performance, in: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Vol. 1, 2016, pp. 654–665. [59] D. Newman, J. H. Lau, K. Grieser, T. Baldwin, Automatic evaluation of topic coherence, in: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, 2010, pp. 100–108. [60] D. Mimno, H. M. Wallach, E. Talley, M. Leenders, A. McCallum, Optimizing semantic coherence in topic models, in: Proceedings of the Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics, 2011, pp. 262–272. [61] J. Chang, S. Gerrish, C. Wang, J. L. Boyd-Graber, D. M. Blei, Reading tea leaves: How humans interpret topic models, in: Advances in neural information processing systems, 2009, pp. 288–296. [62] J. Chuang, C. D. Manning, J. Heer, Termite: Visualization techniques for assessing textual topic models, in: Proceedings of the international working conference on advanced visual interfaces, ACM, 2012, pp. 74– 77. [63] C. Sievert, K. Shirley, Ldavis: A method for visualizing and interpreting topics, in: Proceedings of the workshop on interactive language learning, visualization, and interfaces, 2014, pp. 63–70.", "meta": {"paper_id": "1904.07695v1", "chunk_index": 29, "filename": "1904.07695v1.pdf", "chunk_length": 470}}
{"id": "1904.07695v1_chunk_30", "text": "language learning, visualization, and interfaces, 2014, pp. 63–70.", "meta": {"paper_id": "1904.07695v1", "chunk_index": 30, "filename": "1904.07695v1.pdf", "chunk_length": 8}}
{"id": "1904.07904v1_chunk_0", "text": "MITIGATING THE IMPACT OF SPEECH RECOGNITION ERRORS ON SPOKEN QUESTION ANSWERING BY ADVERSARIAL DOMAIN ADAPTATION Chia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee College of Electrical Engineering and Computer Science National Taiwan University, Taiwan chiahsuan.li@gmail.com , y.v.chen@ieee.org , tlkagkb93901106@gmail.com ABSTRACT Spoken question answering (SQA) is challenging due to com- plex reasoning on top of the spoken documents. The recent studies have also shown the catastrophic impact of automatic speech recognition (ASR) errors on SQA. Therefore, this work proposes to mitigate the ASR errors by aligning the mismatch between ASR hypotheses and their corresponding reference transcriptions. An adversarial model is applied to this domain adaptation task, which forces the model to learn domain-invariant features the QA model can effectively uti- lize in order to improve the SQA results. The experiments successfully demonstrate the effectiveness of our proposed model, and the results are better than the previous best model by 2% EM score. Index Terms— adversarial learning, spoken question an- swering, SQA, domain adaptation 1. INTRODUCTION Question answering (QA) has drawn a lot of attention in the past few years. QA tasks on images [1] have been widely studied, but most focused on understanding text docu- ments [2]. A representative dataset in text QA is SQuAD [2], in which several end-to-end neural models have accomplished promising performance [3]. Although there is a signiﬁcant progress in machine comprehension (MC) on text documents, MC on spoken content is a much less investigated ﬁeld. In spoken question answering (SQA), after transcribing spoken content into text by automatic speech recognition (ASR), typ- ical approaches use information retrieval (IR) techniques [4] to ﬁnd the proper answer from the ASR hypotheses. One attempt towards QA of spoken content is TOEFL listening comprehension by machine [5]. TOEFL is an English exami- nation that tests the knowledge and skills of academic English for English learners whose native languages are not English. Another SQA corpus is Spoken-SQuAD[6], which is auto- matically generated from SQuAD dataset through Google Text-to-Speech (TTS) system. Recently ODSQA, a SQA corpus recorded by real speakers, is released [7]. To mitigate the impact of speech recognition errors, us- ing sub-word units is a popular approach for speech-related downstream tasks. It has been applied to spoken document retrieval [8] and spoken term detection [9] The prior work showed that, using phonectic sub-word units brought im- provements for both Spoken-SQuAD and ODSQA [6]. Instead of considering sub-word features, this paper pro- poses a novel approach to mitigate the impact of ASR errors. We consider reference transcriptions and ASR hypotheses as two domains, and adapt the source domain data (reference transcriptions) to the target domain data (ASR hypotheses) by projecting these two domains in the shared common space. Therefore, it can effectively beneﬁt the SQA model by im- proving the robustness to ASR errors in the SQA model. Domain adaptation has been successfully applied on com- puter vision [10] and speech recognition [11]. It is also widely studied on NLP tasks such as sequence tagging and pars- ing [12, 13, 14]. Recently, adversarial domain adaptation has already been explored on spoken language understanding (SLU). Liu", "meta": {"paper_id": "1904.07904v1", "chunk_index": 0, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_1", "text": "the SQA model. Domain adaptation has been successfully applied on com- puter vision [10] and speech recognition [11]. It is also widely studied on NLP tasks such as sequence tagging and pars- ing [12, 13, 14]. Recently, adversarial domain adaptation has already been explored on spoken language understanding (SLU). Liu and Lane learned domain-general features to ben- eﬁt from multiple dialogue datasets [15]; Zhu et al. learned to transfer the model from the transcripts side to the ASR hy- potheses side [16]; Lan et al. constructed a shared space for slot tagging and language model [17]. This paper extends the capability of adversarial domain adaptation for SQA, which has not been explored yet. 2. SPOKEN QUESTION ANSWERING In SQA, each sample is a triple, (q, d, a), where q is a question in either spoken or text form, d is a multi-sentence spoken- form document, and a is the answer in text from. The task of this work is extractive SQA; that means a is a word span from the reference transcription of d. An overview framework of SQA is shown in Figure 1. In this paper, we frame the source domain as reference transcriptions and the target domain as ASR hypotheses. Hence, we can collect source domain data more easily, and adapt the model to the target domain. In this task, when the machine is given a spoken docu- ment, it needs to ﬁnd the answer of a question from the spo- arXiv:1904.07904v1 [cs.CL] 16 Apr 2019 Predicted Answer Spoken Question Spoken Document Question Transcription Document Transcription ASR QA Model Text Question or Fig. 1. Flow diagram of the SQA system. ken document. SQA can be solved by the concatenation of an ASR module and a question answering module. Given the ASR hypotheses of a spoken document and a question, the question answering module can output a text answer. The most intuitive way to evaluate the text answer is to di- rectly compute the Exact Match (EM) and Macro-averaged F1 scores (F1) between the predicted text answer and the ground-truth text answer. We used the standard evaluation script from SQuAD [2] to evaluate the performance. 3. QUESTION ANSWERING MODEL The used architecture of the QA model is brieﬂy summarized below. Here we choose QANet [3] as the base model due to the following reasons: 1) it achieves the second best per- formance on SQuAD, and 2) since there are completely no recurrent networks in QANet, its training speed is 5x faster than BiDAF [18] when reaching the same performance on SQuAD. The network architecture is illustrated in Figure 2. The left blocks and the right blocks form two QANets, each of which takes a document and a question as the input and out- puts an answer. In QANet, ﬁrstly, an embedding encoder ob- tains word and character embeddings for each word in q or d and then models the temporal interactions between words and reﬁnes word vectors to contextualized word representations. All encoder blocks used in QANet are composed exclusively of depth-wise separable convolutions and self-attention. The intuition here is that", "meta": {"paper_id": "1904.07904v1", "chunk_index": 1, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_2", "text": "encoder ob- tains word and character embeddings for each word in q or d and then models the temporal interactions between words and reﬁnes word vectors to contextualized word representations. All encoder blocks used in QANet are composed exclusively of depth-wise separable convolutions and self-attention. The intuition here is that convolution components can model lo- cal interactions and self-attention components focus on mod- eling global interactions. The context-query attention layer generates the question-document similarity matrix and com- putes the question-aware vector representations of the context words. After that, a model encoder layer containing seven encoder blocks captures the interactions among the context words conditioned on the question. Finally, the output layer predicts a start position and an end position in the document to extract the answer span from the document. Source Embedding Encoder Domain Discriminator From Source? From Target? Source Context Query Attention Layer Source Model Encoder Layer Source Output Layer 𝜓𝑠𝑟𝑐(𝑞) 𝜓𝑠𝑟𝑐(𝑑) 𝑄𝑢𝑒𝑠𝑡𝑖𝑜𝑛𝑞𝐷𝑜𝑐𝑑 Tied / Untied 𝐴𝑛𝑠𝑤𝑒𝑟𝑎 Target Embedding Encoder Target Context Query Attention Layer Target Model Encoder Layer Target Output Layer 𝜓𝑡𝑎𝑟(𝑞) 𝜓𝑡𝑎𝑟(𝑑) 𝑄𝑢𝑒𝑠𝑡𝑖𝑜𝑛𝑞𝐷𝑜𝑐𝑑 𝐴𝑛𝑠𝑤𝑒𝑟𝑎 Fig. 2. The overall architecture of the proposed QA model with a domain discriminator. Each layer can be tied or untied between the source and target models. 4. DOMAIN ADAPTATION APPROACH The main focus of this paper is to apply domain adaptation for SQA. In this approach, we have two SQA models (QANets), one trained from target domain data (ASR hypotheses) and another trained from source domain data (reference transcrip- tions). Because the two domains share common information, some layers in these two models can be tied in order to model the shared features. Hence, we can choose whether each layer in the QA model should be shared. Tying the weights between the source layer and the target layer in order to learn a sym- metric mapping is to project both source and target domain data to a shared common space. Different combinations will be investigated in our experiments. More speciﬁcally, we incorporate a domain discriminator into the SQA model shown in Figure 2, which can enforce the embedding encoder to project the sentences from both source and target domains into a shared common space and conse- quentially to be ASR-error robust. Although the embedding encoder for both domains may implicitly learn some common latent representations, adversarial learning can provide a more direct training signal for aligning the output distribution of the embedding encoder from both domains. The embedding encoder takes in a sequence of word vectors and generates a sequence of hidden vectors with the same length. We use Ψtar(q) and Ψtar(d) (Ψsrc(q) and Ψsrc(d)) to represent the hid- den vector sequence given the question q and the document d in the target (source) domain respectively. The domain discriminator D focuses on identifying the domain of the vector sequence is from given Ψtar or Ψsrc, where the objective is to minimize Ldis. Ldis = E(q,d,a)∼tar [log D(Ψtar(q)) + log D(Ψtar(d))] (1) +E(q,d,a)∼src [log(1 −D(Ψsrc(q)) + log(1 −D(Ψsrc(d))]. Given a training example from the target domain ((q, d, a) ∼ tar), D learns to assign", "meta": {"paper_id": "1904.07904v1", "chunk_index": 2, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_3", "text": "domain of the vector sequence is from given Ψtar or Ψsrc, where the objective is to minimize Ldis. Ldis = E(q,d,a)∼tar [log D(Ψtar(q)) + log D(Ψtar(d))] (1) +E(q,d,a)∼src [log(1 −D(Ψsrc(q)) + log(1 −D(Ψsrc(d))]. Given a training example from the target domain ((q, d, a) ∼ tar), D learns to assign a lower score to q and d in that exam- ple, that is, to minimize D(Ψtar(q)) and D(Ψtar(d)). On the other hand, given a training example from the source domain ((q, d, a) ∼src), D learns to assign a larger value to q and d. Furthermore, we update the parameters of the embedding encoders to maximize the domain classiﬁcation loss Ldis, which works adversarially towards the domain discriminator. We thus expect the model to learn features and structures that can generalize across domains when the outputs of Ψsrc are indistinguishable from the outputs of Ψtar. The loss function for embedding encoder, Lenc, is formulated as Lenc = Lqa −λGLdis, (2) where λG is a hyperparameter. The two embedding encoders in the QA model are learned to maximize Ldis while mini- mizing the loss for QA, Lqa. Because the parameters of other layers in QA model are independent to the loss of the do- main discriminator, the loss function of other layers, Lother, is equivalent to Lqa, that is, Lother = Lqa. Although the discriminator is applied to the output of em- bedding encoder in Figure 2, it can be also applied to other layers.1 Considering that almost all QA model contains such embedding encoders, the proposed approach is expected to generalize to other QA models in addition to QANet. 5. EXPERIMENTS 5.1. Corpus Spoken-SQuAD is chosen as the target domain data for train- ing and testing. Spoken-SQuAD [6] is an automatically gen- 1In the experiments, we found that applying the domain discriminator to embedding encoders yielded the best performance. Table 1. Illustration of domain mismatch, where the models are trained on the source domain (Text-SQuAD; T-SQuAD) or the target domain (Spoken-SQuAD; S-SQuAD) and then evaluated on both source and target domains. Model T-SQuAD S-SQuAD Training EM F1 EM F1 T-SQuAD (a) 61.31 72.66 42.27 55.61 S-SQuAD (b) 45.52 57.39 48.93 61.20 Finetune (c) 54.83 66.45 49.60 61.85 erated corpus in which the document is in spoken form and the question is in text form. The reference transcriptions are from SQuAD [2]. There are 37,111 and 5,351 question an- swer pairs in the training and testing sets respectively, and the word error rate (WER) of both sets is around 22.7%. The original SQuAD, Text-SQuAD, is chosen as the source domain data, where only question answering pairs appearing in Spoken-SQuAD are utilized. In our task setting, during training we train the proposed QA model on both Text-SQuAD and Spoken-SQuAD training sets. While in the testing stage, we evaluate the performance on Spoken- SQuAD testing set. 5.2. Experiment Setup We utilize fasttext [19] to generate the embeddings of all words from both Text-SQuAD and Spoken-SQuAD. We adopt the phoneme sequence embeddings to replace the orig- inal character sequence embeddings using the method pro- posed by", "meta": {"paper_id": "1904.07904v1", "chunk_index": 3, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_4", "text": "testing stage, we evaluate the performance on Spoken- SQuAD testing set. 5.2. Experiment Setup We utilize fasttext [19] to generate the embeddings of all words from both Text-SQuAD and Spoken-SQuAD. We adopt the phoneme sequence embeddings to replace the orig- inal character sequence embeddings using the method pro- posed by Li et al. [6]. The source domain model and the target domain model share the same set of word embedding matrix to improve the alignment between these two domains. W-GAN is adopted for our domain discriminator [20], which stacks 5 residual blocks of 1D convolutional layers with 96 ﬁlters and ﬁlter size 5 followed by one linear layer to convert each input vector sequence into one scalar value. All models used in the experiments are trained with batch size 20, using adam with learning rate 1e −3 and the early stop strategy. The dimension of the hidden state is set to 96 for all layers, and the number of self-attention heads is set to 2. The setup is slightly different but better than the setting suggested by the original QAnet. 5.3. Results 5.3.1. Domain Mismatch First, we highlight the domain mismatch phenomenon in our experiments shown in Table 1. Row (a) is when QANet is trained on Text-SQuAD, row (b) is when QANet is trained on Spoken-SQuAD, and row (c) is when QANet is trained on Text-SQuAD and then ﬁnetuned on Spoken-SQuAD. The Table 2. The EM/F1 scores of proposed adversarial domain adaptation approaches over Spoken-SQuAD. Model EM F1 Baseline S-SQuAD (a) 48.93 61.20 Finetune (b) 49.60 61.85 Li et al. [6] (c) 49.07 61.16 Adverarial Lan et al. [17] (d) 49.13 61.80 Completely Shared (e) 49.57 61.48 (e) + GAN on Embedding (f) 51.10 63.11 (e) + GAN on Attention (g) 48.30 61.11 columns show the evaluation on the testing sets of Text- SQuAD and Spoken-SQuAD. It is clear that the performance drops a lot when the train- ing and testing data mismatch, indicating that model train- ing on ASR hypotheses can not generalize well on reference transcriptions. The performance gap is nearly 20% F1 score (72% to 55%). The row (c) shows the improved performance when testing on S-SQuAD due to the transfer learning via ﬁne-tuning. 5.3.2. Effectiveness of Adversarial Domain Adaptation To better demonstrate the effectiveness of the proposed model, we compare with baselines and show the results in Table 2. The baselines are: (a) trained on S-SQuAD, (b) trained on T-SQuAD and then ﬁne-tuned on S-SQuAD, and (c) previous best model trained on S-SQuAD [6] by using Dr.QA [21]. We also compare to the approach proposed by Lan et al. [17] in the row (d). This approach is originally proposed for spoken language understanding, and we adopt the same approach on the setting here. The approach models domain-speciﬁc features from the source and target domains separately by two different embedding encoders with a shared embedding encoder for modeling domain-general features. The domain-general parameters are adversarially trained by domain discriminator. Row (e) is the model that the weights of all layers are tied between the source domain and the", "meta": {"paper_id": "1904.07904v1", "chunk_index": 4, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_5", "text": "features from the source and target domains separately by two different embedding encoders with a shared embedding encoder for modeling domain-general features. The domain-general parameters are adversarially trained by domain discriminator. Row (e) is the model that the weights of all layers are tied between the source domain and the target domain. Row (f) uses the same architecture as row (e) with an additional do- main discriminator applied to the embedding encoder. It can be found that row (f) outperforms row (e), indicating that the proposed domain adversarial learning is helpful. Therefore, our following experiments contain domain adversarial learn- ing. The proposed approach (row (f)) outperforms previous best model (row (c)) by 2% EM score and over 1.5% F1 score. We also show the results of applying the domain discriminator to the top of context query attention layer in row (g), which obtains poor performance. To sum it up, incorporating adver- sarial learning by applying the domain discriminator on top Table 3. Investigation of different layer tying mechanisms, where ✓means that weights of the layer are tied between the source model and the target model. (L1: embedding encoder, L2: context query attention layer, L3: model encoder layer, L4: output layer.) Combination L1 L2 L3 L4 EM F1 (a) ✓ ✓ ✓ ✓ 51.10 63.11 (b) - ✓ ✓ ✓ 50.25 62.41 (c) - - ✓ ✓ 49.72 61.97 (d) - ✓ - ✓ 48.83 61.80 (e) - ✓ ✓ - 51.09 62.97 (f) ✓ - - ✓ 49.01 61.40 (g) ✓ - ✓ - 49.28 61.71 (h) ✓ ✓ - - 49.61 61.72 of the embedding encoder layer is effective. 5.3.3. Which Layer to Share? Layer weight tying or untying within the model indicates dif- ferent levels of symmetric mapping between the source and target domains. Different combinations are investigated and shown in Table 3. The row (a) in which all layers are tied is the row (e) of Table 2. The results show that untying context- query attention layer L2 (rows (c, f, g)) or model encoder layer L3 (rows (d, f, h)) lead to degenerated solutions in com- parison to row (a) where all layers are tied. Untying both of them simultaneously leads to the worst performance which is even worse than the ﬁnetuning (row (g) v.s. (c) from Table 2). These results imply that sharing the context-query attention layer and the model encoder layer are important for domain adaptation on SQA. We conjecture that these two layers ben- eﬁt from training on source domain data where there are no ASR errors, so the QA model learns to conduct attention or further reason well on target domain data with ASR errors. Overall, it is not beneﬁcial to untie any layer, because no information can be shared across different domains. Untying the embedding encoder L1 and the output layer L4 leads to the least degradation in comparison to row (a). 6. CONCLUSION In this work, we incorporate a domain discriminator to align the mismatched domains between ASR hypotheses and refer- ence transcriptions. The adversarial learning allows the end- to-end QA model", "meta": {"paper_id": "1904.07904v1", "chunk_index": 5, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_6", "text": "the embedding encoder L1 and the output layer L4 leads to the least degradation in comparison to row (a). 6. CONCLUSION In this work, we incorporate a domain discriminator to align the mismatched domains between ASR hypotheses and refer- ence transcriptions. The adversarial learning allows the end- to-end QA model to learn domain-invariant features and im- prove the robustness to ASR errors. The experiments demon- strate that the proposed model successfully achieves superior performance and outperforms the previous best model by 2% EM score and over 1.5% F1 score. 7. REFERENCES [1] C Lawrence Zitnick, Ramakrishna Vedantam, and Devi Parikh, “Adopting abstract images for semantic scene understanding,” IEEE transactions on pattern analysis and machine intelligence, vol. 38, no. 4, pp. 627–638, 2016. [2] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang, “Squad: 100,000+ questions for machine comprehension of text,” arXiv preprint arXiv:1606.05250, 2016. [3] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le, “Qanet: Combining local convolution with global self- attention for reading comprehension,” arXiv preprint arXiv:1804.09541, 2018. [4] Sz-Rung Shiang, Hung-yi Lee, and Lin-shan Lee, “Spo- ken question answering using tree-structured condi- tional random ﬁelds and two-layer random walk,” in Fifteenth Annual Conference of the International Speech Communication Association, 2014. [5] Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and Lin-Shan Lee, “Towards machine comprehension of spoken content: Initial toeﬂlistening comprehension test by machine,” arXiv preprint arXiv:1608.06378, 2016. [6] Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung- yi Lee, “Spoken squad: A study of mitigating the im- pact of speech recognition errors on listening compre- hension,” arXiv preprint arXiv:1804.00320, 2018. [7] Chia-Hsuan Lee, Shang-Ming Wang, Huan-Cheng Chang, and Hung-Yi Lee, “Odsqa: Open-domain spoken question answering dataset,” arXiv preprint arXiv:1808.02280, 2018. [8] Kenney Ng and Victor W Zue, “Subword unit represen- tations for spoken document retrieval,” in Fifth Euro- pean Conference on Speech Communication and Tech- nology, 1997. [9] Charl van Heerden, Damianos Karakos, Karthik Narasimhan, Marelie Davel, and Richard Schwartz, “Constructing sub-word units for spoken term detec- tion,” in Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on. IEEE, 2017, pp. 5780–5784. [10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc¸ois Laviolette, Mario Marchand, and Victor Lempitsky, “Domain- adversarial training of neural networks,” The Journal of Machine Learning Research, vol. 17, no. 1, pp. 2096– 2030, 2016. [11] Yusuke Shinohara, “Adversarial multi-task learning of deep neural networks for robust speech recognition.,” in INTERSPEECH, 2016, pp. 2369–2372. [12] Zhilin Yang, Ruslan Salakhutdinov, and William W Cohen, “Transfer learning for sequence tagging with hierarchical recurrent networks,” arXiv preprint arXiv:1703.06345, 2017. [13] David McClosky, Eugene Charniak, and Mark Johnson, “Automatic domain adaptation for parsing,” in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics. Association for Computa- tional Linguistics, 2010, pp. 28–36. [14] Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan, “Do- main adaptation of rule-based annotators for named- entity recognition tasks,” in Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics,", "meta": {"paper_id": "1904.07904v1", "chunk_index": 6, "filename": "1904.07904v1.pdf", "chunk_length": 512}}
{"id": "1904.07904v1_chunk_7", "text": "Computational Linguistics. Association for Computa- tional Linguistics, 2010, pp. 28–36. [14] Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li, Frederick Reiss, and Shivakumar Vaithyanathan, “Do- main adaptation of rule-based annotators for named- entity recognition tasks,” in Proceedings of the 2010 conference on empirical methods in natural language processing. Association for Computational Linguistics, 2010, pp. 1002–1012. [15] Bing Liu and Ian Lane, “Multi-domain adversarial learning for slot ﬁlling in spoken language understand- ing,” arXiv preprint arXiv:1711.11310, 2017. [16] Su Zhu, Ouyu Lan, and Kai Yu, “Robust spoken lan- guage understanding with unsupervised asr-error adap- tation,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 6179–6183. [17] Ouyu Lan, Su Zhu, and Kai Yu, “Semi-supervised training using adversarial multi-task learning for spoken language understanding,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 6049–6053. [18] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi, “Bidirectional attention ﬂow for machine comprehension,” arXiv preprint arXiv:1611.01603, 2016. [19] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov, “Enriching word vectors with subword information,” arXiv preprint arXiv:1607.04606, 2016. [20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin- cent Dumoulin, and Aaron C Courville, “Improved training of wasserstein gans,” in Advances in Neural Information Processing Systems, 2017, pp. 5767–5777. [21] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes, “Reading wikipedia to answer open-domain questions,” arXiv preprint arXiv:1704.00051, 2017.", "meta": {"paper_id": "1904.07904v1", "chunk_index": 7, "filename": "1904.07904v1.pdf", "chunk_length": 231}}
{"id": "1904.07982v1_chunk_0", "text": "Query Expansion for Cross-Language Question Re-Ranking Muhammad Mahbubur Rahman Sorami Hisamoto Kevin Duh mahbubur@jhu.edu,s@89.io,kevinduh@cs.jhu.edu Johns Hopkins University Baltimore, MD, USA ABSTRACT Community question-answering (CQA) platforms have become very popular forums for asking and answering questions daily. While these forums are rich repositories of community knowl- edge, they present challenges for finding relevant answers and similar questions, due to the open-ended nature of informal dis- cussions. Further, if the platform allows questions and answers in multiple languages, we are faced with the additional challenge of matching cross-lingual information. In this work, we focus on the cross-language question re-ranking shared task, which aims to find existing questions that may be written in different languages. Our contribution is an exploration of query expansion techniques for this problem. We investigate expansions based on Word Embed- dings, DBpedia concepts linking, and Hypernym, and show that they outperform existing state-of-the-art methods. KEYWORDS Query Expansion, Cross-Language Information Retrieval, Commu- nity Question-Answering, DBpedia Concept Linking 1 INTRODUCTION Due to the huge popularity of community question-answering (CQA) platforms, such as Quora and Stack Overflow, it has captured the attention of researchers as an area with social impact. Users who ask questions receive quick and useful answers from these community platforms. Since these platforms are open-ended hav- ing crowd-source nature, it is a challenging task to find relevant questions and answers. To get the best use of these community knowledge repositories, it is very important to find the most rele- vant questions and retrieve their relevant answers to a new question. The informal writing makes this a challenge. To deal with the need of real applications in CQA, we focus on the task of question re-ranking. As defined by SemEval-2016, given (i) a new question and (ii) a large collection of existing questions in the CQA platform, rerank all existing questions by similarity to the new question, with the hope that they may provide the answers to the new question [15]. This addresses the challenge that there exists many ways to ask the same question, and an existing question- answer pair may already satisfy the information need. If a CQA platform supports text entry in multiple languages, this becomes a type of cross-language information retrieval (CLIR) prob- lem. A machine translation (MT) model may be used to translate one language into another language prior to indexing and search, and translation errors may lead to degradation in either precision or recall. For example, Fig 1 shows a question in English, Arabic, and MT English from [5], who extends the SemEval-2016 task to cross-language (CL) settings: the collection of existing questions are in English, and the user queries are simulated as new questions Figure 1: English-Arabic-MT Question written in Arabic.1 We can see that there are various translation errors in the MT English compared to the original English question. In order to address the complexity on question re-ranking in a CLIR setting for CQA platforms, we explore different query ex- pansion (QE) techniques. Our hypothesis is that mis-translations are often different nuances of related concepts, so by expansion with similar terms, we may recover", "meta": {"paper_id": "1904.07982v1", "chunk_index": 0, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_1", "text": "to the original English question. In order to address the complexity on question re-ranking in a CLIR setting for CQA platforms, we explore different query ex- pansion (QE) techniques. Our hypothesis is that mis-translations are often different nuances of related concepts, so by expansion with similar terms, we may recover the underlying terms needed for matching. We investigate Word Embedding, DBpedia, and Hy- pernym knowledge graph to expand query in a question-question re-ranking task. To the best of our knowledge, we first propose QE techniques for CL question re-ranking on CQA platforms. Our QE work flow is given in Fig 2. We adopt a query translation ap- proach, followed by QE: Given an initial query (e.g. MT English), we expand each term with information from outside resources, then match against the existing questions which are indexed as English documents in a search server like ElasticSearch. We develop baseline and aggregated systems using QE methods and evaluate our approaches on the CL extension of the SemEval- 2016 dataset [5, 15]. The evaluation results show that our QE sys- tems achieve significant improvement over existing methods on CL question re-ranking. 2 RELATED WORK Aiming to retrieve information in a language different from the query language, a wide range of research has been done in CLIR [2, 11, 13, 16, 19, 20]. To improve the search performance of a CLIR system, researchers have been giving more importance on QE techniques, such as, using of external lexical database [14], co- occurrence analysis [23] and pseudo-relevance feedback [10, 24]. Zamani et al. [1], Kuzi et al. [9] and Diaz et al. [6] presented QE techniques using Word Embedding. A different approach using external knowledge base were developed by Xiong et al. [22] and Zhang et al. [25] to expand queries in CLIR. 1In the original SemEval-2016 dataset, both new and existing questions were in English, but for the CL extension in [5], the new questions were replaced with their manual Arabic translations, and Arabic-to-English MT results were added to simulate a CLIR setup. arXiv:1904.07982v1 [cs.IR] 16 Apr 2019 Figure 2: Query Expansion Work Flow [Query = Question and Documents = Existing Questions] Although, CQA [3, 4, 8, 12, 21] is a popular research area, there has not been much work done in CL CQA task. The CL CQA heav- ily depends on CL question ranking. SemEval-2016 introduced a shared task on CQA in English and Arabic [15]. They also had a subtask question-question similarity ranking [7]. A later work was done by Martino et al. [5] in CL question re-ranking using a CL tree kernel. There is still a big scope of improving the performance of community question re-ranking task. To the best of our knowl- edge, no research has been done on QE for community question re-ranking task. 3 APPROACH 3.1 Query Expansion Query Expansion is a technique of improving the search perfor- mance in IR by augmenting the initial query using different methods. To achieve a better search performance, it is important to expand the initial query to retrieve more relevant documents. Specially it is", "meta": {"paper_id": "1904.07982v1", "chunk_index": 1, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_2", "text": "re-ranking task. 3 APPROACH 3.1 Query Expansion Query Expansion is a technique of improving the search perfor- mance in IR by augmenting the initial query using different methods. To achieve a better search performance, it is important to expand the initial query to retrieve more relevant documents. Specially it is very useful CL settings since the initial query may miss important clue to retrieve more relevant documents in a different language than the original query. To expand any query we use different ex- pansion techniques and take the union of expanded terms. For any query Q the expanded query is given in the equation 1. QEQ = kwQ Ø qeQ we Ø qeQ db Ø qeQ h (1) Where Ð is a union operator. QEQ is the union of all expanded queries along with the original keyword query kwQ. And qeQ we, qeQ db and qeQ h are QE using Word Embedding, DBpedia concepts linking and Hypernym respectively. 3.1.1 Word Embedding. Word embedding is a type of word rep- resentation that transforms a text into a numerical vector of real numbers where semantic similar words are grouped together. By the characteristics of word embedding, two words having semantic similar meaning share a fair amount of surrounding context. It implies that search engine should return relevant documents to the query term using the similar word. In a CL setting, due to MT, the translated text may miss the original word, and produce a different word of similar meaning, the indexed documents may not have the exact keyword. Hence the retrieval engine won’t return documents of out of vocabulary. From these intuitions and inspired by [1, 9], we use a pre-trained word embedding GloVe vectors [17] based on Wikipedia and Giga- word corpus to get the most similar words for each of the query terms from the original query. The dimension of the vector is 100 and it contains 400k vocab. For a query Q having query terms q1, q2 ...... qn, let tqi 1 and tqi 2 are the 2 most similar terms for query term qi obtained from the pre-trained word embedding model, then QE using word embedding is computed by the equation 2. qeQ we = n Ø i=1 {tqi 1 ,tqi 2 } (2) As an example, a query term \"travel\" from a query, Im likely to travel in the month of june... just wanna know some good places to visit...., is expanded using \"travelers\" and \"trips\" where \"travelers\" and \"trips\" are 2 most similar terms for the query term \"travel\". 3.1.2 DBpedia. DBpedia 2 is a structured open knowledge base derived from the Wikipedia. The current version of DBpedia has 4.58 million English concepts and 38.3 millions of concepts in 125 different languages. Motivated by the work presented in [22] to associate a term with entities from Freebase knowledge graph, we use DBpedia to extract concepts and linked each query term with DBpedia concepts. Given a query Q having query terms q1, q2 ...... qn, we retrieve DBpedia concepts for each of the query terms qi. Each of the", "meta": {"paper_id": "1904.07982v1", "chunk_index": 2, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_3", "text": "[22] to associate a term with entities from Freebase knowledge graph, we use DBpedia to extract concepts and linked each query term with DBpedia concepts. Given a query Q having query terms q1, q2 ...... qn, we retrieve DBpedia concepts for each of the query terms qi. Each of the returned concepts is associated with different types and properties that reflect the concept. In the expansion term selection, we choose a simple but useful strategy. We select a property called dct : subject that links a concept with relevant subjects. We use the relevant subjects to expand the query term, qi. The intuition is that the concepts associated with query terms are able to capture more relevant documents. The QE is computed by the equation 3. qeQ db = n Ø i=1 kØ j=1 {subject : qj i } (3) Where k is the number of subjects for a query term qi and qj i is a subject of a concept associated with the query term qi. As an example, a query term \"travel\" from a query, Im likely to travel in the month of june... just wanna know some good places to visit...., is expanded using \"Tourism\", \"Tourist activities\" and \"Trans- port culture\" where \"Tourism\", \"Tourist activities\" and \"Transport culture\" are the subjects of a concept associated with \"Travel\". 3.1.3 Hypernym. Hyponym is a specific word or phrase of a more generic term. The generic term is called hypernym. Due to MT, the translated term may have a different form of the original term. We use a publicly available hypernym knowledge graph3 developed by [18] to extract hyponym labels with a high confidence score for each of the query terms and include them in the QE. The motivation is to retrieve more relevant documents which may contain hyponym terms of an original query term. For a query Q having q1, q2 ...qi... qn query terms, we expand Q using the equation 4. qeQ h = n Ø i=1 kØ j=1 {hyponym : qj i | cs(qj i,qi) >= 0.75} (4) Where, k is the number of hyponym labels for a query term qi, qj i is a hyponym label and cs(qj i,qi) is a function that gives a confidence score for qj i with respect to qi. 2https://wiki.dbpedia.org/ 3http://webisa.webdatacommons.org/ 2 As an example, a query term \"travel\" from a query, Im likely to travel in the month of june... just wanna know some good places to visit...., is expanded using \"operating expense\", \"related expense\" and \"personal expense\" where \"operating expense\", \"related expense\" and \"personal expense\" are hyponym labels for \"travel\". 3.2 Search and Ranking To index documents, search queries and rank retrieved documents, we use Elasticsearch 4, a Lucene based distributed, RESTful search and analytics engine. We use a built-in English analyzer, which is used to converting documents into tokens to the inverted index for searching. The same analyzer is applied to the query string at the search time. As a ranking or scoring algorithm, we use BM25 similarity algorithm. We also configure the similarity algorithm by hyper-parameter tuning. 4 DATASET", "meta": {"paper_id": "1904.07982v1", "chunk_index": 3, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_4", "text": "analyzer, which is used to converting documents into tokens to the inverted index for searching. The same analyzer is applied to the query string at the search time. As a ranking or scoring algorithm, we use BM25 similarity algorithm. We also configure the similarity algorithm by hyper-parameter tuning. 4 DATASET To evaluate our systems, we use SemEval-2016 Task 3 CQA dataset (Subtask B) [15] and MT version of human translated Arabic ques- tions [5]. In [5], a new question is written in Arabic, and the goal is to retrieve similar questions in English; an MT system was avail- able for translating the new Arabic question into English. In this research, the new question is considered as a query and a set of the first ten existing questions are considered as documents. The task is to re-rank the documents using different QE techniques. The dataset has 267 train, 50 dev and 70 test queries; there are 10 existing questions to be re-ranked for each new question, leading to 2670 train, 500 dev and 700 test \"documents\". In this research, we use only the dev and test datasets. To setup a CL environment, we choose MT version of 50 dev and 70 test questions from [5] and consider them as machine translated queries. 5 EXPERIMENTAL SETUP The datasets explained in the previous section were used for our experiments in the cross-language question re-ranking task. Ini- tially we indexed all existing questions, which are considered as documents in this research, using the approach described in the Search and Ranking section. We observed that both English and machine translated queries might have punctuation and common words. To build a more clean queries, we filtered out punctuation and common English words from the initial queries. Then we split each query into words which are considered as keyword query, our baseline system. We experimented in two scenarios: a) English query, the original SemEval-2016 task B, and b) Machine translated query, where Arabic version of the English queries are translated back into English. The second scenario is the CLIR setting we focus here, while the first scenario provides comparison results in the monolingual setting. We configured 18 systems for English and MT queries, by com- bining each of the four basic systems: (a) Keyword, (b) Word Embed- ding, (c) DBpedia and (d) Hypernym. The system (a) is the baseline system whereas (b), (c) and (d) are systems based on QE using Word Embedding, DBpedia and Hypernym knowledge graph respectively. The average query lengths for baseline systems are 18.67(EN) and 4https://www.elastic.co/products/elasticsearch System QR MAP ∆ Dev Test 1. Keyword(KW) (Baseline) EN 72.60 71.43 00.00 2. Word Embedding(WE) EN 64.40 63.86 -07.57 3. DBpedia(DB) EN 41.00 45.29 -26.14 4. Hypernym(HN) EN 21.00 27.86 -34.57 5. 1 + 2 (KW+WE) EN 80.20 79.86 +08.43 6. 1 + 3 (KW+DB) EN 76.00 75.29 +03.86 7. 1 + 4 (KW+HN) EN 75.20 76.00 +04.57 8. 2 + 3 + 4 (WE+DB+HN) EN 72.20 75.86 +04.43 9. 1 + 2 + 3 + 4 (Best) EN 84.00 82.00 +10.57 10. UH-PRHLT(SemEval[7, 15]) EN 75.90 76.70", "meta": {"paper_id": "1904.07982v1", "chunk_index": 4, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_5", "text": "79.86 +08.43 6. 1 + 3 (KW+DB) EN 76.00 75.29 +03.86 7. 1 + 4 (KW+HN) EN 75.20 76.00 +04.57 8. 2 + 3 + 4 (WE+DB+HN) EN 72.20 75.86 +04.43 9. 1 + 2 + 3 + 4 (Best) EN 84.00 82.00 +10.57 10. UH-PRHLT(SemEval[7, 15]) EN 75.90 76.70 - 11. SVM + TK [5] EN 73.02 77.41 - 12. Keyword(KW) (Baseline) MT 72.20 67.57 00.00 13. Word Embedding(WE) MT 64.40 63.43 -04.14 14. DBpedia(DB) MT 43.20 45.71 -21.86 15. Hypernym(HN) MT 26.80 32.71 -34.86 16. 12 + 13 (KW+WE) MT 79.20 75.57 +08.00 17. 12 + 14 (KW+DB) MT 75.40 71.43 +03.86 18. 12 + 15 (KW+HN) MT 76.40 72.29 +04.72 19. 13 + 14 + 15 (WE+DB+HM) MT 77.60 73.14 +05.57 20. 12 + 13 + 14 + 15(Best) MT 84.00 78.29 +10.72 21. SVM+TK([5]) MT 72.94 76.67 - Table 1: MAP scores for various QE on English (EN) ques- tions (monolingual setup) and MT questions (CLIR setup). 19.96(MT) words. And the average word additions are Word Embed- ding: 30.98(EN) and 35.34(MT); DBpedia: 25.81(EN) and 31.32(MT); Hypernym: 21.58(EN) and 29.04(MT); Best system: 78.3(EN) and 95.6(MT). The combination of 18 systems are given in table 1. All the systems are experimented in two settings - Dev and Test. The search ranking scores are calculated for all 18 systems using BM25 algorithm. We tuned BM25 hyper-parameters, k1 and b on Dev set to get the optimized values where k1 controls non-linear term frequency normalization and b controls to what degree document length normalizes t f values. The score for each query is calculated based on 10 existing documents to re-rank them. 6 RESULTS AND ANALYSIS Table 1 compares the MAP scores on the dev and test sets. The \"System\" column compares QE, Keyword baseline without QE, and previously-published state-of-the-art methods in this task (UH- PRHLT, SVM+TK); the QR column shows whether the query was English (monolingual setup) or Machine-translated English (cross- language setup). The ∆column displays the difference between test MAP against the baseline. The results on the original English queries are shown in rows 1 to 11. Row 1 is the baseline system using keyword query without any QE. Rows 2 to 4 show scores for QE using Word Embedding, DBpedia and Hypernym respectively. We observe that each of the individual systems from rows 2 to 4 has a negative ∆score. That means QE using any single approach doesn’t beat the baseline sys- tem. Combination of baseline and any QE method are shown in rows 5 to 7 where each of the combinations has better performance 3 Figure 3: MAP scores over queries than the baseline system. Among them, combination of word em- bedding and the baseline system has the highest ∆score which is +08.43. Row 8 shows QE using a combination of Word Embedding, DBpedia and Hypernym, which also beats the baseline system by +04.43 ∆score. An aggregation of all systems from rows 1 to 4 is shown in row 9, which is the best performing system. The best system beats our baseline system by +10.57 ∆score which is", "meta": {"paper_id": "1904.07982v1", "chunk_index": 5, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_6", "text": "a combination of Word Embedding, DBpedia and Hypernym, which also beats the baseline system by +04.43 ∆score. An aggregation of all systems from rows 1 to 4 is shown in row 9, which is the best performing system. The best system beats our baseline system by +10.57 ∆score which is a substantial improvement over the baseline system. The best performing systems from the SemEval-2016 Task 3 [7, 15] and the state-of-the-art system [5] in the community ques- tion re-ranking task are shown in row 10 and 11 respectively. Our best system outperforms them by +05.30 and +04.59 MAP scores respectively which is an effective improvement in the community question re-ranking task. The results on MT queries are presented in rows 12 to 21 where the baseline system is shown in row 12. Individual QE using Word Embedding, DBpedia and Hypernym are displayed in rows 13 to 15. We find a similar pattern between these and the QE in English that is negative ∆score which implies individual QE technique also doesn’t perform well for MT queries. Union of the baseline system and any other expansion method from rows 13 to 15 are given in rows 16 to 18. Similar to the English query, we also achieve positive ∆scores for each of them where expansion using Word Embedding has the highest ∆score +08.00. Row 19 shows a combination of QE methods which also beats the baseline by +05.57 MAP score. The best system, union of baseline and all QE given in row 20, improves the performance by +10.72 ∆ score. Our best system in MT setting also outperforms the state-of- the-art system given in row 21 by +01.62 MAP score. The significant improvement compared to the baseline and the state-of-the-art, implies that our QE approaches are also strong to MT. In the comparison of baseline systems in English and MT (row 1 and 12), we notice that the MT baseline system has a lower MAP score by -3.86. We also observe that the MT best system degrades the MAP score by -03.71 than that of English. We assume the reason behind these low map scores for MT systems is the effect of the output of machine translation. We see that individual QE using DBpedia and Hypernym have slightly better performance in MT than English by +00.42 (diff. between row 14 and row 3) and +04.85 (diff. between row 15 and row 4) map scores respectively. Most importantly, we find that our best systems (English and MT), outperform the baseline systems and state-of-the-art results in community question re-ranking task. These indicate that our QE methods are robust and effective in both monolingual and CL settings. Figure 3 shows MAP scores for each query of baseline and best systems for both English and MT. One interesting observation is that we get MAP score 0 for 5 out of 70 test queries, and all these are for either MT baseline or MT best system. Only query 329 has a 0 MAP for English baseline system along with MT best system. 7 CONCLUSION We investigate different query expansion", "meta": {"paper_id": "1904.07982v1", "chunk_index": 6, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_7", "text": "observation is that we get MAP score 0 for 5 out of 70 test queries, and all these are for either MT baseline or MT best system. Only query 329 has a 0 MAP for English baseline system along with MT best system. 7 CONCLUSION We investigate different query expansion techniques for improving cross-language question re-ranking in community question answer- ing platforms. Our techniques, though simple, outperform current state-of-the-art on SemEval-2016 Task 3 and its CLIR extension. As a future work, we plan to improve methods for candidate terms selection for each of the different query expansions types. ACKNOWLEDGMENTS This work is supported in part by the Office of the Director of Na- tional Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract #FA8650-17-C-9115. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, ei- ther expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein. REFERENCES [1] Saeid Balaneshin-kordan and Alexander Kotov. 2017. Embedding-based Query Expansion for Weighted Sequential Dependence Retrieval Model. In Proceedings of SIGIR. ACM, 1213–1216. [2] Lisa Ballesteros and Bruce Croft. 1996. Dictionary methods for cross-lingual information retrieval. In Proceedings of DEXA. Springer, 791–801. [3] Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding the right facts in the crowd: factoid question answering over social media. In Proceedings of WWW. ACM, 467–476. [4] David Carmel, Avihai Mejer, Yuval Pinter, and Idan Szpektor. 2014. Improv- ing term weighting for community question answering search using syntactic analysis. In Proceedings of CIKM. ACM, 351–360. [5] Giovanni Da San Martino, Salvatore Romeo, Alberto Barroón-Cedeño, Shafiq Joty, Lluís Maàrquez, Alessandro Moschitti, and Preslav Nakov. 2017. Cross-Language Question Re-Ranking. In Proceedings of SIGIR. ACM, 1145–1148. [6] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query expansion with locally-trained word embeddings. arXiv preprint arXiv:1605.07891 (2016). [7] Marc Franco-Salvador, Sudipta Kar, Thamar Solorio, and Paolo Rosso. 2018. UH- PRHLT at SemEval-2016 Task 3: Combining lexical and semantic-based features for community question answering. arXiv preprint arXiv:1807.11584 (2018), 1–8. [8] Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model for question retrieval in community question answering. In Proceedings of CIKM. ACM, 2471–2474. [9] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query expansion using word embeddings. In Proceedings of CIKM. ACM, 1929–1932. [10] Victor Lavrenko and W Bruce Croft. 2017. Relevance-based language models. In SIGIR Forum, Vol. 51. ACM, 260–267. 4 [11] Bo Li and Ping Cheng. 2018. Learning Neural Representation for CLIR with Adversarial Framework. In Proceedings of EMNLP. 1861–1870. [12] Baichuan Li, Tan Jin, Michael R Lyu, Irwin King, and Barley Mak. 2012. Analyzing and predicting question quality in community question answering services. In Proceedings of WWW. ACM, 775–782. [13] Robert Litschko, Goran Glavaš, Simone Paolo Ponzetto, and Ivan Vulić. 2018. Unsupervised Cross-Lingual Information Retrieval Using Monolingual Data Only. In Proceedings of SIGIR. ACM, 1253–1256. [14] George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38,", "meta": {"paper_id": "1904.07982v1", "chunk_index": 7, "filename": "1904.07982v1.pdf", "chunk_length": 512}}
{"id": "1904.07982v1_chunk_8", "text": "community question answering services. In Proceedings of WWW. ACM, 775–782. [13] Robert Litschko, Goran Glavaš, Simone Paolo Ponzetto, and Ivan Vulić. 2018. Unsupervised Cross-Lingual Information Retrieval Using Monolingual Data Only. In Proceedings of SIGIR. ACM, 1253–1256. [14] George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39–41. [15] Preslav Nakov, LluÃŋs MÃărquez, Alessandro Moschitti, Walid Magdy, Hamdy Mubarak, Abed A. Freihat, James Glass, and Bilal Randeree. 2016. SemEval-2016 Task 3: Community Question Answering. Proceedings of SemEval (2016), 525–545. [16] Jian-Yun Nie. 2010. Cross-Language Information Retrieval. Morgan and Claypool Publishers. [17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of EMNLP. 1532–1543. [18] Julian Seitner, Christian Bizer, Kai Eckert, Stefano Faralli, Robert Meusel, Heiko Paulheim, and Simone Paolo Ponzetto. 2016. A Large DataBase of Hypernymy Relations Extracted from the Web. In LREC. [19] Ralf Steinberger, Bruno Pouliquen, and Johan Hagman. 2002. Cross-lingual document similarity calculation using the multilingual thesaurus eurovoc. In Proceedings of CICLing. Springer, 415–424. [20] Ivan Vulić and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual Information Retrieval Models Based on (Bilingual) Word Embeddings. In Proceed- ings of SIGIR. ACM, 363–372. [21] Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang Qin. 2017. Answer Selection in Community Question Answering via Attentive Neural Networks. Proceedings of Signal Processing Letters 24, 4 (2017), 505–509. [22] Chenyan Xiong and Jamie Callan. 2015. Query expansion with Freebase. In Proceedings of ICTIR. ACM, 111–120. [23] Jinxi Xu and W Bruce Croft. 2017. Quary expansion using local and global document analysis. In SIGIR forum, Vol. 51. ACM, 168–175. [24] Chengxiang Zhai and John Lafferty. 2001. Model-based feedback in the language modeling approach to information retrieval. In Proceedings of CIKM. ACM, 403– 410. [25] Lei Zhang, Michael Färber, and Achim Rettinger. 2016. Xknowsearch!: exploiting knowledge bases for entity-based cross-lingual information retrieval. In Proceed- ings of CIKM. ACM, 2425–2428. 5", "meta": {"paper_id": "1904.07982v1", "chunk_index": 8, "filename": "1904.07982v1.pdf", "chunk_length": 315}}
{"id": "1904.08051v1_chunk_0", "text": "Posterior-regularized REINFORCE for Instance Selection in Distant Supervision Qi Zhang1, Siliang Tang1∗, Xiang Ren3,Fei Wu1, Shiliang Pu2 & Yueting Zhuang1 1Zhejiang University, 2Hikvision, 3University of Southern California, {zhangqihit,siliang,wufei,yzhuang}@zju.edu.cn, pushiliang@hikvision.com, xiangren@usc.edu Abstract This paper provides a new way to improve the efﬁciency of the REINFORCE training pro- cess. We apply it to the task of instance selec- tion in distant supervision. Modeling the in- stance selection in one bag as a sequential de- cision process, a reinforcement learning agent is trained to determine whether an instance is valuable or not and construct a new bag with less noisy instances. However unbiased methods, such as REINFORCE, could usually take much time to train. This paper adopts posterior regularization (PR) to integrate some domain-speciﬁc rules in instance selection us- ing REINFORCE. As the experiment results show, this method remarkably improves the performance of the relation classiﬁer trained on cleaned distant supervision dataset as well as the efﬁciency of the REINFORCE training. 1 Introduction Relation extraction is a fundamental work in natu- ral language processing. Detecting and classifying the relation between entity pairs from the unstruc- tured document, it can support many other tasks such as question answering. While relation extraction requires lots of la- beled data and make methods labor intensive, (Mintz et al., 2009) proposes distant supervision (DS), a widely used automatic annotating way. In distant supervision, knowledge base (KB) , such as Freebase, is aligned with nature documents. In this way, the sentences which contain an en- tity pair in KB all express the exact relation that the entity pair has in KB. We usually call the set of instances that contain the same entity pair a bag. In this way, the training instances can be di- vided into N bags B = {B1, B2, ..., BN}. Each bag Bk are corresponding to an unique entity pair ∗Corresponding author Ek = (ek 1, ek 2) and contains a sequence of in- stances {xk 1, xk 2, ..., xk |Bk|} . However, distant su- pervision may suffer a wrong label problem. In other words, the instances in one bag may not ac- tually have the relation. To resolve the wrong label problem, just like Fig.2 shows, (Feng et al., 2018) model the instance selection task in one bag Bk as a sequential deci- sion process and train an agent π(a|s, θπ) denoting the probability Pπ(At = a, |St = s, θt = θπ) that action a is taken at time t given that the agent is in state s with parameter vector θπ by REINFORCE algorithm (Sutton et al., 1998). The action a can only be 0 or 1 indicating whether an instance xk i is truly expressing the relation and whether it should be selected and added to the new bag Bk. The state s is determined by the entity pair cor- responding to the bag, the candidate instance to be selected and the instances that have already been selected. Accomplishing this task, the agent gets a new bag Bk at the terminal of the trajec- tory with less wrong labeled instances. With", "meta": {"paper_id": "1904.08051v1", "chunk_index": 0, "filename": "1904.08051v1.pdf", "chunk_length": 512}}
{"id": "1904.08051v1_chunk_1", "text": "state s is determined by the entity pair cor- responding to the bag, the candidate instance to be selected and the instances that have already been selected. Accomplishing this task, the agent gets a new bag Bk at the terminal of the trajec- tory with less wrong labeled instances. With the newly constructed dataset B = {B1, B2, ..., BN} with less wrong labeling instances, we can train bag level relation predicting models with better performance. Meanwhile, the relation predicting model gives reward to the instance selection agent. Therefore, the agent and the relation classiﬁer can be trained jointly. However, REINFORCE is a Monte Carlo algo- rithm and need stochastic gradient method to op- timize. It is unbiased and has good convergence properties but also may be of high variance and slow to train (Sutton et al., 1998). Therefore, we train a REINFORCE based agent by integrating some other domain-speciﬁc rules to accelerate the training process and guide the agent to explore more effectively and learn a bet- ter policy. Here we use a rule pattern as the Fig.1 shows (Liu et al., 2017). The instances that return arXiv:1904.08051v1 [cs.CL] 17 Apr 2019 true (match the pattern and label in any one of the rules) are denoted as xMI and we adopt posterior regularization method (Ganchev, 2010) to regular- ize the posterior distribution of π(a|s, θπ) on xMI. In this way, we can construct a rule-based agent πr. πr tends to regard the instances in xMI valu- able and select them without wasting time in trial- and-error exploring. The number of such rules is 134 altogether and can match nearly four percents of instances in the training data. Our contributions include: • We propose PR REINFORCE by integrating domain-speciﬁc rules to improve the perfor- mance of the original REINFORCE. • We apply the PR REINFORCE to the in- stance selection task for DS dataset to alle- viate the wrong label problem in DS. 2 RELATED WORK Among the previous studies in relation extraction, most of them are supervised methods that need a large amount of annotated data (Bach et al., 2007). Distant supervision is proposed to allevi- ate this problem by aligning plain text with Free- base. However, distant supervision inevitably suf- fers from the wrong label problem. Some previous research has been done in han- dling noisy data in distant supervision. An expressed-at-least-once assumption is employed in (Mintz et al., 2009): if two entities partici- pated in a relation, at least one instance in the bag might express that relation. Many follow-up stud- ies adopt this assumption and choose a most credi- ble instance to represent the bag. (Lin et al., 2016; Ji et al., 2017) employs the attention mechanism to put different attention weight on each sentence in one bag and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 1: Rule Pattern Examples Figure 2: Overall Framework neural", "meta": {"paper_id": "1904.08051v1", "chunk_index": 1, "filename": "1904.08051v1.pdf", "chunk_length": 512}}
{"id": "1904.08051v1_chunk_2", "text": "and assume each sentence is related to the relation but have a different correlation. Another key issue for relation extraction is how to model the instance and extract features, (Zeng et al., 2014, 2015; Zhou et al., 2016) adopts deep Figure 1: Rule Pattern Examples Figure 2: Overall Framework neural network including CNN and RNN, these methods perform better than conventional feature- based methods. Reinforcement learning has been widely used in data selection and natural language process- ing. (Feng et al., 2018) adopts REINFORCE in instance selection for distant supervision which is the basis of our work. Posterior regularization (Ganchev, 2010) is a framework to handle the problem that a variety of tasks and domains require the creation of large problem-speciﬁc annotated data. This framework incorporates external problem-speciﬁc informa- tion and put a constraint on the posterior of the model. In this paper, we propose a rule-based RE- INFORCE based on this framework. 3 Methodology In this section, we focus on the model details. Be- sides the interacting process of the relation clas- siﬁer and the instance selector, we will introduce how to model the state, action, reward of the agent and how we add rules for the agent in training pro- cess. 3.1 basic relation classiﬁer We need a pretrained basic relation classiﬁer to de- ﬁne the reward and state. In this paper, we adopt the BGRU with attention bag level relation clas- siﬁer fb (Zhou et al., 2016). With o denoting the output of fb corresponding to the scores associated to each relation, the conditional probability can be written as follows: Pfb(r|Bk, θb) = exp(or) Pnr k=1 exp(ok) (1) where r is relation type, nr is the number of re- lation types, θb is the parameter vector of the basic relation classiﬁer fb and Bk denotes the input bag of the classiﬁer. In the basic classiﬁer, the sentence rep- resentation is calculated by the sentence encoder network BGRU, the BGRU takes the instance xk i as input and output the sen- tence representation BGRU(xk i ). And then the sentence level(ATT) attention will take {BGRU(xk 1), BGRU(xk 2), ..., BGRU(xk |Bk|)} as input and output o which is the ﬁnal output of fb corresponding to the scores associated to each relation. 3.2 Original REINFORCE Original REINFORCE agent training process is quite similar to (Feng et al., 2018). The instance selection process for one bag is completed in one trajectory. Agent π(a|s, θπ) is trained as an in- stance selector. The key of the model is how to represent the state in every step and the reward at the terminal of the trajectory. We use the pretrained fb to address this key problem. The reward deﬁned by the basic relation classiﬁer is as follows: R = log Pfb(rk|Bk, θb) (2) In which rk denotes the corresponding relation of Bk. The state s mainly contained three parts: the representation of the candidate instance, the rep- resentation of the relation and the representation of the instances that have been selected. The representation of the candidate instance are also deﬁned by the basic relation", "meta": {"paper_id": "1904.08051v1", "chunk_index": 2, "filename": "1904.08051v1.pdf", "chunk_length": 512}}
{"id": "1904.08051v1_chunk_3", "text": "which rk denotes the corresponding relation of Bk. The state s mainly contained three parts: the representation of the candidate instance, the rep- resentation of the relation and the representation of the instances that have been selected. The representation of the candidate instance are also deﬁned by the basic relation classiﬁer fb. At time step t, we use BGRU(xk t ) to represent the can- didate instance xk t and the same for the selected in- stances. As for the embedding of relation, we use the entity embedding method introduced in TransE model (Bordes et al., 2013) which is trained on the Freebase triples that have been mentioned in the training and testing dataset, and the relation em- bedding rek will be computed by the difference of the entity embedding element-wise. The policy π with parameter θπ = {W, b} is deﬁned as follows: Pπ(At|St, θπ) = softmax(WSt + b) (3) With the model above, the parameter vector can be updated according to REINFORCE algorithm (Sutton et al., 1998). 3.3 Posterior Regularized REINFORCE REINFORCE uses the complete return, which in- cludes all future rewards up until the end of the tra- jectory. In this sense, all updates are made after the trajectory is completed (Sutton et al., 1998). These stochastic properties could make the training slow. Fortunately, we have some domain-speciﬁc rules that could help to train the agent and adopt pos- terior regularization framework to integrate these rules. The goal of this framework is to restrict the posterior of π. It can guide the agent towards de- sired behavior instead of wasting too much time in meaninglessly exploring. Since we assume that the domain-speciﬁc rules have high credibility, we designed a rule-based policy agent πr to emphasize their inﬂuences on π. The posterior constrains for π is that the pol- icy posterior for xMI is expected to be 1 which indicates that agent should select the xMI. This expectation can be written as follows: EPπ[l(At = 1)] = 1 (4) where l here is the indicator function. In order to transfer the rules into a new policy πr, the KL di- vergence between the posterior of π and πr should be minimized, this can be formally deﬁned as minKL(Pπ(At|St, θπ)||Pπr(At|St, θπ)) (5) Optimizing the constrained convex problem de- ﬁned by Eq.(4) and Eq.(5), we get a new policy πr: Pπr(At|St, θπ) = Pπ(At|St, θπ)exp(l(At = 1) −1) Z (6) where Z is a normalization term. Z = 1 X At=0 Pπr(At|X, θπ)exp(l(At = 1) −1) Algorithm 1 formally deﬁne the overall frame- work of the rule-based data selection process. 4 Experiment Our experiment is designed to demonstrate that our proposed methodologies can train an instance selector more efﬁciently. Data: Original DS Dataset: B = {B1, B2, ..., BN}, Max Episode:M, Basic Relation Classiﬁer:fb, Step Size: α Result: An Instance Selector initialization policy weight θ′ π = θπ; initialization classiﬁer weight θ′ b = θb; for episode m=1 to M do for Bk in B do Bk = {xk 1, xk 2, ..., xk |Bk|}, Bk = {}; for step i in", "meta": {"paper_id": "1904.08051v1", "chunk_index": 3, "filename": "1904.08051v1.pdf", "chunk_length": 512}}
{"id": "1904.08051v1_chunk_4", "text": "Relation Classiﬁer:fb, Step Size: α Result: An Instance Selector initialization policy weight θ′ π = θπ; initialization classiﬁer weight θ′ b = θb; for episode m=1 to M do for Bk in B do Bk = {xk 1, xk 2, ..., xk |Bk|}, Bk = {}; for step i in |Bk| do construct si by Bk, xk i , rek; if xk i ∈xMI then construct πr; sample action Ai follow πr(a|si, θ′ π); else sample action Ai follow π(a|si, θ′ π); end if Ai=1 then Add xk i in Bk; end end Get terminal reward: R = log Pfb(rk|Bk, θ′ b); Get step delayed reward: Ri=R; Update agent: θπ ←θπ + α P|Bk| i=1 Ri∇θπ log π end θ′ π = τθπ + (1 −τ)θ′ π; Update the classiﬁer fb; end Algorithm 1: PR REINFORCE We tuned our model using three-fold cross val- idation on the training set. For the parameters of the instance selector, we set the dimension of en- tity embedding as 50, the learning rate as 0.01. The delay coefﬁcient τ is 0.005. For the parame- ters of the relation classiﬁer, we follow the settings that are described in (Zhou et al., 2016). The comparison is done in rule-based rein- forcement learning method, original reinforce- ment learning and method with no reinforce- ment learning which is the basic relation classiﬁer trained on original DS dataset. We use the last as the baseline. Figure 3: Precision/Recall Curves 4.1 Dataset A widely used DS dataset, which is developed by (Riedel et al., 2010), is used as the original dataset to be selected. The dataset is generated by aligning Freebase with New York Times corpus. 4.2 Metric and Performance Comparison We compare the data selection model performance by the ﬁnal performance of the basic model trained on newly constructed dataset selected by different models. We use the precision/recall curves as the main metric. Fig.3 presents this comparison. PR REINFORCE constructs cleaned DS dataset with less noisy data compared with the original RE- INFORCE so that the BGRU+2ATT classiﬁer can reach better performance. 5 Conclusions In this paper, we develop a posterior regular- ized REINFORCE methodology to alleviate the wrong label problem in distant supervision. Our model makes full use of the hand-crafted domain- speciﬁc rules in the trial and error search dur- ing the training process of REINFORCE method for DS dataset selection. The experiment re- sults show that PR REINFORCE outperforms the original REINFORCE. Moreover, PR REIN- FORCE greatly improves the efﬁciency of the RE- INFORCE training. Acknowledgments This work has been supported in part by NSFC (No.61751209, U1611461), 973 program (No. 2015CB352302), Hikvision-Zhejiang University Joint Research Center, Chinese Knowledge Cen- ter of Engineering Science and Technology (CK- CEST), Engineering Research Center of Digital Library, Ministry of Education. Xiang Ren’s re- search has been supported in part by National Sci- ence Foundation SMA 18-29268. References Bach et al. 2007. A review of relation extraction. Lit- erature review for Language and Statistics II, 2. Bordes et al. 2013. Translating embeddings for mod- eling multi-relational data. In Advances in neural information processing", "meta": {"paper_id": "1904.08051v1", "chunk_index": 4, "filename": "1904.08051v1.pdf", "chunk_length": 512}}
{"id": "1904.08051v1_chunk_5", "text": "re- search has been supported in part by National Sci- ence Foundation SMA 18-29268. References Bach et al. 2007. A review of relation extraction. Lit- erature review for Language and Statistics II, 2. Bordes et al. 2013. Translating embeddings for mod- eling multi-relational data. In Advances in neural information processing systems, pages 2787–2795. Feng et al. 2018. Reinforcement learning for relation classiﬁcation from noisy data. Kuzman Ganchev. 2010. Posterior regularization for learning with side information and weak supervi- sion. Ph.D. thesis, University of Pennsylvania. Ji et al. 2017. Distant supervision for relation extrac- tion with sentence-level attention and entity descrip- tions. In AAAI, pages 3060–3066. Lin et al. 2016. Neural relation extraction with selec- tive attention over instances. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), vol- ume 1, pages 2124–2133. Liu et al. 2017. Heterogeneous supervision for relation extraction: A representation learning approach. Mintz et al. 2009. Distant supervision for relation ex- traction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Vol- ume 2-Volume 2, pages 1003–1011. Association for Computational Linguistics. Riedel et al. 2010. Modeling relations and their men- tions without labeled text. In Joint European Con- ference on Machine Learning and Knowledge Dis- covery in Databases, pages 148–163. Springer. Sutton et al. 1998. Reinforcement learning: An intro- duction, volume 1. MIT press Cambridge. Zeng et al. 2014. Relation classiﬁcation via convolu- tional deep neural network. In Proceedings of COL- ING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 2335–2344. Zeng et al. 2015. Distant supervision for relation extraction via piecewise convolutional neural net- works. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Process- ing, pages 1753–1762. Zhou et al. 2016. Attention-based bidirectional long short-term memory networks for relation classiﬁca- tion. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 2: Short Papers), volume 2, pages 207–212.", "meta": {"paper_id": "1904.08051v1", "chunk_index": 5, "filename": "1904.08051v1.pdf", "chunk_length": 344}}
{"id": "1904.08075v1_chunk_0", "text": "End-to-End Speech Translation with Knowledge Distillation Yuchen Liu1,2, Hao Xiong4, Zhongjun He4, Jiajun Zhang1,2, Hua Wu4, Haifeng Wang4 and Chengqing Zong1,2,3 1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China 2University of Chinese Academy of Sciences, China 3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China 4Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China {yuchen.liu, jjzhang, cqzong}@nlpr.ia.ac.cn, {xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com Abstract End-to-end speech translation (ST), which directly translates from source language speech into target language text, has at- tracted intensive attentions in recent years. Compared to con- ventional pipepine systems, end-to-end ST models have ad- vantages of lower latency, smaller model size and less error propagation. However, the combination of speech recognition and text translation in one model is more difﬁcult than each of these two tasks. In this paper, we propose a knowledge distilla- tion approach to improve ST model by transferring the knowl- edge from text translation model. Speciﬁcally, we ﬁrst train a text translation model, regarded as a teacher model, and then ST model is trained to learn output probabilities from teacher model through knowledge distillation. Experiments on English- French Augmented LibriSpeech and English-Chinese TED cor- pus show that end-to-end ST is possible to implement on both similar and dissimilar language pairs. In addition, with the in- struction of teacher model, end-to-end ST model can gain sig- niﬁcant improvements by over 3.5 BLEU points. Index Terms: Speech recognition, Speech translation, Knowl- edge distillation, Transformer 1. Introduction Conventional speech translation system is a pipeline of two main components: an automatic speech recognition (ASR) model which provides transcripts of source language utterances, and a text machine translation (MT) model which translates the transcripts to target language [1, 2, 3, 4, 5]. This pipeline system usually suffers from time delay, parameter redundancy and error accumulation. In contrast, end-to-end ST, based on an encoder- decoder architecture with attention mechanism, is more com- pact and efﬁcient. It can directly generate translations from raw audio and jointly optimize parameters on the ﬁnal goal. There- fore, this model has become a new trend in speech translation research studies [6, 7, 8, 9, 10, 11]. However, despite appealing advantages of end-to-end ST model, its performance is generally inferior. One of the im- portant reasons is due to extremely scarce data which includes speech in source language paired with text in target language. Previous studies resort pretraining or multi-task learning ap- proaches to improve the translation quality. They either pretrain ASR task on high-resource data [11], or use multi-task learn- ing to train ST model with ASR or MT model simultaneously [9, 10]. Nevertheless, they only gain limited improvements and do not take full advantage of text data. We notice that the per- formance between end-to-end ST and MT model exists a huge gap, thus how to utilize MT model to help instruct end-to-end ST model is of great signiﬁcance. It is a challenge to train an end-to-end ST model directly from speech signal without text guidance while achieving com- parable performance as text translation model. Given that text translation", "meta": {"paper_id": "1904.08075v1", "chunk_index": 0, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_1", "text": "exists a huge gap, thus how to utilize MT model to help instruct end-to-end ST model is of great signiﬁcance. It is a challenge to train an end-to-end ST model directly from speech signal without text guidance while achieving com- parable performance as text translation model. Given that text translation models are superior to ST model, we consider ST model can be improved by leveraging knowledge distillation. In knowledge distillation, there is usually a big teacher model with a small student model. It has been shown that the output probabilities of teacher model are smooth, which are easier for student model to learn from than ground-truth text [12]. Thus, a student model can be taught by imitating the behaviour of teacher model, such as output probabilities [12, 13], hidden rep- resentation [14, 15], or generated sequence [16], and alleviate the performance gap between itself and the teacher model. In this paper, we present a method based on knowledge dis- tillation for end-to-end ST model to learn knowledge from text translation model. We ﬁrst train a text translation model on par- allel text data (regarded as teacher) and then an end-to-end ST model (regarded as student) is trained by learning from ground- truth translations and the outputs of teacher model simultane- ously. Experiments conducted on 100h English-French Aug- mented LibriSpeech corpus and 542h English-Chinese TED corpus show that it is possible to train a compact end-to-end speech translation model on both similar and dissimilar lan- guage pairs. With the instruction of teacher model, end-to-end ST model can gain signiﬁcant improvements, approaching to the traditional pipeline system. 2. Related Work End-to-end model has already become a dominant paradigm in machine translation task, which adopts an encoder-decoder ar- chitecture and generates target words from left to right at each step [1, 3, 5]. This model has also achieved promising results in ASR ﬁelds [2, 4, 17]. Recent works purpose a further attempt to combine these two tasks together by building an end-to-end speech-to-text translation without the use of source language translation during learning or decoding. Anastasopoulos et al. [6] use k-means clustering to cluster repeated audio patterns and automatically align spoken words with their translations. Duong et al. [7] focus on the alignment between speech and translated phrase but not to directly pre- dict the ﬁnal translations. B´erard et al. [8] give the ﬁrst proof of the potential for end-to-end speech-to-text translation with- out using source language. They further conduct experimetns on a larger English-to-French dataset and pre-train encoder and arXiv:1904.08075v1 [cs.CL] 17 Apr 2019 decoder which improves performance [10]. Weiss et al. [9] also use multi-task learning and show that end-to-end model can outperform a cascade of independently trained pipeline system on Fisher Callhome Spanish-English speech translation task. Bansal et al. [11] ﬁnd pretraining encoder on higher-resource language ASR training data can achieve gains in low-resource speech translation system. However, these work mainly focus on pretraining acoustic encoder and do not take full advantage of text data. Knowledge distillation is ﬁrst adopted to apply for model compression, the main idea of which is to", "meta": {"paper_id": "1904.08075v1", "chunk_index": 1, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_2", "text": "encoder on higher-resource language ASR training data can achieve gains in low-resource speech translation system. However, these work mainly focus on pretraining acoustic encoder and do not take full advantage of text data. Knowledge distillation is ﬁrst adopted to apply for model compression, the main idea of which is to train a smaller stu- dent model to mimic a larger teacher model, by minimizing the loss between the teacher and student predictions. It has soon been applied to a variety of tasks, like image classiﬁcation [12, 18, 19, 20], speech recognition [12] and natural language processing [13, 16, 21]. The teacher and student model in con- ventional knowledge distillation usually handle the same task, while in our method the teacher model and student model have different input modalities where teacher uses text as input and student uses speech. 3. Models In this paper, we apply end-to-end models with the same archi- tecture for all three tasks (ASR, ST and MT). The model archi- tecture is similar with Transformer [5], which is the state-of-art model in MT task. Recently, this model also begins to be used in ASR task, showing a decent performance [22, 23]. In this section, we ﬁrst describe the core architecture of Transformer and then show how this model is applied to ASR/ST and MT task. 3.1. Core Module of Transformer Transformer is an encoder-decoder architecture which entirely relies on self-attention mechanism including scaled dot-product attention and multi-head attention. It consists of N stacked en- coder and decoder layers. Each encoder layer has two blocks, which is a self-attention block followed by a feed-forward block. Decoder layer has the same architecture with encoder layer except an extra encoder-decoder attention block to per- form attention over the output of the top encoder layer. Resid- ual connection and layer normalization are employed around each block. In addition, the self-attention block in the decoder is modiﬁed with mask to prevent present positions attending to future positions during training. To be detailed, multi-head attention technique is applied in self-attention and encoder-decoder attention blocks to obtain in- formation from different representation subspaces at different positions. Each head is corresponding to a scaled dot-product attention, which operates on query Q, key K and value V: Attention(Q, K, V) = softmax(QKT √dk )V (1) where dk is the dimension of the key. Then the output values are concatenated, MultiHead(Q, K, V) = Concat(head1, · · · , headh)WO where headi = Attention(QWQ i , KWK i , VWV i ) (2) where the WQ i ∈Rdmodel×dq, WK i ∈Rdmodel×dk, WV i ∈ Rdmodel×dv and WO ∈Rdv×dmodel are projection matrices Multi-Head Attention Linear Positional Encoding Multi-Head Attention Feed Forward Networks Feed Forward Networks Multi-Head Attention softmax N N ST encoder ST decoder Multi-Head Attention Word Embedding Positional Encoding Multi-Head Attention Feed Forward Networks Feed Forward Networks Multi-Head Attention softmax N MT encoder MT decoder N Text Distillation Loss Figure 1: Model architecture of our method. The left part is ST model, regarded as a student model, whose input is speech sig- nal. The right part is MT model,", "meta": {"paper_id": "1904.08075v1", "chunk_index": 2, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_3", "text": "Multi-Head Attention Feed Forward Networks Feed Forward Networks Multi-Head Attention softmax N MT encoder MT decoder N Text Distillation Loss Figure 1: Model architecture of our method. The left part is ST model, regarded as a student model, whose input is speech sig- nal. The right part is MT model, regarded as a teacher model, whose input is the source sentence corresponding to the input of student model. The output of both student model and teacher model is target sentence. The top part is distillation loss, where the student model not only matches the ground-truth, but also the output probabilities of the teacher model. that are learned. dq = dk = dv = dmodel/h, h is the number of heads. Position-wise feed-forward block is composed of two linear transformations with a ReLU activation in between. FFN(x) = max(0, xW1 + b1)W2 + b2 (3) where the weights W1 ∈Rdmodel×dff , W2 ∈Rdff ×dmodel and the biases b1 ∈Rdff , b2 ∈Rdmodel. 3.2. ASR/ST Model The ASR/ST model is shown in the left part of Figure 1, whose input is a series of discrete-time speech signal. We ﬁrst use log-Mel ﬁlterbank to convert raw speech signal into a sequence of acoustic features and then apply mean and variance normal- ization. To prevent the GPU memory overﬂow and produce ap- proximate hidden representation length against target length, we apply frame stack and downsample similar to [24, 25]. The ﬁnal acoustic feature sequence is S = (s1, s2, · · · , sn) with dimen- sion of dfilterbank × numstack. Then the feature sequence is fed into a linear transformation with a normalization layer to map with model dimension dmodel. In addition, positional en- codings are added to the feature sequence in order to enable the model to attend by relative positions. This sequence is ﬁnally treated as the input into Transformer model. Other parts are the same with Transformer model. For ASR the input to decoder is source language text, while the input to decoder in ST is target language text. 3.3. MT Model We also use Transformer to train a baseline MT model, as shown in the right part of Figure 1. The difference between MT model and ASR/ST model is the input to the encoder. In MT model, X = (x1, x2, · · · , xn) is a sequence of tokens, rep- resenting source sentence. We embed the words in sequence X into a real continuous space with the dimension of Rd model, which can be fed into a neural network. 3.4. Knowledge Distillation Training an end-to-end ST model is considerably difﬁcult than MT model. The accuracy of the later model is usually much higher than the former. Therefore, we present MT model as a teacher to teach ST model. Here we give a description of the idea of knowledge distillation. Denote D = (s, x, y) as the corpus of triple data corre- sponding to speech signal, transcription in source language and its translation. The log-likelihood loss of ST model can be for- mulated as follows: LST(D; θ) = −", "meta": {"paper_id": "1904.08075v1", "chunk_index": 3, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_4", "text": "we give a description of the idea of knowledge distillation. Denote D = (s, x, y) as the corpus of triple data corre- sponding to speech signal, transcription in source language and its translation. The log-likelihood loss of ST model can be for- mulated as follows: LST(D; θ) = − X (s,y)∈D logP(y|s; θ) (4) logP(y|s; θ) = N X t=1 |V | X k=1 1(yt = k)logP(yt = k|y<t, s; θ) (5) where s is the acoustic feature sequence of source speech signal, y is the target translated sentence, N is the length of the output sequence, |V | is the vocabulary size of the output language, yt is the t-th output token, 1(yt = k) is an indicator function which indicates whether the output token is equal to the ground- truth. We denote the output distribution of teacher model for to- ken yt as Q(yt|y<t, x; θT ), and x is the source transcribed sen- tence which corresponds to speech signal s. Then the cross entropy between the distributions of teacher and student is: LKD(D; θ, θT ) = − X (x,y)∈D N X t=1 |V | X k=1 Q(yt = k|y<t, x; θT ) logP(yt = k|y<t, x; θ) (6) In distillation loss, the student not only matches the out- put of ground-truth, but also the output probabilities of teacher model, which is more smooth and yields smaller variance in gradients [12]. Then the total loss function is, LALL(D; θ; θT ) = (1−λ)LST(D; θ)+λLKD(D; θ, θT ) (7) where λ is a hyper-parameter to trade off these two loss terms. 4. Experiments 4.1. Datasets We conduct experiments on Augmented LibriSpeech which is collected by [26] and available for free. This corpus is built by automatically aligning e-books in French with English ut- terances of LibriSpeech, which contains 236 hours of speech in total. They provide quadruplet: English speech signal, En- glish transcription, French text translations from alignment of e-books and Google Translate references. Following [10], We only use the 100 hours clean train set for training, with 2 hours development set and 4 hours test set, which corresponds to 47,271, 1071 and 2048 utterances respectively. To be consistent with their settings, we also double the training size by concate- nating the aligned references with the Google Translate refer- ences. To verify whether the end-to-end speech translation model can handle on dissimilar language pairs, we build a corpus in English-Chinese direction. The raw data (including video, sub- titles and timestamps) are crawled from TED website1. For each talk, we build a wav audio ﬁle extracted from video by ffmpeg2. We also collect its corresponding transcript and save in txt for- mat. We divide each audio ﬁle into small segments based on timestamps instead of voice activity detection (VAD), because it eliminates the inﬂuence of improper fragments and guaran- tees each utterance containing complete semantic information, which is important for translation. In the end, we totally get 317,088 utterances (∼542 hours). Development and test sets are split according to the partition in IWSLT. We use dev2010 as development set and", "meta": {"paper_id": "1904.08075v1", "chunk_index": 4, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_5", "text": "it eliminates the inﬂuence of improper fragments and guaran- tees each utterance containing complete semantic information, which is important for translation. In the end, we totally get 317,088 utterances (∼542 hours). Development and test sets are split according to the partition in IWSLT. We use dev2010 as development set and tst2015 as test set, which has 835 ut- terances (∼1.48 hours) and 1,223 utterances (∼2.37 hours) re- spectively. The remaining data are put into training set. We will release this dataset to public as a benchmark soon. 4.2. Experimental Setup Our acoustic features are 80-dimensional log-Mel ﬁlterbanks extracted with a step size of 10ms and window size of 25ms and extended with mean subtraction and variance normaliza- tion. The features are stacked with 3 frames to the left and downsample to a 30ms frame rate. For text data, we lowercase all the texts, tokenize and apply normalize punctuations with the Moses scripts3. For Augmented LibriSpeech corpus, we ap- ply BPE [27] on the combination of English and French text to obtain subword units. The number of merge operations in BPE is set to 8K, resulting in a shared vocabulary with 8,159 sub- words. For TED English-Chinese, the merge number is 30K, and vocabulary size are 28,912 and 30,000, respectively. We re- port case-insensitive BLEU scores [28] by multi-bleu.pl script for the evaluation of ST and MT tasks and use word error rates (WER) to evaluate ASR task. Because the size of Augmented LibriSpeech is relatively small, we set the hidden size dmodel = 256, the ﬁlter size in feed-forward layer dff = 1024, the head number h = 8, the residual dropout and attention dropout are 0.1. For TED English-Chinese, we set the hidden size dmodel = 512 with the ﬁlter size dff = 2048. MT model, as a teacher model, can use bigger parameters. We use 512 hidden sizes, 2048 ﬁlter sizes with 8 heads.The number of encoder layers and decoder layers in above models are all set to 6. We train our models with Adam optimizer [29] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9 on 2 NVIDIA V100 GPUs. 4.3. Results Table 1 shows the results for the ASR and MT tasks on Aug- mented LibriSpeech. It can be seen that Transformer model signiﬁcantly outperforms in both ASR and MT tasks, with 0.92 WER reduction and 4.1 BLEU scores improvement in beam search compared to [10]. We contribute it to the superior per- formance of Transformer model which is good at modeling long distance in sequence-to-sequence tasks, especially for MT 1https://www.ted.com 2http://ffmpeg.org 3https://www.statmt.org/moses/ Table 1: ASR and MT results on test set of Augmented Lib- riSpeech. LibriSpeech Method WER(↓) BLEU(↑) B´erard [10] greedy 19.9 19.2 beam search 17.9 18.8 Ours greedy 21.46 21.35 beam search 16.98 22.91 Table 2: ST results on Augmented LibriSpeech test. KD denotes knowledge distillation. LibriSpeech Method greedy beam ensemble B´erard [10] Pipeline 14.6 14.6 15.8 End-to-end 12.3 12.9 15.5 Pre-trained 12.6 13.3 Ours Pipeline 15.75 17.85 18.4 End-to-end 10.19 13.15 17.8 Pre-trained 13.89 14.30 KD 14.96 17.02 tasks. Contrary to [10]", "meta": {"paper_id": "1904.08075v1", "chunk_index": 5, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_6", "text": "22.91 Table 2: ST results on Augmented LibriSpeech test. KD denotes knowledge distillation. LibriSpeech Method greedy beam ensemble B´erard [10] Pipeline 14.6 14.6 15.8 End-to-end 12.3 12.9 15.5 Pre-trained 12.6 13.3 Ours Pipeline 15.75 17.85 18.4 End-to-end 10.19 13.15 17.8 Pre-trained 13.89 14.30 KD 14.96 17.02 tasks. Contrary to [10] which uses characters as output units, we consider subword units can also obtain improvements. For ST task, we have four settings. The pipeline model uses ASR outputs as MT inputs, where ASR model and MT model are described above. The end-to-end model is directly trained on source speech signal paired with target text transla- tions. The pre-trained model is identical to end-to-end model, but it is initialized with ASR and MT models. Knowledge dis- tillation (KD) is our method which uses MT model as teacher model to instruct end-to-end ST model. As shown in Table 2, all four settings surpass the results in [10]. Noticing that there exists a huge gap between the perfor- mance of the end-to-end ST model and MT model, even if the end-to-end ST model is pretrained, thus we conduct knowledge distillation to instruct ST model with MT model. The result shows that this method can bring signiﬁcant improvement on the BLEU score which increases from 14.30 to 17.02. With the instruction of MT model, the performance gap is alleviated, approaching to the pipeline system. It demonstrates the effec- tiveness of our method. We also conduct experiments on English-Chinese to verify our methods. Table 3 presents the results of MT and ST mod- els. Pipeline model combines both the ASR (WER is 18.2%) and MT models. It is difﬁcult to train end-to-end ST model from random initialization parameters, for the reordering be- tween dissimilar language pairs is difﬁcult to align with frame based speech representations. The end-to-end ST model here is pretrained with ASR. With knowledge distillation, it can ob- tain signiﬁcant simprovements, proving the generality of our method. Although end-to-end ST does not outperform pipeline system, it shows the potential to implement a compact end-to- end model even on dissimilar language pairs. 4.4. Analysis To evaluate the effect of teacher model, we explore different hyper-parameters λ of the distillation loss on Augmented Lib- riSpeech. With λ increasing, ST will pay more attention to the teacher model. When λ equals 0, it is the pre-trained end-to- end model; when λ is 1, it will ignore ground-truth and only learn from the teacher. As Table 4 shows, the performance be- comes better with the increasing of λ. End-to-end ST obtains Table 3: MT and ST results on TED English-Chinese test. TED MT Pipeline End-to-end KD BLEU 27.08 22.28 16.80 19.55 Table 4: The effect of teacher model weight on ST results. λ 0.0 0.2 0.4 0.6 0.8 1.0 BLEU 14.30 15.68 16.73 16.62 16.93 17.02 the best performance when it only learns the output distributions of teacher model. We further analyze how knowledge from MT model helps ST through visualizations of the encoder-decoder attention. Figure 2 shows an example. The attentions of ASR (a) and MT (c) models have", "meta": {"paper_id": "1904.08075v1", "chunk_index": 6, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_7", "text": "14.30 15.68 16.73 16.62 16.93 17.02 the best performance when it only learns the output distributions of teacher model. We further analyze how knowledge from MT model helps ST through visualizations of the encoder-decoder attention. Figure 2 shows an example. The attentions of ASR (a) and MT (c) models have more conﬁdent than ST model. Each output token in the former two model concentrates on speciﬁc frames or tokens, especially for MT model, while the attention in ST (b) model tends to be smoothed out across many input frames. However, with the help of MT model, the attention of ST model with KD (d) becomes more concentrated. For example, the speech frames l = 45 ∼55 are corresponding to “was talking” in ASR (a), which can be translated to “se parlait” in French (c). The attention in ST model with KD has more weights on frames l = 45 ∼55 than that in original ST model. 0 20 40 60 80 i think she was talking to herself <eos> (a) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 20 40 60 80 je crois qu' elle se parlait à elle-même . <eos> (b) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 i think she wastalking to herself <eos> je crois qu' elle se parlait à elle-même . <eos> (c) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0 20 40 60 80 je crois qu' elle se parlait à elle-même . <eos> (d) 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Figure 2: The visualizations of attention in different models. (a), (b), (c), (d) are the encoder-decoder attention of ASR, end- to-end ST, MT and end-to-end ST with KD, respectively. 5. Conclusions In this work, we present knowledge distillation method to im- prove the end-to-end ST model by transferring the knowledge from MT model. Experiments on two language pairs demon- strate that with the instruction of MT model, end-to-end ST model can gain signiﬁcant improvements. Although the end- to-end ST does not outperform pipeline system, it shows the potential to come close in performance. In the future we will utilize other knowledges like the outputs from ASR model to further improve the performance of ST model. 6. References [1] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015. [2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, 2016. [3] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, and e. a. Moham- mad Norouzi, Googles neural machine translation system: bridg- ing the gap between human and machine translation. arXiv preprint arXix:1609.08144, 2016. [4] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Gonina et al., “State- of-the-art speech recognition with sequence-to-sequence models,” in Proc. ICASSP, 2017. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention", "meta": {"paper_id": "1904.08075v1", "chunk_index": 7, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_8", "text": "Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Gonina et al., “State- of-the-art speech recognition with sequence-to-sequence models,” in Proc. ICASSP, 2017. [5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in Neural Information Processing Systems, pp. 5998– 6008, 2017. [6] A. Anastasopoulos, D. Chiang, and L. Duong, “An unsuper- vised probability model for speech-to-translation alignment of low-resource languages,” in Proc. EMNLP, 2016. [7] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn, “An attentional model for speech translation without transcrip- tion,” in Proc. NAACL, 2016. [8] A. B´erard, O. Pietquin, C. Servan, and L. Besacier, “Listen and translate: A proof of concept for end-to-end speech-to-text trans- lation,” in NeurIPS Workshop on End-to-end Learning for Speech and Audio Processing, 2016. [9] R. J. Weiss, J. Chorowski, N. Jaitly, Y. Wu, and Z. Chen, “Sequence-to-sequence models can directly translate foreign speech,” in Proc. Interspeech, 2017. [10] A. B´erard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin, “End-to-end automatic speech translation of audiobooks,” in Proc. ICASSP, 2018. [11] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Gold- water, “Pre-training on high-resource speech recognition im- proves low-resource speech-to-text translation,” arXiv preprint arXiv:1809.01431, 2018. [12] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural network,” arXiv preprint arXiv:1503.02531, 2015. [13] M. Freitag, Y. Al-Onaizan, and B. Sankaran, “Ensemble distillation for neural machine translation,” arXiv preprint arXiv:1702.01802, 2017. [14] J. Yim, D. Joo, J. Bae, and J. Kim, “A gift from knowledge distilla- tion: Fast optimization, network minimization and transfer learn- ing,” in Proc. CVPR, pp. 4133–4141, 2017. [15] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio, “Fitnets: Hints for thin deep nets,” in Proc. ICLR, 2015. [16] Y. Kim and A. M. Rush, “Sequence-level knowledge distillation,” in Proc. EMNLP, 2016. [17] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Ben- gio, “End-to-end attention-based large vocabulary speech recog- nition,” in Proc. ICASSP, 2016. [18] Y. Li, J. Yang, Y. Song, L. Cao, J. Luo, and L.-J. Li, “Learn- ing from noisy labels with distillation,” in Proc. ICCV, pp. 1910– 1918, 2017. [19] C. Yang, L. Xie, S. Qiao, and A. Yuille, “Knowledge distillation in generations: More tolerant teachers educate better students,” in Proc. CVPR, 2018. [20] R. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E. Dahl, and G. E. Hinton, “Large scale distributed neural network training through online distillation,” arXiv preprint arXiv:1804.03235, 2018. [21] X. Tan, Y. Ren, D. He, T. Qin, Z. Zhao, and T.-Y. Liu, “Multilin- gual neural machine translation with knowledge distillation,” in Proc. ICLR, 2019. [22] L. Dong, S. Xu, and B. Xu, “Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP, pp. 5884–5888, 2018. [23] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence- to-sequence speech recognition with the transformer in mandarin chinese,” in Proc. Interspeech, 2018. [24] H. Sak, A. Senior, K. Rao, and F. Beaufays,", "meta": {"paper_id": "1904.08075v1", "chunk_index": 8, "filename": "1904.08075v1.pdf", "chunk_length": 512}}
{"id": "1904.08075v1_chunk_9", "text": "Xu, “Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP, pp. 5884–5888, 2018. [23] S. Zhou, L. Dong, S. Xu, and B. Xu, “Syllable-based sequence- to-sequence speech recognition with the transformer in mandarin chinese,” in Proc. Interspeech, 2018. [24] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” in Proc. ICASSP, 2015. [25] A. Kannan, Y. Wu, P. Nguyen, T. N. Sainath, Z. Chen, and R. Prabhavalkar, “An analysis of incorporating an external lan- guage model into a sequence-to-sequence model,” in Proc. ICASSP, pp. 1–5828, 2018. [26] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, “Augmenting lib- rispeech with french translations: A multimodal corpus for direct speech translation evaluation,” Language Resources and Evalua- tion, 2018. [27] R. Sennrich, B. Haddow, and A. Birch, “Neural machine transla- tion of rare words with subword units,” in Proc. ACL, 2016. [28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in Proc. ACL, pp. 311–318, 2002. [29] D. P. Kingma and J. Ba, “Adam: A method for stochastic opti- mization,” in Proc. ICLR, 2015.", "meta": {"paper_id": "1904.08075v1", "chunk_index": 9, "filename": "1904.08075v1.pdf", "chunk_length": 193}}
{"id": "1904.08109v1_chunk_0", "text": "Contextual Aware Joint Probability Model Towards Question Answering System Liu Yang Department of Computer Science Stanford University liuyeung@stanford.edu Lijing Song Department of Computer Science Stanford University lisasong@stanford.edu Abstract In this paper, we address the question answering challenge with the SQuAD 2.0 dataset. We design a model architecture which leverages BERT’s [2] capability of context-aware word embeddings and BiDAF’s [8] context interactive exploration mechanism. By integrating these two state-of-the-art architectures, our system tries to extract the contextual word representation at word and character levels, for better comprehension of both question and context and their correlations. We also propose our original joint posterior probability predictor module and its associated loss functions. Our best model so far obtains F1 score of 75.842% and EM score of 72.24% on the test PCE leaderboad. 1 Introduction With increasing popularity of intelligent mobile devices like smartphones, Google Assistant, and Alexa, machine question answering is one of the most popular ﬁeld of deep learning research. SQuAD 2.0 [6] challenges a question answering system in two respects: ﬁrst, to predict whether a question is answerable based on a given context; second, to ﬁnd a span of answer in the context if the question is predicted as answerable. It augments the 100,000 questions in SQuAD 1.1 [7] with 50,000 more unanswerable questions written in purpose to look like answerable ones. Because of the existence of those unanswerable questions which account for 1/3 of the train set and 1/2 of the dev and test sets, we design an elaborate joint probability predictor layer on top of our BERT-BiDAF architecture to solve the answerable classiﬁcation and span prediction problem in a probability inference way. There are many efforts tackling the question answer systems and some work already achieve human level performance. However, common practice of these works actually base on several weird assumptions which we argue that will introduce tough constraints to the model and limit the model’s representation capability. In this paper, we explicitly point out these weird assumptions and give a thorough discussion about their weakness. We propose our solution to model the question answer problem in a brand new joint probability way. Given a speciﬁc (question, context) pair, we try to make the model to learn the joint posterior distribution of the binary answerable or not variable and the start/end span variables. We propose a BERT-BiDAF [2][8] hybrid model to capture the question aware context information at both character and word levels with the expectation to gather strong signals to help our joint probability model make decisions. 2 Related Work On SQuAD leaderboards, all the top works apply BERT word embedding layer. As a pre-trained bidirectional transformer language model released in late 2018, BERT [2] is highly respectable to produce context-aware word representation. Adding n-gram masking and synthetic self-training arXiv:1904.08109v1 [cs.CL] 17 Apr 2019 techniques onto the BERT framework, the current state-of-art model has achieved near human F1 and EM accuracy with ensembling. On top of BERT emdedding layer, we apply bidirectional attention mechanism, because it co-attends to both paragraphs and questions simultaneously and is used by all the top-ranking", "meta": {"paper_id": "1904.08109v1", "chunk_index": 0, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_1", "text": "[cs.CL] 17 Apr 2019 techniques onto the BERT framework, the current state-of-art model has achieved near human F1 and EM accuracy with ensembling. On top of BERT emdedding layer, we apply bidirectional attention mechanism, because it co-attends to both paragraphs and questions simultaneously and is used by all the top-ranking models for SQuAD. Among them, BiDAF [8] is one of the ﬁrst and most important models. The central idea behind BiDAF and its variants [5] is the Attention Flow layer, that generates a question-aware context representation by incorporating both context-to-question and question-to-context attentions. Other competitive approaches to question answering include QANet [9] and Attention-over-Attention (AoA) [1]. QANet speeds up training by excluding RNNs and using only convolution and self- attention, where convolution models local interactions and self-attention models global interactions. AoA models the importance distribution of primary attentions by stacking additional attentions on top of the primary attentions. 3 Approach In this section, we formally deﬁne the terminology \"context\" as the paragraph in which we want to ﬁnd the answer while \"question\" as it literally is. We propose the Joint BERT-BiDAF Question Answering neural model, Fig.1 illustrates its overall architecture. We keep the \"Attention Flow layer\" of BiDAF [8] to produce the question aware context representation at character level. As BERT [2] can directly provide question aware context at word level, we simply combine the two features together and feed them into BiDAF’s \"Modeling layer\" [8] with the expectation to obtain mutually enhanced context representation (mutual awareness of character/word features, mutual positional interactions, etc.). Intuitively, the word piece strategy of BERT [2] doesn’t split the normal words, thus including character embedding can be a good supplement. In Fig.1, the top layer is our proposed joint probability prediction structure. Unlike the common practice of inserting a \"no-answer\" token (e.g baseline and Bert squad) and making the start/end position of the no-answer case converge to that special token, we try to model the joint distribution of the binary variable A (has/no answer), the N (context length) value variables X1 (start) and X2 (end) in a more natural way. We shall give a thorough discussion about this structure in section.3.1.4. The purpose of introducing the new variable A is to make the model align to the causality that, given a speciﬁc question and context, the existence of a valid <start, end> span is depend on the answerable property other than the other way around. It is a big change which result in that we can’t unify the no-answer loss under the general position entropy loss [2][8], we must design new loss functions which align to our joint probability structure. Loss function is essential since it exactly determine every step of the learning update. It is quite trick but interesting to try to ﬁgure out a loss function which value indeed reﬂect the model’s performance while consistent with the model’s design philosophy. We shall discuss our loss function exploration in details in section.3.2. What should be emphasized is that we don’t want to simply reproduce BERT’s success in question answer problem on SQuAD 2.0 [2]. The", "meta": {"paper_id": "1904.08109v1", "chunk_index": 1, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_2", "text": "loss function which value indeed reﬂect the model’s performance while consistent with the model’s design philosophy. We shall discuss our loss function exploration in details in section.3.2. What should be emphasized is that we don’t want to simply reproduce BERT’s success in question answer problem on SQuAD 2.0 [2]. The main difference between our solution and BERT’s QA solution 1 is BERT use representations of both question and context to vote for the span prediction while we only use that of the context. What we really want to do is to verify our original ideas, to test how well BERT can let the context \"aware\" the existence of question and to see whether the two embeddings at different granularity can achieve mutual enhancement. This section is scheduled in the following way: in section3.1, we discuss about our original model architecture in general and our effort to implement this architecture. We go through the essential layers and their back-end techniques but leave the BiDAF[8] details for reference reading. Especially in section3.1.4, we shall focus on our original joint probability predictor and how we tackle the answerable problem and the span prediction problem. In section3.2, we shall show several our original loss functions and their considerations. In section3.3, we introduce the baseline we use and in section3.4, we brieﬂy summarize the workload of our project. Please keep in mind that we deﬁne our terminology incrementally, once a symbol has been deﬁned, it will be used throughout the rest of paper. 1https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_ squad.py 2 3.1 Model Architecture As illustrated in Fig. 1, we keep the main framework of BiDAF [8] but replace its GloVe [4] word embedding with BERT contextual word embedding [2] and stack joint probability predictor on top of it. There are 3 hyper-parameters that determine the computational complexity of our system and we formally deﬁne them to be dlstm, dchar_emb and dbert. dlstm is a uniform hidden dimension for all LSTM components [3]. dchar_emb and dbert are the dimension of the character word embedding (output of character level CNN [10]) and the hidden dimension of the BERTBASE[2], respectively. 3.1.1 Embedding Layer Our embedding layer is built based on BERT base uncased tokenizer. (i) Word Level Embedding: More precisely, this is token level embedding. Yet BERT doesn’t split common words thus it is still can be treated as word representation. (ii) Character Level Embedding: BERT split special words such as name entity into pieces started with \"##\", e.g \"Giselle\" to [’gi’, ’##selle’]. However, each piece is still a character string from which a dchar_emb dimensional embedding can be generated with character CNN [10]. Note that \"##\" in word pieces is ignored. 3.1.2 Joint Contextual Representation Layer In this layer, we denote the lengths of question and context as Lq and Lc respectively, the character level representations of question and context are Rcq ∈RLq×dchar_emb and Rcc ∈RLc×dchar_emb respectively. We apply the \"Attention Flow Layer\" of BiDAF[8] to obtain TChar ∈R8dchar_emb×Lc as context representation conditioned on question at character level. We simply keep the context part from the ﬁnal encoder layer of BERTBASE to obtain", "meta": {"paper_id": "1904.08109v1", "chunk_index": 2, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_3", "text": "character level representations of question and context are Rcq ∈RLq×dchar_emb and Rcc ∈RLc×dchar_emb respectively. We apply the \"Attention Flow Layer\" of BiDAF[8] to obtain TChar ∈R8dchar_emb×Lc as context representation conditioned on question at character level. We simply keep the context part from the ﬁnal encoder layer of BERTBASE to obtain TW ord ∈ Rdbert×Lc as context representation conditioned on question at word level. We concatenate the two representations together to produce the joint contextual representation Gctx = [TW ord, TChar] ∈ R(8dchar_emb+dbert)×Lc. Figure 1: The Joint BERT-BiDAF Question Answering Model 3 3.1.3 Modeling Layer We keep the \"BiDAF Modeling Layer\"[8] for two reasons. (i). Although BERT word TW ord can be treat it as positional aware representation but the character feature TChar can not. We expect TW ord propagates the positional information to TChar in the mixture process in this layer. (ii). We want the ﬁne granularity information captured at character level propagates to TW ord. Actually we expect the two level representations aware the mutual existence and learn how to cooperate to produce the augmented feature. The output of the Modeling Layer is F ∈R(2dlstm+8dchar_emb+dbert)×Lc 3.1.4 Joint Probability Predictor Layer There are 3 components in our joint probability predictor layer which are responsible to compute the P(A), P(X1|A) (start position) and P(X2|A, X1) (end position) respectively. We propose this structure based on the observation that common practices highly rely on the special \"sentry\" token and actually make a solid assumption that P(X1 = 0, X1 = 0|A = 0). We argue that assumption like this introduces a tough constraint and the effort the model spends on push the (X1, x2) to (0, 0) will deﬁnitely impact the model’s internal mechanism to generate X1, X2 condition on A = 1. Our method gives up the special \"sentry\" token, we choose the natural domain of X1, X2 to be {0, 1, · · · , Lc −1}2, and relax the P(X1 = 0, X2 = 0|A = 0) constraint to P(X1 = i, X2 = j|A = 0), ∀i > j, that is, the predicted start position should be larger than the predicted end position condition on no-answer. The fact P(X1 = i, X2 = j|A = 1), ∀i ≤j becomes a natural extension of our concept. Thus we claim that our method does not introduce any inherent contradiction and any bias assumption. The connection of the 3 predictors are consistent with the chain rule, the P(X1|A) predictor relies on the P(A) predictor, while the P(X2|A, X1) predictor relies on the other two both. Thus the structure is expected to simulate the joint probability P(A, X1, X2) = P(A)P(X1|A)P(X2|A, X1). All the 3 predictors take the output F ∈R(2dlstm+8dchar_emb+dbert)×Lc of the \"Modeling Layer\" as input. For simplicity, we deﬁne f = 2dlstm + 8dchar_emb + dbert. I. P(A) Predictor In order to compute the binary classiﬁcation logits, we use a self attention mechanism by setting a learn-able parameter wA ∈Rf, the attention vector attA ∈RLc is computed by Eq.[1] attA[i] = exp(wT AF[:, i]) PLc−1 k=0 exp(wT AF[:, k]) , i ∈{0, 1,", "meta": {"paper_id": "1904.08109v1", "chunk_index": 3, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_4", "text": "8dchar_emb + dbert. I. P(A) Predictor In order to compute the binary classiﬁcation logits, we use a self attention mechanism by setting a learn-able parameter wA ∈Rf, the attention vector attA ∈RLc is computed by Eq.[1] attA[i] = exp(wT AF[:, i]) PLc−1 k=0 exp(wT AF[:, k]) , i ∈{0, 1, · · · , Lc −1]} (1) We compute the context summary vector by SA = FattA ∈Rf. Then we use a learn-able matrix Wlogit ∈R2×f to compute the classiﬁcation logits lA = WlogitsSA ∈R2, thus we obtain P(A) as Eq.[2] P(A = 0) = exp(lA[0]) exp(lA[0]) + exp(lA[1]), P(A = 1) = exp(lA[1]) exp(lA[0]) + exp(lA[1]) (2) In order to propagate the decision of P(A) predictor to P(X1|A) predictor, we use a learn-able matrix W A prop ∈R2dlstm×f to generate a tuple (hA, cA) = W A propSA to initialize the LSTM layer of the P(X1|A) predictor. II. P(X1|A) Predictor By the bidirectional LSTM layer in P(X1|A) predictor, we obtain M1 = LSTM(F, (hA, cA)) ∈ R2dlstm×Lc. We use a learn-able vector w1 ∈R2dlstm to compute the distribution of X1 in Eq.[3] P(X1 = i|A) = exp(wT 1 M1[:, i]) PLc−1 k=0 exp(wT 1 M1[:, k]) , i ∈{0, 1, · · · , Lc −1}) (3) 4 We use the P(X1|A) ∈ RLc itself as attention to generate the context summary S1 = MP(X1|A) ∈R2dlstm. We concatenate the two context summaries obtained so far as Sjoint = [SA, S1] and propagate the decisions of predictors P(A) and P(X1|A) to pre- dictor P(X2|A, X1) by initialize its bidirectional LSTM by (h1, c1) = W 1 propSjoint. Here W 1 prop ∈R2dlstm×(f+2dlstm) is another learn-able matrix parameter in this predictor. III. P(X2|A, X1) Predictor By the bidirectional LSTM layer in P(X2|A, X1) predictor, we obtain M2 = LSTM(F, (h1, c1)) ∈ R2dlstm×Lc. We use a learn-able vector w2 ∈R2dlstm to compute the distribution of X2 in Eq.[4] P(X2 = i|A, X1) = exp(wT 2 M2[:, i]) PLc−1 k=0 exp(wT 2 M2[:, k]) , i ∈{0, 1, · · · , Lc −1}) (4) IV. How to Predict For a speciﬁc instance D[m], from Eq. 2, Eq. 3 and Eq. 4 we have the joint posterior probability as Eq. 5: P(A = a, X1 = i, X2 = j|D[m]) = P(A = a|D[m])P(X1 = i|A = a, D[m])P(X2 = j|A = a, X1 = i, D[m]) a ∈{0, 1}, (i, j) ∈{0, 1, · · · Lc −1}2 (5) We ﬁnd the maximal negative likelihood p0 = maxi>j{P(A = 0, X1 = i, X2 = j)|D} and the maximal positive likelihood p1 = maxi≤j{P(A = 1, X1 = i, X2 = j|D)} . If p1 > p0 then we predict the instance D[m] has answer, otherwise, D[m] has no answer. If D predicted as has answer, then (start, end) = argmaxi≤j{P(A = 1, X1 = i, X2 = j|D[m])}. 3.2 Loss Function Assume for a speciﬁc instance D[m], the ground truth is (A, X1, X2) = (a, i, j). We propose our ﬁrst loss function of our joint probability predictor in Eq. 6.", "meta": {"paper_id": "1904.08109v1", "chunk_index": 4, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_5", "text": "as has answer, then (start, end) = argmaxi≤j{P(A = 1, X1 = i, X2 = j|D[m])}. 3.2 Loss Function Assume for a speciﬁc instance D[m], the ground truth is (A, X1, X2) = (a, i, j). We propose our ﬁrst loss function of our joint probability predictor in Eq. 6. Intuitively, in addition to the normal binary cross entropy, when an instance D[m] has no answer, we want the probability concentrates on the maximum likelihood estimation (MLE); when an instance has answer, we just punish the position loss as common practice. loss1(D[m]) = −log(P(A = a|D[m])) + 1(a = 0){−log(maxi>j{P(A = 0, X1 = i, X2 = j|D[m])})} + 1(a = 1){−log(P(X1 = i|A = 1, D[m]) −log(X2 = j|A = 1, X1 = i, D[m])} (6) For our second loss function, we introduce the distribution U(i, j) as Eq. 7. U(i, j) is a partial uniform distribution whose probability mass concentrates in the conﬁdence area of P(A = 0|D[m]). U(i, j) = ( 2 (Lc−1)Lc , i > j 0, i ≤j (7) When an instance has no answer, we want the model produces a distribution P(X1, X2|D[m]) close to U. With the Kullback-Leibler divergence, we give our loss2 as Eq. 8: loss2(D[m]) = −log(P(A = a|D[m])) + 1(a = 0){KL(U(X1, X2)||P(X1, X2|D[m]))} + 1(a = 1){−log(P(X1 = i|A = 1, D[m]) −log(X2 = j|A = 1, X1 = i, D[m])} (8) 3.3 Baseline Our baseline is a modiﬁed BiDAF [8] model with word-level embedding provided by cs224n teaching staff 2. With the default hyper-parameters, e.g learning rate, decay weight etc. after 30 epochs of training we get the performance metrics of baseline listed in Table 1. 2Chapter 4 in DFP Handout and repository cloned from https://github.com/chrischute/squad.git 5 3.4 Workload In our project, we install pre-trained BERT from huggingface3. When reusing the training framework of the baseline, however, we totally change its tokenization logic in order to integrate BERT. We write a lot of such foundation code to make the two different system consistent. We write all other code to implement our proposed BERT-BiDAF hybrid architecture, our joint probability predictor and loss functions. For ablation purpose, we also implement another answer/no-answer classiﬁer as a candidate for comparison. 4 Experiments 4.1 Dataset The dataset we use is Stanford Question Answering Dataset 2.0 (SQuAD 2.0) [6], a reading compre- hension dataset boosted with additional 50, 000 questions which can not be answered based on the given context. The publicly available data are split into 129, 941, 6, 078, and 5, 915 examples acting as the train/dev/test datasets respectively. Performance is evaluated by two metrics: F1 and EM score. F1 is the harmonic average of precision and recall. EM (Exact Match) measures whether the system output matches the ground truth answer exactly, and is stricter than F1. AvNA (Answer vs. No Answer) is an auxiliary metric that measures how well a model works on labeling a question as has-answer or no-answer. 4.2 Experimental Details In our experiment, we use BERTBASE [2] which conﬁguration (hidden_size=768, intermediate_- size=3072, layers=12). For the 3 model complexity identiﬁers we", "meta": {"paper_id": "1904.08109v1", "chunk_index": 5, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_6", "text": "stricter than F1. AvNA (Answer vs. No Answer) is an auxiliary metric that measures how well a model works on labeling a question as has-answer or no-answer. 4.2 Experimental Details In our experiment, we use BERTBASE [2] which conﬁguration (hidden_size=768, intermediate_- size=3072, layers=12). For the 3 model complexity identiﬁers we deﬁned in section3.1, we have dbert = 768. In order to determine dchar_emb and dlstm and the learning rate, we do grid search in the range lr ∈{ke−5|k ∈{2, 5, 6, 7, 10}}, dchar_dim ∈{16, 32, 64, 80}, dlstm ∈{32, 64, 128}. We have 60 setting here and for each setting we run 1 epoch on a Nvidia Titan V GPU and sort the settings by their F1 scores. It takes us about 4 days to get a sorting list and we then ﬁne tune the model with 3 epochs by trying the high scored settings in order. Finally, we obtain our best model with lr = 5e−5, dchar_emb = 16, dlstm = 64. Fig.2 depicts the performance of our model on dev dataset along with the training process going on [tensorboard visual output in ’Relative’ mode]. The blue curves represent the baseline while the yellow ones represent our best model. For all the 3 metrics AvNA, EM and F1, our model beats the baseline by a signiﬁcant margin. For the rightmost ’NNL’ subplot, we can identify that the baseline tends to overﬁt the dataset in early stage of training, however, for our best model, the signals of overﬁtting appear much latter. Figure 2: Model Performance on dev-set During Training We also conduct two experiments for ablation. In the ﬁrst one, we set dchar_emb = 2, that actually makes the effect of character level embedding negligible. Then the experiment shows both F1 and EM scores punished by 2+ decrements. In the second one, because BERT[2] claims that its hidden state of the ﬁnal layer corresponding to ’[CLS]’ token can be used as the aggregated sequence representation for classiﬁcation task, we simply use it for binary answerable classiﬁcation. In this case, experiment 3https://github.com/huggingface/pytorch-pretrained-BERT 6 shows AvNA decreases by 1.5 while EM and F1 both drop around 2, which indicates the advance of our output layer. 4.3 Results We submitted our best model so far to the test PCE leaderboard and obtained the results F1: 75.84, EM: 72.24. However, in this section, in order to compare with other local implementation, we report the performance data on dev set in Table 1. We list the performance data of the baseline, our best model, and the two ablation experiments. The performance differences have been described in the last section. Although the overall performance differences are consistent to our arguments that character level embedding can help and that the joint probability predictor has decent performance, we believe the performance of our joint probability model can be improved by overcoming 2 difﬁculties. First, the overﬁtting problem. We note that the overﬁtting problem is signiﬁcant in our system in the last epoch. Our best guess towards this problem is the unbalance parameter updating. As BERT uses a strategy which", "meta": {"paper_id": "1904.08109v1", "chunk_index": 6, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_7", "text": "the performance of our joint probability model can be improved by overcoming 2 difﬁculties. First, the overﬁtting problem. We note that the overﬁtting problem is signiﬁcant in our system in the last epoch. Our best guess towards this problem is the unbalance parameter updating. As BERT uses a strategy which pushes learning rate to 0 when approaching to the end of training. Thus the gradient may vanish for BERT module but our upper stacked layers still keep learning, because they are closer to the gradient source (loss function). This unbalance learning makes the BERT module and the upper layer out of sync and the discrepancy within our system become bigger and bigger. Second, due to the limitation of the hardware, we can’t run the size of batch as large as Google. Thus our parameter updating suffers from the variance problem which makes our system unstable. Table 1: Model Performance on Dev Set Metrics Model F1 EM AvNA Baseline 59.94 56.55 66.90 Our Best Model 75.48 71.96 79.68 Without CharEmb 73.25 69.87 78.01 [CLS] AvNA Classiﬁer 73.49 69.76 78.23 Human [6] 89.45 86.83 - 5 Qualitative Analysis of Model Errors 5.1 Error Type 1: Wrong Inference Wrong inference happens when the question and context pair is interpreted incorrectly by our model, and in some cases attention mechanism focuses on the wrong part of the context. This could happen because of underﬁtting in embedding or bidirectional attention, or when the sentence that includes the answer span is interpreted wrongly due to syntactic complication. I. False positive example of wrong inference When reading the long list of Harvard’s alumni, our system, same with original BERT, gets wrong on many of questions similar to the following ones. When the question asks for \"actor starred in The Men in Black\" (or \"directer of Noah in 2014\"), the system answers \"Tatyana Ali\" (or \"Darren Aronofsky\") simply because s/he is an \"actor\" (or \"ﬁlm director\") according to the context, although \"The Men in Black\" or \"Noah\" is not even mentioned in the paragraph. Because of lack of well-trained embeddings of entities like ﬁlm titles, the attention ﬂow layer only focuses on key words such as \"actor\" and \"director\", and generates false positive results distracted by disturbance terms like \"The Men in Black\" or \"Noah\". Question: What actor starred in the Movie Saving Private Ryan? What actor starred in The Men in Black? (two questions) Context: ... comedian, television show host and writer Conan O’Brien; actors Tatyana Ali, Nestor Carbonell, Matt Damon, ... (1. Harvard University in Appx. A) Ground Truth: <No Answer>; <No Answer> (respectively) Prediction: Tatyana Ali; Tatyana Ali (respectively) 7 II. False negative example of wrong inference In the context, there is no appearance of the key word \"status\", but the question is looking for such an answer. Our model fails to draw the correlation between the phrase and the word \"status\". This implies insufﬁciency either in the word embedding of \"status\" or in the bidirectional attention. Question: What status has the Brotherhood obtained in the Islamic world? Context: Despite periodic repression, the Brotherhood has become one", "meta": {"paper_id": "1904.08109v1", "chunk_index": 7, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_8", "text": "answer. Our model fails to draw the correlation between the phrase and the word \"status\". This implies insufﬁciency either in the word embedding of \"status\" or in the bidirectional attention. Question: What status has the Brotherhood obtained in the Islamic world? Context: Despite periodic repression, the Brotherhood has become one of the most inﬂuential movements in the Islamic world ... (2. Islamism in Appx. A) Ground Truth: one of the most inﬂuential movements Prediction: N/A 5.2 Error Type 2: Imprecise Answer Boundaries The model points to the right context part but results in imprecise answer span. More sophisticated techniques to predict start/end positions will help. I. Example of imprecise answer boundaries The prediction gives an unnecessary word \"that\". Question: What was Shrewsbury’s conclusion? Context: ... Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations... (3. Black Death in Appx. A) Ground Truth: contemporary accounts were exaggerations Prediction: that contemporary accounts were exaggerations 5.3 Error Type 3: Confusing Context Meanwhile, our system also makes mistakes that are prone to human readers. Sometimes we don’t even agree with the ground truth answers. This error happens because of ambiguous context and tricky questions. I. Example of confusing context with imprecise answer boundaries We and our model don’t agree with the 3 ground truth answers and suspect it makes a coordination attachment error. The context is confusing with four and’s in a single sentence. Question: What gained ground when Arab nationalism suffered? Context: ... Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground. (4. Islamism in Appx. A) Ground Truth: anti-democratic Islamist movements, anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb, anti-democratic Islamist movements Prediction: different democratic and anti-democratic Islamist movements II. Example of confusing context without any error Here is another example in which we don’t agree with the ground truth answer, although our prediction is exactly the same as the ground truth. According to the context (and China’s history), Kusala is Tugh Temür’s brother, and was thought to be killed by El Temür; Tugh Temür stayed alive after his brother’s death. Based on this comprehension, the right answer should be \"No Answer\" because Tugh Temür was never killed in the paragraph. Question: Who was thought to have killed Tugh Temur? Context: ... Tugh Temür abdicated in favour of his brother Kusala, who was backed by Chagatai Khan Eljigidey, and announced Khanbaliq’s intent to welcome him. However, Kusala suddenly died only four days after a banquet with Tugh Temür. He was supposedly killed with poison by El Temür, and Tugh Temür then remounted the throne. ... (5. Yuan Dynasty in Appx. A) Ground Truth: El Temür (should be \"No Answer\") Prediction: El Temür 8 6 Conclusion In this paper, we propose our BERT-BiDAF hybrid architecture, thoroughly explain the intuition, and show the theoretical derivation of our joint probability predictor and its loss functions. Furthermore, we demonstrate that our ideas can", "meta": {"paper_id": "1904.08109v1", "chunk_index": 8, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_9", "text": "Appx. A) Ground Truth: El Temür (should be \"No Answer\") Prediction: El Temür 8 6 Conclusion In this paper, we propose our BERT-BiDAF hybrid architecture, thoroughly explain the intuition, and show the theoretical derivation of our joint probability predictor and its loss functions. Furthermore, we demonstrate that our ideas can obtain decent performance on a difﬁcult and highly reputable problem (SQuAD 2.0) and verify our arguments by ablation experiments. We learn much about model design and how to conduct a effective learning. We identify that the overﬁtting and high variance problems are the two main difﬁculties which prohibit us form further progress. Our next step is to dig deeply into the difﬁculties we are facing and may take question representation into consideration. Acknowledgments Thanks to all the CS224 teaching staff for guidance and Microsoft Azure for their great generosity. References [1] Yiming Cui et al. “Attention-over-attention neural networks for reading comprehension”. In: arXiv preprint arXiv:1607.04423 (2016). [2] Jacob Devlin et al. “Bert: Pre-training of deep bidirectional transformers for language under- standing”. In: arXiv preprint arXiv:1810.04805 (2018). [3] Sepp Hochreiter and Jürgen Schmidhuber. “LSTM can solve hard long time lag problems”. In: Advances in neural information processing systems. 1997, pp. 473–479. [4] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. “GloVe: Global Vectors for Word Representation”. In: Empirical Methods in Natural Language Processing (EMNLP). 2014, pp. 1532–1543. URL: http://www.aclweb.org/anthology/D14-1162. [5] Matthew E Peters et al. “Deep contextualized word representations”. In: arXiv preprint arXiv:1802.05365 (2018). [6] Pranav Rajpurkar, Robin Jia, and Percy Liang. “Know What You Don’t Know: Unanswerable Questions for SQuAD”. In: arXiv preprint arXiv:1806.03822 (2018). [7] Pranav Rajpurkar et al. “Squad: 100,000+ questions for machine comprehension of text”. In: arXiv preprint arXiv:1606.05250 (2016). [8] Minjoon Seo et al. “Bidirectional attention ﬂow for machine comprehension”. In: arXiv preprint arXiv:1611.01603 (2016). [9] Adams Wei Yu et al. “Qanet: Combining local convolution with global self-attention for reading comprehension”. In: arXiv preprint arXiv:1804.09541 (2018). [10] Xiang Zhang, Junbo Zhao, and Yann LeCun. “Character-level convolutional networks for text classiﬁcation”. In: (2015), pp. 649–657. A Appendix: Full Paragraphs in Analysis 1. Harvard University: Other: Civil rights leader W. E. B. Du Bois; philosopher Henry David Thoreau; authors Ralph Waldo Emerson and William S. Burroughs; educators Werner Baer, Harlan Hanson; poets Wallace Stevens, T. S. Eliot and E. E. Cummings; conductor Leonard Bernstein; cellist Yo Yo Ma; pianist and composer Charlie Albright; composer John Alden Carpenter; comedian, television show host and writer Conan O’Brien; actors Tatyana Ali, Nestor Carbonell, Matt Damon, Fred Gwynne, Hill Harper, Rashida Jones, Tommy Lee Jones, Ashley Judd, Jack Lemmon, Natalie Portman, Mira Sorvino, Elisabeth Shue, and Scottie Thompson; ﬁlm directors Darren Aronofsky, Terrence Malick, Mira Nair, and Whit Stillman; architect Philip Johnson; musicians Rivers Cuomo, Tom Morello, and Gram Parsons; musician, producer and composer Ryan Leslie; serial killer Ted Kaczynski; programmer and activist Richard Stallman; NFL quarterback Ryan Fitzpatrick; NFL center Matt Birk; NBA player Jeremy Lin; US Ski Team skier Ryan Max Riley; physician Sachin H. Jain; physicist J. Robert Oppenheimer; computer pioneer and inventor An Wang; Tibetologist George de Roerich; and Marshall", "meta": {"paper_id": "1904.08109v1", "chunk_index": 9, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_10", "text": "Ryan Leslie; serial killer Ted Kaczynski; programmer and activist Richard Stallman; NFL quarterback Ryan Fitzpatrick; NFL center Matt Birk; NBA player Jeremy Lin; US Ski Team skier Ryan Max Riley; physician Sachin H. Jain; physicist J. Robert Oppenheimer; computer pioneer and inventor An Wang; Tibetologist George de Roerich; and Marshall Admiral Isoroku Yamamoto. 2. Islamism: Despite periodic repression, the Brotherhood has become one of the most inﬂuential movements in the Islamic world, particularly in the Arab world. For many years 9 it was described as \"semi-legal\" and was the only opposition group in Egypt able to ﬁeld candidates during elections. In the Egyptian parliamentary election, 2011–2012, the political parties identiﬁed as \"Islamist\" (the Brotherhood’s Freedom and Justice Party, SalaﬁAl-Nour Party and liberal Islamist Al-Wasat Party) won 75% of the total seats. Mohamed Morsi, an Islamist democrat of Muslim Brotherhood, was the ﬁrst democratically elected president of Egypt. He was deposed during the 2013 Egyptian coup d’état. 3. Black Death: The plague theory was ﬁrst signiﬁcantly challenged by the work of British bacteriologist J. F. D. Shrewsbury in 1970, who noted that the reported rates of mortality in rural areas during the 14th-century pandemic were inconsistent with the modern bubonic plague, leading him to conclude that contemporary accounts were exaggerations. In 1984 zoologist Graham Twigg produced the ﬁrst major work to challenge the bubonic plague theory directly, and his doubts about the identity of the Black Death have been taken up by a number of authors, including Samuel K. Cohn, Jr. (2002), David Herlihy (1997), and Susan Scott and Christopher Duncan (2001). 4. Islamism: The quick and decisive defeat of the Arab troops during the Six-Day War by Israeli troops constituted a pivotal event in the Arab Muslim world. The defeat along with economic stagnation in the defeated countries, was blamed on the secular Arab nationalism of the ruling regimes. A steep and steady decline in the popularity and credibility of secular, socialist and nationalist politics ensued. Ba’athism, Arab socialism, and Arab nationalism suffered, and different democratic and anti-democratic Islamist movements inspired by Maududi and Sayyid Qutb gained ground. 5. Yuan Dynasty: When Yesün Temür died in Shangdu in 1328, Tugh Temür was recalled to Khanbaliq by the Qipchaq commander El Temür. He was installed as the emperor (Emperor Wenzong) in Khanbaliq, while Yesün Temür’s son Ragibagh succeeded to the throne in Shangdu with the support of Yesün Temür’s favorite retainer Dawlat Shah. Gaining support from princes and ofﬁcers in Northern China and some other parts of the dynasty, Khanbaliq- based Tugh Temür eventually won the civil war against Ragibagh known as the War of the Two Capitals. Afterwards, Tugh Temür abdicated in favour of his brother Kusala, who was backed by Chagatai Khan Eljigidey, and announced Khanbaliq’s intent to welcome him. However, Kusala suddenly died only four days after a banquet with Tugh Temür. He was supposedly killed with poison by El Temür, and Tugh Temür then remounted the throne. Tugh Temür also managed to send delegates to the western Mongol khanates such as Golden Horde and Ilkhanate to be accepted as", "meta": {"paper_id": "1904.08109v1", "chunk_index": 10, "filename": "1904.08109v1.pdf", "chunk_length": 512}}
{"id": "1904.08109v1_chunk_11", "text": "Kusala suddenly died only four days after a banquet with Tugh Temür. He was supposedly killed with poison by El Temür, and Tugh Temür then remounted the throne. Tugh Temür also managed to send delegates to the western Mongol khanates such as Golden Horde and Ilkhanate to be accepted as the suzerain of Mongol world. However, he was mainly a puppet of the powerful ofﬁcial El Temür during his latter three-year reign. El Temür purged pro-Kusala ofﬁcials and brought power to warlords, whose despotic rule clearly marked the decline of the dynasty. 10", "meta": {"paper_id": "1904.08109v1", "chunk_index": 11, "filename": "1904.08109v1.pdf", "chunk_length": 93}}
{"id": "1904.12213v1_chunk_0", "text": "arXiv:1904.12213v1 [cs.CL] 27 Apr 2019 Towards Recognizing Phrase Translation Processes: Experiments on English-French Yuming Zhai⋆, Pooyan Safari⋆, Gabriel Illouz, Alexandre Allauzen, and Anne Vilnat LIMSI-CNRS, Univ. Paris-Sud, Univ. Paris-Saclay, France {firstname.lastname}@limsi.fr Abstract. When translating phrases (words or group of words), human transla- tors, consciously or not, resort to different translation processes apart from the literal translation, such as Idiom Equivalence, Generalization, Particularization, Semantic Modulation, etc. Translators and linguists (such as Vinay and Darbel- net, Newmark, etc.) have proposed several typologies to characterize the different translation processes. However, to the best of our knowledge, there has not been effort to automatically classify these ﬁne-grained translation processes. Recently, an English-French parallel corpus of TED Talks has been manually annotated with translation process categories, along with established annotation guidelines. Based on these annotated examples, we propose an automatic classiﬁcation of translation processes at subsentential level. Experimental results show that we can distinguish non-literal translation from literal translation with an accuracy of 87.09%, and 55.20% for classifying among ﬁve non-literal translation processes. This work demonstrates that it is possible to automatically classify translation processes. Even with a small amount of annotated examples, our experiments show the directions that we can follow in future work. One of our long term objectives is leveraging this automatic classiﬁcation to better control paraphrase extraction from bilingual parallel corpora. Keywords: Translation processes · Non-literal translation · Automatic classiﬁ- cation 1 Introduction Since 1958, translators and linguists have published work on translation processes [35,23,8,22]. They distinguish literal translations from other translation processes at subsentential level. Consider these two human non-literal translation examples: the ﬁrst translation preserves exactly the meaning, where the ﬁxed expression à la hauteur de ‘to the height of’ has a ﬁgurative sense which means capable of solving; while the second one is more complicated, there exists a textual inference between the source text and the translation. (1.EN) a solution that’s big enough to solve our problems (1.FR) une solution à la hauteur de nos problèmes (2.EN) and that scar has stayed with him for his entire life (2.FR) et que, toute sa vie, il a souffert de ce traumatisme (‘he has suffered from this traumatism’) Non-literal translations can bring difﬁculties for automatic word alignment [11,10], or cause meaning changes in certain cases. However, to the best of our knowledge, there ⋆Both authors contributed equally to this article. has not been effort to automatically classify these ﬁne-grained translation processes to beneﬁt downstream natural language processing tasks. For example, Machine Trans- lation (MT) techniques have been leveraged for paraphrase extraction from bilingual parallel corpora [1,20]. The assumption is that two monolingual segments are potential paraphrases if they share common translations in another language. Currently the largest paraphrase resource, PPDB (ParaPhrase DataBase) [12], has been built following this method. Nonetheless, Pavlick et al. [26] revealed that there exist other relations (i.e. Entailment (in two directions), Exclusion, Other related and Independent)1 than strict equivalence (paraphrase) in PPDB. Non-literal pivot translations inside the parallel cor- pora could break the strict equivalence between the candidate paraphrases extracted, whereas they have not received enough attention during", "meta": {"paper_id": "1904.12213v1", "chunk_index": 0, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_1", "text": "al. [26] revealed that there exist other relations (i.e. Entailment (in two directions), Exclusion, Other related and Independent)1 than strict equivalence (paraphrase) in PPDB. Non-literal pivot translations inside the parallel cor- pora could break the strict equivalence between the candidate paraphrases extracted, whereas they have not received enough attention during this corpora exploitation. From a linguistic point of view, apart from the word-for-word literal translation, different versions of human translations reﬂect the richness of human language expres- sions, where various translation processes could be employed. Furthermore, because of the existing differences between languages and cultures, non-literal translation pro- cesses are sometimes inevitable to produce correct and natural translations. The ﬁne- grained phrase-level translation processes could help foreign language learners to better compare the language being learned with another language already mastered. Based on the theories developed in translation studies and through manually anno- tating and analyzing an English-French parallel corpus, Zhai et al. [37] have proposed a typology of translation processes adapted to their corpus. In this work our main contri- bution is proposing an automatic classiﬁcation of translation processes at subsentential level, based on these annotated examples. From the aspect of granularity and our goal of better controlling paraphrasing process or helping foreign language learners, it is different from the task of ﬁltering semantically divergent parallel sentence pairs to im- prove the performance of MT systems [6,36,29]. Experimental results show that we can distinguish non-literal translation processes from literal translation with an accuracy of 87.09%, and 55.20% for classifying among non-literal multi-classes. In the present paper, after reviewing related work, we describe the manual annota- tion and the data set. Exploited features and different neural network architectures will be presented, followed by experimental results and error analysis. Finally we conclude and present the perspectives of this work. 2 Related Work Translators and linguists have proposed several typologies to characterize different translation processes. Vinay and Darbelnet [35] identiﬁed direct and oblique transla- tion processes, the latter being employed when a literal translation is unacceptable, or when structural or conceptual asymmetries arising between the source language and the target language are non-negligible. Following studies include, among others, the work of Newmark [23,24], Chuquet and Paillard [8]. More recently, Molina and Hurtado Al- bir [22] proposed their own categorization based on studying the translation of cultural elements in the novel A Hundred Years of Solitude from Spanish to Arabic. 1 Exclusion: X is the contrary of Y; X is mutually exclusive with Y. Other related: X is related in some other way to Y. (e.g. country / patriotic). Independent: X is not related to Y. 2 / 12 Non-literal translations or cross-language divergences have been studied to improve MT related techniques. In order to enable more accurate word-level alignment, Dorr et al. [11] proposed to transform English sentence structure to more closely resemble an- other language. A translation literalness measure was proposed to select appropriate sentences or phrases for automatically constructing MT knowledge [15]. Using a hi- erarchically aligned parallel treebank, Deng and Xue [10] semi-automatically identify, categorize and quantify seven types of translation", "meta": {"paper_id": "1904.12213v1", "chunk_index": 1, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_2", "text": "to transform English sentence structure to more closely resemble an- other language. A translation literalness measure was proposed to select appropriate sentences or phrases for automatically constructing MT knowledge [15]. Using a hi- erarchically aligned parallel treebank, Deng and Xue [10] semi-automatically identify, categorize and quantify seven types of translation divergences between Chinese and En- glish.2 Based on the syntactic and semantic similarity between bilingual sentences, Carl and Schaeffer [5] developed a metric of translation literality. We have drawn inspiration from these preceding work for our feature engineering. Recently, different models have been proposed to automatically detect translation divergence in parallel corpora, with the goal of automatically ﬁltering out divergent sentence pairs to improve MT systems’ performance. An SVM-based cross-lingual di- vergence detector was introduced [6], using word alignments and sentence length fea- tures. Their following work [36] proposed a Deep Neural Network-based approach. This system could be trained for any parallel corpus without any manual annotation. They conﬁrmed that these divergences are a source of performance degradation in neu- ral machine translation. Pham et al. [29] built cross-lingual sentence embeddings ac- cording to the word similarity with a neural architecture in an unsupervised way. They measure the semantic equivalence of a sentence pair to decide whether to ﬁlter it out. Another task studying human translations concerns automatic post-editing [7]. The aim is evaluating systems for automatically correcting translation errors of an unknown “black box” MT engine, by learning from human revisions of translations produced by the same engine. Evaluation metrics include TER [31], BLEU [25] and manual evalua- tion. The task that we propose here is different from these attempts, which either ﬁlter semantically divergent sentence pairs to improve the performance of MT systems; or automatically correct machine translation errors to improve the translation quality. Our task of classifying translation processes (in two classes or in multi-classes) at subsen- tential level is a stand-alone task. One of our long term objectives is leveraging this automatic classiﬁcation to better control phrase-level paraphrase extraction from bilin- gual parallel corpora. 3 Manual Annotation and Data Description In order to model translation choices made by human translators at subsentential level, Zhai et al. [37] have annotated a trilingual parallel (English-French, English-Chinese) corpus of TED Talks3 with translation processes. The corpus is composed of transcrip- tions and human translations of oral presentations. The inter-annotator agreement (Co- hen’s Kappa) [9] for annotating the English-French and English-Chinese control corpus is 0.67 and 0.61, both around the substantial agreement threshold. This indicates that the task of manual annotation is already complicated. Readers can ﬁnd more details of corpus construction in the article [37]. 2 Lexical encoding; difference in transitivity; absence of language-speciﬁc function words; dif- ference in phrase types; difference in word order; dropped elements; structural paraphrases. 3 https://www.ted.com/ 3 / 12 The automatic classiﬁcation is conducted on the English-French pair in this work. We present in the table 1 a brief deﬁnition, a typical example and the number of in- stances for each category to be automatically classiﬁed.4 We combine Transposition and Mod+Trans in a category Contain_Transposition, where", "meta": {"paper_id": "1904.12213v1", "chunk_index": 2, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_3", "text": "3 / 12 The automatic classiﬁcation is conducted on the English-French pair in this work. We present in the table 1 a brief deﬁnition, a typical example and the number of in- stances for each category to be automatically classiﬁed.4 We combine Transposition and Mod+Trans in a category Contain_Transposition, where Modulation is considered as a neutral part. We will work on the classiﬁcation of the pair English-Chinese once the annotation phase is ﬁnished. In this work, we conduct experiments in a simpliﬁed scenario, where we already know the boundaries of bilingual pairs, and we only predict the translation process. For example, given the pair deceptive →une illusion in a pair of bilingual sentences, the goal is to predict its label Contain_Transposition. Table 1: Deﬁnition, typical example and number of instances for each translation process to be automatically classiﬁed. The instances were manually annotated in an English-French parallel corpus of TED Talks. We combine Transposition and Mod+Trans in a category Contain_Transposition for the automatic classiﬁcation. Translation Process Deﬁnition and typical example Literal Word-for-word translation, also concerns lexical units in multiword form. (3771) certain kinds of →certains types de Equivalence Non-literal translation of proverbs or ﬁxed expressions; a word-for-word translation makes sense but the translator expresses differently, without changing the meaning and the grammatical classes. (289) back then →à l’époque (‘at that time’) Generalization Several source words or expressions could be translated into a more general target word or expression, the translator uses the latter to translate. (86) as we sit here in ... →alors que nous sommes à ... (‘as we are at ...’) Particularization The source word or expression could be translated into several target words or expressions with a more speciﬁc meaning, and the translator chooses one of them according to the context. (215) the idea I want to put out is ... →l’idée que je veux diffuser c’est ... (‘the idea I want to spread is ...’) Modulation Metonymical and grammatical modulation [8]; change the point of view; the meaning could be changed. (195) that scar has stayed with him →il a souffert de ce traumatisme (‘he has suffered from this traumatism’) Transposition Change grammatical classes without changing the meaning. (289) unless something changes →à moins qu’un changement ait lieu (‘unless a change occurs’) Mod+Trans Combine the transformations of Modulation and of Transposition, which could make the alignment difﬁcult. (53) this is a completely unsustainable pattern →il est absolument impossible de continuer sur cette tendance (‘it is completely impossible to continue on this trend’) 4 Note that there are other detailed annotation rules in the annotation guidelines. 4 / 12 4 Automatic Classiﬁcation We have tried two approaches for the automatic classiﬁcation. Since the size of the cross validation data set is quite small, we ﬁrst compare different statistical machine learning techniques with feature engineering. We also build different neural network architectures which we explain below. 4.1 Feature Engineering with Statistical Machine Learning Techniques We describe below the features exploited in this work. The tag sets of English and French for part-of-speech (PoS) tagging, constituency parsing and dependency parsing have", "meta": {"paper_id": "1904.12213v1", "chunk_index": 3, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_4", "text": "learning techniques with feature engineering. We also build different neural network architectures which we explain below. 4.1 Feature Engineering with Statistical Machine Learning Techniques We describe below the features exploited in this work. The tag sets of English and French for part-of-speech (PoS) tagging, constituency parsing and dependency parsing have been converted into three compact and uniﬁed tag sets [28]. 1) The PoS tagging is done by Stanford CoreNLP [21] for the two languages. On source and target side, for each PoS tag, the number of its occurrence is counted in a vector. We also calculate the cosine similarity between these two vectors (on all words and only on content words).5 2) We verify the pattern of PoS tag sequence changing according to a manual list, for example the pair methodologically →de façon méthodologique ‘methodologically’ corresponds to the pattern ADV →ADP NOUN ADJ. 3) The number of tokens in the two segments (le, lf), the ratio of these numbers (le/lf, lf/le), the distance Levenshtein [18] between the segments. 4) The constituency parsing is done by Bonsai [4] for French, by Stanford CoreNLP for English. We compare the PoS tags for a pair of words, the non-terminal node tags for a pair of segments, the tag category (e.g. verb →verb phrase) for a word translated by a segment or vice versa. 5) The dependency parsing is done by Stanford CoreNLP for the two languages. Inside the segments, the number of occurrence of each dependency relation is counted. Outside the segments, among the words linked at source and target side, we ﬁlter those which are aligned in the sentence context. Then the number of occurrence of each de- pendency relation between the words in segments and these context words is counted. 6) The cosine similarity is calculated between the embeddings from ConceptNet Numberbatch [32]. This resource is multilingual and the system based on ConceptNet took the ﬁrst place in the task “Multilingual and Cross-lingual Semantic Word Sim- ilarity” of SemEval2017 [3,33]. Certain multi-word expressions have their own em- beddings in this resource. Otherwise, we calculate the average of embeddings only on content words. The same features are calculated for lemmatized segments.6 7) The resource ConceptNet [32] also provides assertions in triplet: a pair of words or expressions linked by a relation. In this multilingual resource, we verify if an English- French pair is directly linked; indirectly linked by another French segment or simply not linked.7 Three forms are tested: original form, lemmatized form and lemmatized ﬁltered form.8 5 The tags of content words include: ADJ, ADV, NOUN, PROPN, VERB. If a segment does not contain any content word, the original segment is used. 6 The lemmatization is done by Stanford CoreNLP and Tree Tagger [30] for English and French. 7 The EN-FR and FR-FR assertions are used in this work. 8 We ﬁlter the words in a manual list, for example the light verbs, determinants, pronouns, etc. 5 / 12 8) On the lemmatized ﬁltered form, we calculate the percentage of tokens which are linked with a relation of derivation, based on the resource", "meta": {"paper_id": "1904.12213v1", "chunk_index": 4, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_5", "text": "assertions are used in this work. 8 We ﬁlter the words in a manual list, for example the light verbs, determinants, pronouns, etc. 5 / 12 8) On the lemmatized ﬁltered form, we calculate the percentage of tokens which are linked with a relation of derivation, based on the resource ConceptNet. For example deceptive and illusion ‘illusion’ are not directly linked in the resource, but they are both linked to illusoire ‘illusory’. Hence we consider that there exists a link of derivation between them. For the three following features, we have exploited the lexical translation probabil- ity table generated by the statistical word alignment tool Berkeley Word Aligner [19], trained on an English-French parallel corpus composed of TED Talks and a part of Paracrawl corpus (in total 1.8M parallel sentence pairs and 41M English tokens).9 9) The entropy of the distributions of lexical translation probabilities [13,5], calcu- lated according to this equation: H(X) = P i P(xi)I(xi) = −P i P(xi)logeP(xi). We calculate the average entropy on content words. A bigger entropy indicates that the words have more general meanings or they are polysemous. The same feature is calculated on the lemmatized content words. 10) The bidirectional lexical weighting on content words, by supposing a n-m align- ment a between the segments (¯e and ¯f). In the scheme proposed by Koehn et al. [16] (equation 1), to calculate the direct lexical weighting, each of the English words ei is generated by aligned foreign words fj with the word translation probability w(ei|fj). And similarly for the reverse lexical weighting lex( ¯f|¯e, a). The same feature is calcu- lated for lemmatized content words. This feature could reﬂect the alignment conﬁdence between a pair of segments. lex(¯e| ¯f, a) = length(¯e) Y i=1 1 |{j|(i, j) ∈a}| X ∀(i,j)∈a w(ei|fj) (1) 11) The sum of lexical translation probability differences between the human trans- lation and the most probable translation according to the probability table. For each source word, we take the target word in human translation with the biggest probability. According to this method, we also count the unaligned words to calculate a ratio on the total number of tokens on each side. These features are calculated in the two directions of translation. We use the toolkit Scikit-Learn [27] to train different statistical machine learning classiﬁers.10 4.2 End-to-end Neural Network Architectures The source and target phrases are encoded using a bidirectional encoder with Gated Recurrent Unit (GRU) (size 10). The outputs of forward and backward recurrent net- works are concatenated to form the source and target phrase representations (size 20). After the encoder layer we have tried two different architectures. The ﬁrst one is to build an alignment matrix for the source-target phrases, using the dot product of the two representations, inspired by these two work [17,29]. Then a Convolutional Neural 9 https://wit3.fbk.eu/, https://paracrawl.eu/index.html 10 The code and data set is publicly available at https://github.com/YumingZHAI/ctp. 6 / 12 Network (CNN) classiﬁer is applied to this alignment matrix, which is composed of one convolution layer followed by pooling. Since the shape of the alignment matrix varies", "meta": {"paper_id": "1904.12213v1", "chunk_index": 5, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_6", "text": "work [17,29]. Then a Convolutional Neural 9 https://wit3.fbk.eu/, https://paracrawl.eu/index.html 10 The code and data set is publicly available at https://github.com/YumingZHAI/ctp. 6 / 12 Network (CNN) classiﬁer is applied to this alignment matrix, which is composed of one convolution layer followed by pooling. Since the shape of the alignment matrix varies from one source-target pair to another, an adaptive pooling is used [14]. The output of the pooling layer is fed into a fully-connected layer followed by a linear layer as the output. In the second architecture, the source and target outputs of the encoder layer are averaged over time steps to produce two ﬁxed-dimensional vectors, which are fur- ther concatenated (size 40) and fed into a Multi-layer Perceptron (MLP) classiﬁer. The hidden layer of MLP includes 10 hidden units with tanh non-linearity. The length of our phrases is usually short, especially for word-for-word Literal instances. In order to build a more robust alignment matrix and to avoid the out-of- vocabulary problem, we ﬁnally choose to use character embeddings. As shown in ta- ble 2, for the embedding layer, we have tried respectively randomly initialized character embeddings (size 10), and training our own word embeddings using skipgram model of FastText [2] on a TED Talks corpus (around 3M tokens for both English and French), with a word-embedding size of 100, minimum n-gram of 3, and maximum n-gram of 6. All the models have been trained in 200 epochs, with a learning rate of 0.0001 using Adam optimizer and the minibatch size of 20. Dropout has been applied to all layers except the output and embedding layers. 5 Experimental Results and Analysis The table 2 and 3 show the results of our classiﬁers using end-to-end neural network architectures, for binary classiﬁcation (balanced distribution) and multi-class classiﬁ- cation. For the binary classiﬁcation, Non_literal (NL) class has in total 1127 instances, and 1127 Literal (L) instances are randomly chosen. Besides the preprocessing steps of lowercasing and correcting minor spelling errors, for the neural classiﬁers, we also normalized the clitic forms to complete words (e.g. ’re →are), and normalized digits to letter form (e.g. 42 →four two). The architecture using word embeddings and MLP obtain better results and is faster than the other two architectures. However, the current data set is too small for neural architectures to produce satisfactory results. Table 2: Binary classiﬁcation (balanced distribution) Architecture Accuracy F1 (L) F1 (NL) Randomly initialized character embedding CNN 59.99% 0.60 0.60 MLP 71.16% 0.71 0.71 Pre-trained fasttext word embedding MLP 71.25% 0.71 0.71 Table 3: Multi-class classiﬁcation (ﬁve non-literal classes) Architecture Accuracy Micro-F1 Macro-F1 Randomly initialized character embedding CNN 34.08% 0.34 0.20 MLP 40.74% 0.41 0.34 Pre-trained fasttext word embedding MLP 43.22% 0.43 0.34 The number of all non-literal instances (1127) is only one third of Literal instances (3771). Considering this important difference, for the statistical machine learning clas- siﬁers, we ﬁrst evaluated them under these conﬁgurations: - six classes (Literal, Equivalence, Generalization, Particularization, Modulation, Contain_Transposition). We ﬁrst put all Literal instances. Then to have an approxi- mately balanced class distribution, we randomly take 200 instances for Literal.", "meta": {"paper_id": "1904.12213v1", "chunk_index": 6, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_7", "text": "(3771). Considering this important difference, for the statistical machine learning clas- siﬁers, we ﬁrst evaluated them under these conﬁgurations: - six classes (Literal, Equivalence, Generalization, Particularization, Modulation, Contain_Transposition). We ﬁrst put all Literal instances. Then to have an approxi- mately balanced class distribution, we randomly take 200 instances for Literal. 7 / 12 - two classes (Literal and Non_literal), with three distributions (3:1, 2:1, 1:1). The distribution 3:1 is the natural distribution in the data set. The instances of Literal have been extracted randomly for the last two distributions. - ﬁve classes (only non-literal categories). For each conﬁguration, we have tuned the hyperparameters of different classiﬁers. We evaluate them by ﬁve-fold cross-validation,11 using the metrics such as the average accuracy of ﬁve folds, the micro average and macro average F1-score [34]. The Dum- myClassiﬁer is used as a baseline, which generates random predictions by respecting the distribution of training classes. First, we attempted a direct classiﬁcation into six classes (see table 4). The best results by RandomForest reﬂect the difﬁculty of the task in multi-classes. On the other hand, we observe the potential of our features on classifying the category Literal when the number of instances increases. As a result, we decide to divide the problem: conduct ﬁrst a binary classiﬁcation, and secondly a multi-class classiﬁcation among the non- literal categories. Table 4: Classiﬁcation results under different conﬁgurations, using all features Distribution of classes Classiﬁer Accuracy Micro-F1 Macro-F1 Six classes six classes, with 3771 Literal Dummy 60.76% 0.61 0.15 RandomForest 83.10% 0.83 0.44 six classes, with 200 Literal Dummy 18.92% 0.19 0.16 RandomForest 57.04% 0.57 0.52 Two classes Literal (3) : Non_literal (1) Dummy 65.84% 0.66 0.52 RandomForest 90.16% 0.90 0.86 Literal (2) : Non_literal (1) Dummy 56.43% 0.56 0.51 RandomForest 88.85% 0.89 0.88 Literal (1) : Non_literal (1) Dummy 53.19% 0.53 0.53 RandomForest 87.09% 0.87 0.87 Five classes Five non-literal classes Dummy 20.32% 0.20 0.18 RandomForest 55.10% 0.55 0.47 For the binary classiﬁcation, the two best classiﬁers are RandomForest and MLP. Furthermore, RandomForest has better performance than the two combined by the method hard voting or soft voting. The table 4 presents the results under three different class distributions. From the natural distribution (3:1) to our artiﬁcial balanced distribu- tion by randomly choosing Literal instances (thus both class have 1127 instances), the average F1-score for the class Non_literal increases from 0.78 to 0.88. We will continue to test this tendency when a larger data set is available. Table 4 also shows the results for the classiﬁcation into ﬁve non-literal classes using all features, and the average F1- score for each non-literal category are shown in table 5. The category Generalization 11 StratiﬁedKFold is used for cross-validation, where the folds are made by preserving the per- centage of samples for each class. 8 / 12 has many fewer instances than the other categories, which need to be augmented; there exist many confusions between Modulation and the other categories, which suggests rather a review of annotation guidelines. Table 5: Average F1-score for each non-literal class, using all features Category Equivalence Generalization Particularization Modulation", "meta": {"paper_id": "1904.12213v1", "chunk_index": 7, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_8", "text": "8 / 12 has many fewer instances than the other categories, which need to be augmented; there exist many confusions between Modulation and the other categories, which suggests rather a review of annotation guidelines. Table 5: Average F1-score for each non-literal class, using all features Category Equivalence Generalization Particularization Modulation Contain_Transposition Nb. instances 289 86 215 195 342 Average F1 0.51 0.25 0.56 0.36 0.68 The table 6 recapitulates the best performance on binary classiﬁcation (balanced distribution) and on the classiﬁcation of ﬁve non-literal classes, using the most helpful set of features. With the best performing classiﬁer RandomForest, we have investigated the performance of features one by one and also grouped them: PoS_tagging (feature 1, 2), surface (feature 3), syntactic_analysis (feature 4, 5), external_resource (feature 6, 7, 8) and word_alignment (feature 9, 10, 11). For binary classiﬁcation, feature 10 (bidi- rectional lexical weighting) is most helpful, which generates average F1-score of 0.78 for Literal and 0.80 for Non_literal by itself. The group of features word_alignment contributes the most for the binary classiﬁcation. The combination of all features gen- erates the best results, which remain the same if we remove the feature 4 (constituency parsing), 7 (how the pair is linked in the resource ConceptNet) and the features on PoS tagging apart from the vector counting the occurrence of each tag. The features in ﬂoat form generally perform better than those in discrete form (e.g. 0, 1, etc.). Concerning the classiﬁcation into ﬁve non-literal classes, the combination of all features except the group external_resource leads to the best results, where the group PoS_tagging and syntactic_analysis contribute more than the group word_alignment and surface. The accuracy changes from 55.10% to 55.20% after feature ablation (see table 4). Our error analysis shows that in binary classiﬁcation, it is difﬁcult to distinguish Literal and Equivalence; in multi-class classiﬁcation, the biggest confusion is between Equivalence and Contain_Transposition. Consequently, we conducted another three bi- nary classiﬁcation experiments (see table 7), where in all conﬁgurations each class has 549 instances to make the results comparable: i) Literal vs Non_literal ii) Literal com- bined with Equivalence (E), vs the other classes iii) Literal combined with Equivalence and Transposition (T), vs the other classes. The third conﬁguration is more interesting, because the group of translation processes LET do not bring meaning changes, while the processes non-LET could. The results show that by including Transposition (change grammatical classes without changing the meaning), the performance gets better than only grouping Literal and Equivalence, since we avoid the confusion between Equiv- alence and Transposition. The better results of binary classiﬁcation (L vs NL, LET vs non-LET) indicate that in future work we can develop cascading classiﬁers, namely ﬁrst separating word-for-word literal translations, or those which do not cause meaning changes, then conducting a ﬁner-grained classiﬁcation among the other categories. 6 Conclusion and Perspectives We have proposed a new Natural Language Processing task of automatically classify- ing translation processes at subsentential level, based on manually annotated examples 9 / 12 Table 6: Classiﬁcation results after feature ablation study average accuracy average F1-scores binary classiﬁcation (balanced", "meta": {"paper_id": "1904.12213v1", "chunk_index": 8, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_9", "text": "classiﬁcation among the other categories. 6 Conclusion and Perspectives We have proposed a new Natural Language Processing task of automatically classify- ing translation processes at subsentential level, based on manually annotated examples 9 / 12 Table 6: Classiﬁcation results after feature ablation study average accuracy average F1-scores binary classiﬁcation (balanced distribution) 87.09% 0.87 (Literal) 0.88 (Non_literal) ﬁve non-literal classes 55.20% 0.55 (micro average) 0.48 (macro average) Table 7: Classiﬁcation results after grouping classes, every class has 549 instances Conﬁguration average accuracy average F1 (class1) average F1 (class2) Dummy 48.63% 0.49 0.49 L vs NL 85.24% 0.84 0.86 LE vs non-LE 75.32% 0.74 0.77 LET vs non-LET 79.42% 0.78 0.81 from a parallel English-French TED Talks corpus. To the best of our knowledge, these translation processes have not been explicitly exploited during paraphrase extraction from bilingual parallel corpora. With the best performing classiﬁer RandomForest and feature engineering, our empirical results show a best accuracy of 87.09% for binary classiﬁcation (Literal vs Non_literal) and 55.20% for multi-class classiﬁcation (Equiv- alence, Generalization, Particularization, Modulation, Contain_Transposition), which are much better than the baseline random classiﬁer. This task is complicated, and our exploratory work is restrained by the limited amount of annotated examples. However, our work demonstrates that automatically classifying translation processes seem possible, and the experiments show the direc- tions that we can follow in future work. There is much room to constitute an augmented and balanced data set, on which we will evaluate our classiﬁer to observe the perfor- mance. The ﬁner error analysis of the classiﬁcation results is useful to help the research on corpus annotation and linguistic analysis. We will continue to improve the classiﬁer on English-French, by implementing other features for multi-class classiﬁcation, and explore more neural architectures. We will also extend our work to English-Chinese translation pairs. One of our long term objectives is leveraging this automatic classiﬁ- cation to better control paraphrase extraction from bilingual parallel corpora. References 1. Bannard, C., Callison-Burch, C.: Paraphrasing with bilingual parallel corpora. In: Proceed- ings of the 43rd Annual Meeting on Association for Computational Linguistics. pp. 597–604. Association for Computational Linguistics (2005) 2. Bojanowski, P., Grave, E., Joulin, A., Mikolov, T.: Enriching word vectors with subword in- formation. Transactions of the Association for Computational Linguistics 5, 135–146 (2017) 3. Camacho-Collados, J., Pilehvar, M.T., Collier, N., Navigli, R.: Semeval-2017 task 2: Multi- lingual and cross-lingual semantic word similarity. In: Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017). pp. 15–26. Association for Computa- tional Linguistics (2017). https://doi.org/10.18653/v1/S17-2002 4. Candito, M., Nivre, J., Denis, P., Anguiano, E.H.: Benchmarking of statistical dependency parsers for french. In: Proceedings of the 23rd International Conference on Computational 10 / 12 Linguistics: Posters. pp. 108–116. Association for Computational Linguistics, Chinese In- formation Processing Society of China (2010) 5. Carl, M., Schaeffer, M.J.: Why translation is difﬁcult: A corpus-based study of non-literality in post-editing and from-scratch translation. HERMES-Journal of Language and Communi- cation in Business (56), 43–57 (2017) 6. Carpuat, M., Vyas, Y., Niu, X.: Detecting cross-lingual semantic divergence for neural ma- chine translation. In: Proceedings of the First Workshop on", "meta": {"paper_id": "1904.12213v1", "chunk_index": 9, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_10", "text": "Schaeffer, M.J.: Why translation is difﬁcult: A corpus-based study of non-literality in post-editing and from-scratch translation. HERMES-Journal of Language and Communi- cation in Business (56), 43–57 (2017) 6. Carpuat, M., Vyas, Y., Niu, X.: Detecting cross-lingual semantic divergence for neural ma- chine translation. In: Proceedings of the First Workshop on Neural Machine Translation. pp. 69–79. Association for Computational Linguistics (2017) 7. Chatterjee, R., Negri, M., Rubino, R., Turchi, M.: Findings of the WMT 2018 Shared Task on Automatic Post-Editing. In: Proceedings of the Third Conference on Machine Translation. Association for Computational Linguistics, Belgium, Brussels (October 2018) 8. Chuquet, H., Paillard, M.: Approche linguistique des problèmes de traduction anglais- français. Ophrys (1989) 9. Cohen, J.: A coefﬁcient of agreement for nominal scales. Educational and Psychological Measurement 20, 37–46 (1960) 10. Deng, D., Xue, N.: Translation divergences in chinese–english machine translation: An em- pirical investigation. Computational Linguistics 43(3), 521–565 (2017) 11. Dorr, B.J., Pearl, L., Hwa, R., Habash, N.: Duster: A method for unraveling cross-language divergences for statistical word-level alignment. In: Conference of the Association for Ma- chine Translation in the Americas. pp. 31–43. Springer (2002) 12. Ganitkevitch, J., Van Durme, B., Callison-Burch, C.: PPDB: The paraphrase database. In: Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. pp. 758–764 (2013) 13. Gray, R.M.: Entropy and Information Theory. Springer-Verlag, Berlin, Heidelberg (1990) 14. He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 37(9), 1904–1916 (Sept 2015). https://doi.org/10.1109/TPAMI.2015.2389824 15. Imamura, K., Sumita, E., Matsumoto, Y.: Automatic construction of machine translation knowledge using translation literalness. In: Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume 1. pp. 155–162. Associa- tion for Computational Linguistics (2003) 16. Koehn, P., Och, F.J., Marcu, D.: Statistical phrase-based translation. In: Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Lin- guistics on Human Language Technology-Volume 1. pp. 48–54. Association for Computa- tional Linguistics (2003) 17. Legrand, J., Auli, M., Collobert, R.: Neural Network-based Word Alignment through Score Aggregation. In: Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Germany. pp. 66–73. The Association for Computer Linguistics (2016) 18. Levenshtein, V.I.: Binary codes capable of correcting deletions, insertions, and reversals. Soviet physics doklady 10(8), 707–710 (1966) 19. Liang, P., Taskar, B., Klein, D.: Alignment by agreement. In: Proceedings of the main con- ference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics. pp. 104–111. Association for Computational Lin- guistics (2006) 20. Mallinson, J., Sennrich, R., Lapata, M.: Paraphrasing revisited with neural machine transla- tion. In: Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. vol. 1, pp. 881–893 (2017) 21. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S.J., McClosky, D.: The Stanford CoreNLP natural language processing toolkit. In: Association for Computational Linguistics (ACL) System Demonstrations. pp. 55–60", "meta": {"paper_id": "1904.12213v1", "chunk_index": 10, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_11", "text": "Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. vol. 1, pp. 881–893 (2017) 21. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S.J., McClosky, D.: The Stanford CoreNLP natural language processing toolkit. In: Association for Computational Linguistics (ACL) System Demonstrations. pp. 55–60 (2014) 11 / 12 22. Molina, L., Hurtado Albir, A.: Translation Techniques Revisited: A Dynamic and Function- alist Approach. Meta 47(4), 498–512 (2002). https://doi.org/10.7202/008033ar 23. Newmark, P.: Approaches to Translation (Language Teaching Methodology Senes). Oxford: Pergamon Press (1981) 24. Newmark, P.: A textbook of translation, vol. 66. Prentice Hall New York (1988) 25. Papineni, K., Roukos, S., Ward, T., Zhu, W.J.: Bleu: a method for automatic evaluation of machine translation. In: Proceedings of the 40th annual meeting on association for computa- tional linguistics. pp. 311–318. Association for Computational Linguistics (2002) 26. Pavlick, E., Bos, J., Nissim, M., Beller, C., Van Durme, B., Callison-Burch, C.: Adding semantics to data-driven paraphrasing. In: Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). vol. 1, pp. 1512–1522 (2015) 27. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., Duchesnay, E.: Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12, 2825–2830 (2011) 28. Petrov, S., Das, D., McDonald, R.T.: A universal part-of-speech tagset. In: Calzolari, N., Choukri, K., Declerck, T., Dogan, M.U., Maegaard, B., Mariani, J., Odijk, J., Piperidis, S. (eds.) Proceedings of the Eighth International Conference on Language Resources and Eval- uation, LREC 2012, Istanbul, Turkey, May 23-25, 2012. pp. 2089–2096. European Language Resources Association (ELRA) (2012) 29. Pham, M.Q., Crego, J., Senellart, J., Yvon, F.: Fixing translation divergences in parallel cor- pora for neural mt. In: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. pp. 2967–2973 (2018) 30. Schmid, H.: Improvements In Part-of-Speech Tagging With an Application To German. In: Proceedings of the ACL SIGDAT-Workshop. pp. 47–50 (1995) 31. Snover, M., Dorr, B., Schwartz, R., Micciulla, L., Makhoul, J.: A study of translation edit rate with targeted human annotation. In: Proceedings of association for machine translation in the Americas. vol. 200 (2006) 32. Speer, R., Chin, J., Havasi, C.: Conceptnet 5.5: An open multilingual graph of general knowl- edge. In: Thirty-First AAAI Conference on Artiﬁcial Intelligence. pp. 4444–4451 (2017) 33. Speer, R., Lowry-Duda, J.: Conceptnet at semeval-2017 task 2: Extending word embeddings with multilingual relational knowledge. In: Bethard, S., Carpuat, M., Apidianaki, M., Mo- hammad, S.M., Cer, D.M., Jurgens, D. (eds.) Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval@ACL 2017, Vancouver, Canada, August 3-4, 2017. pp. 85–89. Association for Computational Linguistics (2017) 34. Tsoumakas, G., Katakis, I., Vlahavas, I.: Random k-labelsets for multilabel classiﬁcation. IEEE Transactions on Knowledge and Data Engineering 23(7), 1079–1089 (2011) 35. Vinay, J.P., Darbelnet, J.: Stylistique comparée du français et de l’anglais: méthode de tra- duction. Bibliothèque de stylistique comparée, Didier (1958) 36. Vyas, Y.,", "meta": {"paper_id": "1904.12213v1", "chunk_index": 11, "filename": "1904.12213v1.pdf", "chunk_length": 512}}
{"id": "1904.12213v1_chunk_12", "text": "Computational Linguistics (2017) 34. Tsoumakas, G., Katakis, I., Vlahavas, I.: Random k-labelsets for multilabel classiﬁcation. IEEE Transactions on Knowledge and Data Engineering 23(7), 1079–1089 (2011) 35. Vinay, J.P., Darbelnet, J.: Stylistique comparée du français et de l’anglais: méthode de tra- duction. Bibliothèque de stylistique comparée, Didier (1958) 36. Vyas, Y., Niu, X., Carpuat, M.: Identifying Semantic Divergences in Parallel Text without Annotations. In: Walker, M.A., Ji, H., Stent, A. (eds.) Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers). pp. 1503–1515. Association for Computational Linguistics (2018) 37. Zhai, Y., Max, A., Vilnat, A.: Construction of a multilingual corpus annotated with transla- tion relations. In: First Workshop on Linguistic Resources for Natural Language Processing. pp. 102–111 (2018) 12 / 12", "meta": {"paper_id": "1904.12213v1", "chunk_index": 12, "filename": "1904.12213v1.pdf", "chunk_length": 142}}
{"id": "1904.12550v1_chunk_0", "text": "arXiv:1904.12550v1 [cs.CL] 29 Apr 2019 Semantic Matching of Documents from Heterogeneous Collections: A Simple and Transparent Method for Practical Applications Mark-Christoph M¨uller Heidelberg Institute for Theoretical Studies gGmbH Heidelberg, Germany mark-christoph.mueller@h-its.org Abstract We present a very simple, unsupervised method for the pairwise matching of documents from het- erogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classiﬁcation task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-speciﬁc modiﬁcations, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efﬁcient, because it is based on the aggregation of (pre-computable) word-level similarities. 1 Introduction We present a simple and efﬁcient unsupervised method for pairwise matching of documents from het- erogeneous collections. Following Gong et al. (2018), we consider two document collections heteroge- neous if their documents differ systematically with respect to vocabulary and / or level of abstraction. With these deﬁning differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Examples include collections in which expert an- swers are mapped to non-expert questions (e.g. InsuranceQA by Feng et al. (2015)), but also so-called community QA collections (Blooma and Kurian (2011)), where the lexical mismatch between Q and A documents is often less pronounced than the length difference. Like many other approaches, the proposed method is based on word embeddings as universal meaning representations, and on vector cosine as the similarity metric. However, instead of computing pairs of document representations and measuring their similarity, our method assesses the document-pair simi- larity on the basis of selected pairwise word similarities. This has the following advantages, which make our method a viable candidate for practical, real-world applications: efﬁciency, because pairwise word similarities can be efﬁciently (pre-)computed and cached, and transparency, because the selected words from each document are available as evidence for what the similarity computation was based on. We demonstrate our method with the Concept-Project matching task (Gong et al. (2018)), which is de- scribed in the next section. 2 Task, Data Set, and Original Approach The Concept-Project matching task is a binary classiﬁcation task where each instance is a pair of het- erogeneous documents: one concept, which is a short science curriculum item from NGSS1, and one project, which is a much longer science project description for school children from ScienceBuddies2. 1https://www.nextgenscience.org 2https://www.sciencebuddies.org CONCEPT LABEL: ecosystems: - ls2.a: interdependent relationships in ecosystems CONCEPT DESCRIPTION: Ecosystems have carrying capacities , which are limits to the numbers of organisms and populations they can support . These limits result from such factors as the availability of living and nonliving resources and from such challenges such as predation , competition , and disease . Organisms would have the capacity to produce populations of great size were it not for the fact that environments and resources are ﬁnite . This fundamental tension affects the abundance ( number of individuals ) of species in any", "meta": {"paper_id": "1904.12550v1", "chunk_index": 0, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_1", "text": "such challenges such as predation , competition , and disease . Organisms would have the capacity to produce populations of great size were it not for the fact that environments and resources are ﬁnite . This fundamental tension affects the abundance ( number of individuals ) of species in any given ecosystem . PROJECT LABEL: Primary Productivity and Plankton PROJECT DESCRIPTION: Have you seen plankton? I am not talking about the evil villain trying to steal the Krabby Patty recipe from Mr. Krab. I am talking about plankton that live in the ocean. In this experiment you can learn how to collect your own plankton samples and see the wonderful diversity in shape and form of planktonic organisms. The oceans contain both the earth’s largest and smallest organisms. Interestingly they share a delicate relationship linked together by what they eat. The largest of the ocean’s inhabitants, the Blue Whale, eats very small plankton, which themselves eat even smaller phytoplankton. All of the linkages between predators, grazers, and primary producers in the ocean make up an enormously complicated food web.The base of this food web depends upon phytoplankton, very small photosynthetic organisms which can make their own energy by using energy from the sun. These phytoplankton provide the primary source of the essential nutrients that cycle through our ocean’s many food webs. This is called primary productivity, and it is a very good way of measuring the health and abundance of our ﬁsheries.There are many different kinds of phytoplankton in our oceans. [...] One way to study plankton is to collect the plankton using a plankton net to collect samples of macroscopic and microscopic plankton organisms. The net is cast out into the water or trolled behind a boat for a given distance then retrieved. Upon retrieving the net, the contents of the collecting bottle can be removed and the captured plankton can be observed with a microscope. The plankton net will collect both phytoplankton (photosynthetic plankton) and zooplankton (non-photosynthetic plankton and larvae) for observation.In this experiment you will make your own plankton net and use it to collect samples of plankton from different marine or aquatic locations in your local area. You can observe both the abundance (total number of organisms) and diversity (number of different kinds of organisms) of planktonic forms to make conclusions about the productivity and health of each location. In this experiment you will make a plankton net to collect samples of plankton from different locations as an indicator of primary productivity. You can also count the number of phytoplankton (which appear green or brown) compared to zooplankton (which are mostly marine larval forms) and compare. Do the numbers balance, or is there more of one type than the other? What effect do you think this has on productivity cycles? Food chains are very complex. Find out what types of predators and grazers you have in your area. You can ﬁnd this information from a ﬁeld guide or from your local Department of Fish and Game. Can you use this information to construct a food web for your", "meta": {"paper_id": "1904.12550v1", "chunk_index": 1, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_2", "text": "productivity cycles? Food chains are very complex. Find out what types of predators and grazers you have in your area. You can ﬁnd this information from a ﬁeld guide or from your local Department of Fish and Game. Can you use this information to construct a food web for your local area? Some blooms of phytoplankton can be harmful and create an anoxic environment that can suffocate the ecosystem and leave a ”Dead Zone” behind. Did you ﬁnd an excess of brown algae or diatoms? These can be indicators of a harmful algal bloom. Re-visit this location over several weeks to report on an increase or decrease of these types of phytoplankton. Do you think that a harmful algal bloom could be forming in your area? For an experiment that studies the relationship between water quality and algal bloom events, see the Science Buddies project Harmful Algal Blooms in the Chesapeake Bay. Figure 1: C-P Pair (Instance 261 of the original data set.) The publicly available data set3 contains 510 labelled pairs4 involving C = 75 unique concepts and P = 230 unique projects. A pair is annotated as 1 if the project matches the concept (57%), and as 0 otherwise (43%). The annotation was done by undergrad engineering students. Gong et al. (2018) do not provide any speciﬁcation, or annotation guidelines, of the semantics of the ’matches’ relation to be annotated. Instead, they create gold standard annotations based on a majority vote of three manual anno- tations. Figure 1 provides an example of a matching C-P pair. The concept labels can be very speciﬁc, potentially introducing vocabulary that is not present in the actual concept descriptions. The extent to which this information is used by Gong et al. (2018) is not entirely clear, so we experiment with several setups (cf. Section 4). 2.1 Gong et al. (2018)’s Approach The approach by Gong et al. (2018) is based on the idea that the longer document in the pair is reduced to a set of topics which capture the essence of the document in a way that eliminates the effect of a potential length difference. In order to overcome the vocabulary mismatch, these topics are not based on words and their distributions (as in LSI (Deerwester et al. (1990)) or LDA (Blei et al. (2003))), but on word embedding vectors. Then, basically, matching is done by measuring the cosine similarity between the topic vectors and the short document words. Gong et al. (2018) motivate their approach mainly with the length mismatch argument, which they claim makes approaches relying on document representations (incl. vector averaging) unsuitable. Accordingly, they use Doc2Vec (Le and Mikolov (2014)) as one of their baselines, and show that its performance is inferior to their method. They do not, however, provide a much simpler averaging-based baseline. As a second baseline, they use Word Mover’s Distance (Kusner et al. (2015)), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method. Gong et al. (2018) use two different sets", "meta": {"paper_id": "1904.12550v1", "chunk_index": 2, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_3", "text": "a much simpler averaging-based baseline. As a second baseline, they use Word Mover’s Distance (Kusner et al. (2015)), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method. Gong et al. (2018) use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles. 3 Our Method We develop our method as a simple alternative to that of Gong et al. (2018). We aim at comparable or better classiﬁcation performance, but with a simpler model. Also, we design the method in such a way that it provides human-interpretable results in an efﬁcient way. One common way to compute 3https://github.com/HongyuGong/Document-Similarity-via-Hidden-Topics 4Of the original 537 labelled pairs, 27 were duplicates, which we removed. the similarity of two documents (i.e. word sequences) c and p is to average over the word embeddings for each sequence ﬁrst, and to compute the cosine similarity between the two averages afterwards. In the ﬁrst step, weighting can be applied by multiplying a vector with the TF, IDF, or TF*IDF score of its pertaining word. We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by Gong et al. (2018). It yields a single scalar similarity score. The core idea of our alternative method is to turn the above process upside down, by computing the cosine similarity of selected pairs of words from c and p ﬁrst, and to average over the similarity scores afterwards (cf. also Section 6). More precisely, we implement a measure TOP n COS SIM AVG as the average of the n highest pairwise cosine similarities of the n top-ranking words in c and p. Ranking, again, is done by TF, IDF, and TF*IDF. For each ranking, we take the top-ranking n words from c and p, compute n × n similarities, rank by decreasing similarity, and average over the top n similarities. This measure yields both a scalar similarity score and a list of < cx, py, sim > tuples, which represent the qualitative aspects of c and p on which the similarity score is based. 4 Experiments Setup All experiments are based on off-the-shelf word-level resources: We employ WOMBAT (M¨uller and Strube (2018)) for easy access to the 840B GloVe (Pennington et al. (2014)) and the GoogleNews5 Word2Vec (Mikolov et al. (2013)) embeddings. These embedding resources, while slightly outdated, are still widely used. However, they cannot handle out-of-vocabulary tokens due to their ﬁxed, word-level lexicon. Therefore, we also use a pretrained English fastText model6 (Bojanowski et al. (2017); Grave et al. (2018)), which also includes subword information. IDF weights for approx. 12 mio. different words were obtained from the English Wikipedia dump provided by the Polyglot project (Al-Rfou et al. (2013)). All resources are case-sensitive, i.e. they might contain different entries for words that only differ in case (cf. Section 5). We run experiments in different setups, varying", "meta": {"paper_id": "1904.12550v1", "chunk_index": 3, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_4", "text": "weights for approx. 12 mio. different words were obtained from the English Wikipedia dump provided by the Polyglot project (Al-Rfou et al. (2013)). All resources are case-sensitive, i.e. they might contain different entries for words that only differ in case (cf. Section 5). We run experiments in different setups, varying both the input representation (GloVe vs. Google vs. fastText embeddings, ± TF-weighting, and ± IDF-weighting) for concepts and projects, and the extent to which concept descriptions are used: For the latter, Label means only the concept label (ﬁrst and second row in the example), Description means only the textual description of the concept, and Both means the concatenation of Label and Description. For the projects, we always use both label and description. For the project descriptions, we extract only the last column of the original ﬁle (CONTENT), and remove user comments and some boiler-plate. Each instance in the resulting data set is a tuple of < c, p, label >, where c and p are bags of words, with case preserved and function words7 removed, and label is either 0 or 1. Parameter Tuning Our method is unsupervised, but we need to deﬁne a threshold parameter which controls the minimum similarity that a concept and a project description should have in order to be considered a match. Also, the TOP n COS SIM AVG measure has a parameter n which controls how many ranked words are used from c and p, and how many similarity scores are averaged to create the ﬁnal score. Parameter tuning experiments were performed on a random subset of 20% of our data set (54% positive). Note that Gong et al. (2018) used only 10% of their 537 instances data set as tuning data. The tuning data results of the best-performing parameter values for each setup can be found in Tables 1 and 2. The top F scores per type of concept input (Label, Description, Both) are given in bold. For AVG COS SIM and TOP n COS SIM AVG, we determined the threshold values (T) on the tuning data by doing a simple .005 step search over the range from 0.3 to 1.0. For TOP n COS SIM AVG, we additionally varied the value of n in steps of 2 from 2 to 30. 5https://code.google.com/archive/p/word2vec/ 6https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz 7We use the list provided by Gong et al. (2018), with an additional entry for cannot. Results The top tuning data scores for AVG COS SIM (Table 1) show that the Google embeddings with TF*IDF weighting yield the top F score for all three concept input types (.881 - .945). Somewhat expectedly, the best overall F score (.945) is produced in the setting Both, which provides the most information. Actually, this is true for all four weighting schemes for both GloVe and Google, while fastText consistently yields its top F scores (.840 - .911) in the Label setting, which provides the least information. Generally, the level of performance of the simple baseline measure AVG COS SIM on this data set is rather striking. Concept Input → Label Description Both Embeddings TF", "meta": {"paper_id": "1904.12550v1", "chunk_index": 4, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_5", "text": "and Google, while fastText consistently yields its top F scores (.840 - .911) in the Label setting, which provides the least information. Generally, the level of performance of the simple baseline measure AVG COS SIM on this data set is rather striking. Concept Input → Label Description Both Embeddings TF IDF T P R F T P R F T P R F GloVe - - .635 .750 .818 .783 .720 .754 .891 .817 .735 .765 .945 .846 + - .640 .891 .745 .812 .700 .831 .891 .860 .690 .813 .945 .874 - + .600 .738 .873 .800 .670 .746 .909 .820 .755 .865 .818 .841 + + .605 .904 .855 .879 .665 .857 .873 .865 .715 .923 .873 .897 Google - - .440 .813 .945 .874 .515 .701 .982 .818 .635 .920 .836 .876 + - .445 .943 .909 .926 .540 .873 .873 .873 .565 .927 .927 .927 - + .435 .839 .945 .889 .520 .732 .945 .825 .590 .877 .909 .893 + + .430 .943 .909 .926 .530 .889 .873 .881 .545 .945 .945 .945 fastText - - .440 .781 .909 .840 .555 .708 .927 .803 .615 .778 .891 .831 + - .435 .850 .927 .887 .520 .781 .909 .840 .530 .803 .964 .876 - + .435 .850 .927 .887 .525 .722 .945 .819 .600 .820 .909 .862 + + .420 .895 .927 .911 .505 .803 .891 .845 .520 .833 .909 .870 Table 1: Tuning Data Results AVG COS SIM. Top F per Concept Input Type in Bold. For TOP n COS SIM AVG, the tuning data results (Table 2) are somewhat more varied: First, there is no single best performing set of embeddings: Google yields the best F score for the Label setting (.953), while GloVe (though only barely) leads in the Description setting (.912). This time, it is fastText which produces the best F score in the Both setting, which is also the best overall tuning data F score for TOP n COS SIM AVG (.954). While the difference to the Google result for Label is only minimal, it is striking that the best overall score is again produced using the ’richest’ setting, i.e. the one involving both TF and IDF weighting and the most informative input. Concept Input → Label Description Both Embeddings TF IDF T/n P R F T/n P R F T/n P R F GloVe + - .365/6 .797 .927 .857 .690/14 .915 .782 .843 .675/16 .836 .927 .879 - + .300/30 .929 .236 .377 .300/30 .806 .455 .581 .300/30 .778 .636 .700 + + .330/6 .879 .927 .903 .345/6 .881 .945 .912 .345/6 .895 .927 .911 Google + - .345/22 .981 .927 .953 .480/16 .895 .927 .911 .520/16 .912 .945 .929 - + .300/30 1.00 .345 .514 .300/8 1.00 .345 .514 .300/30 1.00 .600 .750 + + .300/10 1.00 .509 .675 .300/14 .972 .636 .769 .350/22 1.00 .836 .911 fastText + - .415/22 .980 .873 .923 .525/14 .887 .855 .870 .535/20 .869 .964 .914 - + .350/24 1.00 .309 .472 .300/30 1.00 .382 .553 .300/28 1.00 .673 .804 + + .300/20 1.00 .800", "meta": {"paper_id": "1904.12550v1", "chunk_index": 5, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_6", "text": ".600 .750 + + .300/10 1.00 .509 .675 .300/14 .972 .636 .769 .350/22 1.00 .836 .911 fastText + - .415/22 .980 .873 .923 .525/14 .887 .855 .870 .535/20 .869 .964 .914 - + .350/24 1.00 .309 .472 .300/30 1.00 .382 .553 .300/28 1.00 .673 .804 + + .300/20 1.00 .800 .889 .300/10 .953 .745 .837 .310/14 .963 .945 .954 Table 2: Tuning Data Results TOP n COS SIM AVG. Top F per Concept Input Type in Bold. We then selected the best performing parameter settings for every concept input and ran experiments on the held-out test data. Since the original data split used by Gong et al. (2018) is unknown, we cannot exactly replicate their settings, but we also perform ten runs using randomly selected 10% of our 408 instances test data set, and report average P, R, F, and standard deviation. The results can be found in Table 3. For comparison, the two top rows provide the best results of Gong et al. (2018). The ﬁrst interesting ﬁnding is that the AVG COS SIM measure again performs very well: In all three settings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that is adapted to the science domain (topic science), with again the Both setting yielding the best over- all result (.926). Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018). This result corroborates our ﬁndings on the tuning data, and clearly con- tradicts the (implicit) claim made by Gong et al. (2018) regarding the infeasibility of document-level matching for documents of different lengths. The second, more important ﬁnding is that our pro- posed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by Gong et al. (2018) in two out of three settings. It only fails in the setting using only the Description P R F Gong et al. (2018) topic science .758 ±.012 .885 ±.071 .818 ±.028 topic wiki .750 ±.009 .842 ±.010 .791 ±.007 Method Embeddings Settings T/n Conc. Input AVG COS SIM Google +TF +IDF .515 Label .939 ±.043 .839 ±.067 .884 ±.038 Google +TF +IDF .520 Description .870 ±.068 .834 ±.048 .849 ±.038 Google +TF +IDF .545 Both .915 ±.040 .938 ±.047 .926 ±.038 TOP n COS SIM AVG Google +TF -IDF .345/22 Label .854 ±.077 .861 ±.044 .856 ±.054 GloVe +TF +IDF .345/6 Description .799 ±.063 .766 ±.094 .780 ±.068 fastText +TF +IDF .310/14 Both .850 ±.059 .918 ±.049 .881 ±.037 Table 3: Test Data Results input.8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while Gong et al. (2018) reach their best results with a much more sophisticated system and with em- beddings that were custom-trained for the science domain. Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by Gong et al. (2018), it is itself outper- formed by the ’baseline’ AVG COS SIM method with appropriate weighting. However, apart from raw classiﬁcation performance, our", "meta": {"paper_id": "1904.12550v1", "chunk_index": 6, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_7", "text": "custom-trained for the science domain. Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by Gong et al. (2018), it is itself outper- formed by the ’baseline’ AVG COS SIM method with appropriate weighting. However, apart from raw classiﬁcation performance, our method also aims at providing human-interpretable information on how a classiﬁcation was done. In the next section, we perform a detail analysis on a selected setup. 5 Detail Analysis The similarity-labelled word pairs from concept and project description which are selected during clas- siﬁcation with the TOP n COS SIM AVG measure provide a way to qualitatively evaluate the basis on which each similarity score was computed. We see this as an advantage over average-based comparison (like AVG COS SIM), since it provides a means to check the plausibility of the decision. Here, we are mainly interested in the overall best result, so we perform a detail analysis on the best-performing Both setting only (fastText, TF*IDF weighting, T = .310, n = 14). Since the Concept-Project matching task is a binary classiﬁcation task, its performance can be qualitatively analysed by providing examples for instances that were classiﬁed correctly (True Positive (TP) and True Negative (TN)) or incorrectly (False Positive (FP) and False Negative (FN)). Table 5 shows the concept and project words from selected instances (one TP, FP, TN, and FN case each) of the tuning data set. Concept and project words are ordered alphabetically, with concept words appearing more than once being grouped together. According to the selected setting, the number of word pairs is n = 14. The bottom line in each column provides the average similarity score as computed by the TOP n COS SIM AVG measure. This value is compared against the threshold T = .310. The similarity is higher than T in the TP and FP cases, and lower otherwise. Without going into too much detail, it can be seen that the selected words provide a reasonable idea of the gist of the two documents. Another observation relates to the effect of using unstemmed, case-sensitive documents as input: the top-ranking words often contain inﬂectional variants (e.g. enzyme and enzymes, level and levels in the example), and words differing in case only can also be found. Currently, these are treated as distinct (though semantically similar) words, mainly out of compatibility with the pretrained GloVe and Google embeddings. However, since our method puts a lot of emphasis on individual words, in particular those coming from the shorter of the two documents (the concept), results might be improved by somehow merging these words (and their respective embedding vectors) (see Section 7). 6 Related Work While in this paper we apply our method to the Concept-Project matching task only, the underlying task of matching text sequences to each other is much more general. Many existing approaches follow 8Remember that this setup was only minimally superior (.001 F score) to the next best one on the tuning data. TP (.447 > .310) FP (.367 > .310) TN (.195 < .310) FN (.278", "meta": {"paper_id": "1904.12550v1", "chunk_index": 7, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_8", "text": "task of matching text sequences to each other is much more general. Many existing approaches follow 8Remember that this setup was only minimally superior (.001 F score) to the next best one on the tuning data. TP (.447 > .310) FP (.367 > .310) TN (.195 < .310) FN (.278 < .310) Concept Project Sim Concept Project Sim Concept Project Sim Concept Project Sim Word Word Word Word Word Word Word Word cells enzymes .438 co-evolution dynamic .299 energy allergy .147 area water .277 cells genes .427 continual dynamic .296 energy juice .296 climate water .269 molecules DNA .394 delicate detail .306 energy leavening .186 earth copper .254 molecules enzyme .445 delicate dynamic .326 energy substitutes .177 earth metal .277 molecules enzymes .533 delicate texture .379 surface average .212 earth metals .349 molecules gene .369 dynamic dynamic 1.00 surface baking .216 earth water .326 molecules genes .471 dynamic image .259 surface egg .178 extent concentration .266 multiple different .550 dynamic range .377 surface leavening .158 range concentration .255 organisms enzyme .385 dynamic texture .310 surface thickening .246 range ppm .237 organisms enzymes .512 surface level .323 transfer baking .174 systems metals .243 organisms genes .495 surface texture .383 transfer substitute .192 systems solution .275 organs enzymes .372 surface tiles .321 transfer substitutes .157 typical heavy .299 tissues enzymes .448 systems dynamic .272 warms baking .176 weather heavy .248 tissues genes .414 systems levels .286 warms thickening .214 weather water .308 Avg. Sim .447 Avg. Sim .367 Avg. Sim .195 Avg. Sim .278 Table 4: TOP n COS SIM AVG Detail Results of Best-performing fastText Model on Both. the so-called compare-aggregate framework (Wang and Jiang (2017)). As the name suggests, these ap- proaches collect the results of element-wise matchings (comparisons) ﬁrst, and create the ﬁnal result by aggregating these results later. Our method can be seen as a variant of compare-aggregate which is characterized by extremely simple methods for comparison (cosine vector similarity) and aggregation (averaging). Other approaches, like He and Lin (2016) and Wang and Jiang (2017), employ much more elaborated supervised neural networks methods. Also, on a simpler level, the idea of averaging similarity scores (rather than scoring averaged representations) is not new: Camacho-Collados and Navigli (2016) use the average of pairwise word similarities to compute their compactness score. 7 Conclusion and Future Work We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by Gong et al. (2018). Although much simpler, our method clearly outperformed the original system in most input settings. Another result is that, contrary to the claim made by Gong et al. (2018), the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied. Due to its simplicity, we believe that our method can also be applied to other text matching tasks, including more ’standard’ ones which do not necessarily involve heterogeneous document collections. This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which", "meta": {"paper_id": "1904.12550v1", "chunk_index": 8, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_9", "text": "we believe that our method can also be applied to other text matching tasks, including more ’standard’ ones which do not necessarily involve heterogeneous document collections. This seems desirable because our method offers additional transparency by providing not only a similarity score, but also the subset of words on which the similarity score is based. Future work includes detailed error analysis, and exploration of methods to combine complementary information about (grammatically or orthographically) related words from word embedding resources. Also, we are currently experimenting with a pretrained ELMo (Peters et al. (2018)) model as another word embedding resource. ELMo takes word embeddings a step further by dynamically creating contextualized vectors from input word sequences (normally sentences). Our initial experiments have been promising, but since ELMo tends to yield different, context-dependent vectors for the same word in the same document, ways have still to be found to combine them into single, document-wide vectors, without (fully) sacriﬁcing their context-awareness. The code used in this paper is available at https://github.com/nlpAThits/TopNCosSimAvg. Acknowledgements The research described in this paper was funded by the Klaus Tschira Foundation. We thank the anonymous reviewers for their useful comments and suggestions. References Al-Rfou, R., B. Perozzi, and S. Skiena (2013, August). Polyglot: Distributed word representations for multilingual nlp. In Proceedings of the Seventeenth Conference on Computational Natural Language Learning, Soﬁa, Bulgaria, pp. 183–192. Association for Computational Linguistics. Blei, D. M., A. Y. Ng, and M. I. Jordan (2003). Latent dirichlet allocation. Journal of Machine Learning Research 3, 993–1022. Blooma, M. J. and J. C. Kurian (2011). Research issues in community based question answering. In P. B. Seddon and S. Gregor (Eds.), Paciﬁc Asia Conference on Information Systems, PACIS 2011: Qual- ity Research in Paciﬁc Asia, Brisbane, Queensland, Australia, 7-11 July 2011, pp. 29. Queensland University of Technology. Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov (2017). Enriching word vectors with subword information. TACL 5, 135–146. Camacho-Collados, J. and R. Navigli (2016). Find the word that does not belong: A framework for an intrinsic evaluation of word vector representations. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP, RepEval@ACL 2016, Berlin, Germany, August 2016, pp. 43– 50. Association for Computational Linguistics. Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman (1990). Indexing by latent semantic analysis. JASIS 41(6), 391–407. Feng, M., B. Xiang, M. R. Glass, L. Wang, and B. Zhou (2015). Applying deep learning to answer selection: A study and an open task. In 2015 IEEE Workshop on Automatic Speech Recognition and Understanding, ASRU 2015, Scottsdale, AZ, USA, December 13-17, 2015, pp. 813–820. IEEE. Gong, H., T. Sakakini, S. Bhat, and J. Xiong (2018). Document similarity for texts of varying lengths via hidden topics. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2341–2351. Association for Computational Linguistics. Grave, E., P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov (2018). Learning word vectors for 157 languages. In N. Calzolari, K. Choukri, C. Cieri, T. Declerck, S. Goggi, K. Hasida, H. Isahara,", "meta": {"paper_id": "1904.12550v1", "chunk_index": 9, "filename": "1904.12550v1.pdf", "chunk_length": 512}}
{"id": "1904.12550v1_chunk_10", "text": "Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2341–2351. Association for Computational Linguistics. Grave, E., P. Bojanowski, P. Gupta, A. Joulin, and T. Mikolov (2018). Learning word vectors for 157 languages. In N. Calzolari, K. Choukri, C. Cieri, T. Declerck, S. Goggi, K. Hasida, H. Isahara, B. Mae- gaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis, and T. Tokunaga (Eds.), Proceedings of the Eleventh International Conference on Language Resources and Evaluation, LREC 2018, Miyazaki, Japan, May 7-12, 2018. European Language Resources Association (ELRA). He, H. and J. J. Lin (2016). Pairwise word interaction modeling with deep neural networks for semantic similarity measurement. In K. Knight, A. Nenkova, and O. Rambow (Eds.), NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pp. 937–948. The Association for Computational Linguistics. Kusner, M. J., Y. Sun, N. I. Kolkin, and K. Q. Weinberger (2015). From word embeddings to document distances. In F. R. Bach and D. M. Blei (Eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, Volume 37 of JMLR Workshop and Conference Proceedings, pp. 957–966. JMLR.org. Le, Q. V. and T. Mikolov (2014). Distributed representations of sentences and documents. In Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, Volume 32 of JMLR Workshop and Conference Proceedings, pp. 1188–1196. JMLR.org. Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean (2013). Distributed representations of words and phrases and their compositionality. In Proceedings of Advances in Neural Information Processing Systems 26. Lake Tahoe, Nev., 5–8 December 2013, pp. 3111–3119. M¨uller, M. and M. Strube (2018). Transparent, efﬁcient, and robust word embedding access with WOM- BAT. In D. Zhao (Ed.), COLING 2018, The 27th International Conference on Computational Linguis- tics: System Demonstrations, Santa Fe, New Mexico, August 20-26, 2018, pp. 53–57. Association for Computational Linguistics. Pennington, J., R. Socher, and C. D. Manning (2014). Glove: Global vectors for word representation. In A. Moschitti, B. Pang, and W. Daelemans (Eds.), Proceedings of the 2014 Conference on Empir- ical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL, pp. 1532–1543. ACL. Peters, M. E., M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer (2018). Deep contextualized word representations. In M. A. Walker, H. Ji, and A. Stent (Eds.), Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pp. 2227–2237. Association for Computational Linguistics. Wang, S. and J. Jiang (2017). A compare-aggregate model for matching text sequences. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.", "meta": {"paper_id": "1904.12550v1", "chunk_index": 10, "filename": "1904.12550v1.pdf", "chunk_length": 492}}
{"id": "1904.12550v1_chunk_11", "text": "and J. Jiang (2017). A compare-aggregate model for matching text sequences. In 5th Inter- national Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.", "meta": {"paper_id": "1904.12550v1", "chunk_index": 11, "filename": "1904.12550v1.pdf", "chunk_length": 30}}
{"id": "1904.12848v6_chunk_0", "text": "Unsupervised Data Augmentation for Consistency Training Qizhe Xie1,2, Zihang Dai1,2, Eduard Hovy2, Minh-Thang Luong1, Quoc V. Le1 1 Google Research, Brain Team, 2 Carnegie Mellon University {qizhex, dzihang, hovy}@cs.cmu.edu, {thangluong, qvl}@google.com Abstract Semi-supervised learning lately has shown much promise in improving deep learn- ing models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, speciﬁcally those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAug- ment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classiﬁcation dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when ﬁnetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used.1 1 Introduction A fundamental weakness of deep learning is that it typically requires a lot of labeled data to work well. Semi-supervised learning (SSL) [5] is one of the most promising paradigms of leveraging unlabeled data to address this weakness. The recent works in SSL are diverse but those that are based on consistency training [2, 49, 32, 58] have shown to work well on many benchmarks. In a nutshell, consistency training methods simply regularize model predictions to be invariant to small noise applied to either input examples [41, 51, 7] or hidden states [2, 32]. This framework makes sense intuitively because a good model should be robust to any small change in an input example or hidden states. Under this framework, different methods in this category differ mostly in how and where the noise injection is applied. Typical noise injection methods are additive Gaussian noise, dropout noise or adversarial noise. In this work, we investigate the role of noise injection in consistency training and observe that advanced data augmentation methods, speciﬁcally those work best in supervised learning [56, 31, 9, 66], also perform well in semi-supervised learning. There is indeed a strong correlation between the performance of data augmentation operations in supervised learning and their performance in consistency training. We, hence, propose to substitute the traditional noise injection methods with high quality data augmentation methods in order to improve consistency training. To emphasize the 1Code is available at https://github.com/google-research/uda. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1904.12848v6 [cs.LG] 5 Nov 2020 use of better data augmentation in consistency training, we name our method Unsupervised", "meta": {"paper_id": "1904.12848v6", "chunk_index": 0, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_1", "text": "methods with high quality data augmentation methods in order to improve consistency training. To emphasize the 1Code is available at https://github.com/google-research/uda. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. arXiv:1904.12848v6 [cs.LG] 5 Nov 2020 use of better data augmentation in consistency training, we name our method Unsupervised Data Augmentation or UDA. We evaluate UDA on a wide variety of language and vision tasks. On six text classiﬁcation tasks, our method achieves signiﬁcant improvements over state-of-the-art models. Notably, on IMDb, UDA with 20 labeled examples outperforms the state-of-the-art model trained on 1250x more labeled data. On standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all existing semi-supervised learning methods by signiﬁcant margins and achieves an error rate of 5.43 and 2.72 with 250 labeled examples respectively. Finally, we also ﬁnd UDA to be beneﬁcial when there is a large amount of supervised data. For instance, on ImageNet, UDA leads to improvements of top-1 accuracy from 58.84 to 68.78 with 10% of the labeled set and from 78.43 to 79.05 when we use the full labeled set and an external dataset with 1.3M unlabeled examples. Our key contributions and ﬁndings can be summarized as follows: • First, we show that state-of-the-art data augmentations found in supervised learning can also serve as a superior source of noise under the consistency enforcing semi-supervised framework. See results in Table 1 and Table 2. • Second, we show that UDA can match and even outperform purely supervised learning that uses orders of magnitude more labeled data. See results in Table 4 and Figure 4. State-of-the-art results for both vision and language tasks are reported in Table 3 and 4. The effectiveness of UDA across different training data sizes are highlighted in Figure 4 and 7. • Third, we show that UDA combines well with transfer learning, e.g., when ﬁne-tuning from BERT (see Table 4), and is effective at high-data regime, e.g. on ImageNet (see Table 5). • Lastly, we also provide a theoretical analysis of how UDA improves the classiﬁcation performance and the corresponding role of the state-of-the-art augmentation in Section 3. 2 Unsupervised Data Augmentation (UDA) In this section, we ﬁrst formulate our task and then present the key method and insights behind UDA. Throughout this paper, we focus on classiﬁcation problems and will use x to denote the input and y∗to denote its ground-truth prediction target. We are interested in learning a model pθ(y | x) to predict y∗based on the input x, where θ denotes the model parameters. Finally, we will use pL(x) and pU(x) to denote the distributions of labeled and unlabeled examples respectively and use f ∗to denote the perfect classiﬁer that we hope to learn. 2.1 Background: Supervised Data Augmentation Data augmentation aims at creating novel and realistic-looking training data by applying a trans- formation to an example, without changing its label. Formally, let q(ˆx | x) be the augmentation transformation from which one can draw augmented examples ˆx based on an original example x. For an augmentation transformation to be valid, it is required that any example", "meta": {"paper_id": "1904.12848v6", "chunk_index": 1, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_2", "text": "training data by applying a trans- formation to an example, without changing its label. Formally, let q(ˆx | x) be the augmentation transformation from which one can draw augmented examples ˆx based on an original example x. For an augmentation transformation to be valid, it is required that any example ˆx ∼q(ˆx | x) drawn from the distribution shares the same ground-truth label as x. Given a valid augmentation transformation, we can simply minimize the negative log-likelihood on augmented examples. Supervised data augmentation can be equivalently seen as constructing an augmented labeled set from the original supervised set and then training the model on the augmented set. Therefore, the augmented set needs to provide additional inductive biases to be more effective. How to design the augmentation transformation has, thus, become critical. In recent years, there have been signiﬁcant advancements on the design of data augmentations for NLP [66], vision [31, 9] and speech [17, 45] in supervised settings. Despite the promising results, data augmentation is mostly regarded as the “cherry on the cake” which provides a steady but limited performance boost because these augmentations has so far only been applied to a set of labeled examples which is usually of a small size. Motivated by this limitation, via the consistency training framework, we extend the advancement in supervised data augmentation to semi-supervised learning where abundant unlabeled data is available. 2.2 Unsupervised Data Augmentation As discussed in the introduction, a recent line of work in semi-supervised learning has been utilizing unlabeled examples to enforce smoothness of the model. The general form of these works can be summarized as follows: 2 Labeled Data Unsupervised Consistency Loss Supervised Cross-entropy Loss Unlabeled Data x x* Augmentations Back translation RandAugment TF-IDF word replacement Final Loss x y∗ $% & () $%+ & () $% & (*) M M M Figure 1: Training objective for UDA, where M is a model that predicts a distribution of y given x. • Given an input x, compute the output distribution pθ(y | x) given x and a noised version pθ(y | x, ϵ) by injecting a small noise ϵ. The noise can be applied to x or hidden states. • Minimize a divergence metric between the two distributions D (pθ(y | x) ∥pθ(y | x, ϵ)). This procedure enforces the model to be insensitive to the noise ϵ and hence smoother with respect to changes in the input (or hidden) space. From another perspective, minimizing the consistency loss gradually propagates label information from labeled examples to unlabeled ones. In this work, we are interested in a particular setting where the noise is injected to the input x, i.e., ˆx = q(x, ϵ), as considered by prior works [51, 32, 41]. But different from existing work, we focus on the unattended question of how the form or “quality” of the noising operation q can inﬂuence the performance of this consistency training framework. Speciﬁcally, to enforce consistency, prior methods generally employ simple noise injection methods such as adding Gaussian noise, simple input augmentations to noise unlabeled examples. In contrast, we hypothesize that", "meta": {"paper_id": "1904.12848v6", "chunk_index": 2, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_3", "text": "question of how the form or “quality” of the noising operation q can inﬂuence the performance of this consistency training framework. Speciﬁcally, to enforce consistency, prior methods generally employ simple noise injection methods such as adding Gaussian noise, simple input augmentations to noise unlabeled examples. In contrast, we hypothesize that stronger data augmentations in supervised learning can also lead to superior performance when used to noise unlabeled examples in the semi-supervised consistency training framework, since it has been shown that more advanced data augmentations that are more diverse and natural can lead to signiﬁcant performance gain in the supervised setting. Following this idea, we propose to use a rich set of state-of-the-art data augmentations veriﬁed in various supervised settings to inject noise and optimize the same consistency training objective on unlabeled examples. When jointly trained with labeled examples, we utilize a weighting factor λ to balance the supervised cross entropy and the unsupervised consistency training loss, which is illustrated in Figure 1. Formally, the full objective can be written as follows: min θ J (θ) = Ex1∼pL(x) [−log pθ(f ∗(x1) | x1)]+λEx2∼pU(x)Eˆx∼q(ˆx|x2) \u0002 CE \u0000p˜θ(y | x2)∥pθ(y | ˆx) \u0001\u0003 (1) where CE denotes cross entropy, q(ˆx | x) is a data augmentation transformation and ˜θ is a ﬁxed copy of the current parameters θ indicating that the gradient is not propagated through ˜θ, as suggested by VAT [41]. We set λ to 1 for most of our experiments. In practice, in each iteration, we compute the supervised loss on a mini-batch of labeled examples and compute the consistency loss on a mini-batch of unlabeled data. The two losses are then summed for the ﬁnal loss. We use a larger batch size for the consistency loss. In the vision domain, simple augmentations including cropping and ﬂipping are applied to labeled examples. To minimize the discrepancy between supervised training and prediction on unlabeled examples, we apply the same simple augmentations to unlabeled examples for computing p˜θ(y | x). Discussion. Before detailing the augmentation operations used in this work, we ﬁrst provide some intuitions on how more advanced data augmentations can provide extra advantages over simple ones used in earlier works from three aspects: • Valid noise: Advanced data augmentation methods that achieve great performance in supervised learning usually generate realistic augmented examples that share the same ground-truth labels with the original example. Thus, it is safe to encourage the consistency between predictions on the original unlabeled example and the augmented unlabeled examples. • Diverse noise: Advanced data augmentation can generate a diverse set of examples since it can make large modiﬁcations to the input example without changing its label, while simple Gaussian noise only make local changes. Encouraging consistency on a diverse set of augmented examples can signiﬁcantly improve the sample efﬁciency. 3 • Targeted inductive biases: Different tasks require different inductive biases. Data augmentation operations that work well in supervised training essentially provides the missing inductive biases. 2.3 Augmentation Strategies for Different Tasks We now detail the augmentation methods, tailored for different tasks, that we use in this work. RandAugment for Image", "meta": {"paper_id": "1904.12848v6", "chunk_index": 3, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_4", "text": "• Targeted inductive biases: Different tasks require different inductive biases. Data augmentation operations that work well in supervised training essentially provides the missing inductive biases. 2.3 Augmentation Strategies for Different Tasks We now detail the augmentation methods, tailored for different tasks, that we use in this work. RandAugment for Image Classiﬁcation. We use a data augmentation method called RandAug- ment [10], which is inspired by AutoAugment [9]. AutoAugment uses a search method to combine all image processing transformations in the Python Image Library (PIL) to ﬁnd a good augmentation strategy. In RandAugment, we do not use search, but instead uniformly sample from the same set of augmentation transformations in PIL. In other words, RandAugment is simpler and requires no labeled data as there is no need to search for optimal policies. Back-translation for Text Classiﬁcation. When used as an augmentation method, back- translation [54, 15] refers to the procedure of translating an existing example x in language A into another language B and then translating it back into A to obtain an augmented example ˆx. As observed by [66], back-translation can generate diverse paraphrases while preserving the semantics of the original sentences, leading to signiﬁcant performance improvements in question answering. In our case, we use back-translation to paraphrase the training data of our text classiﬁcation tasks.2 We ﬁnd that the diversity of the paraphrases is important. Hence, we employ random sampling with a tunable temperature instead of beam search for the generation. As shown in Figure 2, the paraphrases generated by back-translation sentence are diverse and have similar semantic meanings. More speciﬁcally, we use WMT’14 English-French translation models (in both directions) to perform back- translation on each sentence. To facilitate future research, we have open-sourced our back-translation system together with the translation checkpoints. Back-translation Given the low budget and production limitations, this movie is very good. Since it was highly limited in terms of budget, and the production restrictions, the film was cheerful. There are few budget items and production limitations to make this film a really good one. Due to the small dollar amount and production limitations the ouest film is very beautiful. RandAugment Figure 2: Augmented examples using back-translation and RandAugment. Word replacing with TF-IDF for Text Classiﬁcation. While back-translation is good at maintaining the global semantics of a sentence, there is little control over which words will be retained. This requirement is important for topic classiﬁcation tasks, such as DBPedia, in which some keywords are more informative than other words in determining the topic. We, therefore, propose an augmentation method that replaces uninformative words with low TF-IDF scores while keeping those with high TF-IDF values. We refer readers to Appendix A.2 for a detailed description. 2.4 Additional Training Techniques In this section, we present additional techniques targeting at some commonly encountered problems. Conﬁdence-based masking. We ﬁnd it to be helpful to mask out examples that the current model is not conﬁdent about. Speciﬁcally, in each minibatch, the consistency loss term is computed only on examples whose highest probability among classiﬁcation categories is greater than a threshold β. We set", "meta": {"paper_id": "1904.12848v6", "chunk_index": 4, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_5", "text": "commonly encountered problems. Conﬁdence-based masking. We ﬁnd it to be helpful to mask out examples that the current model is not conﬁdent about. Speciﬁcally, in each minibatch, the consistency loss term is computed only on examples whose highest probability among classiﬁcation categories is greater than a threshold β. We set the threshold β to a high value. Speciﬁcally, β is set to 0.8 for CIFAR-10 and SVHN and 0.5 for ImageNet. 2We also note that while translation uses a labeled dataset, the translation task itself is quite distinctive from a text classiﬁcation task and does not make use of any text classiﬁcation label. In addition, back-translation is a general data augmentation method that can be applied to many tasks with the same model checkpoints. 4 Sharpening Predictions. Since regularizing the predictions to have low entropy has been shown to be beneﬁcial [16, 41], we sharpen predictions when computing the target distribution on unlabeled examples by using a low Softmax temperature τ. When combined with conﬁdence-based masking, the loss on unlabeled examples Ex∼pU(x)Eˆx∼q(ˆx|x) \u0002 CE \u0000p˜θ(y | x)∥pθ(y | ˆx) \u0001\u0003 on a minibatch B is computed as: 1 |B| X x∈B I(max y′ p˜θ(y′ | x) > β)CE \u0010 p(sharp) ˜θ (y | x)∥pθ(y | ˆx) \u0011 p(sharp) ˜θ (y | x) = exp(zy/τ) P y′ exp(zy′/τ) where I(·) is the indicator function, zy is the logit of label y for example x. We set τ to 0.4 for CIFAR-10, SVHN and ImageNet. Domain-relevance Data Filtering. Ideally, we would like to make use of out-of-domain unlabeled data since it is usually much easier to collect, but the class distributions of out-of-domain data are mismatched with those of in-domain data, which can result in performance loss if directly used [44]. To obtain data relevant to the domain for the task at hand, we adopt a common technique for detecting out-of-domain data. We use our baseline model trained on the in-domain data to infer the labels of data in a large out-of-domain dataset and pick out examples that the model is most conﬁdent about. Speciﬁcally, for each category, we sort all examples based on the classiﬁed probabilities of being in that category and select the examples with the highest probabilities. 3 Theoretical Analysis In this section, we theoretically analyze why UDA can improve the performance of a model and the required number of labeled examples to achieve a certain error rate. Following previous sections, we will use f ∗to denote the perfect classiﬁer that we hope to learn, use pU to denote the marginal distribution of the unlabeled data and use q(ˆx | x) to denote the augmentation distribution. To make the analysis tractable, we make the following simplistic assumptions about the data augmen- tation transformation: • In-domain augmentation: data examples generated by data augmentation have non-zero probability under pU, i.e., pU(ˆx) > 0 for ˆx ∼q(ˆx | x), x ∼pU(x). • Label-preserving augmentation: data augmentation preserves the label of the original example, i.e., f ∗(x) = f ∗(ˆx) for ˆx ∼q(ˆx | x), x ∼pU(x). • Reversible augmentation: the data augmentation operation can", "meta": {"paper_id": "1904.12848v6", "chunk_index": 5, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_6", "text": "augmentation have non-zero probability under pU, i.e., pU(ˆx) > 0 for ˆx ∼q(ˆx | x), x ∼pU(x). • Label-preserving augmentation: data augmentation preserves the label of the original example, i.e., f ∗(x) = f ∗(ˆx) for ˆx ∼q(ˆx | x), x ∼pU(x). • Reversible augmentation: the data augmentation operation can be reversed, i.e., if q(ˆx | x) > 0 then q(x | ˆx) > 0 . As the ﬁrst step, we hope to provide an intuitive sketch of our formal analysis. Let us deﬁne a graph GpU where each node corresponds to a data sample x ∈X and an edge (ˆx, x) exists in the graph if and only if q(ˆx | x) > 0. Due to the label-preserving assumption, it is easy to see that examples with different labels must reside on different components (disconnected sub-graphs) of the graph GpU . Hence, for an N-category classiﬁcation problems, the graph has N components (sub-graphs) when all examples within each category can be traversed by the augmentation operation. Otherwise, the graph will have more than N components. Given this construction, notice that for each component Ci of the graph, as long as there is a single labeled example in the component, i.e. (x∗, y∗) ∈Ci, one can propagate the label of the node to the rest of the nodes in Ci by traversing Ci via the augmentation operation q(ˆx | x). More importantly, if one only performs supervised data augmentation, one can only propagate the label information to the directly connected neighbors of the labeled node. In contrast, performing unsupervised data augmentation ensures the traversal of the entire sub-graph Ci. This provides the ﬁrst high-level intuition how UDA could help. Taking one step further, in order to ﬁnd a perfect classiﬁer via such label propagation, it requires that there exists at least one labeled example in each component. In other words, the number of components lower bounds the minimum amount of labeled examples needed to learn a perfect classiﬁer. Importantly, number of components is actually decided by the quality of the augmentation operation: an ideal augmentation should be able to reach all other examples of the same category given a starting instance. This well matches our discussion of the beneﬁts of state-of-the-art data 5 augmentation methods in generating more diverse examples. Effectively, the augmentation diversity leads to more neighbors for each node, and hence reduces the number of components in a graph. Since supervised data augmentation only propagates the label information to the directly connected neighbors of the labeled nodes. Advanced data augmentation that has a high accuracy must lead to a graph where each node has more neighbors. Effectively, such a graph has more edges and better connectivity. Hence, it is also more likely that this graph will have a smaller number of components. To further illustrate this intuition, in Figure 3, we provide a comparison between different algorithms. 6 (a) Supervised learn- ing. (4/15) 2 (b) Advanced super- vised augmentation. (9/15) 3 (c) UDA with ad- vanced augmentation. (15/15) 4 (d) Simple super- vised augmentation. (7/15) 5 (e) UDA with simple", "meta": {"paper_id": "1904.12848v6", "chunk_index": 6, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_7", "text": "of components. To further illustrate this intuition, in Figure 3, we provide a comparison between different algorithms. 6 (a) Supervised learn- ing. (4/15) 2 (b) Advanced super- vised augmentation. (9/15) 3 (c) UDA with ad- vanced augmentation. (15/15) 4 (d) Simple super- vised augmentation. (7/15) 5 (e) UDA with simple augmentation. (10/15) Figure 3: Prediction results of different settings, where green and red nodes are labeled nodes, white nodes are unlabeled nodes whose labels cannot be determined and light green nodes and light red nodes are unlabeled nodes whose labels can be correctly determined. The accuracy of different settings are shown in (·). With the intuition described, we state our formal results. Without loss of generality, assume there are k components in the graph. For each component Ci(i = 1, . . . , k), let Pi be the total probability mass that an observed labeled example fall into the i-th component, i.e., Pi = P x∈Ci pL(x). The following theorem characterizes the relationship between UDA error rate and the amount of labeled examples. Theorem 1. Under UDA, let Pr(A) denote the probability that the algorithm cannot infer the label of a new test example given m labeled examples from PL. Pr(A) is given by Pr(A) = X i Pi(1 −Pi)m. In addition, O(k/ϵ) labeled examples can guarantee an error rate of O(ϵ), i.e., m = O(k/ϵ) =⇒Pr(A) = O(ϵ). Proof. Please see Appendix. C for details. From the theorem, we can see the number of components, i.e. k, directly governs the amount of labeled data required to reach a desired performance. As we have discussed above, the number of components effectively relies on the quality of an augmentation function, where better augmentation functions result in fewer components. This echoes our discussion of the beneﬁts of state-of-the-art data augmentation operations in generating more diverse examples. Hence, with state-of-the-art augmentation operations, UDA is able to achieve good performance using fewer labeled examples. 4 Experiments In this section, we evaluate UDA on a variety of language and vision tasks. For language, we rely on six text classiﬁcation benchmark datasets, including IMDb, Yelp-2, Yelp-5, Amazon-2 and Amazon-5 sentiment classiﬁcation and DBPedia topic classiﬁcation [37, 71]. For vision, we employ two smaller datasets CIFAR-10 [30], SVHN [43], which are often used to compare semi-supervised algorithms, as well as ImageNet [13] of a larger scale to test the scalability of UDA. For ablation studies and experiment details, we refer readers to Appendix B and Appendix E. 4.1 Correlation between Supervised and Semi-supervised Performances As the ﬁrst step, we try to verify the fundamental idea of UDA, i.e., there is a positive correlation of data augmentation’s effectiveness in supervised learning and semi-supervised learning. Based on Yelp-5 (a language task) and CIFAR-10 (a vision task), we compare the performance of different data augmentation methods in either fully supervised or semi-supervised settings. For Yelp-5, apart from back-translation, we include a simpler method Switchout [61] which replaces a token with a random 6 Augmentation Sup Semi-Sup (# Sup examples) (50k) (4k) Crop & ﬂip 5.36 10.94 Cutout 4.42 5.43 RandAugment 4.23", "meta": {"paper_id": "1904.12848v6", "chunk_index": 7, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_8", "text": "of different data augmentation methods in either fully supervised or semi-supervised settings. For Yelp-5, apart from back-translation, we include a simpler method Switchout [61] which replaces a token with a random 6 Augmentation Sup Semi-Sup (# Sup examples) (50k) (4k) Crop & ﬂip 5.36 10.94 Cutout 4.42 5.43 RandAugment 4.23 4.32 Table 1: Error rates on CIFAR-10. Augmentation Sup Semi-sup (# Sup examples) (650k) (2.5k) \u0017 38.36 50.80 Switchout 37.24 43.38 Back-translation 36.71 41.35 Table 2: Error rate on Yelp-5. token uniformly sampled from the vocabulary. For CIFAR-10, we compare RandAugment with two simpler methods: (1) cropping & ﬂipping augmentation and (2) Cutout. Based on this setting, Table 1 and Table 2 exhibit a strong correlation of an augmentation’s effec- tiveness between supervised and semi-supervised settings. This validates our idea of stronger data augmentations found in supervised learning can always lead to more gains when applied to the semi-supervised learning settings. 4.2 Algorithm Comparison on Vision Semi-supervised Learning Benchmarks With the correlation established above, the next question we ask is how well UDA performs compared to existing semi-supervised learning algorithms. To answer the question, we focus on the most commonly used semi-supervised learning benchmarks CIFAR-10 and SVHN. Vary the size of labeled data. Firstly, we follow the settings in [44] and employ Wide-ResNet-28- 2 [67, 18] as the backbone model and evaluate UDA with varied supervised data sizes. Speciﬁcally, we compare UDA with two highly competitive baselines: (1) Virtual adversarial training (VAT) [41], an algorithm that generates adversarial Gaussian noise on input, and (2) MixMatch [3], a parallel work that combines previous advancements in semi-supervised learning. The comparison is shown in Figure 4 with two key observations. • First, UDA consistently outperforms the two baselines given different sizes of labeled data. • Moreover, the performance difference between UDA and VAT shows the superiority of data augmentation based noise. The difference of UDA and VAT is essentially the noise process. While the noise produced by VAT often contain high-frequency artifacts that do not exist in real images, data augmentation mostly generates diverse and realistic images. 250 500 1000 2000 4000 # Labeled examples 5.0 10.0 15.0 20.0 25.0 30.0 35.0 40.0 Error rate VAT MixMatch UDA (RandAugment) (a) CIFAR-10 250 500 1000 2000 4000 # Labeled examples 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 Error rate VAT MixMatch UDA (RandAugment) (b) SVHN Figure 4: Comparison with two semi-supervised learning methods on CIFAR-10 and SVHN with varied number of labeled examples. Vary model architecture. Next, we directly compare UDA with previously published results under different model architectures. Following previous work, 4k and 1k labeled examples are used for CIFAR-10 and SVHN respectively. As shown in Table 3, given the same architecture, UDA outperforms all published results by signiﬁcant margins and nearly matches the fully supervised performance, which uses 10x more labeled examples. This shows the huge potential of state-of-the-art data augmentations under the consistency training framework in the vision domain. 4.3 Evaluation on Text Classiﬁcation Datasets Next, we further evaluate UDA in the language domain. Moreover, in order to test whether UDA can", "meta": {"paper_id": "1904.12848v6", "chunk_index": 8, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_9", "text": "fully supervised performance, which uses 10x more labeled examples. This shows the huge potential of state-of-the-art data augmentations under the consistency training framework in the vision domain. 4.3 Evaluation on Text Classiﬁcation Datasets Next, we further evaluate UDA in the language domain. Moreover, in order to test whether UDA can be combined with the success of unsupervised representation learning, such as BERT [14], we further consider four initialization schemes: (a) random Transformer; (b) BERTBASE; (c) BERTLARGE; (d) 7 Method Model # Param CIFAR-10 (4k) SVHN (1k) Π-Model [32] Conv-Large 3.1M 12.36 ± 0.31 4.82 ± 0.17 Mean Teacher [58] Conv-Large 3.1M 12.31 ± 0.28 3.95 ± 0.19 VAT + EntMin [41] Conv-Large 3.1M 10.55 ± 0.05 3.86 ± 0.11 SNTG [35] Conv-Large 3.1M 10.93 ± 0.14 3.86 ± 0.27 ICT [60] Conv-Large 3.1M 7.29 ± 0.02 3.89 ± 0.04 Pseudo-Label [33] WRN-28-2 1.5M 16.21 ± 0.11 7.62 ± 0.29 LGA + VAT [25] WRN-28-2 1.5M 12.06 ± 0.19 6.58 ± 0.36 ICT [60] WRN-28-2 1.5M 7.66 ± 0.17 3.53 ± 0.07 MixMatch [3] WRN-28-2 1.5M 6.24 ± 0.06 2.89 ± 0.06 Mean Teacher [58] Shake-Shake 26M 6.28 ± 0.15 - Fast-SWA [1] Shake-Shake 26M 5.0 - MixMatch [3] WRN 26M 4.95 ± 0.08 - UDA (RandAugment) WRN-28-2 1.5M 4.32 ± 0.08 2.23 ± 0.07 UDA (RandAugment) Shake-Shake 26M 3.7 - UDA (RandAugment) PyramidNet 26M 2.7 - Table 3: Comparison between methods using different models where PyramidNet is used with ShakeDrop regularization. On CIFAR-10, with only 4,000 labeled examples, UDA matches the performance of fully supervised Wide-ResNet-28-2 and PyramidNet+ShakeDrop, where they have an error rate of 5.4 and 2.7 respectively when trained on 50,000 examples without RandAugment. On SVHN, UDA also matches the performance of our fully supervised model trained on 73,257 examples without RandAugment, which has an error rate of 2.84. BERTFINETUNE: BERTLARGE ﬁne-tuned on in-domain unlabeled data3. Under each of these four initialization schemes, we compare the performances with and without UDA. Fully supervised baseline Datasets IMDb Yelp-2 Yelp-5 Amazon-2 Amazon-5 DBpedia (# Sup examples) (25k) (560k) (650k) (3.6m) (3m) (560k) Pre-BERT SOTA 4.32 2.16 29.98 3.32 34.81 0.70 BERTLARGE 4.51 1.89 29.32 2.63 34.17 0.64 Semi-supervised setting Initialization UDA IMDb Yelp-2 Yelp-5 Amazon-2 Amazon-5 DBpedia (20) (20) (2.5k) (20) (2.5k) (140) Random \u0017 43.27 40.25 50.80 45.39 55.70 41.14 \u0013 25.23 8.33 41.35 16.16 44.19 7.24 BERTBASE \u0017 18.40 13.60 41.00 26.75 44.09 2.58 \u0013 5.45 2.61 33.80 3.96 38.40 1.33 BERTLARGE \u0017 11.72 10.55 38.90 15.54 42.30 1.68 \u0013 4.78 2.50 33.54 3.93 37.80 1.09 BERTFINETUNE \u0017 6.50 2.94 32.39 12.17 37.32 - \u0013 4.20 2.05 32.08 3.50 37.12 - Table 4: Error rates on text classiﬁcation datasets. In the fully supervised settings, the pre-BERT SOTAs include ULMFiT [23] for Yelp-2 and Yelp-5, DPCNN [26] for Amazon-2 and Amazon-5, Mixed VAT [50] for IMDb and DBPedia. All of our experiments use a sequence length of 512. The results are presented in Table 4 where we would like to emphasize three observations: • First, even with very few labeled examples, UDA can offer decent or even competitive performances compared to", "meta": {"paper_id": "1904.12848v6", "chunk_index": 9, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_10", "text": "Mixed VAT [50] for IMDb and DBPedia. All of our experiments use a sequence length of 512. The results are presented in Table 4 where we would like to emphasize three observations: • First, even with very few labeled examples, UDA can offer decent or even competitive performances compared to the SOTA model trained with full supervised data. Particularly, on binary sentiment analysis tasks, with only 20 supervised examples, UDA outperforms the previous SOTA trained with full supervised data on IMDb and is competitive on Yelp-2 and Amazon-2. • Second, UDA is complementary to transfer learning / representation learning. As we can see, when initialized with BERT and further ﬁnetuned on in-domain data, UDA can still signiﬁcantly reduce the error rate from 6.50 to 4.20 on IMDb. • Finally, we also note that for ﬁve-category sentiment classiﬁcation tasks, there still exists a clear gap between UDA with 500 labeled examples per class and BERT trained on the entire supervised 3One exception is that we do not pursue BERTFINETUNE on DBPedia as ﬁne-tuning BERT on DBPedia does not yield further performance gain. This is probably due to the fact that DBPedia is based on Wikipedia while BERT is already trained on the whole Wikipedia corpus. 8 set. Intuitively, ﬁve-category sentiment classiﬁcations are much more difﬁcult than their binary counterparts. This suggests a room for further improvement in the future. 4.4 Scalability Test on the ImageNet Dataset Then, to evaluate whether UDA can scale to problems with a large scale and a higher difﬁculty, we now turn to the ImageNet dataset with ResNet-50 being the underlying architecture. Speciﬁcally, we consider two experiment settings with different natures: • We use 10% of the supervised data of ImageNet while using all other data as unlabeled data. As a result, the unlabeled exmaples are entirely in-domain. • In the second setting, we keep all images in ImageNet as supervised data. Then, we use the domain-relevance data ﬁltering method to ﬁlter out 1.3M images from JFT [22, 6]. Hence, the unlabeled set is not necessarily in-domain. The results are summarized in Table 5. In both 10% and the full data settings, UDA consistently brings signiﬁcant gains compared to the supervised baseline. This shows UDA is not only able to scale but also able to utilize out-of-domain unlabeled examples to improve model performance. In parallel to our work, S4L [69] and CPC [20] also show signiﬁcant improvements on ImageNet. Methods SSL 10% 100% ResNet-50 \u0017 55.09 / 77.26 77.28 / 93.73 w. RandAugment 58.84 / 80.56 78.43 / 94.37 UDA (RandAugment) \u0013 68.78 / 88.80 79.05 / 94.49 Table 5: Top-1 / top-5 accuracy on ImageNet with 10% and 100% of the labeled set. We use image size 224 and 331 for the 10% and 100% experiments respectively. 5 Related Work Existing works in consistency training does make use of data augmentation [32, 51]; however, they only apply weak augmentation methods such as random translations and cropping. In parallel to our work, ICT [60] and MixMatch [3] also show improvements for semi-supervised learning. These methods employ mixup [70] on", "meta": {"paper_id": "1904.12848v6", "chunk_index": 10, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_11", "text": "Related Work Existing works in consistency training does make use of data augmentation [32, 51]; however, they only apply weak augmentation methods such as random translations and cropping. In parallel to our work, ICT [60] and MixMatch [3] also show improvements for semi-supervised learning. These methods employ mixup [70] on top of simple augmentations such as ﬂipping and cropping; instead, UDA emphasizes on the use of state-of-the-art data augmentations, leading to signiﬁcantly better results on CIFAR-10 and SVHN. In addition, UDA is also applicable to language domain and can also scale well to more challenging vision datasets, such as ImageNet. Other works in the consistency training family mostly differ in how the noise is deﬁned: Pseudo- ensemble [2] directly applies Gaussian noise and Dropout noise; VAT [41, 40] deﬁnes the noise by approximating the direction of change in the input space that the model is most sensitive to; Cross-view training [7] masks out part of the input data. Apart from enforcing consistency on the input examples and the hidden representations, another line of research enforces consistency on the model parameter space. Works in this category include Mean Teacher [58], fast-Stochastic Weight Averaging [1] and Smooth Neighbors on Teacher Graphs [35]. For a complete version of related work, please refer to Appendix D. 6 Conclusion In this paper, we show that data augmentation and semi-supervised learning are well connected: better data augmentation can lead to signiﬁcantly better semi-supervised learning. Our method, UDA, employs state-of-the-art data augmentation found in supervised learning to generate diverse and realistic noise and enforces the model to be consistent with respect to these noise. For text, UDA combines well with representation learning, e.g., BERT. For vision, UDA outperforms prior works by a clear margin and nearly matches the performance of the fully supervised models trained on the full labeled sets which are one order of magnitude larger. We hope that UDA will encourage future research to transfer advanced supervised augmentation to semi-supervised setting for different tasks. 9 Acknowledgements We want to thank Hieu Pham, Adams Wei Yu, Zhilin Yang and Ekin Dogus Cubuk for their tireless help to the authors on different stages of this project and thank Colin Raffel for pointing out the connections between our work and previous works. We also would like to thank Olga Wichrowska, Barret Zoph, Jiateng Xie, Guokun Lai, Yulun Du, Chen Dan, David Berthelot, Avital Oliver, Trieu Trinh, Ran Zhao, Ola Spyra, Brandon Yang, Daiyi Peng, Andrew Dai, Samy Bengio, Jeff Dean and the Google Brain team for insightful discussions and support to the work. Lastly, we thank anonymous reviewers for their valueable feedbacks. Broader Impact This work show that it is possible to achieve great performance with limited labeled data. Hence groups/institutes with limited budgets for annotating data may beneﬁt from this research. To the best of our knowledge, nobody will be put at disadvantage from this research. Our method does not leverage biases in the data. Our tasks include standard benchmarks such as IMDb, CIFAR-10, SVHN and ImageNet. References [1] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson.", "meta": {"paper_id": "1904.12848v6", "chunk_index": 11, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_12", "text": "this research. To the best of our knowledge, nobody will be put at disadvantage from this research. Our method does not leverage biases in the data. Our tasks include standard benchmarks such as IMDb, CIFAR-10, SVHN and ImageNet. References [1] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and Andrew Gordon Wilson. There are many consistent explanations of unlabeled data: Why you should average. ICLR, 2019. [2] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. In Advances in Neural Information Processing Systems, pages 3365–3373, 2014. [3] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint arXiv:1905.02249, 2019. [4] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C Duchi. Unlabeled data improves adversarial robustness. arXiv preprint arXiv:1905.13736, 2019. [5] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. IEEE Transactions on Neural Networks, 20(3):542–542, 2009. [6] François Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 1251–1258, 2017. [7] Kevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc V Le. Semi-supervised sequence modeling with cross-view training. arXiv preprint arXiv:1809.08370, 2018. [8] Ronan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167. ACM, 2008. [9] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501, 2018. [10] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical data augmentation with no separate search. arXiv preprint arXiv:1909.13719, 2019. [11] Andrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural information processing systems, pages 3079–3087, 2015. [12] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov. Good semi-supervised learning that requires a bad gan. In Advances in Neural Information Processing Systems, pages 6510–6520, 2017. [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255. Ieee, 2009. 10 [14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [15] Sergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at scale. arXiv preprint arXiv:1808.09381, 2018. [16] Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In Advances in neural information processing systems, pages 529–536, 2005. [17] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. [19] Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. Sequence to sequence", "meta": {"paper_id": "1904.12848v6", "chunk_index": 12, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_13", "text": "up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014. [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. [19] Xuanli He, Gholamreza Haffari, and Mohammad Norouzi. Sequence to sequence mixture model for diverse machine translation. arXiv preprint arXiv:1810.07391, 2018. [20] Olivier J Hénaff, Ali Razavi, Carl Doersch, SM Eslami, and Aaron van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv preprint arXiv:1905.09272, 2019. [21] Alex Hernández-García and Peter König. Data augmentation instead of explicit regularization. arXiv preprint arXiv:1806.03852, 2018. [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. [23] Jeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classi- ﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 328–339, 2018. [24] Weihua Hu, Takeru Miyato, Seiya Tokui, Eiichi Matsumoto, and Masashi Sugiyama. Learning discrete representations via information maximizing self-augmented training. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1558–1567. JMLR. org, 2017. [25] Jacob Jackson and John Schulman. Semi-supervised learning by label gradient alignment. arXiv preprint arXiv:1902.02336, 2019. [26] Rie Johnson and Tong Zhang. Deep pyramid convolutional neural networks for text catego- rization. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 562–570, 2017. [27] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. In Advances in neural information processing systems, pages 3581–3589, 2014. [28] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. [29] Wouter Kool, Herke van Hoof, and Max Welling. Stochastic beams and where to ﬁnd them: The gumbel-top-k trick for sampling sequences without replacement. arXiv preprint arXiv:1903.06059, 2019. [30] Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009. [31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [32] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint arXiv:1610.02242, 2016. [33] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. In Workshop on Challenges in Representation Learning, ICML, volume 3, page 2, 2013. 11 [34] Davis Liang, Zhiheng Huang, and Zachary C Lipton. Learning noise-invariant representations for robust speech recognition. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 56–63. IEEE, 2018. [35] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors on teacher graphs for semi-supervised learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8896–8905, 2018. [36] Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473, 2016. [37] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for", "meta": {"paper_id": "1904.12848v6", "chunk_index": 13, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_14", "text": "Computer Vision and Pattern Recognition, pages 8896–8905, 2018. [36] Lars Maaløe, Casper Kaae Sønderby, Søren Kaae Sønderby, and Ole Winther. Auxiliary deep generative models. arXiv preprint arXiv:1602.05473, 2016. [37] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1, pages 142–150. Association for Computational Linguistics, 2011. [38] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based recommendations on styles and substitutes. In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 43–52. ACM, 2015. [39] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre- sentations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013. [40] Takeru Miyato, Andrew M Dai, and Ian Goodfellow. Adversarial training methods for semi- supervised text classiﬁcation. arXiv preprint arXiv:1605.07725, 2016. [41] Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori Koyama. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE transactions on pattern analysis and machine intelligence, 2018. [42] Amir Najaﬁ, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial perturbations in learning from incomplete data. arXiv preprint arXiv:1905.13021, 2019. [43] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011. [44] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms. In Advances in Neural Information Processing Systems, pages 3235–3246, 2018. [45] Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. [46] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532–1543, 2014. [47] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Ken- ton Lee, and Luke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365, 2018. [48] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai- assets/research-covers/languageunsupervised/language understanding paper. pdf, 2018. [49] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri Valpola, and Tapani Raiko. Semi- supervised learning with ladder networks. In Advances in neural information processing systems, pages 3546–3554, 2015. [50] Devendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for semi-supervised text classiﬁcation via mixed objective function. 2018. [51] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with stochastic transfor- mations and perturbations for deep semi-supervised learning. In Advances in Neural Information Processing Systems, pages 1163–1171, 2016. [52] Julian Salazar, Davis Liang, Zhiheng Huang, and Zachary C Lipton. Invariant representation learning for robust deep networks. In Workshop on Integration of Deep Learning Theories, NeurIPS, 2018. 12 [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi", "meta": {"paper_id": "1904.12848v6", "chunk_index": 14, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_15", "text": "in Neural Information Processing Systems, pages 1163–1171, 2016. [52] Julian Salazar, Davis Liang, Zhiheng Huang, and Zachary C Lipton. Invariant representation learning for robust deep networks. In Workshop on Integration of Deep Learning Theories, NeurIPS, 2018. 12 [53] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 2234–2242, 2016. [54] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation models with monolingual data. arXiv preprint arXiv:1511.06709, 2015. [55] Tianxiao Shen, Myle Ott, Michael Auli, and Marc’Aurelio Ranzato. Mixture models for diverse machine translation: Tricks of the trade. arXiv preprint arXiv:1902.07816, 2019. [56] Patrice Y Simard, Yann A LeCun, John S Denker, and Bernard Victorri. Transformation invariance in pattern recognition—tangent distance and tangent propagation. In Neural networks: tricks of the trade, pages 239–274. Springer, 1998. [57] Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al. Are labels required for improving adversarial robustness? arXiv preprint arXiv:1905.13725, 2019. [58] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems, pages 1195–1204, 2017. [59] Trieu H Trinh, Minh-Thang Luong, and Quoc V Le. Selﬁe: Self-supervised pretraining for image embedding. arXiv preprint arXiv:1906.02940, 2019. [60] Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio, and David Lopez-Paz. Interpolation consistency training for semi-supervised learning. arXiv preprint arXiv:1903.03825, 2019. [61] Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. Switchout: an efﬁcient data augmentation algorithm for neural machine translation. arXiv preprint arXiv:1808.07512, 2018. [62] Jason Weston, Frédéric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi- supervised embedding. In Neural Networks: Tricks of the Trade, pages 639–655. Springer, 2012. [63] Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with graph embeddings. arXiv preprint arXiv:1603.08861, 2016. [64] Zhilin Yang, Junjie Hu, Ruslan Salakhutdinov, and William W Cohen. Semi-supervised qa with generative domain-adaptive nets. arXiv preprint arXiv:1702.02206, 2017. [65] Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6210–6219, 2019. [66] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi, and Quoc V Le. Qanet: Combining local convolution with global self-attention for reading comprehension. arXiv preprint arXiv:1804.09541, 2018. [67] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. BMVC, 2016. [68] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John Hopcroft, and Liwei Wang. Adversar- ially robust generalization just requires more unlabeled data. arXiv preprint arXiv:1906.00555, 2019. [69] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. In Proceedings of the IEEE international conference on computer vision, 2019. [70] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [71] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015. [72] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised", "meta": {"paper_id": "1904.12848v6", "chunk_index": 15, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_16", "text": "Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017. [71] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015. [72] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In Proceedings of the 20th International conference on Machine learning (ICML-03), pages 912–919, 2003. 13 A Extended Method Details In this section, we present some additional details used in our method. We introduce Training Signal Annealing in Appendix A.1 and details for augmentation strategies in Appendix A.2. A.1 Training Signal Annealing for Low-data Regime In semi-supervised learning, we often encounter a situation where there is a huge gap between the amount of unlabeled data and that of labeled data. Hence, the model often quickly overﬁts the limited amount of labeled data while still underﬁtting the unlabeled data. To tackle this difﬁculty, we introduce a new training technique, called Training Signal Annealing (TSA), which gradually releases the “training signals” of the labeled examples as training progresses. Intuitively, we only utilize a labeled example if the model’s conﬁdence on that example is lower than a predeﬁned threshold which increases according to a schedule. Speciﬁcally, at training step t, if the model’s predicted probability for the correct category pθ(y∗| x) is higher than a threshold ηt, we remove that example from the loss function. Suppose K is the number of categories, by gradually increasing ηt from 1 K to 1, the threshold ηt serves as a ceiling to prevent over-training on easy labeled examples. We consider three increasing schedules of ηt with different application scenarios. Let T be the total number of training steps, the three schedules are shown in Figure 5. Intuitively, when the model is prone to overﬁt, e.g., when the problem is relatively easy or the number of labeled examples is very limited, the exp-schedule is most suitable as the supervised signal is mostly released at the end of training. In contrast, when the model is less likely to overﬁt (e.g., when we have abundant labeled examples or when the model employs effective regularization), the log-schedule can serve well. 0.0 0.2 0.4 0.6 0.8 1.0 t T 0.0 0.2 0.4 0.6 0.8 1.0 t log-schedule 0.0 0.2 0.4 0.6 0.8 1.0 t T 0.0 0.2 0.4 0.6 0.8 1.0 t linear-schedule 0.0 0.2 0.4 0.6 0.8 1.0 t T 0.0 0.2 0.4 0.6 0.8 1.0 t exp-schedule Figure 5: Three schedules of TSA. We set ηt = αt ∗(1 −1 K ) + 1 K . αt is set to 1 −exp(−t T ∗5), t T and exp(( t T −1) ∗5) for the log, linear and exp schedules. A.2 Extended Augmentation Strategies for Different Tasks Discussion on Trade-off Between Diversity and Validity for Data Augmentation. Despite that state-of-the-art data augmentation methods can generate diverse and valid augmented examples as discussed in section 2.2, there is a trade-off between diversity and validity since diversity is achieved by changing a part of the original example, naturally", "meta": {"paper_id": "1904.12848v6", "chunk_index": 16, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_17", "text": "Tasks Discussion on Trade-off Between Diversity and Validity for Data Augmentation. Despite that state-of-the-art data augmentation methods can generate diverse and valid augmented examples as discussed in section 2.2, there is a trade-off between diversity and validity since diversity is achieved by changing a part of the original example, naturally leading to the risk of altering the ground-truth label. We ﬁnd it beneﬁcial to tune the trade-off between diversity and validity for data augmentation methods. For text classiﬁcation, we tune the temperature of random sampling. On the one hand, when we use a temperature of 0, decoding by random sampling degenerates into greedy decoding and generates perfectly valid but identical paraphrases. On the other hand, when we use a temperature of 1, random sampling generates very diverse but barely readable paraphrases. We ﬁnd that setting the Softmax temperature to 0.7, 0.8 or 0.9 leads to the best performances. RandAugment Details. In our implementation of RandAugment, each sub-policy is composed of two operations, where each operation is represented by the transformation name, probability, and magnitude that is speciﬁc to that operation. For example, a sub-policy can be [(Sharpness, 0.6, 2), (Posterize, 0.3, 9)]. For each operation, we randomly sample a transformation from 15 possible transformations, a magnitude in [1, 10) and ﬁx the probability to 0.5. Speciﬁcally, we sample from the following 15 transformations: Invert, Cutout, Sharpness, AutoContrast, Posterize, ShearX, TranslateX, TranslateY, ShearY, Rotate, Equalize, Contrast, Color, Solarize, Brightness. We ﬁnd this setting to work well in 14 our ﬁrst try and did not tune the magnitude range and the probability. Tuning these hyperparameters might result in further gains in accuracy. TF-IDF based word replacing Details. Ideally, we would like the augmentation method to generate both diverse and valid examples. Hence, the augmentation is designed to retain keywords and replace uninformative words with other uninformative words. We use BERT’s word tokenizer since BERT ﬁrst tokenizes sentences into a sequence of words and then tokenize words into subwords although the model uses subwords as input. Speciﬁcally, Suppose IDF(w) is the IDF score for word w computed on the whole corpus, and TF(w) is the TF score for word w in a sentence. We compute the TF-IDF score as TFIDF(w) = TF(w)IDF(w). Suppose the maximum TF-IDF score in a sentence x is C = maxi TFIDF(xi). To make the probability of having a word replaced to negatively correlate with its TF-IDF score, we set the probability to min(p(C −TFIDF(xi))/Z, 1), where p is a hyperparameter that controls the magnitude of the augmentation and Z = P i(C −TFIDF(xi))/|x| is the average score. p is set to 0.7 for experiments on DBPedia. When a word is replaced, we sample another word from the whole vocabulary for the replacement. Intuitively, the sampled words should not be keywords to prevent changing the ground-truth labels of the sentence. To measure if a word is keyword, we compute a score of each word on the whole corpus. Speciﬁcally, we compute the score as S(w) = freq(w)IDF(w) where freq(w) is the frequency of word w on the whole corpus. We set", "meta": {"paper_id": "1904.12848v6", "chunk_index": 17, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_18", "text": "to prevent changing the ground-truth labels of the sentence. To measure if a word is keyword, we compute a score of each word on the whole corpus. Speciﬁcally, we compute the score as S(w) = freq(w)IDF(w) where freq(w) is the frequency of word w on the whole corpus. We set the probability of sampling word w as (maxw′ S(w′)−S(w))/Z′ where Z′ = P w maxw′ S(w′) −S(w) is a normalization term. B Extended Experiments B.1 Ablation Studies Ablation Studies for Unlabeled Data Size Here we present an ablation study for unlabeled data sizes. As shown in Table 6 and Table 7, given the same number of labeled examples, reducing the number of unsupervised examples clearly leads to worse performance. In fact, having abundant unsupervised examples is more important than having more labeled examples since reducing the unlabeled data amount leads to worse performance than reducing the labeled data by the same ratio. # Unsup / # Sup 250 500 1,000 2,000 4,000 50,000 5.43 ± 0.96 4.80 ± 0.09 4.75 ± 0.10 4.73 ± 0.14 4.32 ± 0.08 20,000 11.01 ± 1.01 9.46 ± 0.14 8.57 ± 0.14 7.65 ± 0.17 7.31 ± 0.24 10,000 23.17 ± 0.71 18.43 ± 0.43 15.46 ± 0.58 12.52 ± 0.13 10.32 ± 0.20 5,000 35.41 ± 0.75 28.35 ± 0.60 22.06 ± 0.71 17.36 ± 0.15 13.19 ± 0.12 Table 6: Error rate (%) for CIFAR-10 with different amounts of labeled data and unlabeled data. # Unsup / # Sup 250 500 1,000 2,000 4,000 73,257 2.72 ± 0.40 2.27 ± 0.09 2.23 ± 0.07 2.20 ± 0.06 2.28 ± 0.10 20,000 5.59 ± 0.74 4.43 ± 0.15 3.81 ± 0.11 3.86 ± 0.14 3.64 ± 0.20 10,000 17.13 ± 12.85 7.59 ± 1.01 5.76 ± 0.29 5.17 ± 0.12 5.40 ± 0.12 5,000 31.58 ± 7.39 12.66 ± 0.81 6.28 ± 0.25 8.35 ± 0.36 7.76 ± 0.28 Table 7: Error rate (%) for SVHN with different amounts of labeled data and unlabeled data. Ablations Studies on RandAugment We hypothesize that the success of RandAugment should be credited to the diversity of the augmentation transformations, since RandAugment works very well for multiple different datasets while it does not require a search algorithm to ﬁnd out the most effective policies. To verify this hypothesis, we test UDA’s performance when we restrict the number of possible transformations used in RandAugment. As shown in Figure 6, the performance gradually improves as we use more augmentation transformations. 15 2 4 6 8 10 12 14 16 Number of possible tranformations 4.25 4.50 4.75 5.00 5.25 5.50 5.75 Error rate # Sup Examples: 1000 # Sup Examples: 4000 Figure 6: Error rate of UDA on CIFAR-10 with different numbers of possible transformations in Ran- dAugment. UDA achieves lower error rate when we increase the number of possible transformations, which demonstrates the importance of a rich set of augmentation transformations. Ablation Studies for TSA We study the effect of TSA on Yelp-5 where we have 2.5k labeled examples and 6m unlabeled examples. We use a randomly initialized transformer in this", "meta": {"paper_id": "1904.12848v6", "chunk_index": 18, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_19", "text": "rate when we increase the number of possible transformations, which demonstrates the importance of a rich set of augmentation transformations. Ablation Studies for TSA We study the effect of TSA on Yelp-5 where we have 2.5k labeled examples and 6m unlabeled examples. We use a randomly initialized transformer in this study to rule out factors of having a pre-trained representation. As shown in Table 8, on Yelp-5, where there is a lot more unlabeled data than labeled data, TSA reduces the error rate from 50.81 to 41.35 when compared to the baseline without TSA. More speciﬁcally, the best performance is achieved when we choose to postpone releasing the supervised training signal to the end of the training, i.e, exp-schedule leads to the best performance. TSA schedule Yelp-5 \u0017 50.81 log-schedule 49.06 linear-schedule 45.41 exp-schedule 41.35 Table 8: Ablation study for Training Signal Annealing (TSA) on Yelp-5 and CIFAR-10. The shown numbers are error rates. B.2 More Results on CIFAR-10, SVHN and Text Classiﬁcation Datasets Results with varied label set sizes on CIFAR-10 In Table 9, we show results for compared methods of Figure 4a and results of Pseudo-Label [33], Π-Model [32], Mean Teacher [58]. Fully supervised learning using 50,000 examples achieves an error rate of 4.23 and 5.36 with or without RandAugment. The performance of the baseline models are reported by MixMatch [3]. To make sure that the performance reported by MixMatch and our results are comparable, we reimplement MixMatch in our codebase and ﬁnd that the results in the original paper is comparable but slightly better than our reimplementation, which results in a more competitive comparison for UDA. For example, our reimplementation of MixMatch achieves an error rate of 7.00 ± 0.59 and 7.39 ± 0.11 with 4,000 and 2,000 examples. Methods / # Sup 250 500 1,000 2,000 4,000 Pseudo-Label 49.98 ± 1.17 40.55 ± 1.70 30.91 ± 1.73 21.96 ± 0.42 16.21 ± 0.11 Π-Model 53.02 ± 2.05 41.82 ± 1.52 31.53 ± 0.98 23.07 ± 0.66 17.41 ± 0.37 Mean Teacher 47.32 ± 4.71 42.01 ± 5.86 17.32 ± 4.00 12.17 ± 0.22 10.36 ± 0.25 VAT 36.03 ± 2.82 26.11 ± 1.52 18.68 ± 0.40 14.40 ± 0.15 11.05 ± 0.31 MixMatch 11.08 ± 0.87 9.65 ± 0.94 7.75 ± 0.32 7.03 ± 0.15 6.24 ± 0.06 UDA (RandAugment) 5.43 ± 0.96 4.80 ± 0.09 4.75 ± 0.10 4.73 ± 0.14 4.32 ± 0.08 Table 9: Error rate (%) for CIFAR-10. 16 Results with varied label set sizes on SVHN In Table 10, we similarly show results for compared methods of Figure 4b and results of methods mentioned above. Fully supervised learning using 73,257 examples achieves an error rate of 2.28 and 2.84 with or without RandAugment. The performance of the baseline models are reported by MixMatch [3]. Our reimplementation of MixMatch also resulted in comparable but higher error rates than the reported ones. Methods / # Sup 250 500 1,000 2,000 4,000 Pseudo-Label 21.16 ± 0.88 14.35 ± 0.37 10.19 ± 0.41 7.54 ± 0.27 5.71 ± 0.07 Π-Model 17.65 ± 0.27 11.44 ± 0.39 8.60", "meta": {"paper_id": "1904.12848v6", "chunk_index": 19, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_20", "text": "[3]. Our reimplementation of MixMatch also resulted in comparable but higher error rates than the reported ones. Methods / # Sup 250 500 1,000 2,000 4,000 Pseudo-Label 21.16 ± 0.88 14.35 ± 0.37 10.19 ± 0.41 7.54 ± 0.27 5.71 ± 0.07 Π-Model 17.65 ± 0.27 11.44 ± 0.39 8.60 ± 0.18 6.94 ± 0.27 5.57 ± 0.14 Mean Teacher 6.45 ± 2.43 3.82 ± 0.17 3.75 ± 0.10 3.51 ± 0.09 3.39 ± 0.11 VAT 8.41 ± 1.01 7.44 ± 0.79 5.98 ± 0.21 4.85 ± 0.23 4.20 ± 0.15 MixMatch 3.78 ± 0.26 3.64 ± 0.46 3.27 ± 0.31 3.04 ± 0.13 2.89 ± 0.06 UDA (RandAugment) 2.72 ± 0.40 2.27 ± 0.09 2.23 ± 0.07 2.20 ± 0.06 2.28 ± 0.10 Table 10: Error rate (%) for SVHN. 20 200 2000 25000 # Labeled examples 93.5 94.0 94.5 95.0 95.5 96.0 Accuracy Semi-Supervised (UDA) Superivsed (BERT-finetune) Previous SOTA (a) IMDb 20 200 2000 20000 560000 # Labeled examples 97.0 97.2 97.5 97.8 98.0 98.2 98.5 98.8 Accuracy Semi-Supervised (UDA) Superivsed (BERT-finetune) Previous SOTA (b) Yelp-2 Figure 7: Accuracy on IMDb and Yelp-2 with different number of labeled examples. In the large-data regime, with the full training set of IMDb, UDA also provides robust gains. Experiments on Text Classiﬁcation with Varied Label Set Sizes We also try different data sizes on text classiﬁcation tasks . As show in Figure 7, UDA leads to consistent improvements across all labeled data sizes on IMDb and Yelp-2. C Proof for Theoretical Analysis Here, we provide a full proof for Theorem 1. Theorem 1. Under UDA, let Pr(A) denote the probability that the algorithm cannot infer the label of a new test example given m labeled examples from PL. Pr(A) is given by Pr(A) = X i Pi(1 −Pi)m. In addition, O(k/ϵ) labeled examples can guarantee an error rate of O(ϵ), i.e., m = O(k/ϵ) =⇒Pr(A) = O(ϵ). Proof. Let x′ be the sampled test example. Then the probability of event A is Pr(A) = X i Pr(A and x′ ∈Ci) = X i Pi(1 −Pi)m 17 To bound the probability, we would like to ﬁnd the maximum value of P i Pi(1 −Pi)m. We can deﬁne the following optimization function: min P − X ci Pi(1 −Pi)m s.t. X ci Pi = 1 The problem is a convex optimization problem and we can construct its the Lagrangian dual function: L = X i Pi(1 −Pi)m −λ( X i Pi −1) Using the KKT condition, we can take derivatives to Pi and set it to zero. Then we have λ = (1 −mPi)(1 −Pi)m−1 Hence Pi = Pj for any i ̸= j. Using the fact that P i Pi = 1, we have Pi = 1 k Plugging the result back into Pr(A) = P i Pi(1 −Pi)m, we have Pr(A) ≤(1 −1 k )m = exp(m log(1 −1 k )) ≤exp(−m k ) Hence when m = O( k ϵ ), we have Pr(A) = O(ϵ) D Extended Related Work Semi-supervised Learning. Due to the long history of semi-supervised learning (SSL), we", "meta": {"paper_id": "1904.12848v6", "chunk_index": 20, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_21", "text": "= P i Pi(1 −Pi)m, we have Pr(A) ≤(1 −1 k )m = exp(m log(1 −1 k )) ≤exp(−m k ) Hence when m = O( k ϵ ), we have Pr(A) = O(ϵ) D Extended Related Work Semi-supervised Learning. Due to the long history of semi-supervised learning (SSL), we refer read- ers to [5] for a general review. More recently, many efforts have been made to renovate classic ideas into deep neural instantiations. For example, graph-based label propagation [72] has been extended to neural methods via graph embeddings [62, 63] and later graph convolutions [28]. Similarly, with the variational auto-encoding framework and reinforce algorithm, classic graphical models based SSL methods with target variable being latent can also take advantage of deep architectures [27, 36, 64]. Besides the direct extensions, it was found that training neural classiﬁers to classify out-of-domain examples into an additional class [53] works very well in practice. Later, Dai et al. [12] shows that this can be seen as an instantiation of low-density separation. Apart from enforcing consistency on the noised input examples and the hidden representations, another line of research enforces consistency under different model parameters, which is complementary to our method. For example, Mean Teacher [58] maintains a teacher model with parameters being the ensemble of a student model’s parameters and enforces the consistency between the predictions of the two models. Recently, fast-SWA [1] improves Mean Teacher by encouraging the model to explore a diverse set of plausible parameters. In addition to parameter-level consistency, SNTG [35] also enforces input-level consistency by constructing a similarity graph between unlabeled examples. Data Augmentation. Also related to our work is the ﬁeld of data augmentation research. Besides the conventional approaches and two data augmentation methods mentioned in Section 2.1, a recent approach MixUp [70] goes beyond data augmentation from a single data point and performs interpo- lation of data pairs to achieve augmentation. Recently, it has been shown that data augmentation can be regarded as a kind of explicit regularization methods similar to Dropout [21]. Diverse Back Translation. Diverse paraphrases generated by back-translation has been a key component in the signiﬁcant performance improvements in our text classiﬁcation experiments. We use random sampling instead of beam search for decoding similar to [15]. There are also recent 18 works on generating diverse translations [19, 55, 29] that might lead to further improvements when used as data augmentations. Unsupervised Representation Learning. Apart from semi-supervised learning, unsupervised rep- resentation learning offers another way to utilize unsupervised data. Collobert and Weston [8] demonstrated that word embeddings learned by language modeling can improve the performance signiﬁcantly on semantic role labeling. Later, the pre-training of word embeddings was simpliﬁed and substantially scaled in Word2Vec [39] and Glove [46]. More recently, pre-training using language modeling and denoising auto-encoding has been shown to lead to signiﬁcant improvements on many tasks in the language domain [11, 47, 48, 23, 14]. There is also a growing interest in self-supervised learning for vision [69, 20, 59]. Consistency Training in Other Domains. Similar ideas of consistency training has also been applied in other", "meta": {"paper_id": "1904.12848v6", "chunk_index": 21, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_22", "text": "has been shown to lead to signiﬁcant improvements on many tasks in the language domain [11, 47, 48, 23, 14]. There is also a growing interest in self-supervised learning for vision [69, 20, 59]. Consistency Training in Other Domains. Similar ideas of consistency training has also been applied in other domains. For example, recently, enforcing adversarial consistency on unsupervised data has also been shown to be helpful in adversarial robustness [57, 68, 42, 4]. Enforcing consistency w.r.t data augmentation has also been shown to work well for representation learning [24, 65]. Invariant representation learning [34, 52] applies the consistency loss not only to the predicted distributions but also to representations and has been shown signiﬁcant improvements on speech recognition. E Experiment Details E.1 Text Classiﬁcations Datasets. In our semi-supervised setting, we randomly sampled labeled examples from the full supervised set4 and use the same number of examples for each category. For unlabeled data, we use the whole training set for DBPedia, the concatenation of the training set and the unlabeled set for IMDb and external data for Yelp-2, Yelp-5, Amazon-2 and Amazon-5 [38]5. Note that for Yelp and Amazon based datasets, the label distribution of the unlabeled set might not match with that of labeled datasets since there are different number of examples in different categories. Nevertheless, we ﬁnd it works well to use all the unlabeled data. Preprocessing. We ﬁnd the sequence length to be an important factor in achieving good performance. For all text classiﬁcation datasets, we truncate the input to 512 subwords since BERT is pretrained with a maximum sequence length of 512. Further, when the length of an example is greater than 512, we keep the last 512 subwords instead of the ﬁrst 512 subwords as keeping the latter part of the sentence lead to better performances on IMDb. Fine-tuning BERT on in-domain unsupervised data. We ﬁne-tune the BERT model on in-domain unsupervised data using the code released by BERT. We try learning rate of 2e-5, 5e-5 and 1e-4, batch size of 32, 64 and 128 and number of training steps of 30k, 100k and 300k. We pick the ﬁne-tuned models by the BERT loss on a held-out set instead of the performance on a downstream task. Random initialized Transformer. For the experiments with randomly initialized Transformer, we adopt hyperparameters for BERT base except that we only use 6 hidden layers and 8 attention heads. We also increase the dropout rate on the attention and the hidden states to 0.2, When we train UDA with randomly initialized architectures, we train UDA for 500k or 1M steps on Amazon-5 and Yelp-5 where we have abundant unlabeled data. BERT hyperparameters. Following the common BERT ﬁne-tuning procedure, we keep a dropout rate of 0.1, and try learning rate of 1e-5, 2e-5 and 5e-5 and batch size of 32 and 128. We also tune the number of steps ranging from 30 to 100k for various data sizes. UDA hyperparameters. We set the weight on the unsupervised objective λ to 1 in all of our experiments. We use a batch size of 32", "meta": {"paper_id": "1904.12848v6", "chunk_index": 22, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_23", "text": "and 5e-5 and batch size of 32 and 128. We also tune the number of steps ranging from 30 to 100k for various data sizes. UDA hyperparameters. We set the weight on the unsupervised objective λ to 1 in all of our experiments. We use a batch size of 32 for the supervised objective since 32 is the smallest batch size on v3-32 Cloud TPU Pod. We use a batch size of 224 for the unsupervised objective when the Transformer is initialized with BERT so that the model can be trained on more unlabeled data. We ﬁnd that generating one augmented example for each unlabeled example is enough for BERTFINETUNE. 4http://bit.ly/2kRWoof, https://ai.stanford.edu/~amaas/data/sentiment/ 5https://www.kaggle.com/yelp-dataset/yelp-dataset, http://jmcauley.ucsd.edu/data/ amazon/ 19 All experiments in this part are performed on a v3-32 Cloud TPU Pod. E.2 Semi-supervised learning benchmarks CIFAR-10 and SVHN Hyperparameters for Wide-ResNet-28-2. We train our model for 500K steps. We apply Exponen- tial Moving Average to the parameters with a decay rate of 0.9999. We use a batch size of 64 for labeled data and a batch size of 448 for unlabeled data. The softmax temperature τ is set to 0.4. The conﬁdence threshold β is set to 0.8. We use a cosine learning rate decay schedule: cos( 7t 8T ∗π 2 ) where t is the current step and T is the total number of steps. We use a SGD optimizer with nesterov momentum with the momentum hyperparameter set to 0.9. In order to reduce training time, we generate augmented examples before training and dump them to disk. For CIFAR-10, we generate 100 augmented examples for each unlabeled example. Note that generating augmented examples in an online fashion is always better or as good as using dumped augmented examples since the model can see different augmented examples in different epochs, leading to more diverse samples. We report the average performance and the standard deviation for 10 runs. Experiments in this part are performed on a Tesla V100 GPU. Hyperparameters for Shake-Shake and PyramidNet. For the experiments with Shake-Shake, we train UDA for 300k steps and use a batch size of 128 for the supervised objective and use a batch size of 512 for the unsuperivsed objective. For the experiments with PyramidNet+ShakeDrop, we train UDA for 700k steps and use a batch size of 64 for the supervised objective and a batch size of 128 for the unsupervised objective. For both models, we use a learning rate of 0.03 and use a cosine learning decay with one annealing cycle following AutoAugment. Experiments in this part are performed on a v3-32 Cloud TPU v3 Pod. E.3 ImageNet 10% Labeled Set Setting. Unless otherwise stated, we follow the standard hyperparameters used in an open-source implementation of ResNet.6 For the 10% labeled set setting, we use a batch size of 512 for the supervised objective and a batch size of 15,360 for the unsupervised objective. We use a base learning rate of 0.3 that is decayed by 10 for four times and set the weight on the unsupervised objective λ to 20. We mask out unlabeled examples whose", "meta": {"paper_id": "1904.12848v6", "chunk_index": 23, "filename": "1904.12848v6.pdf", "chunk_length": 512}}
{"id": "1904.12848v6_chunk_24", "text": "size of 512 for the supervised objective and a batch size of 15,360 for the unsupervised objective. We use a base learning rate of 0.3 that is decayed by 10 for four times and set the weight on the unsupervised objective λ to 20. We mask out unlabeled examples whose highest probabilities across categories are less than 0.5 and set the Softmax temperature to 0.4. The model is trained for 40k steps. Experiments in this part are performed on a v3-64 Cloud TPU v3 Pod. Full Labeled Set Setting. For experiments on the full ImageNet, we use a batch size of 8,192 for the supervised objective and a batch size of 16,384 for the unsupervised objective. The weight on the unsupervised objective λ is set to 1. We use entropy minimization to sharpen the prediction. We use a base learning rate of 1.6 and decay it by 10 for four times. Experiments in this part are performed on a v3-128 Cloud TPU v3 Pod. 6https://github.com/tensorﬂow/tpu/tree/master/models/ofﬁcial/resnet 20", "meta": {"paper_id": "1904.12848v6", "chunk_index": 24, "filename": "1904.12848v6.pdf", "chunk_length": 166}}
{"id": "1905.00422v1_chunk_0", "text": "Time-series Insights into the Process of Passing or Failing Online University Courses using Neural-Induced Interpretable Student States Byungsoo Jeon Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA byungsoj@cs.cmu.edu Eyal Shafran Western Governors University 4001 S 700 East Salt Lake City, UT eyal.shafran@wgu.edu Luke Breitfeller Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA mbreitfe@cs.cmu.edu Jason Levin Western Governors University 4001 S 700 East Salt Lake City, UT jason.levin@wgu.edu Carolyn P. Rosé Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA cprose@cs.cmu.edu ABSTRACT This paper addresses a key challenge in Educational Data Mining, namely to model student behavioral trajectories in order to provide a means for identifying students most at- risk, with the goal of providing supportive interventions. While many forms of data including clickstream data or data from sensors have been used extensively in time series mod- els for such purposes, in this paper we explore the use of textual data, which is sometimes available in the records of students at large, online universities. We propose a time series model that constructs an evolving student state repre- sentation using both clickstream data and a signal extracted from the textual notes recorded by human mentors assigned to each student. We explore how the addition of this textual data improves both the predictive power of student states for the purpose of identifying students at risk for course failure as well as for providing interpretable insights about student course engagement processes. Keywords Student State, Clickstream Data, Mentor’s Notes, LDA, Time-series Modeling, Deep Learning 1. INTRODUCTION With the rapidly-changing landscape of work opportunities causing increased concerns related to unemployment, work- ers have a greater need to seek further education. Online universities [1] like Western Governor’s University (WGU) [18] play crucial roles in helping workers achieve their career success by providing a personal and aﬀordable education based on real-world competencies. In such online educa- tional contexts, modeling the population of students at scale is an important challenge, for example, in order to identify students most at-risk and to provide appropriate interven- tions to improve their chances of earning a degree in a timely fashion. In this respect, a plethora of approaches for click- stream analysis [13, 30, 31] have been published in the ﬁeld of Educational Data Mining, which address questions about modeling student course engagement processes. Some of this work has produced time series models producing predictive signals related to dropout or disengagement, which can then be used as triggers for interventions [25]. While clickstream data is the most readily available, and while some success has been achieved using it for this purpose, its low level in- dicators provide only glimpses related to student progress, challenges, and aﬀect as we would hope to observe and model them. In this paper, we explore the extent to which we may achieve richer insights by adding textual data to the foun- dation provided by clickstream data. One advantage to modeling student behavior and states from a for-pay platform is that the level of support provided to students is greater than in freely available contexts like Mas- sive Open Online Courses (MOOCs),", "meta": {"paper_id": "1905.00422v1", "chunk_index": 0, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_1", "text": "achieve richer insights by adding textual data to the foun- dation provided by clickstream data. One advantage to modeling student behavior and states from a for-pay platform is that the level of support provided to students is greater than in freely available contexts like Mas- sive Open Online Courses (MOOCs), and this more inten- sive engagement provides richer data sources that can be leveraged. In our work, we make use of a new data source provided by the Western Governor’s University (WGU) plat- form, where each student is assigned a human mentor, and the notes from each biweekly encounter between student and mentor are recorded and made part of the time series data available for each student. Thus, even if we do not have access to the full transcript of the interactions between stu- dents and their mentors, we can leverage the documentation of provided support in order to enhance the richness and ulti- mately the interpretability of student states we may induce from other low level behavioral indicators we can extract from traces of learning platform interactions. A major thrust of our work has been to develop a tech- nique for leveraging this form of available textual data. We refer to this data as Mentor’s Notes. In particular, we pro- arXiv:1905.00422v1 [cs.CL] 1 May 2019 pose a sequence model to integrate available data traces over time, which we refer to as Click2State, which serves a dual purpose. The ﬁrst aim is to induce predictive stu- dent states, which provide substantial traction towards pre- dicting whether a student is on a path towards passing or failing a course. Another is to provide us with insights into the process of passing or failing a course over time, and in particular leveraging the insights of human mentors whose observations give deeper meaning to the click level behav- ioral data, which is otherwise impoverished from an inter- pretability standpoint. In the remainder of the paper we discuss related work in the ﬁelds of Educational Data Mining of large scale online course data to contextualize our speciﬁc work. Next we discuss the speciﬁc data we are working with and how it relates to the context in which it was collected. Then we explain from a technical level the modeling approach we are taking in this work. Finally, we present a series of experiments that investigate the following three research questions: • RQ1. How can we leverage contemporary text mining techniques such as Latent Dirichlet Allocation (LDA) [2] to extract information and meaning from mentors’ notes about the formation of student states across time using a time-series model? • RQ2. To what extent does integrating a representation of topical insights from Mentor’s Notes improve the ability of a time series neural model to predict whether students are on a path towards passing or failing a course? • RQ3. How can we use insights about student progress in an online course captured using student state rep- resentations from our integrated model to understand the process of passing or failing a course on the plat- form? 2. RELATED WORK", "meta": {"paper_id": "1905.00422v1", "chunk_index": 1, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_2", "text": "on a path towards passing or failing a course? • RQ3. How can we use insights about student progress in an online course captured using student state rep- resentations from our integrated model to understand the process of passing or failing a course on the plat- form? 2. RELATED WORK Past research aiming at enhancing the learning process of students in online universities has focused on providing an- alytic tools for teachers and administrators. These tools are meant to enhance their ability to oﬀer support and make strategic choices in the administration of learning within the contexts under their care. As just one example, the Analytics4Action Evaluation Framework (A4AEF) model [25], developed at the Open University (UK) [1], provides a university-wide pipeline allowing its users to leverage learn- ing analytics to enable successful interventions and to gain strategic insights from their trials. One of the most im- portant challenges in implementing such a pipeline is to model the population of students in such a way as to provide both predictive power for triggering interventions and inter- pretability for ensuring validity and for supporting decision making. Some past research has already produced models to identify at-risk students and predict student outcomes speciﬁcally in online universities [6, 21]. For example, Smith et el. [29] pro- posed models to predict students’ course outcomes and to identify factors that led to student success in online univer- sity courses. Eagle et al. [12] presented exploratory models to predict outcomes like high scores on the upcoming tests or overall probability of passing a course, and provided ex- amples of strong indicators of student success in the WGU platform where our work is also situated. However, this past work has focused mainly on predictive modeling of student outcomes, whereas our work pursues both predictive power and interpretability. While much work in the ﬁeld of Educational Data Mining explores time series modeling and induction of student state representations from open online platforms such as Massive Open Online Courses (MOOCs) or Intelligent Tutoring Sys- tems, far less has been published from large, online univer- sities such as WGU, which oﬀer complementary insights to the ﬁeld. Student states are triggered by students’ interac- tion with university resources, their progress through course milestones, test outcomes, aﬀect-inducing experiences, and so on. Aﬀect signals in particular have been utilized by many researchers as the basis for induced student states, as this rich source of insight into student experiences has been proven to correlate with several indicators of student accom- plishments [9, 24, 26]. Researchers have investigated aﬀect and developed corresponding detectors using sensors, ﬁeld observation, and self-reported aﬀect. These detectors cap- ture students’ aﬀective signals from vocal patterns [7, 23], posture [11], facial expressions [3, 23], interaction with the platform [4, 5, 14], and physiological cues [7, 20]. Although these signals provide rich insights, the requisite data is some- times expensive or even impractical to obtain, even on for- pay platforms such as WGU, where we conduct our research. The bulk of existing work using sequence modeling to induce student states has focused on the", "meta": {"paper_id": "1905.00422v1", "chunk_index": 2, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_3", "text": "cues [7, 20]. Although these signals provide rich insights, the requisite data is some- times expensive or even impractical to obtain, even on for- pay platforms such as WGU, where we conduct our research. The bulk of existing work using sequence modeling to induce student states has focused on the data that is most readily available, speciﬁcally, clickstream data. For example, Tang et al. [30] have constructed a model to predict a set of stu- dent actions with long short-term memory (LSTM) [15] on student clickstream data from a BerkeleyX MOOC, though the basic LSTM was unable to match the baseline of de- faulting to the majority class for samples of student actions. Fei et al. [13] proposed a sequence model to predict dropout based on clickstream data using deep learning models such as recurrent neural networks (RNNs) and LSTMs, with more success. Wang et al. [31] also built a deep neural network architecture using a combination of convolutional neural net- work (CNN) [19] and RNN for dropout prediction from click- stream data. Though these models have achieved diﬀering success at their predictive tasks, a shortcoming shared by all of these models is the lack of interpretability in the induced student state representations. Some prior work has nevertheless attempted to construct cognitively meaningful representations, such as representa- tions that can be constructed through summarization of raw video clickstream data [27, 28]. Sinha et al. [27] attempted to construct cognitive video watching states to explain the dynamic process of cognition involved in MOOC video click- stream interaction. Building on this work, Sinha et al. [28] explored the combined representations of video clickstream behavior and discussion forum footprint to provide insights about student engagement process in MOOCs. Similar to our own work, their work extracting these abstract feature representations from the raw clickstream data aims (1) to obtain noise-resistant and interpretable features and (2) to # click # focused state # keypress # mouse move # scroll # unfocused state Target course 53 61 0 168 904 1732 Other courses 177 167 0 455 2301 4887 Degree plan 0 0 0 0 0 0 Portal 21 89 0 263 3862 2440 Homepage 36 69 0 122 72 1581 Table 1: Example of clickstream data. Type Description # click The number of mouse clicks. # focused state The number of times a mouse was in a content box. It is incremented by one every time a mouse goes into a content box from somewhere else. # keypress The number of times a keyboard was pressed. # mouse move The number of times a mouse has been moved. # scroll The number of scroll counts. # unfocused state The number of times a mouse was outside a content box. Table 2: Description of click types. transform the unstructured raw clickstream data to struc- tured data appropriate as input to existing statistical or ma- chine learning models. However, this published work does not model the temporal patterns of such cognitively mean- ingful features such as we perform in this paper. Our work extends previous studies by proposing", "meta": {"paper_id": "1905.00422v1", "chunk_index": 3, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_4", "text": "the unstructured raw clickstream data to struc- tured data appropriate as input to existing statistical or ma- chine learning models. However, this published work does not model the temporal patterns of such cognitively mean- ingful features such as we perform in this paper. Our work extends previous studies by proposing a model that enriches temporal signals from clickstream data using the textual mentor’s notes in order to provide a means for interpret- ing student state representations as they evolve over time within courses. 3. DATA Our study is based on data collected by Western Gover- nor’s University (WGU), an online educational platform 1. WGU is an online school with career-focused bachelor’s and master’s degrees—in teaching, nursing, IT, and business— designed to allow working professionals the opportunity to ﬁt an online university education into their busy lives. Stu- dents in WGU earn a competency-based degree by prov- ing knowledge through regular assessments [16], which facil- itates self-paced learning based on their prior experience. To support self-paced learning, students in WGU are as- signed to a program mentor (PM). The PM is in charge of evaluating a student’s progress through their degree and helping to manage obstacles the student faces. A PM and a student generally have bi-weekly live calls, but this may vary depending on the student’s needs and schedule. Each PM writes down a summary of what was discussed, which we refer to as a mentor’s note. An example is given in Fig- ure 1. As in the example, mentor’s notes describe the status and progress of the student and what types of support was oﬀered or what suggestions were made during the call. This information can provide meaningful cues to infer student states over time. 1https://www.wgu.edu/ Figure 1: An example of mentor’s notes. In our modeling work we aim both for predictive power and interpretability, thus it is an important part of our work to build models that induce an interpretable student state representation. In this work we speciﬁcally investigate how the use of the mentor’s note data along side the more fre- quently used clickstream data might enable that important goal. Clickstream data in WGU also provides us with infor- mation on how active students are and where in the WGU platform they spend their time. We collect clickstream data from four diﬀerent types of web pages in the WGU platform: course, degree plan, homepage, and portal. The course web pages cover all pages related to courses in WGU. Degree plan represents a dashboard where students check their progress toward a degree. Homepage is the main page that shows students’ progress in each course and allows access to all provided WGU features. Portal covers any other pages for student support including technical and ﬁnancial assistance. An example of the clickstream data can be seen in Table 1. Each row represents one of ﬁve diﬀerent click sources: tar- get course page, other course page, degree plan page, portal page, and homepage. We divide the course pages into ”tar- get course” and ”other course”. Each column represents one of diﬀerent six click", "meta": {"paper_id": "1905.00422v1", "chunk_index": 4, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_5", "text": "data can be seen in Table 1. Each row represents one of ﬁve diﬀerent click sources: tar- get course page, other course page, degree plan page, portal page, and homepage. We divide the course pages into ”tar- get course” and ”other course”. Each column represents one of diﬀerent six click types: click count, focus state count, keypress count, mousemove count, scroll count, and unfo- cused state count. The values in the table represent the weekly count of diﬀerent type of clicks from each diﬀerent source. The description of these click types are in Table 2. HA CA # of students 6,041 4,062 Length of a term 25 weeks 25 weeks Avg prior units 62 ± 39 11 ± 23 Fail rate 0.185 0.509 Avg # of notes per student 10.9 ± 5.7 11.0 ± 5.8 Avg length of notes (chars) 198 ± 47 194 ± 55 Table 3: Data Statistics For this paper, we have collected the mentor’s notes and clickstream data from two courses conducted in 2017: Health Assessment (HA) and College Algebra (CA). We choose these two courses because they are popular among students and represent diﬀerent levels of overall diﬃculty. Table 3 shows the statistics for the dataset. “Average prior units” is the average number of units students transferred to WGU from prior education when they started the degree, and func- tions as a proxy for the level of student’s prior knowledge. We split the dataset for each course into a training set (80%), a validation set (10%), and a test set (10%). For training, in order to avoid a tendency for trained models to over-predict the majority class, we have resampled the training set so that both the pass state and the fail state are represented equally. 4. PROPOSED METHOD As we have stated above, in our modeling work, we propose a sequence model, Click2State, with two primary purposes. The ﬁrst is to form a student state representation that will allow us to better identify students at risk of failing a course than a baseline model that does not make use of rich textual data. The second is to provide us with a means to interpret the meaning of a student state representation. Figure 2 provides a schematic overview of our proposed model. Note that it is ﬁrst and foremost a sequence model that predicts whether a student will pass or fail a course based on an interpretable student state that evolves from week to week as each week’s clickstream data is input to the recurrent neural model. A summary of the content of the mentor’s note for a week is constructed using a popular topic modeling technique, speciﬁcally Latent Dirichlet Al- location (LDA). In the full model, an intermittent task to predict the topic distribution extracted from the mentor’s notes associated with a time point is introduced. The goal is to use this secondary task to both improve the predictive power of the induced student states over the baseline as well as to enhance the interpretability of the state respresenta- tion. Below we evaluate the extent to", "meta": {"paper_id": "1905.00422v1", "chunk_index": 5, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_6", "text": "the mentor’s notes associated with a time point is introduced. The goal is to use this secondary task to both improve the predictive power of the induced student states over the baseline as well as to enhance the interpretability of the state respresenta- tion. Below we evaluate the extent to which both of these objectives are met in this model. When training a neural model, inputs are presented to the network, activation is propagated forward, then an error is computed at the output, and the error is propagated back- wards through the network. As the error is propagated back- wards, adjustments are made to the weights in order to re- duce the occurrence of such errors. In all of our experiments, the only input at each time step is a representation of the clickstream data from the time step. We have two predic- tive tasks, namely pass prediction and topic prediction (from mentor’s notes). We propagate errors for pass prediction only after the whole sequence of inputs has been provided to the network. Error for topic prediction of mentor’s notes are propagated after each time step where mentor’s notes are provided with the clickstream data. Feature Vector Design We train our model using click- stream feature vectors (as input) and topic distribution vec- tors (as output for the topic prediction task). We design the clickstream feature vector to include both an encoding of click behavior of students from a time period as well as a control variable that represents the prior knowledge of stu- dents as estimated by the number of units they were able to transfer in from their higher education experience prior to starting a degree at WGU. The full clickstream feature vec- tor contains thirty weekly counts for each diﬀerent type and source of click, in addition to the single control variable just mentioned, which is the number of transferred units. We use min-max normalization to scale the values between 0 and 1 in preparation for training. To extract a topic distribution vector for each mentor’s note, we run Latent Dirichlet Allo- cation (LDA)[2] over the whole set of mentor’s notes from the entire training dataset. Formal Deﬁnition of the Model Now we formally spec- ify the model. Denote the student’s clickstream features by C = (c1, c2, ..., cT ), where ct is the clickstream feature vector of tth week, and T is the number of weeks for the term. The clickstream feature vectors are encoded via Gated Recurrent Units (GRU) [8], which are variants of the Recurrent Neural Network (RNN). Each time step is a week, t. Thus, at each time step t, this network constructs a hidden state of the student for the tth week, ht ∈RH, where H is the dimen- sionality of the hidden state representation. We consider ht as the student state representation associated with the tth week. Based on the generated student state representation from RNN (ht), our model is trained to predict a topic dis- tribution of a mentor’s note. As mentioned above, the model is only trained to predict the", "meta": {"paper_id": "1905.00422v1", "chunk_index": 6, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_7", "text": "state representation. We consider ht as the student state representation associated with the tth week. Based on the generated student state representation from RNN (ht), our model is trained to predict a topic dis- tribution of a mentor’s note. As mentioned above, the model is only trained to predict the topic distribution ˆθt for every week t where a student has a mentor’s note in the data. But at test time, a topic distribution can be predicted for ev- ery student state since the representation of student state is always within the same vector space. Similarly, though we only propagate error for the pass or fail prediction task after a series of weeks of data for a student have been input to the RNN, the pass or fail prediction can be made from the student state representation at any time step. Topic Prediction Given the generated hidden states from RNN (ht) for the tth week, the model estimates the true topic distribution (θt ∈RNt) of a mentor’s note on tth week where Nt is the number of topics. The estimated topic dis- tribution ( ˆθt ∈RNt) is computed by taking ht as an input of one fully connected layer (weight matrix: Wθ) whose output dimensionality is Nt followed by a softmax layer. ˆθt = Softmax(Wθht) In training time, the loss function we use for error propa- gation and adjustment of weights is calculated by means of the Kullback-Leibler divergence loss between ˆθt and θt. Fail Prediction As data from a student’s participation in a course is fed into the RNN week by week, the model es- Figure 2: Architecture of Click2State Model. timates the probability of the student failing that course (P(y = 1|C)) at the last timestep T. The estimated prob- ability is computed by taking ht as an input of one fully connected layer (weight matrix: Wy) whose output dimen- sionality is one followed by a sigmoid layer. P(y = 1|C) = Sigmoid(Wyht) In training time, the loss function we use for error propa- gation and adjustment of weights for fail prediction is the computed binary cross entropy loss with P(y = 1|C) and a true label for nth student, yn. Loss The loss function is composed of KL divergence loss for the topic prediction and binary cross-entropy loss for the fail prediction. Assume there are a total of N students. The KL divergence loss of topic distribution of the mentor’s note for nth student at time t is deﬁned as: KLDn,t = DKL(θn,t ∥ˆθn,t), where θn,t and ˆθn,t are the true and estimated topic distri- bution of the mentor’s note at time t for nth student. The binary cross-entropy of the nth student measures the similarity between the predicted P(y = 1|C) and the true y as: BCE n = −yn log PΘ(yn = 1|C) −(1 −yn) log(1 −PΘ(yn = 1|C)), where yn is the true y (∈{0, 1}) of the nth student and PΘ is the probability of the fail predicted by our model with parameters Θ. Assume that there are a total of Nn mentor’s notes for", "meta": {"paper_id": "1905.00422v1", "chunk_index": 7, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_8", "text": "= −yn log PΘ(yn = 1|C) −(1 −yn) log(1 −PΘ(yn = 1|C)), where yn is the true y (∈{0, 1}) of the nth student and PΘ is the probability of the fail predicted by our model with parameters Θ. Assume that there are a total of Nn mentor’s notes for nth student. Combining the two losses, our ﬁnal loss is 1 N N X n=1 [λBCE n + (1 −λ) 1 Nn tn,Nn X t=tn,1 KLDn,t], where tn,i is the timestep when nth student has ith mentor’s note, and λ is the rescaling weight to balance the contribu- tion of two diﬀerent loss functions. 5. RESULTS In this section, we answer our aforementioned research ques- tions one by one. First we describe topics learned from men- tor’s notes and how they may relate to student states. Then we illustrate experiment results to evaluate our Click2State model, along with experimental settings. We conclude by providing methods of extracting insights from these learned student state representations related to the process of pass- ing or failing a course over time. RQ1. What types of information about student states can we extract from mentor’s notes using LDA? Men- tor’s notes are summaries of bi-weekly live calls where pro- gram mentors (PM) interact with students to provide advice and support. On this bi-weekly call, PMs mostly check stu- dents’ progress and help them to establish appropriate study plans to achieve their goals for the term. PMs also diagnose students’ issues and developmental needs to better provide struggling students with tailored instruction and support. With this in mind, we answer the question of how student state information may be extracted from mentor’s notes through application of LDA to the notes. This provides us with topics we can easily describe and interpret to de- duce overall student states. In this section, we illustrate the insights this LDA approach yields. We set the number of topics to ten to maximize the interpretability of the results. Table 4 shows the learned topics with manually assigned la- bels, topical words, and text. Topical words are the top ten words with the highest probability of appearing within each learned topic, and are presented in decreasing order of likeli- hood. The topical text column contains an example snippet from one of top ten mentor’s notes for each topic with the highest topic probability. Frequently in topic models in- duced by LDA, not all of the topics are strongly thematic. Since many words in a text are non-thematic, and since ev- ery word needs to be assigned to a topic by the model, one or more learned topics is not coherent enough to interpret. Thus, we exclude from our interpretation the one topic that was incoherent out of the 10 learned topics, and thus we have nine topics in Table 4. Topic Topical Words Topical Text T1. Revision task, submit, revise, discuss, equate, complete, need, write, practice, paper The ST and I discussed his Task 3 revisions after he made some corrections. The ST still needs to revise the task based on the evaluator’s comments.", "meta": {"paper_id": "1905.00422v1", "chunk_index": 8, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_9", "text": "have nine topics in Table 4. Topic Topical Words Topical Text T1. Revision task, submit, revise, discuss, equate, complete, need, write, practice, paper The ST and I discussed his Task 3 revisions after he made some corrections. The ST still needs to revise the task based on the evaluator’s comments. He plans to do more revisions that align with the task rubric and submit the task soon. T2. Question student, question, call, email, send, course, discuss, appoint, speak, assist Student emailed for help with getting started. CM called to oﬀer support. Student could not talk for long. CM emailed welcome letter and scheduling link and encouraged for student to make an appointment T3. Assessment week, goal, today, schedule, pass, take, exam, ﬁnal, work, talk C278: took and did not pass preassessment, did not take ﬁnal. NNP C713: took and did not pass the preassessment. Passed LMC1 PA with a 65 on 02/27. LMC1 exam scheduled for 02/27 T4. Review for exam student, review, assess, plan, study, attempt, discuss, complete, take, report Student scheduled appointment to review for ﬁrst OA attempt but had taken and not passed the attempt by the time of the appointment. T5. Term plan student, discuss, course, complete, engage, college, term, plan, pass, progress Discussed ﬁnal term courses. Discussed starting C229 and working through hours and then working through C349 course. T6. Course progress goal, course, progress, current, complete, previous, work, date, pass, module Current course: C349 Previous goal: completed modules 1-3 and engage shadow health by next appt date Progress toward goal: Yes New Goal: shadow health completed and engaged in video assessment T7. Term progress term, course, complete, date, goal, week, progress, current, leave, remain Date: 8/22/17 Term Ends: 5 weeks OTP Progress: 5/14 cu completed Engaged Course: C785 Goal Progress: did not pass PA T8. Time constraint work, week, lesson, complete, go, progress, plan, ﬁnish, time, goal NNP stated he was not able to make forward progress in course related to personal situation and time constraints from an unexpected event. T9. Goal setting goal, week, work, complete, task, progress, pass, accomplish, ﬁnish, contact Previous goal: ﬁnish shadow health, ﬁnish and submit video by next call, start c228 next Progress/concerns: states working on c349 SH, discussed deadlines Goal: ﬁnish shadow health Table 4: LDA Topics Learned From Mentor’s Notes Note that there are two topics related to student progress, course progress (T6) and term progress (T7). Course progress (T6) focuses on progress towards modules in a particular course, along with past and present goals about the course itself. Term progress (T7) emphasizes the number of course units that have been achieved, course units that remain, and goals about courseload within in a term. There is a clear util- ity to these topics as an interpretation tool for regulation of the student’s process moving through the curriculum–if a student hits an impasse in their studies, mentor’s notes are expected to focus on what challenges the student experi- enced and what was discussed to address these challenges. We also ﬁnd two topics directly associated with plans and goals, which", "meta": {"paper_id": "1905.00422v1", "chunk_index": 9, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_10", "text": "regulation of the student’s process moving through the curriculum–if a student hits an impasse in their studies, mentor’s notes are expected to focus on what challenges the student experi- enced and what was discussed to address these challenges. We also ﬁnd two topics directly associated with plans and goals, which are term plan (T5) and goal setting (T9). Term plan (T5) includes discussions about plans for a term, such as course selection and long-term degree planning. Goal setting (T9) is similar in focus to course progress (T6), but is not constrained to a single course. As with the previous topics, these reﬂect serious investment from the student and make useful cues for favorable progress. The remaining six topics provide insight on speciﬁc issues and circumstances a student may be facing at a particu- RQ2. Does the task of topic prediction construct better student state representation than our base- line, as evaluated by the ability to predict student failure? One method of evaluating the quality of a state representation is by measuring its predictive power on a task, such as the task of predicting whether a student will fail a course. We measure the predictive power of learned student state representation from our model and compare with that of our baseline, which shares the same neural architecture but is not trained on the extra task of topic prediction. The speciﬁc predictive task is to determine whether a student fails a course within a given term given a sequence of weeks of student clickstream data. For this analysis, we trained separate models to make a prediction after a set number of weeks so that we could evaluate the diﬀerence in predictive power of student states depending on how many weeks worth of data were used in the prediction. The classiﬁer associated with the i-th week is trained on the clickstream data of all students up until the i-th week in the training set, to pre- dict failure as determined at the end of the term. During the training step, we mark the clickstream sequences of the students who failed in the given term as positive instances, and those of students who did not as negative instances. (a) Fail Prediction Performance on Health Assessment by Weeks (b) Fail Prediction Performance on College Algebra by Weeks (c) Topic Prediction Performance on Both Courses by Weeks Figure 3: Performance of Fail Prediction and Topic Prediction by Weeks (a) Topic ”Revision” (b) Topic ”Question” (c) Topic ”Assessment” (d) Topic ”Review for exam” (e) Topic ”Term plan” (f) Topic ”Course progress” (g) Topic ”Term progress” (h) Topic ”Time constraints” (i) Topic ”Goal setting” Figure 4: Standard Score of Each Topic Probability across Weeks for P and F Students We evaluate our model and baseline using data from two WGU courses: Health Assessment and College Algebra. As our evaluation metric, we use the AUC score (Area Under the Receiver Operating Characteristic Curve), which mea- sures the probability of a positive instance being classiﬁed as more likely to fail a course than a negative instance. For clickstream feature encoding, Gated Recurrent", "meta": {"paper_id": "1905.00422v1", "chunk_index": 10, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_11", "text": "courses: Health Assessment and College Algebra. As our evaluation metric, we use the AUC score (Area Under the Receiver Operating Characteristic Curve), which mea- sures the probability of a positive instance being classiﬁed as more likely to fail a course than a negative instance. For clickstream feature encoding, Gated Recurrent Units (GRU) [8] with hidden state sizes 20, 40, 60, 80, and 100 are ex- plored. Optimization is performed using Adam [17] with the initial learning rate 0.001 without weight decay. The rescaling weight (λ) is 0.5, and the minibatch size is 1. Figure 3(a) shows the AUC scores across time steps for the Health Assessment course while Figure 3(b) shows the AUC scores across time steps for College Algebra. For the Health Assessment course, our model achieves a statistically signif- icant improvement (p-value < 0.05) in performance over the baseline model after the 5th week. For College Algebra, our model achieves a statistically signiﬁcant improvement after the 17th week. This diﬀerence in model performance be- tween the Health Assessment and College Algebra courses suggests the result from College Algebra-speciﬁc topic data adds limited predictive power to the model. It is possible the clickstream data of students taking College Algebra al- ready contains enough information about whether a student is going to fail, a conclusion supported by the fact that AUC scores of the baseline model for College Algebra are always better across time steps than those for Health Assessment. We also plot the KL Divergence loss of our model across time steps to determine how well our model is predicting the topic distribution of each mentor’s note. The KL Di- vergence is a typical performance measure when predicting a discrete probability distribution, which measures the en- tropy increase due to the use of an approximation to the true distribution rather than the true distribution itself. Figure 3(c) shows the minimum KL Divergence loss of the topic prediction task for both courses at each week. Though we determined that adding this task improves the fail predic- tion task, results on this task speciﬁcally are not impres- sive, which demonstrates the relative diﬃculty of predicting mentor’s notes from click data. The KL Divergence loss increases as the number of weeks increases. A possible ex- planation is that topics become more diverse as the weeks pass and therefore become harder to predict. In addition, the KL Divergence loss is lower in College Algebra than in Health Assessment, which could be explained by students in College Algebra having less variance in the topics of focus in their mentors’ notes. RQ3. What insights do we gain about the process of passing or failing a course over time from pre- dicted mentor’s notes topic distributions over time from the model? To answer the question of character- izing students likely to pass or fail a course, we perform three diﬀerent experiments on the dataset of clickstream and mentors’ notes data of students taking the College Algebra course. We choose this course because our topic prediction loss was lower (and thus, accuracy higher) for the course. First, we determine", "meta": {"paper_id": "1905.00422v1", "chunk_index": 11, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_12", "text": "students likely to pass or fail a course, we perform three diﬀerent experiments on the dataset of clickstream and mentors’ notes data of students taking the College Algebra course. We choose this course because our topic prediction loss was lower (and thus, accuracy higher) for the course. First, we determine what topics inferred from our model correlate with whether a student will pass or fail a course (Experiment 1). Then we ﬁnd sequences of standardized topic probabilities of each topic inferred by our model that characterize students likely to pass or fail (Experiment 2). Lastly, we report how early we can identify students likely to fail based on our learned student state representation (Ex- periment 3). State Revision Question Assessment P 0.4795 0.0346 -0.1486 F 0.4914 0.1090 0.1735 State Review for exam Term plan Course progress P -0.1469 -0.0481 0.6301 F -0.0752 -0.1426 -0.0049 State Term progress Time constraint Goal setting P 0.2298 -0.4811 0.0817 F -0.1647 -0.2186 0.0661 Table 5: Standard Score of Inferred Topic Probabil- ities from P and F State Experiment 1. In the ﬁrst experiment, we ﬁnd the two stu- dent state representations which minimize or maximize the probability of failing a course. We call the state representa- tions that minimize and maximize the probability of failure as a P state and F state, respectively. Then, we show what topic distributions are inferred from P and F states. We represent emphasis, or a lack thereof, on a topic by stan- dardizing topic probabilities and observing the number of standard deviations above and below the mean of a topic probability. Table 5 shows the number of standard deviations above or below the mean (also called standard score) for each inferred topic probability from the P and F state. While the standard score of some topics are similar for the P and F state, some vary wildly. For example, the standard score of assessment topic (T3) for the P state is negative and for the F state is positive. One interpretation of this results is that students likely to fail have more trouble in passing assessments, and thus talked to their mentors more about assessment topic (T3). As shown in Table 4, the topical text of assessment (T3) includes a general description of students’ progress in assessments. The standard score of course progress (T6) and term progress (T7) for the P state is positive and negative for the F state. This suggests that students more likely to pass show smoother progress instead of reporting ongoing issues, so their typical mentors’ notes can focus exclusively on progress reports. Evidently, the topical text of course and term progress (T6, T7) in Table 4 is a progress report without ongoing issues. Experiment 2. We investigate the inferred probability of each topic from learned student state representation by our model. We compare the trajectory of inferred probability of each topic from students who passed (P students) and failed (F students) a course. Figure 4 shows the average standard score of topic probability per topic for P and F students over time. We can see", "meta": {"paper_id": "1905.00422v1", "chunk_index": 12, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_13", "text": "student state representation by our model. We compare the trajectory of inferred probability of each topic from students who passed (P students) and failed (F students) a course. Figure 4 shows the average standard score of topic probability per topic for P and F students over time. We can see through this experiment clear, distinct patterns for the frequency of each topic over time that make intuitive sense given the format of online courses. For example, term plan (T5) is high frequency for the ﬁrst week and plunges right after, since most students and mentors will naturally discuss plans for a term at the start of each term. The topical text of term plan (T5) actually exhibits the discus- sion of course choice for the ﬁnal term, expected to happen at the beginning of the ﬁnal term. The standard scores of other topics related to goal and progress (T6,T7,T9) also decrease over time, likely for similar reasons. The standard scores of revision (T1), question (T2), and assessment (T3), meanwhile, increase over time, which may indicate students seek help more actively as they approach the end of a term. The standard scores of review for exam (T4) increase dra- matically until the third week, decrease for few weeks, and ﬁnally level oﬀ. As the only condition for students in WGU to pass a course is to pass the ﬁnal assessment, it may be that many students take their ﬁnal assessments during the earlier weeks so they can pass a course as early as possi- ble. The standard scores of time constraints (T8) steeply increase until the fourth week, and then gradually decrease over time. This suggests that when students begin a term they do not expect to have time constraints, but accumu- late unanticipated issues in their personal lives as the course goes on. It is possible that students mention time constraints less near the end of the term either because they have al- ready abandoned the course or they better planned out their schedules near the end of the term in order to complete a term. However, we need to keep in mind that it may not be that they actually have time constraints, but that they just needed an excuse for slow progress. The topical text in Table 4 also shows time constraints may be discussed as the potential reason for slow progress though it may not be the real reason for the issue. For most topics, the P and F students exhibit distinct di- vergences in topic patterns. For topics related to goal and progress (T6, T7, T9), the gap between P students and F students increases over time–suggesting that as time goes on F students will be reporting issues and obstacles to their mentors instead of positive progress. The gap between P stu- dents and F students for question (T2) increases over time, likely for similar reasons. For revision (T1), P students gen- erally have higher standard scores than F students over time, which supports the idea that P students actively seek oppor- tunities for revision and review towards", "meta": {"paper_id": "1905.00422v1", "chunk_index": 13, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_14", "text": "gap between P stu- dents and F students for question (T2) increases over time, likely for similar reasons. For revision (T1), P students gen- erally have higher standard scores than F students over time, which supports the idea that P students actively seek oppor- tunities for revision and review towards the end of a term. For assessment (T3), standard score for F students increases over time while score for P students decreases. This could suggest that F students are more likely to procrastinate and struggle with their assessments than P students. Finally, for time constraints (T8) F students show higher standard score as time goes on. A likely interpretation is that students who encounter time constraints due to a demanding job or other outstanding personal issues cannot devote focus to a course and are more likely to fail. Experiment 3. We report how early we can identify stu- dents likely to fail based on our learned student state repre- sentation. We compare the Euclidean distance between stu- dent state representation and the representation of P and F states. Then we plot the average distance of students who actually passed and failed (P student and F student) across time steps. Figure 5 shows how the Euclidean distance between the stu- dent state and the P and F states changes over time. We ﬁnd that F students are closer to the F state over all diﬀer- ent weeks except second week. P students on average are approximately the same distance between the P and F state until about 15 weeks. Based on these observations, we can see in early stages whether a student is likely to fail from the diﬀerence between the distance of our model’s student state representation from P and F states. This is especially evident in early stages (week 5 onward), where we see a substantial distance between the average F student’s state representation from the P state compared to the F state, while there is much more ambiguity for P students. (a) P Students (b) F Students Figure 5: Average Euclidean Distance between State Representation of P and F Students and P and F State Representation 6. CONCLUSION In this paper, we propose and evaluate a sequence model, Click2State, which aims to build an interpretable student state representation by leveraging mentor’s notes to give deeper meaning to impoverished clickstream data. We also introduce a methodology for interpreting the learned rep- resentation from our model that extracts time-sensitive in- sights about the process of passing or failing a given online course. Our experimental results demonstrate that student state representations learned by our model have better pre- dictive power on the task of determining student failure rate than a baseline that only uses click stream data. We also present how individual topic-based insights into the process of passing or failing a course let us construct a rich charac- terization of a student likely to fail or pass an online course. We see many possible ways to build on the work in this paper. For instance, instead of learning topics using a sim-", "meta": {"paper_id": "1905.00422v1", "chunk_index": 14, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_15", "text": "insights into the process of passing or failing a course let us construct a rich charac- terization of a student likely to fail or pass an online course. We see many possible ways to build on the work in this paper. For instance, instead of learning topics using a sim- ple LDA approach, which is purely unsupervised, we might train our model to learn topics that directly correlate with a student’s likelihood of failure [10, 22]. In this case, the key challenge might be to train a model in a way that enable us to maintain the level of interpretability in topics that we see from LDA. Another direction could utilize mentor’s notes explicitly as an additional input of a model. This approach might provide us a more targetted student state representa- tion. 7. ACKNOWLEDGEMENT This work was funded in part by NSF grantIIS 1822831. References [1] Alison Ashby. Monitoring student retention in the open university: deﬁnition, measurement, interpretation and action. Open Learning: The Journal of Open, Distance and e-Learning, 19(1):65–77, 2004. [2] David M Blei, Andrew Y Ng, and Michael I Jordan. La- tent dirichlet allocation. Journal of machine Learning research, 3(Jan):993–1022, 2003. [3] Nigel Bosch, Yuxuan Chen, and Sidney D’Mello. Itˆa˘A´Zs written on your face: detecting aﬀective states from facial expressions while learning computer program- ming. In International Conference on Intelligent Tu- toring Systems, pages 39–44. Springer, 2014. [4] Anthony F Botelho, Ryan S Baker, and Neil T Heﬀer- nan. Improving sensor-free aﬀect detection using deep learning. In International Conference on Artiﬁcial In- telligence in Education, pages 40–51. Springer, 2017. [5] Anthony F. Botelho, Ryan Shaun Joazeiro de Baker, Jaclyn Ocumpaugh, and Neil T. Heﬀernan. Studying aﬀect dynamics and chronometry using sensor-free de- tectors. In EDM, 2018. [6] Carol Elaine Calvert. Developing a model and applica- tions for probabilities of student success: a case study of predictive analytics. Open Learning: The Journal of Open, Distance and e-Learning, 29(2):160–173, 2014. [7] Rafael A Calvo and Sidney D’Mello. Aﬀect detection: An interdisciplinary review of models, methods, and their applications. IEEE Transactions on aﬀective com- puting, 1(1):18–37, 2010. [8] Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah- danau, and Yoshua Bengio. On the properties of neu- ral machine translation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014. [9] Scotty Craig, Arthur Graesser, Jeremiah Sullins, and Barry Gholson. Aﬀect and learning: an exploratory look into the role of aﬀect in learning with autotutor. Journal of educational media, 29(3):241–250, 2004. [10] Adji B. Dieng, Chong Wang, Jianfeng Gao, and John William Paisley. Topicrnn: A recurrent neural network with long-range semantic dependency. CoRR, abs/1611.01702, 2016. [11] Sidney D’mello and Arthur Graesser. Mind and body: Dialogue and posture for aﬀect detection in learning environments. Frontiers in Artiﬁcial Intelligence and Applications, 158:161, 2007. [12] Michael Eagle, Ted Carmichael, Jessica Stokes, Mary Jean Blink, John Stamper, and Jason Levin. Predictive student modeling for interventions in online classes. In EDM, 2018. [13] Mi Fei and Dit-Yan Yeung. Temporal models for pre- dicting student dropout in massive open online courses. In ICDMW 2015, 2015. [14] Neil T Heﬀernan and Cristina Lindquist Heﬀernan. The assistments", "meta": {"paper_id": "1905.00422v1", "chunk_index": 15, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_16", "text": "Jean Blink, John Stamper, and Jason Levin. Predictive student modeling for interventions in online classes. In EDM, 2018. [13] Mi Fei and Dit-Yan Yeung. Temporal models for pre- dicting student dropout in massive open online courses. In ICDMW 2015, 2015. [14] Neil T Heﬀernan and Cristina Lindquist Heﬀernan. The assistments ecosystem: Building a platform that brings scientists and teachers together for minimally invasive research on human learning and teaching. Interna- tional Journal of Artiﬁcial Intelligence in Education, 24(4):470–497, 2014. [15] Sepp Hochreiter and J¨urgen Schmidhuber. Long short- term memory. Neural computation, 1997. [16] Douglas Johnstone. A competency alternative: Western governors university. Change: The Magazine of Higher Learning, 37(4):24, 2005. [17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [18] Kevin Kinser. Taking wgu seriously: Implications of the western governors university. Innovative Higher Educa- tion, 26(3):161–173, 2002. [19] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hin- ton. Imagenet classiﬁcation with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097–1105, 2012. [20] Changchun Liu, Pramila Rani, and Nilanjan Sarkar. An empirical study of machine learning techniques for aﬀect recognition in human-robot interaction. In 2005 IEEE/RSJ International Conference on Intelli- gent Robots and Systems, pages 2662–2667. IEEE, 2005. [21] Julia M Matuga. Self-regulation, goal orientation, and academic achievement of secondary students in online university courses. Journal of Educational Technology & Society, 12(3):4, 2009. [22] Jon D Mcauliﬀe and David M Blei. Supervised topic models. In Advances in neural information processing systems, pages 121–128, 2008. [23] Mihalis A Nicolaou, Hatice Gunes, and Maja Pantic. Continuous prediction of spontaneous aﬀect from mul- tiple cues and modalities in valence-arousal space. IEEE Transactions on Aﬀective Computing, 2011. [24] Zachary A Pardos, Ryan SJD Baker, Maria OCZ San Pedro, Sujith M Gowda, and Supreeth M Gowda. Aﬀective states and state tests: Investigating how aﬀect and engagement during the school year predict end-of- year learning outcomes. Journal of Learning Analytics, 1(1):107–128, 2014. [25] Bart Rienties, Avinash Boroowa, Simon Cross, Chris Kubiak, Kevin Mayles, and Sam Murphy. Analyt- ics4action evaluation framework: A review of evidence- based learning analytics interventions at the open uni- versity uk. Journal of Interactive Media in Education, 2016(1), 2016. [26] Ma Mercedes T Rodrigo, Ryan S Baker, Matthew C Jadud, Anna Christine M Amarra, Thomas Dy, Maria Beatriz V Espejo-Lahoz, Sheryl Ann L Lim, Sheila AMS Pascua, Jessica O Sugay, and Emily S Ta- banao. Aﬀective and behavioral predictors of novice programmer achievement. In ACM SIGCSE Bulletin. ACM, 2009. [27] Tanmay Sinha, Patrick Jermann, Nan Li, and Pierre Dillenbourg. Your click decides your fate: Leveraging clickstream patterns in MOOC videos to infer students’ information processing and attrition behavior. CoRR, abs/1407.7131, 2014. [28] Tanmay Sinha, Nan Li, Patrick Jermann, and Pierre Dillenbourg. Capturing ”attrition intensifying” struc- tural traits from didactic interaction sequences of mooc learners. In EMNLP 2014, Workshop on Analysis of Large Scale Social Interaction in MOOCs, 2014. [29] Vernon C Smith, Adam Lange, and Daniel R Huston. Predictive modeling to forecast student outcomes and drive eﬀective interventions in online community college courses. Journal of Asynchronous", "meta": {"paper_id": "1905.00422v1", "chunk_index": 16, "filename": "1905.00422v1.pdf", "chunk_length": 512}}
{"id": "1905.00422v1_chunk_17", "text": "traits from didactic interaction sequences of mooc learners. In EMNLP 2014, Workshop on Analysis of Large Scale Social Interaction in MOOCs, 2014. [29] Vernon C Smith, Adam Lange, and Daniel R Huston. Predictive modeling to forecast student outcomes and drive eﬀective interventions in online community college courses. Journal of Asynchronous Learning Networks, 2012. [30] Steven Tang, Joshua C Peterson, and Zachary A Par- dos. Deep neural networks and how they apply to se- quential education data. In Proceedings of the Third (2016) ACM Conference on Learning@ Scale, pages 321–324. ACM, 2016. [31] Wei Wang, Han Yu, and Chunyan Miao. Deep model for dropout prediction in moocs. In ICCSE 2017, 2017.", "meta": {"paper_id": "1905.00422v1", "chunk_index": 17, "filename": "1905.00422v1.pdf", "chunk_length": 110}}
{"id": "1905.01958v1_chunk_0", "text": "arXiv:1905.01958v1 [cs.CL] 11 Apr 2019 Text2Node: a Cross-Domain System for Mapping Arbitrary Phrases to a Taxonomy Rohollah Soltani and Alexandre Tomberg Knowtions Research Inc. Toronto, Ontario, Canada Abstract Electronic health record (EHR) systems are used extensively throughout the healthcare domain. However, data interchangeability between EHR systems is limited due to the use of diﬀerent coding standards across systems. Existing methods of mapping coding standards based on manual human experts mapping, dictionary mapping, symbolic NLP and classiﬁcation are unscalable and cannot accommodate large scale EHR datasets. In this work, we present Text2Node, a cross-domain mapping system capable of mapping medical phrases to concepts in a large taxonomy (such as SNOMED CT). Te system is designed to generalize from a limited set of training samples and map phrases to elements of the taxonomy that are not covered by training data. As a result, our system is scalable, robust to wording variants between coding systems and can output highly relevant concepts when no exact concept exists in the target taxonomy. Text2Node operates in three main stages: ﬁrst, the lexicon is mapped to word embeddings; second, the taxonomy is vectorized using node embeddings; and ﬁnally, the mapping function is trained to connect the two embedding spaces. We compared multiple algorithms and architectures for each stage of the training, including GloVe and FastText word embeddings, CNN and Bi-LSTM mapping functions, and node2vec for node embeddings. We conﬁrmed the robustness and generalisation properties of Text2Node by mapping ICD-9- CM Diagnosis phrases to SNOMED CT and by zero-shot training at comparable accuracy. Tis system is a novel methodological contribution to the task of normalizing and linking phrases to a taxonomy, advancing data interchangeability in health- care. When applied, the system can use electronic health records to generate an embedding that incorporates taxonomical medical knowledge to improve clinical predictive models. 1 Introduction Electronic heath and medical record (EHR/EMR) systems are steadily gaining in pop- ularity. Ever more facets of healthcare are recorded and coded in such systems, in- cluding patient demographics, disease history and progression, laboratory test results, 1 clinical procedures and medications, and even genetics. Tis trove of information is a unique opportunity to learn paterns that can help improve various aspects of health- care. However, the sheer number of various coding systems used to encode this clin- ical information is a major challenge for anyone trying to analyse structured EHR data. Even the most widely used coding systems have multiple versions to cater to diﬀerent regions of the world. Sofware developed to analyze one version of the cod- ing system may not be used for another version, let alone a diﬀerent coding system. In addition to the public and well documented coding systems, a multitude of private coding mechanisms that have no mappings to any public coding systems are used by insurance companies and certain hospitals. Te eﬀorts to solve this problem range from the development of mapping diction- aries between coding systems to machine learning driven approaches. One example of the former is cTAKES [27], a project that uses annotated lookup tables to map clinical entities to", "meta": {"paper_id": "1905.01958v1", "chunk_index": 0, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_1", "text": "are used by insurance companies and certain hospitals. Te eﬀorts to solve this problem range from the development of mapping diction- aries between coding systems to machine learning driven approaches. One example of the former is cTAKES [27], a project that uses annotated lookup tables to map clinical entities to concepts in a controlled vocabulary such as SNOMED CT (Systematised Nomenclature of Medicine – Clinical Terms, [20]). Te reliance of cTAKES and sim- ilar systems on symbolic natural language processing techniques makes them hard to generalise and scale, especially in view of regular updates and changes to the tar- get vocabulary. An example of the later approach is the work of [39], where word embeddings have been used in a hierarchical structure to annotate and map medical concepts to a reference taxonomy. Teir method is based on classiﬁcation learning and limited in its ability to be applied on controlled vocabularies such as SNOMED CT due to the large size of these vocabularies and small number of terms or phrases associated with each concepts. In this work, we present a system that we call Text2Node that maps phrases from a medical lexicon to a concept in a large target taxonomy (we use SNOMED CT in this paper). Our system is scalable since it relies on lexicon and taxonomy embeddings that are generated in an unsupervised fashion. It is robust to changes in the phrase vocabulary, can output similar concepts in cases when the exact match is not found, and can map to parts of the taxonomy that were not covered by the training data. We achieved this result by relying on node embeddings built from the relationships between the concepts in SNOMED CT, and by replacing the classiﬁcation problem of ﬁnding the best concept with a regression to the nearest node embedding. Applications of the Text2Node system include scalable generation of mapping dic- tionaries for non-public medical coding systems, identiﬁcation and recognition of clinical concepts or named entities in EHRs and EMRs, and generation of phrases representations that are medical concept aware. Technical Signiﬁcance We developed Text2Node as a system that maps phrases from a medical lexicon to concepts in a large target taxonomy. Te system relies on word and node embeddings that are generated in a scalable unsupervised fashion; it is robust to changes in the vocabulary and presence of out-of-vocabulary words; it returns highly relevant concepts even in cases where the exact match is not found or does not exist; and it generalises well as demonstrated by our zero-shot learning experiments. A key ingredient in achieving these results was the transformation of the classiﬁcation task of ﬁnding the best concept into a regression to the nearest node 2 embedding problem. Clinical Relevance Our Text2Node system can be used for generating mapping dictionaries for non-public medical coding systems to large public taxonomies in a scalable way. It is robust to changes in wording between coding systems, it can gen- eralise well from a limited set of training samples, and it can map to elements of taxonomies that are not covered by training", "meta": {"paper_id": "1905.01958v1", "chunk_index": 1, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_2", "text": "dictionaries for non-public medical coding systems to large public taxonomies in a scalable way. It is robust to changes in wording between coding systems, it can gen- eralise well from a limited set of training samples, and it can map to elements of taxonomies that are not covered by training data. Te Text2Node system can also be used for identiﬁcation and recognition of clinical concepts and clinical named entities in electronic medical records. 2 Motivation A natural approach to mapping an arbitrary coding system to a well documented public taxonomy is to start with the phrases that describe each code. Since the coding system must be understandable to its human users, the phrases describing each code must provide enough information for them to be able to use it. Lexical representation Since the seminal work of [24] on Word2Vec, word embed- dings have been widely used to capture the semantic meaning of words, phrases and even sentences. For example, word embeddings have been successfully applied in clin- ical setings to information retrieval, named entity recognition and patient outcome prediction tasks on unstructured text in EMR narratives. Speciﬁcally, this technique assigns real-valued vectors of a ﬁxed length to individual words from a collection of documents, called a corpus. Tis vector representation is such that it captures the se- mantic relationships between words, so that synonyms lie close to each other, while unrelated words are far away. Following Word2Vec, many algorithms have been de- veloped and used successfully to generate word embeddings, including GloVe [26] and FastText [4]. Te deﬁning feature of all of word embedding algorithms is the use of contextual interchangeability as a proxy for relatedness in meaning. However, this feature can be a problem for the task at hand, especially in the medical seting. For example, the terms cold and viral respiratory infection are highly related, but are not ofen used interchangeably in the medical context. Te use of contextual interchangeability as a proxy will lead the word algorithms to incorrectly position the vectors corresponding to these two terms very far from each other in the embedding space. Taxonomical representation In healthcare and biomedical research, the relation- ships between entities contain valuable information, because they describe the inter- actions and causal relationships between diagnosis, medications and procedures, as well as genetic components. To document the complex relationships, large databases have been built, including biomedical knowledge graphs (e.g. PharmGKB [16], Drug- Bank [37] ), ontologies (e.g. Gene Ontology [9]) and taxonomies such as International Statistical Classiﬁcation of Diseases (ICD), and SNOMED CT. Network topology is used to analyse and represent the network structure of these biomedical databases. Such analysis requires high computational costs due to the 3 high dimensionality and sparsity of these databases. Network embedding techno- logies provide new eﬀective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserving its struc- tural properties. Many network embedding algorithms have been developed to embed these graphs into vector spaces (see for example [29]) and used successfully to predict drug-drug interactions [34] and another paper. Mapping between", "meta": {"paper_id": "1905.01958v1", "chunk_index": 2, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_3", "text": "solve the network analysis problem. It converts network into a low-dimensional space while maximally preserving its struc- tural properties. Many network embedding algorithms have been developed to embed these graphs into vector spaces (see for example [29]) and used successfully to predict drug-drug interactions [34] and another paper. Mapping between representations Since contextual interchangeability is not a good metric of medical relatedness, word embeddings cannot be directly used to map between coding systems. However, they can be used to capture semantic information from phrases that are used to describe such systems. In contrast, node embeddings generated from concepts in a medical taxonomy are a beter representation of medical relatedness, because they are built from relationships between medical concepts. In order to bridge the gap between these two embedding spaces, a mapping function is needed. Tis mapping function is to operate on the level of vector representations rather than original phrases and concepts. Tis has two important beneﬁts: these vector spaces are low-dimensional compared to hundreds of thousands of original concepts; the function learned from embeddings should be more generalisable and is in general easier to train. Zero-shot learning Whenever there is scarcity of supervised data, machine learn- ing models ofen fail to carry out reliable generalisations. Obtaining correctly la- belled data is ofen costly and impractical for large datasets. A practical application of concept embedding is the zero-shot transformation of words and concepts that were missing in the training data. It is possible to generalize the mapping function and accurately map unseen concepts, despite having only a few training examples per concept, because embedding training in both domains is an unsupervised task. Tis is done through nearest neighbour retrieval, where the closest embedding in the target space is selected according to some similarity metric. 3 Methods A visual representation of the Text2Node system is provided by Figure 1. Tere are three main stages: ﬁrst, the lexicon is mapped to word embeddings; second, the tax- onomy is vectorized using node embeddings; and ﬁnally, the mapping function is trained to connect the two embedding spaces. 3.1 Word Embeddings Corpus According to [36], word embeddings trained on a biomedical corpus can capture the semantic meaning of medical concepts beter than embeddings trained on an unspecialised set of documents. We have thus constructed a large corpus con- sisting of open access papers from PubMed, free text admission and discharge notes from the MIMIC-III Clinical Database [14, 12], narratives from the US Food and Drug 4 Word embeddings • • • • • Node embeddings • • • • • doc4 doc3 doc2 doc1 Corpus Taxonomy Mapping function Figure 1: Text2Node System overview Administration (FDA) Adverse Event Reporting System (FAERS), and a part of the 2010 Relations Challenge from i2b2 [33]. Te documents from those sources were pre-processed using the Stanford CoreNLP pipeline [23] to split sentences, add spaces around punctuation marks, change all char- acters to lowercase, and reformat to one sentence per line. Finally, all ﬁles were con- catenated into a single document with 235M sentences and 6.25B words. Tis ﬁle was used to train", "meta": {"paper_id": "1905.01958v1", "chunk_index": 3, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_4", "text": "pre-processed using the Stanford CoreNLP pipeline [23] to split sentences, add spaces around punctuation marks, change all char- acters to lowercase, and reformat to one sentence per line. Finally, all ﬁles were con- catenated into a single document with 235M sentences and 6.25B words. Tis ﬁle was used to train both algorithms described below. Algorithms We focused on two algorithms for learning word embeddings: GloVe and FastText. An important distinction between them is the treatment of words that are not part of the training vocabulary: GloVe creates a special out-of-vocabulary token and maps all of these words to this token’s vector, while FastText uses subword information to generate an appropriate embedding. We set the vector space dimensionality to 200 and the minimal number of word occurrences to 10 for both algorithms: this produced a vocabulary of 3.6M tokens. All other algorithm-speciﬁc setings were lef at their default values, since we expected that Text2Node would perform just as well with diﬀerent versions of embeddings. 3.2 Node Embeddings Taxonomy Te taxonomy to which Text2Node maps phrases can be arbitrary. In this paper, we focused on the 2018 international version of SNOMED CT as our target graph G = (V, E). Te vertex set V consists of 392K medical concepts and the edge set E is composed of 1.9M relations between the vertices including is a relationships and atributes such as finding site and due to. Algorithm To construct taxonomy embeddings, we used node2vec method from [13]. We started a random walk on the edges from each vertex v ∈V and stopped it afer a ﬁxed number of steps (20 in our application). All the vertices visited by the walk were considered to be part of the graph neighbourhood N(v) of v. Following the 5 skip–gram architecture from [24], we selected the feature vector assignment function v 7→fn2v(v) ∈R128 by solving the optimisation problem fn2v = argmax f X u∈V log P \u0002 N(u)|f(u) \u0003 , using stochastic gradient descent and negative sampling. 3.3 Mapping Te mapping between phrases and concepts in the target taxonomy was made by associating points in the node embedding vector space to sequences of word embed- dings corresponding to individual words in a phrase. As illustrated in Figure 2, given a phrase consisting of n words with the associated word embeddings w1, . . . , wn, the mapping function was m : (w1, . . . , wn) 7→p, where p ∈R128 is a point in the node embedding vector space. To complete the mapping, we found concepts in the Input Phrase Word 1 Embedding 1 Word 2 Embedding 2 Word 3 Embedding 3 Word 4 Embedding 4 Mapping Function Node Embedding Figure 2: Mapping procedure: the input phrase is split into words that are converted to word embeddings and fed into the mapping function; the output of the function is a point in the node embedding space R128. taxonomy whose node embeddings were the closest to the point p. We experimented with two measures of closeness in the node embedding vector space R128: Euclidean ℓ2 distance and cosine", "meta": {"paper_id": "1905.01958v1", "chunk_index": 4, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_5", "text": "and fed into the mapping function; the output of the function is a point in the node embedding space R128. taxonomy whose node embeddings were the closest to the point p. We experimented with two measures of closeness in the node embedding vector space R128: Euclidean ℓ2 distance and cosine similarity, that is ℓ2 distance (p, q) = ∥p −q∥= p (p −q) · (p −q), cos similarity (p, q) = p · q ∥p∥∥q∥. 6 In some cases, for example to compute the top-k accuracy of the mapping (see Sec- tion 4), a list of k closest concepts was used. Function architectures Te exact form of the mapping function m can vary. In this paper, we tested three architectures: a linear mapping, a convolutional neural network (CNN), and a bidirectional long short term memory network (Bi-LSTM). We padded or truncated all phrases to be exactly 20 words long to represent each phrase by 20 word embeddings w1, . . . , w20 ∈R200 in order to accommodate all three archi- tectures. Linear We concatenated 20 word embeddings into a single 4000 dimensional vector w, and the linear mapping was given by p = m(w) = Mw for a 4000 × 128 matrix M. CNN A convolutional neural network [17] applies convolutional ﬁlters of diﬀerent sizes to the input vectors. Te feature maps produced by the ﬁlters are then fed into a pooling layer followed by a projection layer to obtain an output of desired dimension. Here, we used ﬁlters representing word windows of sizes 1, 2, 3, and 5, followed by a maximum pooling layer and a projection layer to 128 output dimensions. CNN is a non-linear transformation that is more ﬂexible and may be beter suited to capture complex paterns in the input than a linear mapping method. Another useful property of the CNN is its ability to learn invariant features regardless of their position in the phrase. Bi-LSTM A bidirectional long short term memory network (Bi-LSTM) [28] is also a non-linear transformation. Tis type of neural network operates by recursively applying a computation to every element of the input sequence conditioned on the previous computed results in both forward and backward directions. Bi- LSTM excels at learning long distance dependencies in its input. We built a Bi-LSTM to approximate the mapping function m using a single Bi-LSTM cell with 200 hidden units followed by a projection layer to 128 output dimensions. Training We gathered training data consisting of phrase–concept pairs from the taxonomy itself. Because most nodes in SNOMED CT have multiple phrases describ- ing them (synonyms), we considered each synonym–concept pair separately for a total of 269K training examples. To ﬁnd the best mapping function m∗in each of the three architectures described above, we solved the supervised regression problem m∗= argmin m X (phrase,node) m(phrase) −fn2v(node) 2 ℓ2, using the Adam optimizer [15] for 50 epochs. 4 Evaluation and Results First, we evaluated all mapping function architectures to determine the top perform- ing ones using a random hold-out test set of 10K phrase–concept pairs. Next, we 7 tested", "meta": {"paper_id": "1905.01958v1", "chunk_index": 5, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_6", "text": "problem m∗= argmin m X (phrase,node) m(phrase) −fn2v(node) 2 ℓ2, using the Adam optimizer [15] for 50 epochs. 4 Evaluation and Results First, we evaluated all mapping function architectures to determine the top perform- ing ones using a random hold-out test set of 10K phrase–concept pairs. Next, we 7 tested the robustness and generalisability of the winning architectures on a new lex- icon consisting of 8.9K phrases from ICD-9-CM. Finally, we tested the zero-shot learn- ing capability of Text2Node. We did this by randomly selecting 1K concepts from our training set, removing all associated 3.4K phrase–concept pairs from the training dataset, and retraining the system. Te performance was evaluated by mapping the removed phrases to concepts that were never seen before. In these tests we considered two performance metrics: accuracy and mean graph distance. Accuracy is the proportion of test phrases that are mapped exactly to their corres- ponding concept (higher is beter). Tis metric is ofen used for classiﬁcation based approaches and is a standard metric for the mapping problem. Mean graph distance is the average graph distance (length of the shortest path) in the taxonomy between the target concept and the predicted concept (lower is beter). Tis metric tells us how far is the predicted concept from the exact match, and low scores are evidence for predictions that are immediate neigh- bours of the target concept, that is its children or parents. Due to the probabilistic nature of node embeddings and the topology of our target graph, it was possible that two distinct concepts were embedded to the exact same location in the node embedding space. Since the mapping procedure involved nearest neighbour searches, we computed the performance metrics for the top-k results of these searches for k = 1, 5, 10, 20, 50. Tis also allowed us to beter cover the test cases where multiple concepts were been assigned to a phrase. 4.1 Intrinsic Evaluation We used a random hold-out test set of 10K phrase–concept pairs to ﬁnd the top per- forming architectures. Te accuracy results are in Table 1, and the graph distance results are in Table 2. Te overall best system was the Bi-LSTM mapping function on top of FastText embeddings. An interesting ﬁnding was that the cosine similarity yields consistently beter accuracy scores, while the ℓ2 distance provides for slightly smaller average graph distances. Te Bi-LSTM on FastText architecture achieved 23.9% top-1 accuracy with the mean graph distance of 1.27. Tis conﬁrmed our expectation that the majority of cases that were not mapped to the exact target concept were still mapped to one of its immediate neighbours. Te accuracy increased to 67.1% when we considered the top-20 closest matches. 4.2 Extrinsic Evaluation (ICD-9-CM) To check the robustness and generalisability of the Text2Node system, we created an extrinsic evaluation task consisting of 8.9K ICD-9-CM phrases mapped by medical experts from [32] to a unique SNOMED CT concept. Te lexicon of these phrases is diﬀerent from our target taxonomy, because ICD-9-CM was developed for assigning codes to diagnoses and procedures associated with hospital utilization in the United 8", "meta": {"paper_id": "1905.01958v1", "chunk_index": 6, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_7", "text": "extrinsic evaluation task consisting of 8.9K ICD-9-CM phrases mapped by medical experts from [32] to a unique SNOMED CT concept. Te lexicon of these phrases is diﬀerent from our target taxonomy, because ICD-9-CM was developed for assigning codes to diagnoses and procedures associated with hospital utilization in the United 8 Text2Node model Accuracy (top k) Mapping Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 Linear GloVe cos 0.033 0.106 0.163 0.231 0.352 Linear GloVe ℓ2 0.014 0.062 0.099 0.153 0.245 Linear FastText cos 0.029 0.094 0.153 0.226 0.355 Linear FastText ℓ2 0.012 0.050 0.091 0.143 0.237 CNN GloVe cos 0.061 0.176 0.249 0.338 0.485 CNN GloVe ℓ2 0.045 0.150 0.214 0.294 0.419 CNN FastText cos 0.082 0.218 0.304 0.397 0.546 CNN FastText ℓ2 0.067 0.181 0.256 0.350 0.495 Bi-LSTM GloVe cos 0.225 0.442 0.543 0.640 0.762 Bi-LSTM GloVe ℓ2 0.195 0.402 0.497 0.602 0.722 Bi-LSTM FastText cos 0.239 0.466 0.571 0.671 0.785 Bi-LSTM FastText ℓ2 0.201 0.416 0.515 0.616 0.744 Table 1: Intrinsic evaluation results: accuracy on a random hold-out test set of 10K phrase–concept pairs. Text2Node model Mean graph distance (top k) Mapping Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 Linear GloVe cos 2.08 2.23 2.30 2.35 2.42 Linear GloVe ℓ2 1.79 2.07 2.17 2.24 2.32 Linear FastText cos 2.01 2.17 2.23 2.28 2.35 Linear FastText ℓ2 1.78 2.06 2.16 2.23 2.31 CNN GloVe cos 1.71 1.95 2.03 2.11 2.19 CNN GloVe ℓ2 1.60 1.88 1.98 2.06 2.14 CNN FastText cos 1.60 1.86 1.95 2.03 2.13 CNN FastText ℓ2 1.52 1.80 1.91 2.00 2.09 Bi-LSTM GloVe cos 1.28 1.63 1.76 1.88 2.01 Bi-LSTM GloVe ℓ2 1.26 1.61 1.74 1.86 1.98 Bi-LSTM FastText cos 1.27 1.61 1.75 1.87 2.01 Bi-LSTM FastText ℓ2 1.25 1.59 1.73 1.85 1.98 Table 2: Intrinsic evaluation results: mean graph distance on a random hold-out test set of 10K phrase–concept pairs. 9 States. Te accuracyresults are in Table 3, and the graph distance results are in Table 4. Text2Node model Accuracy (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 0.202 0.430 0.541 0.644 0.755 GloVe ℓ2 0.170 0.394 0.501 0.602 0.714 FastText cos 0.210 0.443 0.553 0.658 0.763 FastText ℓ2 0.181 0.403 0.507 0.614 0.725 Table 3: Extrinsic evaluation: accuracy of the Bi-LSTM mapping function on 8.9K ICD-9-CM phrases. Text2Node model Mean graph distance (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 1.42 1.74 1.88 2.00 2.14 GloVe ℓ2 1.40 1.72 1.85 1.97 2.10 FastText cos 1.40 1.73 1.87 1.99 2.14 FastText ℓ2 1.36 1.69 1.84 1.95 2.09 Table 4: Extrinsic evaluation: mean graph distance of the Bi-LSTM mapping function on 8.9K ICD-9-CM phrases. Te Bi-LSTM model on FastText also yielded the best score in this experiment. When using cosine similarity as similarity measurement, it achieved 21% in top-1 ac- curacy and 44% in top-5 accuracy, and when ℓ2", "meta": {"paper_id": "1905.01958v1", "chunk_index": 7, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_8", "text": "4: Extrinsic evaluation: mean graph distance of the Bi-LSTM mapping function on 8.9K ICD-9-CM phrases. Te Bi-LSTM model on FastText also yielded the best score in this experiment. When using cosine similarity as similarity measurement, it achieved 21% in top-1 ac- curacy and 44% in top-5 accuracy, and when ℓ2 metric was used, it achieved 1.36 in graph distance. Te graph distance of 1.36 shows that, although the exact correct match was only 21% at top-1 of Text2Node ranking, all the predicted concepts were close to the exact match in terms of taxonomy distance, that is they are either syn- onyms, parents, or children of the exact match concept from 392K nodes. Tis test set also allowed us to study the eﬀect of extra knowledge of source phrases on mapping to the target taxonomy. In our previous evaluations, we mapped phrases to one of the 392K concepts in the target taxonomy. However, when we reduced the search space to 7.5K concepts that have at least one ICD-9-CM phrase mapped to them, the accuracy and mean graph distance results improved signiﬁc- antly as shown in Tables 5 and 6. Text2Node model Accuracy (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 0.379 0.698 0.793 0.855 0.905 GloVe ℓ2 0.350 0.655 0.754 0.822 0.876 FastText cos 0.389 0.710 0.804 0.863 0.914 FastText ℓ2 0.362 0.658 0.760 0.829 0.887 Table 5: Extrinsic evaluation: accuracy of the Bi-LSTM mapping function on 8.9K ICD-9 phrases with the search space reduced to 7.5K concepts. 10 Text2Node model Mean graph distance (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 1.32 1.80 1.98 2.13 2.29 GloVe ℓ2 1.36 1.82 1.98 2.12 2.26 FastText cos 1.27 1.78 1.98 2.13 2.28 FastText ℓ2 1.31 1.80 1.97 2.11 2.25 Table 6: Extrinsic evaluation: mean graph distance of the Bi-LSTM mapping function on 8.9K ICD-9 phrases with the search space reduced to 7.5K concepts. SNOMED CT consists of a wide category of medical concepts including diseases, procedures, medications, and body structures that are closely related, meaning that these nodes are close in the node embedding space. Terefore, extra knowledge about the category of the taxonomy allowed us to reduce the search space and boost the performance of Text2Node. In this test, the accuracy for top-1 almost doubled to 39% and the graph distance went down to 1.27 by ﬁltering the output concepts to be correlated to the ICD-9 concepts. 4.3 Zero-shot Evaluation To evaluate the zero-shot learning capability, we randomly selected 1K concepts from the taxonomy that appeared in our training set. We then removed all 3.4K phrase– concept pairs associated to the selected concepts from the training set, and used them as the test set. We retrained the Bi-LSTM mapping network from scratch using the new training set, so that all of the targets in the zero-shot test set were never seen before by the mapping function. Te accuracy and mean graph distance results are given", "meta": {"paper_id": "1905.01958v1", "chunk_index": 8, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_9", "text": "training set, and used them as the test set. We retrained the Bi-LSTM mapping network from scratch using the new training set, so that all of the targets in the zero-shot test set were never seen before by the mapping function. Te accuracy and mean graph distance results are given in Tables 7 and 8. Text2Node model Accuracy (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 0.242 0.449 0.538 0.622 0.737 GloVe ℓ2 0.227 0.391 0.481 0.583 0.706 FastText cos 0.221 0.499 0.591 0.671 0.780 FastText ℓ2 0.186 0.439 0.534 0.636 0.734 Table 7: Zero-shot evaluation: accuracyof a newly trained Bi-LSTM mapping function on 3.4K phrases mapping to 1K previously unseen concepts. In this test, we were able achieve comparable results to the intrinsic task evalu- ations from Section 4.1, and we showed that our system is general enough to be able to map to unseen concepts in the training set. 11 Text2Node model Mean graph distance (top k) Word Emb Metric k = 1 k = 5 k = 10 k = 20 k = 50 GloVe cos 1.41 1.74 1.86 1.97 2.09 GloVe ℓ2 1.38 1.70 1.82 1.93 2.04 FastText cos 1.41 1.69 1.81 1.92 2.05 FastText ℓ2 1.38 1.65 1.78 1.88 2.00 Table 8: Zero-shot evaluation: mean graph distance of a newly trained Bi-LSTM map- ping function on 3.4K phrases mapping to 1K previously unseen concepts. 5 Discussion and Related Work 5.1 Discussion In this paper we have described our Text2Node system for mapping arbitrary medical phrases to a large taxonomy. In our system, we ﬁrst used embedding models to pro- ject phrases in the source medical lexicon domain and concept entities in in the target taxonomy domain into two diﬀerent latent spaces. We then learned a mapping func- tion between those latent spaces to associate points in the node embedding space to sequences of word embedding vectors. We used nearest neighbour search in the tar- get domain to ﬁnd candidate medical concepts for a given phrase. We tested a number of word embedding algorithms and mapping function architectures and we obtained the best result using a Bi-LSTM mapping model and FastText word embeddings. In summary, our experiments showed that Text2Node is a robust and generalisable system that achieved a satisfactory result at mapping ICD-9-CM Diagnosis phrases to SNOMED CT and a comparable accuracy at zero-shot training. More generally, we conﬁrmed our hypothesis that utilising the hierarchical structure of a biomedical taxonomy to learn node representations can achieve a beter mapping system between medical phrases and taxonomy concepts. 5.2 Related work Concept mapping is a longstanding problem and there is a variety of existing ap- proaches. For example, cTAKES [27], MetaMap [1], MedLEE [11], KnowledgeMap [10] NOBLE [31], and PDD Graph [35] use annotated lookup tables in order to map clinical entities to concepts in controlled vocabularies such as SNOMED CTand the Uniﬁed Medical Language System (UMLS). To do so, they rely on symbolic natural language processing techniques, which makes them hard to generalise and", "meta": {"paper_id": "1905.01958v1", "chunk_index": 9, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_10", "text": "[10] NOBLE [31], and PDD Graph [35] use annotated lookup tables in order to map clinical entities to concepts in controlled vocabularies such as SNOMED CTand the Uniﬁed Medical Language System (UMLS). To do so, they rely on symbolic natural language processing techniques, which makes them hard to generalise and scale. cTAKES, was developed by Mayo Clinic and later transitioned to an Apache project. MetaMap was developed by the National Library of Medicine for mapping biomedical text to the UMLS Metathesaurus. MedLEE is one of the earliest clinical NLP systems developed and is mostly used for pharmacovigilance. Word embeddings have been widely used in medical domain since they contain rich latent semantic information. A few prominent examples include [25] that trained a set of word embeddings on PubMed data using Word2Vec; [19] that studied drug name recognition utilizing word embeddings; [18] that applied word embedding to 12 extract events from the biomedical text; and [38] that examined the use of word em- beddings for clinical abbreviation disambiguation. Recently researchers have started to explore the possibility of eﬃcient represent- ation learning in the medical domain. [6] utilised a hierarchical structure from the medical term ontology by applying an atention mechanism. [7, 5, 8] learned the rep- resentation of clinical codes using skip-gram from structured visit records of patients. [30] used a modiﬁed restricted RBM for representing EHR concepts using a structured training process. In a similar paper, [21] used AEs to generate concept vectors from word-based concepts extracted from clinical free text. For cross-domain mapping in natural language processing, [2] used linear trans- formation to improve bilingual word embedding mapping, and [3] used adversarial training for cross-lingual word embeddings without parallel data. Finally, in recom- mendation systems, [22] used a cross-domain recommendation system to leverage feedbacks or ratings from multiple domains and improve recommendation perform- ance. 5.3 Text2Node Applications NER Te problem of named entity recognition (NER) is the task of extracting rel- evant concepts from free text. Once extracted, such concepts need to be mapped to a taxonomy of known entities. Our Text2Node system can be used to solve this mapping problem. Medical phrase representation Finding a representation for a phrase that can capture all the information in its sequence of words is a challenging task in natural language processing. Te mapping function in our Text2Node system can be seen as an encoding model that is generating a representations for medical phrases. Since these representations have been trained in a supervised manner using the information from the taxonomy, they also represent their taxonomy position and the structure of their neighbourhood in the taxonomy. Tus this supervised representation of the medical phrases can be used in diﬀerent tasks (i.e. medical outcome prediction) using transfer learning techniques. Zero-shot learning A practical application of embedding mappings is the zero- shot transformation of concepts that were missing in the training data. Our system showed a satisfactory performance therefore it can be used for data pre-processing when the training data is hard to obtain as is ofen the case. 5.4 Future work Our system showed a satisfactory", "meta": {"paper_id": "1905.01958v1", "chunk_index": 10, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_11", "text": "mappings is the zero- shot transformation of concepts that were missing in the training data. Our system showed a satisfactory performance therefore it can be used for data pre-processing when the training data is hard to obtain as is ofen the case. 5.4 Future work Our system showed a satisfactory performance in mapping phrases between two tax- onomies and in a promising result towards solving the zero-shot problem. More work can be done to make it beter. In EHRs, there are ofen phrases with a complex internal structure. For example, a surgery code labelled Eye, Intra-ocular Foreign Body, Removal from Anterior Segment could be decomposed into three parts: location, problem, operation. Te performance 13 of our system is yet to be evaluated by focusing on solely complex phrases. On the other hand, some NLP features such as part of speech can be coupled into our system to solve this problem. In the taxonomy, there exist nodes that have the exact same connections to the rest of the taxonomy. Terefore, they have the same representation in our node em- bedding space. Our current system does not address this issue and these nodes will not diﬀerentiated. Tis problem can be solved by adding extra informative connec- tions to those nodes to make their neighbourhoods diﬀerent. It is also possible to use two taxonomies to train the node embeddings, to diﬀerentiate these nodes and yield beter performance. References [1] Alan R. Aronson and Franois-Michel Lang. An overview of metamap: historical perspective and recent advances. JAMIA, 17(3):229–236, 2010. [2] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. Generalizing and improving bilingual word em- bedding mappings with a multi-step framework of linear transformations. In Tirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. [3] Mikel Artetxe, Gorka Labaka, and Eneko Agirre. A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings. In Proceedings of the 56th Annual Meeting of the Asso- ciation for Computational Linguistics (Volume 1: Long Papers), pages 789–798, Melbourne, Australia, July 2018. Association for Computational Linguistics. [4] Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135–146, 2017. [5] Edward Choi, Mohammad Taha Bahadori, Elizabeth Searles, Catherine Coﬀey, Michael Tompson, James Bost, Javier Tejedor-Sojo, and Jimeng Sun. Multi-layer representation learning for medical concepts. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1495–1504. ACM, 2016. [6] Edward Choi, Mohammad Taha Bahadori, Le Song, Walter F Stewart, and Jimeng Sun. Gram: graph- based atention model for healthcare representation learning. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 787–795. ACM, 2017. [7] Edward Choi, Andy Schuetz, Walter F Stewart, and Jimeng Sun. Medical concept representation learning from electronic health records and its application on heart failure prediction, 2016, arXiv preprint arXiv:1602.03686. [8] Youngduck Choi, Chill Yi-I Chiu, and David Sontag. Learning low-dimensional representations of medical concepts. AMIA Summits on Translational Science Proceedings, 2016:41, 2016. [9] Te Gene Ontology Consortium. Gene ontology consortium: going forward. Nuc- leic Acids Research, 43(D1):D1049–D1056,", "meta": {"paper_id": "1905.01958v1", "chunk_index": 11, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_12", "text": "records and its application on heart failure prediction, 2016, arXiv preprint arXiv:1602.03686. [8] Youngduck Choi, Chill Yi-I Chiu, and David Sontag. Learning low-dimensional representations of medical concepts. AMIA Summits on Translational Science Proceedings, 2016:41, 2016. [9] Te Gene Ontology Consortium. Gene ontology consortium: going forward. Nuc- leic Acids Research, 43(D1):D1049–D1056, 11 2014, htp://oup.prod.sis.lan/nar/article- pdf/43/D1/D1049/7329240/gku1179.pdf. [10] Joshua C. Denny, Plomarz R. Irani, Firas H. Wehbe, Jeﬀrey D. Smithers, and Anderson Spickard III. Te knowledgemap project: Development of a concept-based medical school curriculum database. In AMIA. AMIA, 2003. [11] Carol Friedman, Philip O. Alderson, John H. M. Austin, James J. Cimino, and Stephen B. Johnson. A general natural-language text processor for clinical radiology. Journal of the American Medical Informatics Association, 1(2):161–174, 03 1994. [12] Ary L Goldberger et al. Components of a new research resource for complex physiologic signals, physiobank, physiotoolkit, and physionet, american heart association journals. Circulation, 101(23):1– 9, 2000. [13] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pages 855–864. ACM, 2016. 14 [14] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-wei, Mengling Feng, Mohammad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Anthony Celi, and Roger G Mark. Mimic-iii, a freely accessible critical care database. Scientiﬁc data, 3:160035, 2016. [15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014, arXiv preprint arXiv:1412.6980. [16] Teri E Klein, Jeﬀrey T Chang, Mildred K Cho, Katrina L Easton, Ray Fergerson, Micheal Hewet, Zhen Lin, Y Liu, S Liu, DE Oliver, et al. Integrating genotype and phenotype information: an overview of the pharmgkb project. Te pharmacogenomics journal, 1(3):167, 2001. [17] Yann LeCun, L´eon Botou, Yoshua Bengio, Patrick Haﬀner, et al. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. [18] Chen Li, Runqing Song, Maria Liakata, Andreas Vlachos, Stephanie Seneﬀ, and Xiangrong Zhang. Using word embedding for bio-event extraction. In Proceedings of BioNLP 15, pages 121–126, 2015. [19] Shengyu Liu, Buzhou Tang, Qingcai Chen, and Xiaolong Wang. Eﬀects of semantic features on ma- chine learning-based drug name recognition systems: word embeddings vs. manually constructed dictionaries. Information, 6(4):848–865, 2015. [20] Esther Lozano, Mar Marcos, Begoa Martnez-Salvador, Albert Alonso, and Josep Ramon Alonso. Ex- periences in the development of electronic care plans for the management of comorbidities. In Lecture Notes in Computer Science, volume 5943, pages 113–123, Berlin, Heidelberg, 07 2010. Springer. [21] Xinbo Lv, Yi Guan, Jinfeng Yang, and Jiawei Wu. Clinical relation extraction with deep learning. IJHIT, 9(7):237–248, 2016. [22] Tong Man, Huawei Shen, Xiaolong Jin, and Xueqi Cheng. Cross-domain recommendation: An em- bedding and mapping approach. In IJCAI, pages 2464–2470, 2017. [23] Christopher Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven Bethard, and David Mc- Closky. Te stanford corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55–60, 2014. [24] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word repres- entations in vector space, 2013, arXiv preprint arXiv:1301.3781. [25] SPFGH Moen and", "meta": {"paper_id": "1905.01958v1", "chunk_index": 12, "filename": "1905.01958v1.pdf", "chunk_length": 512}}
{"id": "1905.01958v1_chunk_13", "text": "corenlp natural language processing toolkit. In Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pages 55–60, 2014. [24] Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeﬀrey Dean. Eﬃcient estimation of word repres- entations in vector space, 2013, arXiv preprint arXiv:1301.3781. [25] SPFGH Moen and Tapio Salakoski2 Sophia Ananiadou. Distributional semantics resources for bio- medical text processing. In Proceedings of LBM 2013, pages 39–44, 2013. [26] Jeﬀrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543, 2014. [27] Guergana K Savova, James J Masanz, Philip V Ogren, Jiaping Zheng, Sunghwan Sohn, Karin C Kipper- Schuler, and Christopher G Chute. Mayo clinical text analysis and knowledge extraction system (ctakes): architecture, component evaluation and applications. Journal of the American Medical In- formatics Association, 17(5):507–513, 2010. [28] Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673–2681, 1997. [29] Chang Su, Fei Wang, Jie Tong, Yongjun Zhu, and Peng Cui. Network embedding in biomedical data science, 12 2018, htps://doi.org/10.1093/bib/bby117. [30] Truyen Tran, Tu Dinh Nguyen, Dinh Q. Phung, and Svetha Venkatesh. Learning vector representation of medical objects via emr-driven nonnegative restricted boltzmann machines (enrbm). Journal of Biomedical Informatics, 54:96–105, 2015. [31] Eugene Tseytlin, Kevin Mitchell, Elizabeth Legowski, Julia Corrigan, Girish Chavan, and Rebecca S Jacobson. Noble–ﬂexible concept recognition for large-scale biomedical natural language processing. BMC bioinformatics, 17(1):32, 2016. [32] U.S. National Library of Medicine. Icd-9-cm diagnostic codes to snomed ct map, 2015. [33] ¨Ozlem Uzuner, Bret R South, Shuying Shen, and Scot L DuVall. 2010 i2b2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552–556, 2011. 15 [34] Meng Wang. Predicting rich drug-drug interactions via biomedical knowledge graphs and text jointly embedding, 2017, arXiv preprint arXiv:1712.08875. [35] Meng Wang, Jiaheng Zhang, Jun Liu, Wei Hu, Sen Wang, Xue Li, and Wenqiang Liu. Pdd graph: Bridging electronic medical records and biomedical knowledge graphs via entity linking. In Interna- tional Semantic Web Conference, pages 219–227. Springer, 2017. [36] Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei Wang, Feichen Shen, Paul Kingsbury, and Hongfang Liu. A comparison of word embeddings for the biomedical natural language processing. Journal of biomedical informatics, 87:12–20, 2018. [37] David S Wishart, Craig Knox, An Chi Guo, Savita Shrivastava, Murtaza Hassanali, Paul Stothard, Zhan Chang, and Jennifer Woolsey. Drugbank: a comprehensive resource for in silico drug discovery and exploration. Nucleic acids research, 34(suppl 1):D668–D672, 2006. [38] Jun Xu, Yaoyun Zhang, Hua Xu, et al. Clinical abbreviation disambiguation using neural word em- beddings. In Proceedings of BioNLP 15, pages 171–176, 2015. [39] Sendong Zhao, Ting Liu, Sicheng Zhao, and Fei Wang. A neural multi-task learning frame- work to jointly model medical named entity recognition and normalization, 2018, arXiv preprint arXiv:1812.06081. 16", "meta": {"paper_id": "1905.01958v1", "chunk_index": 13, "filename": "1905.01958v1.pdf", "chunk_length": 472}}
{"id": "1905.01958v1_chunk_14", "text": "named entity recognition and normalization, 2018, arXiv preprint arXiv:1812.06081. 16", "meta": {"paper_id": "1905.01958v1", "chunk_index": 14, "filename": "1905.01958v1.pdf", "chunk_length": 10}}
{"id": "1905.01973v1_chunk_0", "text": "WHO WROTE THIS BOOK? A CHALLENGE FOR E- COMMERCE B´eranger Dumont, Simona Maggio, Ghiles Sidi Said & Quoc-Tien Au Rakuten Institute of Technology Paris {beranger.dumont,simona.maggio}@rakuten.com, {ts-ghiles.sidisaid,quoctien.au}@rakuten.com ABSTRACT Modern e-commerce catalogs contain millions of references, associated with tex- tual and visual information that is of paramount importance for the products to be found via search or browsing. Of particular signiﬁcance is the book category, where the author name(s) ﬁeld poses a signiﬁcant challenge. Indeed, books written by a given author (such as F. Scott Fitzgerald) might be listed with different au- thors’ names in a catalog due to abbreviations and spelling variants and mistakes, among others. To solve this problem at scale, we design a composite system involving open data sources for books as well as machine learning components leveraging deep learning-based techniques for natural language processing. In particular, we use Siamese neural networks for an approximate match with known author names, and direct correction of the provided author’s name using sequence- to-sequence learning with neural networks. We evaluate this approach on product data from the e-commerce website Rakuten France, and ﬁnd that the top proposal of the system is the normalized author name with 72% accuracy. 1 INTRODUCTION Unlike brick-and-mortar stores, e-commerce websites can list hundreds of millions of products, with thousands of new products entering their catalogs every day. The availability and the reliability of the information on the products, or product data, is crucial for the products to be found by the users via textual or visual search, or using faceted navigation. Books constitute a prominent part of many large e-commerce catalogs. Indeed, for the United States alone, more than 300,000 books are published every year and the market value of the book business is estimated to 274 billion euros in 2013 (International Publishers Association (2014)). Relevant book properties include: title, author(s), format, edition, and publication date, among others. In this work, we focus on the names of book authors, as they are found to be extremely relevant to the user and are commonly used in search queries on e-commerce websites, but suffer from considerable variability and noise. To the best of our knowledge, there is no large-scale public dataset for books F. S. Fitzgerald Match with open bibliographic sources Siamese approx. name matching Seq2seq name correction a l d F r a Fitzgerald, F. Scott F. Scott Fitzgerald Frank S. Fitzgerald F. Scott Fitzgerald Ranking ISBN NAME Figure 1: Overview of the system for normalizing author names. Each component is detailed in Section 3. 1 arXiv:1905.01973v1 [cs.CL] 19 Apr 2019 that captures the variability arising on e-commerce marketplaces from user-generated input. Thus, in this work we use product data from Rakuten France (RFR).1 The variability and noise is evident in the RFR dataset. For example, books written by F. Scott Fitzgerald are also listed with the following author’s names: “Francis Scott Fitzgerald” (full name), “Fitzgerald, F. Scott” (inversion of the ﬁrst and last name), “Fitzgerald” (last name only), “F. Scott Fitgerald” (misspelling of the last name), “F SCOTT FITZGERALD” (capitalization and different typological conventions), as well as several combinations", "meta": {"paper_id": "1905.01973v1", "chunk_index": 0, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_1", "text": "Scott Fitzgerald are also listed with the following author’s names: “Francis Scott Fitzgerald” (full name), “Fitzgerald, F. Scott” (inversion of the ﬁrst and last name), “Fitzgerald” (last name only), “F. Scott Fitgerald” (misspelling of the last name), “F SCOTT FITZGERALD” (capitalization and different typological conventions), as well as several combinations of those variations. Less frequently, the author name ﬁeld is missing (displayed as “Unknown”) or incorrectly refers to a different person. The variability of the possible spellings for an author’s name is very hard to capture using rules, even more so for names which are not primarily written in latin alphabet (such as arabic or asian names), for names containing titles (such as “Dr.” or “Pr.”), and for pen names which may not follow the usual conventions. This motivated us to explore automated techniques for normalizing the authors’ names to their best known (“canonical”) spellings. In addition to improvements to the product data, normalizing the authors’ names can be used to help the user ﬁnd other books by the same author. Fortunately, a wealth of open databases exist for books, making it possible to match a signiﬁcant fraction of the books listed in e-commerce catalogs. While not always clean and unambiguous, this information is extremely valuable in itself. Furthermore, along with datasets of name variants, it enables us to train and evaluate machine learning systems to disambiguate and normalize authors’ names. To this end, in addition to the match with open databases, we will explore two different approaches: approximate match with known authors’ names using Siamese neural networks, and direct correction of the provided author’s name using sequence-to-sequence learning with neural networks. Then, we will present our machine learning approach to ranking the results. The rest of the paper is organized as follows: we present the data from RFR and from the open databases in Section 2, before turning to the experimental setup for the overall system and for each of its components in Section 3. Finally, we give results in Section 4, we present related works in Section 5, and conclude in Section 6. 2 BOOK DATA 2.1 RAKUTEN FRANCE DATA The RFR dataset contains 12 million book references. The most relevant product data to help nor- malizing authors’ names is: • ISBN2 in 10 digit or 13 digit format; • product title, which almost always includes the book title, often supplemented with extra information in free text; • author(s) of the book as was provided by the seller (multiple authors should be, but not always are, listed in separate sub-ﬁelds). In particular, the ISBN is a worldwide unique identiﬁer for books, which makes it a prime candidate for unambiguous matching with external sources. In this dataset, an ISBN is present for about 70% of the books. Among the books with no ISBN, 30% are ancient books which are not expected to be associated an ISBN. 2.2 EXTERNAL BIBLIOGRAPHIC RESOURCES There is no central authority providing consistent information on books associated with an ISBN. However, there is a wealth of bibliographic resources and open databases for books. In order to re-", "meta": {"paper_id": "1905.01973v1", "chunk_index": 1, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_2", "text": "no ISBN, 30% are ancient books which are not expected to be associated an ISBN. 2.2 EXTERNAL BIBLIOGRAPHIC RESOURCES There is no central authority providing consistent information on books associated with an ISBN. However, there is a wealth of bibliographic resources and open databases for books. In order to re- trieve the author’s name(s) associated with the books in the RFR dataset, we perform ISBN matching using public APIs on eight of them, listed in Table 1 along with the the fraction of found ISBNs from this dataset. The last three sources are specialized on French books and books translated into French, 1https://fr.shopping.rakuten.com The dataset will be publicly available at https://rit.rakuten.co.jp/data_release. 2International Standard Book Number, see https://www.isbn-international.org 2 Table 1: Performances of the external bibliographic resources used for matching books on RFR via ISBN. Source URL % of ISBNs Open Library https://openlibrary.org 24.9% ISBNdb https://isbndb.com 36.3% Goodreads https://www.goodreads.com 64.7% Google Books https://books.google.com 51.2% OCLC https://www.oclc.org 52.2% BnF http://www.bnf.fr 7.4% Sudoc http://www.sudoc.abes.fr 29.0% Babelio https://www.babelio.com 7.9% which is relevant for the RFR dataset where such books are overrepresented. We ﬁnd that 75% of the books with an ISBN are matched with at least one source. We also ﬁnd the sources to be highly complementary in terms of coverage, and to be independent to a reasonable extent (i.e., returned results can differ signiﬁcantly in case of match with different sources). The match via ISBN on external bibliographic resources is the ﬁrst component of the system depicted in Fig. 1. 2.3 DATASET OF NAME ENTITIES In order to train and evaluate machine learning systems to match or correct authors’ names, a dataset of name entities containing the different surface forms (or variants) of authors’ names is required. The entities should reﬂect as well as possible the variability that can be found in the RFR dataset, as was illustrated in the case of F. Scott Fitzgerald in Section 1. For each entity, a canonical name should be elected and correspond to the name that should be preferred for the purpose of e-commerce (i.e., its most popular variant). Name entities are collected in three distinct ways: 1. ISBN matching: books matched with external sources via ISBN may reveal author names different from one another, and/or compared with the RFR author name ﬁeld, which gives us an entity. The canonical form is the one that is matched with Wikipedia3 or DBpe- dia (Lehmann et al. (2015)); else the one provided by the greatest number of sources. 2. Matching of Rakuten authors: we build entities using fuzzy search on the author name ﬁeld on DBpedia and consider the DBpedia value to be canonical. We limit the number of false positives in fuzzy search by tokenizing both names, and keeping only the names where at least one token from the name on RFR is approximately found in the external resource (i.e., with Levenshtein distance < 2). 3. Name variants: DBpedia, BnF, and JRC-names (Steinberger et al. (2011); Maud et al. (2016)) directly provide data about people (not limited to book authors) and their name variants. While Wikipedia seems more pertinent", "meta": {"paper_id": "1905.01973v1", "chunk_index": 2, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_3", "text": "name on RFR is approximately found in the external resource (i.e., with Levenshtein distance < 2). 3. Name variants: DBpedia, BnF, and JRC-names (Steinberger et al. (2011); Maud et al. (2016)) directly provide data about people (not limited to book authors) and their name variants. While Wikipedia seems more pertinent to select canonical names matching the e-commerce user expectations, specialized librarian data services, such as the Library of Congress Name Authority4, could be used in future research to enrich the dataset of name entities. After creating the name entity dataset, we normalize all names to latin-1. We obtain about 750,000 entities, for a total of 2.1 million names. 2.4 ANNOTATED RAKUTEN FRANCE DATA In order to evaluate the overall system, we need product data from RFR for which the canonical author name has been carefully annotated and can be considered as the ground truth. To this end, we have considered a subset of 1000 books from the RFR dataset, discarding books written by more 3https://www.wikipedia.org 4http://id.loc.gov/authorities/names.html 3 than one author for simplicity.5 We ﬁnd that 467 books have a canonical author name that differs from RFR’s original (unnormalized) author name. Also, 310 do not have an ISBN or do not match on any of the bibliographic resources listed in Section 2.2. Among them, 208 books have a canonical name that differs from the catalog input. 3 EXPERIMENTAL SETUP The overview of the system can be found in Fig. 1. Its ﬁrst component, the matching via ISBN against external databases, has already been presented in Section 2.2. In the rest of this section, we will shed light on the three machine learning components of the system. 3.1 SIAMESE APPROXIMATE NAME MATCHING We want to learn a mapping that assigns a similarity score to a pair of author names such that name variants of the same entity will have high similarity, and names that belong to different entities will have low similarity. To this end, we might use a classical string metric such as the Levenshtein distance or the n-gram distance (Kondrak (2005)). However, those are not speciﬁc to people’s names, and might return a large distance (low similarity) in cases such as the inversion between ﬁrst name and last name or the abbreviation of the ﬁrst name to an initial. Thus, we want to use the dataset of name entities to learn a specialized notion of similarity—this is known as distance metric learning (Kulis et al. (2013)). Once learned, this mapping will enable us to assign an entity to any given name. To learn such a mapping, we use a pair of neural networks with shared weights, or Siamese neu- ral network (Bromley et al. (1994)). Each network is a recurrent neural network (RNN) com- posed of a character-level embedding layer with 256 units, a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber (1997)) with 2 × 128 units, and a dense layer with 256 units. Each network takes a name as input and outputs a representation—the two representations are then compared using cosine similarity. The input to the Siamese network consists of", "meta": {"paper_id": "1905.01973v1", "chunk_index": 3, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_4", "text": "units, a bidirectional long short-term memory (LSTM) (Hochreiter and Schmidhuber (1997)) with 2 × 128 units, and a dense layer with 256 units. Each network takes a name as input and outputs a representation—the two representations are then compared using cosine similarity. The input to the Siamese network consists of a tuple (x1, x2, y), where xi are the input names viewed as a sequence of characters, and the similarity y is equal to 1 for name variants of the same entity, and to 0 otherwise. We preprocess the input by representing all characters in ASCII and lowercase. We consider a sequence length of 32 using zero padding. The weights of the embedding, of the LSTM, and of the dense layers are learned during training. The Siamese network is trained with contrastive loss (Hadsell et al. (2006)) in order to push the similarity towards 1 for similar pairs, and below a certain margin (that we set to 0) for dissimilar pairs. The optimization is done using Adam (Kingma and Ba (2014)), with a learning rate of 10−3 and a gradient clipping value of 5. We use batches of 512 samples, consider a negative to positive pairs ratio of 4 : 1, and randomly generate new negative pairs at every epoch. The architecture is implemented, trained, and tested using Keras6. At test time, we search for the canonical name whose representation is closest to that of the query, using only the high-quality name entities from DBpedia, BnF, and JRC-names. To this end, we do approximate nearest neighbor search using Annoy7. 3.2 NAME CORRECTION WITH SEQ2SEQ NETWORKS We use a generative model to correct and normalize authors’ names directly. The dataset of name entities is again employed to train a sequence-to-sequence (seq2seq) model (Sutskever et al. (2014)) using Keras to produce the canonical form of a name from one of its variants. The dataset is further augmented by including additional variants where the ﬁrst name is abbreviated to an initial. The seq2seq model is an encoder-decoder using RNNs, with a character embedding layer, as in the case of the Siamese network. The encoder is a bi-directional LSTM with 2 × 256 units, while the 5The two RFR datasets (original 12 million books and 1000 annotated books) will be publicly available at https://rit.rakuten.co.jp/data_release. 6https://github.com/keras-team/keras 7https://github.com/spotify/annoy 4 decoder is a plain LSTM with 512 units connected to a softmax layer that computes a probability distribution over the characters. The training is performed by minimizing the categorical cross-entropy loss, using teacher forc- ing (Williams and Zipser (1989)) for faster optimization. The optimization setting is identical to that of the Siamese nework, with batches of 1024 samples. For inference, we collect the 10 output sequences with highest probability using beam search. 3.3 RANKING OF THE PROPOSALS For any given book with an ISBN and an author’s name, all three techniques shown in Fig. 1 provide one or several candidate canonical names. As we aim at providing an automated tool to enhance the quality of the book products, the ﬁnal system should provide a ranked list of candidates with", "meta": {"paper_id": "1905.01973v1", "chunk_index": 4, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_5", "text": "given book with an ISBN and an author’s name, all three techniques shown in Fig. 1 provide one or several candidate canonical names. As we aim at providing an automated tool to enhance the quality of the book products, the ﬁnal system should provide a ranked list of candidates with a calibrated conﬁdence level. For this purpose, we evaluate each candidate canonical name by using logistic regression on a set of 12 features related to their origin. More speciﬁcally, we have binary features for each candidate: 8 indicating whether it is found in the bibliographic sources, one indicating equality with the input name, one indicating if found in the top-10 candidates of the seq2seq model, and one indicating if found by approximate match with the Siamese network. We also have a feature corresponding to the cosine distance between the representation of the proposal and that of the input name. For a given proposal, the selected features reﬂect that the conﬁdence of the global system should increase with (i) the consensus among the different sources, and (ii) the similarity to the input catalog name. We train and evaluate a logistic regression on the annotated dataset introduced in Section 2.4 to estimate the probability that a proposal is the canonical form for an author’s name. This information can then be used to rank all the generated canonical names for a given book. For each book, about 10 proposals are generated, making a dataset of 11185 samples. We split the books between training and test sets, with a ratio of 50% : 50%. As the negative proposals greatly outnumber the positive ones, we use random oversampling on the training data. 4 RESULTS The three machine learning components discussed in the previous section have been individually evaluated on their speciﬁc task. Furthermore the ﬁnal system has been evaluated in terms of correctly normalized book authors in a real case scenario. 4.1 SIAMESE APPROXIMATE NAME MATCHING We evaluate the Siamese network on a held out test set, and compare it to an n-gram distance, by checking that the nearest neighbor of a name variant is the canonical name of the entity to which it belongs. We ﬁnd an accuracy of 79.8% for the Siamese network, against 71.1% for the n-gram baseline with n = 3. We have also checked metrics when introducing a threshold distance above which we consider that no matching entity is found, and found systematic improvement over the baseline. In the ﬁnal system, we set the threshold to inﬁnity. 4.2 NAME CORRECTION WITH SEQ2SEQ NETWORKS Similarly to the previous approach, the seq2seq network is evaluated on a held out test set by check- ing that one of the generated name variants is the canonical name of the entity to which it belongs. As expected, name normalization using seq2seq network gives poorer performances than approxi- mate matching within a dataset of known authors, but constitutes a complementary approach that is useful in case of formatting issues or incomplete names. This approach alone reaches a top-10 accuracy of 42% on the entire test set, 26% on", "meta": {"paper_id": "1905.01973v1", "chunk_index": 5, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_6", "text": "name normalization using seq2seq network gives poorer performances than approxi- mate matching within a dataset of known authors, but constitutes a complementary approach that is useful in case of formatting issues or incomplete names. This approach alone reaches a top-10 accuracy of 42% on the entire test set, 26% on a test set containing only names with initials, and 53% on a test set containing only minor spelling mistakes. 5 Table 2: Global system top-k accuracy at the book level. Type of books # of samples acc@1 acc@3 all 500 72% 85% unnormalized input author 235 49% 67% no ISBN match 151 50% 64% unnormalized + no ISBN 109 35% 49% 4.3 RANKING OF THE PROPOSALS A logistic regression is trained to predict whether a proposed name variant for a given input name is the actual canonical author name, according to features related to the sources providing the proposal, as well as the similarity to the input name. With a decision threshold of p = 0.5, the trained classiﬁer has an accuracy of 93% for both positive and negative candidates in the test set. The coefﬁcients of the estimator reveal the importance of the features and, thus, of the related components. We observe that the three most important contributions are the match with the Siamese network, the match via ISBN in Babelio, and the similarity with the input name in the catalog, conﬁrming the relevance of a multi-approach design choice. Finally, the probability that a proposed name variant is the canonical author name, provided by the resulting classiﬁer, is used as a conﬁdence score to rank the different candidate names returned by the three normalization approaches. 4.4 GLOBAL SYSTEM In order to reﬂect the actual use of the global system on e-commerce catalog data, the ﬁnal evaluation is performed at the book level, by considering all the proposals provided by the different components for a given book. The metric used is the top-k accuracy on the ranked list of proposals for each book; results are summarized in Table 2. We ﬁnd that 72% of the books have the author’s name normalized by the highest ranked proposal. Excluding from the evaluation books where the ground truth for the author’s name equals the catalog value, this accuracy drops to 49%. In the case of books without ISBN or that do not match on any of the bibliographic resources, thus relying on machine learning- based components only, we ﬁnd that 50% of the books are normalized by the top proposal. Finally, for the combination of the above two restrictions, we ﬁnd a top-1 accuracy of 35%. 5 RELATED WORKS There is a long line of work on author name disambiguation for the case of bibliographic citation records (Hussain and Asghar (2017)). While related, this problem differs from the one of book authors. Indeed, unlike most books, research publications usually have several authors, each of them having published papers with other researchers. The relationships among authors, which can be represented as a graph, may be used to help disambiguate the bibliographic citations. In addition, unlike books", "meta": {"paper_id": "1905.01973v1", "chunk_index": 6, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_7", "text": "differs from the one of book authors. Indeed, unlike most books, research publications usually have several authors, each of them having published papers with other researchers. The relationships among authors, which can be represented as a graph, may be used to help disambiguate the bibliographic citations. In addition, unlike books in the context of e-commerce, the author name ﬁeld is usually clean. Named entity linking (Shen et al. (2015)), where one aims at determining the identity of entities (such as a person’s name) mentioned in text, is another related problem. This approach requires a knowledge base to which entity mentions can be linked. The crucial difference with the disambigua- tion of book authors is that entity linking systems leverage the context of the named entity mention to link unambiguously to an entity in the knowledge base. Deep learning approaches similar to ours have been used to tackle related problems. Distance metric learning with neural networks has been used for merging datasets on names (Srinivas et al. (2018)), for normalizing job titles (Neculoiu et al. (2016)), and for the disambiguation of researchers (Zhang et al. (2018)). Sequence-to-sequence learning has been used for the more general task of text nor- malization (Sproat and Jaitly (2016)), and for sentence-level grammar error identiﬁcation (Schmaltz et al. (2016)). To the best of our knowledge, the problem of name normalization speciﬁc to book authors has not been tackled in the previous literature, with the exception of a work on named entity linking for French writers (Frontini et al. (2015)). 6 6 CONCLUSIONS We provided a ﬁrst attempt at solving the problem of author name normalization in the context of books sold on e-commerce websites. To this end, we used a composite system involving open data sources for books, approximate match with Siamese neural networks, name correction with sequence-to-sequence networks, and ranking of the proposals.8 We ﬁnd that 72% of the books have the author’s name normalized by the highest ranked proposal. In order to facilitate future research, we are releasing data from Rakuten France: a large dataset containing product information, and a subset of it with expert human annotation for the authors’ names. They will be accessible at https://rit.rakuten.co.jp/data_release. Multiple challenges remain and are left for future research. First, the system should be extended to handle the case of books with multiple authors. In addition, the book title could be used to help disambiguate between authors and to query external bibliographic resources. This work can also be seen as an intermediate step towards a knowledge base for book author names with name variants, extending public ones such as BnF and DBpedia, using the ISNI9 for easier record linkage whenever available. ACKNOWLEDGMENTS We thank Rapha¨el Ligier-Tirilly for his help with the deployment of the system as microservices, and Laurent Ach for his support. REFERENCES Jane Bromley et al. 1994. Signature veriﬁcation using a “siamese” time delay neural network. NIPS, 737–744. Francesca Frontini et al. 2015. Semantic Web Based Named Entity Linking for Digital Humanities and Heritage Texts. First International Workshop Semantic Web for Scientiﬁc Heritage at the 12th ESWC", "meta": {"paper_id": "1905.01973v1", "chunk_index": 7, "filename": "1905.01973v1.pdf", "chunk_length": 512}}
{"id": "1905.01973v1_chunk_8", "text": "Ach for his support. REFERENCES Jane Bromley et al. 1994. Signature veriﬁcation using a “siamese” time delay neural network. NIPS, 737–744. Francesca Frontini et al. 2015. Semantic Web Based Named Entity Linking for Digital Humanities and Heritage Texts. First International Workshop Semantic Web for Scientiﬁc Heritage at the 12th ESWC Conference. Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality Reduction by Learning an Invariant Mapping. CVPR. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780. Ijaz Hussain and Sohail Asghar. 2017. A Survey of Author Name Disambiguation Techniques: 2010 to 2016. The Knowledge Engineering Review 32 (2017). International Publishers Association. 2014. Annual Report. https: //www.internationalpublishers.org/images/reports/2014/ IPA-annual-report-2014.pdf Diederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. (2014). arXiv:1412.6980 Grzegorz Kondrak. 2005. N-gram similarity and distance. International symposium on string pro- cessing and information retrieval, 115–126. Brian Kulis et al. 2013. Metric learning: A survey. Foundations and Trends in Machine Learning 5, 4 (2013), 287–364. Jens Lehmann et al. 2015. DBpedia - A Large-scale, Multilingual Knowledge Base Extracted from Wikipedia. Semantic Web Journal 6, 2 (2015), 167–195. Ehrmann Maud et al. 2016. JRC-Names: Multilingual Entity Name Variants and Titles as Linked Data. Semantic Web Journal 8, 2 (2016), 283–295. 8The technical debt of such machine learning systems (Sculley et al. (2015)) should be kept in mind. 9International Standard Name Identiﬁer, see http://www.isni.org 7 Paul Neculoiu et al. 2016. Learning Text Similarity with Siamese Recurrent Networks. Proceedings of the 1st Workshop on Representation Learning for NLP. Allen Schmaltz et al. 2016. Sentence-level grammatical error identiﬁcation as sequence-to-sequence correction. 11th Workshop on Innovative Use of NLP for Building Educational Applications. D. Sculley et al. 2015. Hidden Technical Debt in Machine Learning Systems. NIPS. Wei Shen, Jianyong Wang, and Jiawei Han. 2015. Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions. IEEE TKDE (2015). Richard Sproat and Navdeep Jaitly. 2016. RNN Approaches to Text Normalization: A Challenge. (2016). arXiv:1611.00068 Kavitha Srinivas, Abraham Gale, and Julian Dolby. 2018. Merging Datasets Through Deep learning. (2018). arXiv:1809.01604 Ralf Steinberger et al. 2011. JRC-Names: A freely available, highly multilingual named entity resource. RANLP. Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. NIPS. Ronald J Williams and David Zipser. 1989. A learning algorithm for continually running fully recurrent neural networks. Neural computation 1, 2 (1989), 270–280. Yutao Zhang et al. 2018. Name Disambiguation in AMiner: Clustering, Maintenance, and Human in the Loop. KDD. 8", "meta": {"paper_id": "1905.01973v1", "chunk_index": 8, "filename": "1905.01973v1.pdf", "chunk_length": 414}}
{"id": "1905.01984v1_chunk_0", "text": "AI-Powered Text Generation for Harmonious Human- Machine Interaction: Current State and Future Directions Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang, Shaoyang Hao, Zhiwen Yu School of Computer Science, Northwestern Polytechnical University, Xi’an 710129, P.R.China guob@nwpu.edu.cn Abstract—In the last two decades, the landscape of text generation has undergone tremendous changes and is being reshaped by the success of deep learning. New technologies for text generation ranging from template-based methods to neural network-based methods emerged. Meanwhile, the research objectives have also changed from generating smooth and coherent sentences to infusing personalized traits to enrich the diversification of newly generated content. With the rapid development of text generation solutions, one comprehensive survey is urgent to summarize the achievements and track the state of the arts. In this survey paper, we present the general systematical framework, illustrate the widely utilized models and summarize the classic applications of text generation. Keywords—text generation, deep learning, dialog system I. INTRODUCTION Text generation is an important research field in natural language processing (NLP) and has great application prospects which enables computers to learn to express like human with various types of information, such as images, structured data, text, etc., so as to replace human to complete a variety of tasks. The first automatically generated text is dated back to March 17, 2014, when the Los Angeles Times reported the small earthquake occurred near Beverly hills, California by providing detailed information about the time, location and strength of the earthquake. The news was automatically generated by a ‘robot reporter’, which converted the automatically registered seismic data into text by filling in the blanks in the predefined template text [43]. Since then, the landscape of text generation is rapidly expanding. At the initial stage, majority studies focused on how to reduce the grammatical errors of the text to make the generated text more accurate, smooth and coherence. In recent years, deep learning achieved great success in many applications ranging from computer vision, speech processing and natural language processing. Most recent advances in text generation field are based on deep learning technology. Not only the most basic Recurrent Neural Networks (RNN) and Sequence to sequence (Seq2seq), but even the Generative Adversarial Networks (GAN) and the Reinforcement learning are widely used in the field of text generation. With the help of these technologies, the generated text is more coherent, logical and emotionally harmonious. Many dialogue systems have brought great convenience to people's lives such as Microsoft XiaoIce, Contona and Apple siri. They not only help people to accomplish specific tasks, but also communicate with people as a virtual partner. Nowadays, researchers start to consider the research of personalized text generation. Just as we adjust our speaking style according to the characteristics of each other in the daily communication, the text generation process should also dynamically adjust the generation strategy and the final generated content according to the different profiles of the user. Therefore, now the research on personalized text generation is receiving unprecedented attention. Different from prior survey papers on text generation, in this overview, we introduce the most recent progress from the", "meta": {"paper_id": "1905.01984v1", "chunk_index": 0, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_1", "text": "also dynamically adjust the generation strategy and the final generated content according to the different profiles of the user. Therefore, now the research on personalized text generation is receiving unprecedented attention. Different from prior survey papers on text generation, in this overview, we introduce the most recent progress from the methodology perspective and summarize the emerging applications of text generation. According to the difference of data modalities, tasks of text generation can be divided into data-to-text, text-to-text, and image-to-text. Among them, data-to-text tasks include weather forecast generation, financial report generation and so on. Text-to-text tasks include news generation, text summarization, text retelling and review generation are widely studied. While, the image- to-text tasks include image captioning, image questioning & answering, etc. In short, the main contributions of this paper are shown below: • We summarize the most recent progress in text generation and present the widely used models in this field. • We provide one comprehensive collection of primary applications including dialogue systems, text summarization, review generation and image caption & visual question answer, and the key techniques behind them. • Finally, we provide a promising research direction of text generation—the personalized text generation. The remaining of this paper is organized as follows. Section 2 introduces the commonly used models in the field of text generation. Section 3 presents the application scenarios of these models in detail. Section 4 highlights application of personalized text generation in various fields. Section 5 summarizes the evaluation and Section 6 concludes this paper with future work. II. THE TEXT GENERATION MODELS In this section, we will introduce the basic frameworks of the widely applied neural networks models for text generation including Recurrent Neural Networks (RNN), Sequence to sequence (Seq2seq), Generative Adversarial Networks (GAN) and Reinforcement learning. A. Recurrent Neural Network RNN is a special neural network structure, which is proposed according to the view that ‘people's cognition is based on past experience and memory’. Different from deep neural networks (DNN) and convolutional neural networks (CNN), RNN not only considers the input of the previous moment, but also endows the network with a ‘memory’ function of the previous content. The RNN structure is shown in Figure 1. RNN can remember the previous information and apply it to the calculation of the current output. Thus, the nodes between the hidden layer are no longer connectionless but connected, and the input of the hidden layer includes not only the output of the input layer but also the output of the hidden layer at the previous moment. There are also many variations of RNN networks, such as Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). The study in [36] is the pioneering application of RNN for the construction of language models. The experimental results show that the RNN language model outperforms the traditional method. Figure 1. The model structure of RNN B. Sequence to sequence structure Standard seq2seq model used two RNN networks to compose the encoder-decoder structure [7]. The first RNN encoded a sequence of symbols into a fixed length vector representation, and the second RNN decoded the", "meta": {"paper_id": "1905.01984v1", "chunk_index": 1, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_2", "text": "model outperforms the traditional method. Figure 1. The model structure of RNN B. Sequence to sequence structure Standard seq2seq model used two RNN networks to compose the encoder-decoder structure [7]. The first RNN encoded a sequence of symbols into a fixed length vector representation, and the second RNN decoded the representation into another sequence of symbols. The encoder and decoder were jointly trained to maximize the conditional probability of a target sequence given a source sequence. The Seq2seq structure is shown in Figure 2. Figure 2. The model structure of Seq2seq While CNN cannot process sequence data with variable length, and the input and output sequence length of RNN must be the same, using seq2seq model to encode with RNN in encoder stage can receive sequences with indefinite length as input, and in decoder stage can transform the representation vector into sequences without being affected by the input sequence length. Thus, seq2seq is widely used in a variety of tasks including machine translation, text summarization, reading comprehension, and speech recognition, etc. C. GAN and Reinforcement learning GAN[15] proposed by Goodfellow consists of two parts: one generator and one discriminator. The generator is to generate a false sample distribution that is closest to the real samples. The discriminator is used to distinguish generated samples and real samples. The model structure of GAN is shown in Figure 3. While, the original GAN supports well for the continuous data instead of discrete data such as text. To address this problem, researchers have made some fine-tuning to GAN's structure, which brings hope for the generation of discrete data [2; 22]. Zhang et al. used LSTM as generator and CNN as discriminator to implement the task of text generation [70]. The idea of smooth approximation was used to approximate the output of the generator LSTM to solve the gradient inducibility problem caused by the discrete data. Figure 3. The model structure of GAN Reinforcement learning is usually a markov decision process in which performing an action in each state will be rewarded (or get negative reward--punishment). The goal of reinforcement learning is to find the optimal policy to maximize rewards. The dialogue generation task is in line with the operating mechanism of reinforcement learning. The dialogue generation process can be seen as a process of maximizing the expected rewards of generated dialogue content. Through the combination of reinforcement learning and GAN, excellent results have been achieved in the field of text generation. SeqGAN[67] used discriminator in GAN as the source of reward in reinforcement learning. Before the discriminator was updated, the generator continuously optimized itself according to the return score of the current discriminator until the texts generated by the generator were absolutely true. By using the reward mechanism and the policy gradient technologies in reinforcement learning, the problem that the gradient cannot be back propagated when GAN faces discrete data was skillfully avoided. In the interval of training generator with reinforcement learning method, the discriminator was trained with the original method of GAN. III. THE TEXT GENERATION APPLICATIONS In this section, we summarize the classic applications", "meta": {"paper_id": "1905.01984v1", "chunk_index": 2, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_3", "text": "the problem that the gradient cannot be back propagated when GAN faces discrete data was skillfully avoided. In the interval of training generator with reinforcement learning method, the discriminator was trained with the original method of GAN. III. THE TEXT GENERATION APPLICATIONS In this section, we summarize the classic applications of text generation including dialogue systems, text summarization, review generation and image caption & visual question answer. A. Task-oriented Dialogue Systems Dialogue systems attracted more and more attention in recent years. According to different application fields, dialogue systems can be divided into two categories: task- oriented and non-task-oriented dialogue systems (also known as chatbots). Task-oriented dialogue systems help users carry out specific tasks, such as restaurant reservations, travel itineraries, etc. Apple Siri and Microsoft Cortana are the representatives of the task-oriented dialogue system. Recently, deep learning algorithm has been applied to the construction of task-oriented dialogue systems. Deep learning can automatically learn high- dimensional distributed feature representation and reduce the burden of manual design. Using a large amount of dialogue data to build a pure data-driven end-to-end dialogue system to directly map user input to system output is a very popular research direction now. Wen et al. [62] constructed a task-oriented dialogue system by using a modular neural generation model. Neural network is used to realize the process of all modules, and Random Input Real Data Real/ Fake optimize Generator Generated data Discriminator x U s V o W Unfold xt-1 U V W U V W U V W W st-1 st st+1 xt xt+1 ot-1 ot ot+1 Encoder X1 X2 XT1 C X3 Decoder y1 y2 y3 yT2 specific tasks of restaurant reservation was achieved. Bordes et al. used the neural generation model to treat the dialogue process as a mapping between the user input content and the model reply content, and used the encoder-decoder structure to train the mapping relationship [4]. In order to solve the problem of dependence on external knowledge bases in task- oriented dialogue systems, Eric et al. proposed the end-to-end key-value retrieval network in [11], which was equipped with an attention-based key-value retrieval mechanism over entries of a knowledge base, and could extract relevant information from the knowledge base. In addition, the memory network, a variant of RNN, was proposed to store the current user's dialogue context and similar user's conversation history with external memory module [31]. By matching user input and context, appropriate replies could be selected from the alternative reply set. B. Non-task-oriented Dialogue Systems Known as chatbots, non-task-oriented dialogue systems aim to communicate with humans naturally in the open context. Microsoft XiaoIce is a typical chatbots. There are two main design methods for the non-task-oriented dialogue systems: retrieval-based method and generative method. 1) Rtrieval-based methods The retrieval-based method directly selects the corresponding reply from the alternative replies of the given corpus according to the matching principle. In [29], dual encoder model was proposed by Lowe et al. for semantic representation of context and reply content. Context and reply were respectively encoded into semantic vectors by dual RNN model, and then semantic similarity", "meta": {"paper_id": "1905.01984v1", "chunk_index": 3, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_4", "text": "reply from the alternative replies of the given corpus according to the matching principle. In [29], dual encoder model was proposed by Lowe et al. for semantic representation of context and reply content. Context and reply were respectively encoded into semantic vectors by dual RNN model, and then semantic similarity was calculated by matrix transformation. It was found that matching only from the perspective of words could not achieve good results, so Zhou et al. proposed matching through multiple levels (word level and utterance level), and its multi-dimensional thinking provided direction for the following papers [71]. In [72], Zhou et al. used the encoder part of the transformer model obtain the multi-granularity text representation of each context and reply, and then two matching matrices were calculated for the representation under each granularity of utterance-response pair, and the dependency information between the words in utterance and the words in response was also added to the calculation of the alignment matrix as the expression of the words, so as to model a deeper semantic relationship. 2) Generative methods Recently, the data-driven model has been widely studied in the dialogue system. The pure data-driven model directly trains from a large amount of dialogue data without relying on external knowledge. Ritter et al. [49] took the reply generation problem as a translation problem, in which the process of generating replies was regarded to translate the query into corresponding replies. Based on the statistical machine translation model, a generating probability model was proposed to model the dialogue system. The disadvantages of that model are obvious. The most important one is that only one user query is translated into the reply in the translation process without considering the context information in the dialogue, which is obviously unable to work properly in multiple rounds dialogues. With the development of deep learning, neural generation model began to receive attention. Sordoni et al. and Vinyals et al. [53; 59] began to apply RNN to construct the dialogue model, and applied the neural network method to the end-to- end dialogue model for the first time. Based on the Seq2Seq model, the past dialogue history was mapped to the reply. Many existing researches have realized the importance of context. The simplest method is to use RNN to directly encode the dialogue sentences as a whole sequence, and obtain the semantic representation vector of the context, which is treated as additional input in the decoding stage. This method is used by Yan et al. [64] to utilize the context information. Direct concatenation of all sentences may lose the relative relationship between sentences, so researchers have proposed more complex methods to extract context information. Using the multi-layer model to extract context information, the first level is a model of the sentence level to encode the semantic information of a single sentence, and the second is a cross- sentence level model, using the first layer’s output as input, to integrate all the contextual information. Tian et al. [57] carried out experiments on three different cross-sentence methods and came to the conclusion that the performance of the", "meta": {"paper_id": "1905.01984v1", "chunk_index": 4, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_5", "text": "semantic information of a single sentence, and the second is a cross- sentence level model, using the first layer’s output as input, to integrate all the contextual information. Tian et al. [57] carried out experiments on three different cross-sentence methods and came to the conclusion that the performance of the multi-layer context information extraction model outperformed the single-layer model. As stated in [63], in daily human communication, people often associate a dialogue content with related topics in their mind. Based on this assumption, Xing et al. organized content and selected words according to the topics for generating responses. The Latent Dirichlet Allocation (LDA) topic model was used to obtain the topical information in the dialogue sentences, which was taken into consideration as additional input in the decoding process. The experimental results showed that the introduction of topics into the dialogue model is constructive with improved performance. Choudhary et al. Recently, GAN and reinforcement Learning have been applied to dialogue systems. Li et al. through combining GAN and reinforcement learning, jointly trained two models [26]. The generation model aimed to generate reply sequences, and the discriminator was used to distinguish between human- generated and machine-generated dialogue. Li et al. [25] simulated dialogues between two virtual agents, using policy gradient to reward sequences showing three useful dialogue attributes: informativity, coherence, and ease of answering (related to forward-looking function). C. Text Summarization Text summarization is another important research direction in text generation which provides concise description for users by compressing and refining the original text. Text summarization can be regarded as a process of information synthesize, in which one or more input documents are integrated into a short abstract. Banko et al. viewed summarization as a problem analogous to statistical machine translation and generated headlines using statistical models for selecting and ordering the summary words [3]. There are two methods to realize text summarization: retrieval-based method and generative method which will be described in detail in the rest of this chapter. 1) Retrieval-based methods Retrieval-based method is a simple method by selecting a subset of sentences in the original document. This process can be thought of selecting the most central sentences in the document, which contain the necessary and sufficient information related to the subject of the main theme. Nenkova et al. [40] used the word frequency as a feature of the summarization. Three attributes related to word frequency were studied: word frequency, compound function estimating sentence importance from word frequency, and word frequency weight adjusted based on context. Erkan et al. [12] proposed a model based on the centrality (prestige) of eigenvectors, which is known as LexPageRank. This model constructed the sentence connectivity matrix based on cosine similarity. Svore et al. [55] proposed a new automatic summarization method based on neural network, whose name was NetSum. The model retrieved a set of characteristics from each sentence to help determine the importance in the document. In the [6], Cheng et al. used neural network to extract abstract, and word and sentence contents were extracted respectively. What is special about this work is the use of", "meta": {"paper_id": "1905.01984v1", "chunk_index": 5, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_6", "text": "was NetSum. The model retrieved a set of characteristics from each sentence to help determine the importance in the document. In the [6], Cheng et al. used neural network to extract abstract, and word and sentence contents were extracted respectively. What is special about this work is the use of the attention mechanism. They directly used the scores in attention to select sentences in a document, and was actually similar to pointer networks. Cao et al. [5] used the attention mechanism to weight the sentences. The weighted basis was the correlation of document sentences to query (based on attention), and thus extracted the summary by ranking the sentences. The disadvantages of retrieval-based method include the similarity of selected sentences and the lack of logic among the selected sentences. 2) Generative methods Different from retrieval-based method, the generative method is able to generate sentences that are not in the original text, which requires the generative model to have stronger ability of understanding and representation. It is difficult for traditional methods to achieve these abilities. Paulus et al. [45] introduced the application of reinforcement learning method based on Seq2Seq architecture in abstract generation. Pasunuru et al. also used reinforcement learning to generate the summarization of the article [44]. The theme of [50] from Facebook was attention-based NN to generate sentence summarization. Alexander M. Rush et al. proposed a sentence digest model under the framework of encoder-decoder. Later, this method was used in many works to construct training data. Nallapati et al. [39] not only included work on sentence compression, but also presented a new data set about document into a multi-sentence. This paper added a lot of features on it, such as POS tag, TF, IDF, NER tag, etc. The feature-rich encoder proposed in this paper was also of great significance for other work. D. Review Generation Review generation belongs to data-to-text natural language generation [14]. Within the field of recommender systems, a promising application is to estimate (or generate) personalized reviews that a user would write about a product, to discover their nuanced opinions about each of its individual aspects [41]. In order to recommend products to users, we need to ultimately predict how users will react to new products. However, traditional methods often discard comment text, which makes the underlying dimensions of users and products difficult to explain [35]. Huang et al. [16] determined the meaning of words through the context of local and global documents of words and explained homonyms and polysemy by learning multiple embedding of each word. Tang et al. [56] generated natural language in a specific context or context, which introduced two text generation models, C2S and gC2S. The C2S model produced semantically and syntactically coherent sentences, and gC2C did better when the sequence became very long. After identifying the product domain, name, and user rating, the model could generate a review of the corresponding rating like “I love Disney movies but this one was not at all what I expected. The story line was so predictable, it was dumb.” Jaech et al. [17] made full use of", "meta": {"paper_id": "1905.01984v1", "chunk_index": 6, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_7", "text": "identifying the product domain, name, and user rating, the model could generate a review of the corresponding rating like “I love Disney movies but this one was not at all what I expected. The story line was so predictable, it was dumb.” Jaech et al. [17] made full use of RNN and concatenated the context with the word embedding at the input layer of RNN. Experiments on language modeling and classification tasks using three different corpora demonstrated the advantages of this method. Almahairi et al. [1] developed two new models (BoWLF and LMLF) to normalize the rating predictions for the Amazon review data set using text reviews. Lei Zheng et al. [23] proposed a new method for modeling ratings, reviews and their temporal dynamics in conjunction with RNN was proposed. A recurrent network was used to capture the temporal evolution of user and movie states, which were directly used to predict ratings. The user’s movie rating history was used as the input of the updated status. The problem with review generation is how to use fine- grained attributes as input to generate more diverse and user- specific comments. Generation of long comments is also a challenge. E. Image Captioning & Visual Question Answering With the development of social network, the task that generates captions for images received a lot of attention. 1) Image Captioning Image caption is a basic multimodal problem in the field of artificial intelligence, which connects computer vision with natural language generation. It can be divided into two steps, feature extraction and natural language generation. CNN is usually used as the feature extraction sub-model. It can extract significant features, usually represented by the context vector of fixed length. This is followed by a RNN model to generate the corresponding sentence. The whole structure is similar to encoder-decoder structure. Jaech et al. proposed a deep Boltzmann machine in [54] to learn how to generate such multimodal data and shows that the model can be used to create a fused representation by combining features across modes. Kiros et al. [19] introduced the neural language model of multimodal constraint and used CNN to learn the word representation and image features together. Vinyals et al. [60] proposed a generation model based on deep RNN architecture. Given the training image, the model could be trained to maximize the probability of the target sentence. Socher et al. introduced a model that recognized objects in images even when there was no training data available in the object class [52]. And in the completely unsupervised model, the accuracy was up to 90%. Mao et al. [34] proposed a multimodal RNN (m-RNN) model to generate new sentence descriptions explaining the content of images. The model was composed of two subnetworks: sentence depth RNN and image depth CNN. Kulkarni et al. [21] introduced an automatic natural language description generation system based on image, which used a lot of statistical information of text data and computer vision recognition algorithm. The system was very effective in generating image-related sentences. Mitchel et al. [37] used a new method to generate language, in", "meta": {"paper_id": "1905.01984v1", "chunk_index": 7, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_8", "text": "et al. [21] introduced an automatic natural language description generation system based on image, which used a lot of statistical information of text data and computer vision recognition algorithm. The system was very effective in generating image-related sentences. Mitchel et al. [37] used a new method to generate language, in which syntactic models were linked to computer vision detection to generate well-formed descriptions of images by filtering out unlikely attributes and putting objects into ordered syntactic structures. Frome et al. [13] proposed a new deep visual semantic embedding model which used annotated image data and semantic information extracted from unannotated text to identify visual objects. 2) Visual Question Answering Visual question answering (VQA) aims to answer questions about image. The inputs are one image and one question associated with the image, and the output is one answer to the question. The deep learning model of VQA usually uses CNN to acquire image’s information and RNN to encode the question. Ma et al. applied CNN to VQA tasks [32] and provided an end-to-end convolutional framework for learning not only images and problem representations, but also the modal interactions between them to generate answers. Malinowsk et al. [33] also used CNN to encode image and feed the question together with the image representation into the LSTM network. The system was trained to give correct answers to questions about images. Ren et al. [48] used neural networks and visual semantic embedding and not included intermediate stages, such as object construction and image segmentation. There are other methods besides CNN to implement VQA task. Noh et al. [42] used an independent parametric predictive network with a GRU with the question as input and a fully connected layer generating as output. By combining hashing techniques, they reduced the complexity of constructing a parameter prediction network with a large number of parameters. Yang et al. [66] proposed a learning method based on hierarchical attention network, which could help models to answer natural language questions from images. Zhu et al. [73] evaluated several basic patterns of personnel performance and QA tasks and proposed a new LSTM model with spatial attention that can handle 7W quality assurance tasks. IV. PERSONALIZED TEXT GENERATION With the development of deep learning techniques, we hope computers to automatically write high-quality natural language text, but much of the previous research has focused on the generated text content, not the user's personality. In our daily conversation, we will not only consider the fact content to produce the corresponding dialogue content, but also consider other’s personalized profiles, to adjust our dialogue style and strategy. The purpose of personalized text generation is to let the computer imitate the behavior of human beings and take the personalized characteristics of users into consideration when generating text content, so as to dynamically adjust the generated text content and generate more high-quality text. Personalized text generation are embodied in many applications, which will be briefly introduced below. A. Personalized Dialogue Systems Dialogue system is one of the text generation applications that best reflects the user's personalized profiles. In the process of chatting", "meta": {"paper_id": "1905.01984v1", "chunk_index": 8, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_9", "text": "dynamically adjust the generated text content and generate more high-quality text. Personalized text generation are embodied in many applications, which will be briefly introduced below. A. Personalized Dialogue Systems Dialogue system is one of the text generation applications that best reflects the user's personalized profiles. In the process of chatting with different users, if chatbots want to bring pleasant interactive experience to users, it needs to adjust its dialogue strategies and reply content according to different characteristics of users. In [24], the personalized characteristics of the user were modeled for the first time by Li et al., and the persona-based model was proposed. Different user was embedded into the hidden vector space by the similar word embedding method. The user embedding vector was used to adjust the dialogue style and content of the dialogue agent. Kottur et al. [20] extended the previous model, and also carried out vector embedding of user features. Combined with the Hierarchical Recurrent Encoder-Decoder structure, the model could better capture context-related information and considered user personalized features to generate more high-quality dialogue content. Considering the lack of dialogue data with user personalized characteristics, Luan et al. [30] applied the multi- task learning mechanism to the personalized reply generation. A small amount of personalized dialogue data was used to train the reply generation model firstly, and then an auto- encoder model was trained with non-conversational data and the parameters of the two models were shared by the multi- task learning mechanism to obtain the generation model of personalized reply. Mo et al. and Yang et al. made use of the idea of transfer learning [38; 65]. First, they trained a large number of general dialogue data to generate a general reply model, and then used a small amount of personalized dialogue data to fine-tune the model with transfer learning, so that users' personalized information could be considered when generating reply. Considering the different influence of user characteristics on the reply content, Qian et al. [46] applied the supervision mechanism to judge when to express the appropriate user profiles in the reply generation process. Liu et al. [28] built a two-branch neural network to automatically learn user profiles from user dialogues, and then the deep neural network was used to further learn fusion representation the user queries, replies and user profiles, so as to realize the dialogue process from the user's perspective. Zhang et al. All the work above was carried out based on chatbots, and the task-oriented dialogue system is also an important research direction in the dialogue system, but there are few studies that consider the user's personalized information in it. Joshi et al. [18] published the dataset of task-oriented dialogue system, in which each conversation contained the user's personalized information, providing data support for subsequent research. Luo et al. [31] made use of a variant of RNN--memory network to realize the task-oriented personalized dialogue system. The Profile Model was used to encode the user's personalized information, and the Preference Model was used to solve the ambiguity problem of the same query when facing different users. At the", "meta": {"paper_id": "1905.01984v1", "chunk_index": 9, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_10", "text": "et al. [31] made use of a variant of RNN--memory network to realize the task-oriented personalized dialogue system. The Profile Model was used to encode the user's personalized information, and the Preference Model was used to solve the ambiguity problem of the same query when facing different users. At the same time, the similar user’s dialogue history was stored. When the reply content was extracted, the personalized reply content for different users was generated by combining the similar user dialogue history and the user personalized feature information. B. Personalized Review Generation The findings of Tintarev et al. [58] indicated that users mentioned different movie features when describing their favorite movies and short, personalized arguments for users were more persuasive. Therefore, personalized user review generation contributes to better recommend products. Radford et al. [47] demonstrated the direct influence of emotional units on the process of model generation. Lipton et al. [27] built a system of giving user/item combinations to generate the comments that users would write when reviewing the product. They designed a character-level RNN to generate personalized product reviews. The model learned the styles and opinions of nearly a thousand different authors, using a large number of comments from BeerAdvocate.com. The model in [9] was able to generate sentences which was close to a real user's written comments and could identify spelling errors and domain-specific words. Besides RNN, the decoder structure can be LSTM and GRU. Zang et al. [68] introduced a deep neural network model to generate Chinese comments from emotional scores representing user opinions. In this paper, a hierarchical (LSTM) decoder with attention consistency was proposed. Dong et al. [10] proposed an attention-enhanced attribute-to- sequence model to generate product reviews for given attribute information, such as users, products, and ratings. Attribute encoder learned to represent input attributes as vectors. The sequence decoder then generated comments by adjusting the output of these vectors. They also introduced an attention mechanism to syndicate comments and align words with input attributes. Sharma et al. [51] used the model similar to [10] and added loss terms to generate more compliant comments. Ni et al. [41] designed a review generation model that could make use of user and project information as well as auxiliary text input and aspect perception knowledge. In the encoding stage of the model, there were three encoders (sequence encoder, attribute encoder and aspect encoder) for information integration. The decoder's processing of the encoded information biased the GRU model toward generating phrases and statements closest to the input. In addition, GAN can also be used to generate personal reviews. Wang et al. [61] proposed a new punishment-based goal that took a more rational approach to minimizing overall punishment rather than maximizing rewards. Experiments and theories have shown that based on the punishment, it could force each generator to produce multiple texts of specific emotional tags, rather than producing repeated but \"safe\" and \"good\" examples. In addition, multi-class discriminator target allowed the generator to focus more on generating its own specific emotional tag examples. The model could generate a variety of different emotional tags", "meta": {"paper_id": "1905.01984v1", "chunk_index": 10, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_11", "text": "could force each generator to produce multiple texts of specific emotional tags, rather than producing repeated but \"safe\" and \"good\" examples. In addition, multi-class discriminator target allowed the generator to focus more on generating its own specific emotional tag examples. The model could generate a variety of different emotional tags of high-quality text perceptual language. V. EVALUATION METRICS With the continuous development of text generation technology, the corresponding evaluation method has gradually become an active research direction. Researchers need to use the established evaluation method to judge the quality of the proposed models. Good evaluation metric is a key factor to promote the research progress. To date, there are two main methods for text generation evaluation: objective evaluation metric and artificial evaluation. Objective evaluation metric is mainly divided into two aspects, the first is the word overlapping evaluation matrix, such as BLEU and ROUGE, the second is based on word vector evaluation matrix, such as Greedy Matching and Embedding business. A. Word overlap evaluation metrics 1) BLEU (bilingual evaluation understudy) BLEU is a method to compare the n-gram of model output and reference output and calculate the number of matched fragments. To calculate this metric, you need to use machine- translated text (called candidate docs) and some text translated by professional translators (called reference docs). In essence, BLEU is used to measure the degree of similarity between machine translation text and reference text. Its value ranges from 0 to 1, and the closer the value is to 1, the better the machine translation results will be. BLEU adopts the n-gram matching rule, through which it calculates a proportion of N groups of words similar between the comparison translation and the reference translation. BLEU algorithm can give relatively valuable evaluation scores quickly. 2) ROUGE (Recall-Oriented Understudy for Gisting Evaluation) ROUGE evaluates the abstract based on the co-occurrence information of n-gram, which is an evaluation method oriented to the recall rate of n-gram words. Its basic idea is to generate an abstract set of standard abstracts by a number of experts, respectively, and then to compare that automatically generated abstract of the system with the artificially generated standard abstract. The quality of abstract is evaluated by counting the number of overlapping basic units (n-gram, word sequence and word pair) between the two kind of abstract. The stability and robustness of the system are improved by comparing the multi-expert manual abstracts. This method has become one of the general notes of abstract evaluation technique. B. Word vectors evaluation metrics In addition to the word overlap, another way to evaluate the response effect is to judge the relevance of the response by knowing the meaning of each word, and the word vector is the basis of this evaluation method. In accordance with semantic distributions, a vector is assigned to each word by a method such as word2vec, which is used to represent that word, which is represented approximately by calculating the frequency that the word appears in the corpus. All the word vector metrics can be approximated as sentence vectors at the sentence level through vector", "meta": {"paper_id": "1905.01984v1", "chunk_index": 11, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_12", "text": "is assigned to each word by a method such as word2vec, which is used to represent that word, which is represented approximately by calculating the frequency that the word appears in the corpus. All the word vector metrics can be approximated as sentence vectors at the sentence level through vector connection. In this way, the sentence vectors of candidate and target reply sentences can be obtained respectively, and the similarity between them can be obtained by comparing them with cosine distance. 1) Greedy matching The greedy matching method is a matrix matching method based on word level. In the two sentences given 𝑟 and 𝑟̂, every word w ∈r will converse into word vector 𝑒𝑤 after a conversion. At the same time, cosine similarity matching is carried out to the maximum extent with the word vector 𝑒𝑤̂ of each word sequence 𝑤̂ ∈𝑟̂ in 𝑟̂, and the final result is the mean value after all words are matched. Greedy matching was first proposed in the intelligent navigation system, and subsequent studies have found that the optimal solution of this method tends to be the result with a large semantic similarity between the center word and the reference answer. 2) Embedding average The embedding average is the way to calculate a sentence eigenvector by the word vector in the sentence. The vector of a sentence is calculated by averaging the vectors of each word in the sentence. It's a method that has been used in many NLP domains beyond the dialogue system, like calculating the similarity of the text. When comparing two sentences, embedding average can be calculated respectively and then put the cosine similarity of both as indicators to evaluate their similarity. VI. FUTURE DIRECTIONS AND CONCLUSION Within the last two decades, although great achievements of deep learning spur the development of text generation, it is still at the preliminary stage with a large number of open issues. In this section, we will highlight some pending questions to underpin the future of research work. A. Dataset Deficiency Different from computer vision and machine translation, there is a lack of high-quality data in the field of text generation and it is difficult to manually label data. How to use a small amount of data to complete the efficient training of the model is the primary research direction in the future. B. Ultra-long Dialogue Context Human-computer dialogue is a hot area of text generation research. Although current chatbots can preliminarily understand the context, it is still difficult to grasp the ultra- long text. How to effectively capture the semantic information in the text and ensure the consistency of language and logic in the whole dialogue process is a hot topic in the future. C. Co-textual Information Most of the existing studies only focus on the text content, but ignore their co-textual information. However, in reality, natural language is usually generated in a specific environment, such as time, place, emotion or emotion. Therefore, only by considering these co-textual information can the syntactically correct, semantically reasonable and reasonable text content in a specific context be generated.", "meta": {"paper_id": "1905.01984v1", "chunk_index": 12, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_13", "text": "the text content, but ignore their co-textual information. However, in reality, natural language is usually generated in a specific environment, such as time, place, emotion or emotion. Therefore, only by considering these co-textual information can the syntactically correct, semantically reasonable and reasonable text content in a specific context be generated. D. Evaluation Metrics Text generation field is lack of unified evaluation metrics system, the best evaluation method is conducted by artificial judgement. High-quality evaluation metric is crucial to the research in the field of artificial intelligence. Only through reasonable and unified evaluation metric can researchers know whether their research work is reasonable or not. This is a major research gap in the future. E. Personalized text generation Research on personalized text generation is attracting more and more attention. Most of the existing research is based on the encoding of user personalized profiles. How to effectively obtain the relationship between personalized profiles and text content is the focus of future research. Another problem is the impact of the lack of personalized data on model training. How to use a small amount of personalized data to achieve personalized text generation is the focus of researchers. This paper gives a comprehensive introduction to the basic concepts, commonly used models, and popular applications in the text generation. At the same time, some unsolved problems are put forward. Since there are many researchers in each of these work, the relevant research results are also endless, so the text is inevitably missing. We hope that this paper will provide some help to relevant researchers in this field. ACKNOWLEDGMENT This work was partially supported by the National Key R&D Program of China(2017YFB1001800), the National Natural Science Foundation of China (No. 61772428, 61725205). REFERENCES [1] Almahairi, A., Kastner, K., Cho, K., and Courville, A. (2015). Learning distributed representations from reviews for collaborative filtering. In Proceedings of the 9th ACM Conference on Recommender Systems. [2] Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein gan. arXiv preprint arXiv:1701.07875. [3] Banko, M., Mittal, V. O., and Witbrock, M. J. (2000). Headline generation based on statistical translation. In Proceedings of the 38th Annual Meeting on Association for Computational Linguistics. [4] Bordes, A., Boureau, Y.-L., and Weston, J. (2016). Learning end-to- end goal-oriented dialog. arXiv preprint arXiv:1605.07683. [5] Cao, Z., Li, W., Li, S., Wei, F., and Li, Y. (2016). Attsum: Joint learning of focusing and summarization with neural attention. arXiv preprint arXiv:1604.00125. [6] Cheng, J., and Lapata, M. (2016). Neural summarization by extracting sentences and words. arXiv preprint arXiv:1603.07252. [7] Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., et al. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. [8] Choudhary, S., Srivastava, P., Ungar, L., and Sedoc, J. (2017). Domain aware neural dialog system. arXiv preprint arXiv:1708.00897. [9] Costa, F., Ouyang, S., Dolog, P., and Lawlor, A. (2018). Automatic generation of natural language explanations. In Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion. [10] Dong, L., Huang, S., Wei, F., Lapata, M., Zhou, M., and Xu, K. (2017). Learning", "meta": {"paper_id": "1905.01984v1", "chunk_index": 13, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_14", "text": "dialog system. arXiv preprint arXiv:1708.00897. [9] Costa, F., Ouyang, S., Dolog, P., and Lawlor, A. (2018). Automatic generation of natural language explanations. In Proceedings of the 23rd International Conference on Intelligent User Interfaces Companion. [10] Dong, L., Huang, S., Wei, F., Lapata, M., Zhou, M., and Xu, K. (2017). Learning to generate product reviews from attributes. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers. [11] Eric, M., and Manning, C. D. (2017). Key-value retrieval networks for task-oriented dialogue. arXiv preprint arXiv:1705.05414. [12] Erkan, G., and Radev, D. R. (2004). Lexpagerank: Prestige in multi- document text summarization. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing. [13] Frome, A., Corrado, G. S., Shlens, J., Bengio, S., Dean, J., and Mikolov, T. (2013). Devise: A deep visual-semantic embedding model. In Advances in neural information processing systems. [14] Gatt, A., and Krahmer, E. (2018). Survey of the state of the art in natural language generation: Core tasks, applications and evaluation. Journal of Artificial Intelligence Research, 61, 65-170. [15] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al. (2014). Generative adversarial nets. In Advances in neural information processing systems. [16] Huang, E. H., Socher, R., Manning, C. D., and Ng, A. Y. (2012). Improving word representations via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. [17] Jaech, A., and Ostendorf, M. (2017). Improving Context Aware Language Models. [18] Joshi, C. K., Mi, F., and Faltings, B. (2017). Personalization in goal- oriented dialog. arXiv preprint arXiv:1706.07503. [19] Kiros, R., Salakhutdinov, R., and Zemel, R. (2014). Multimodal neural language models. In International Conference on Machine Learning. [20] Kottur, S., Wang, X., and Carvalho, V. (2017). Exploring Personalized Neural Conversational Models. In IJCAI. [21] Kulkarni, G., Premraj, V., Dhar, S., Li, S., Choi, Y., Berg, A. C., et al. (2011). Baby talk: Understanding and generating image descriptions. In Proceedings of the 24th CVPR. [22] Kusner, M. J., and Hernández-Lobato, J. M. (2016). Gans for sequences of discrete elements with the gumbel-softmax distribution. arXiv preprint arXiv:1611.04051. [23] Lei, Z., Noroozi, V., and Yu, P. S. (2017). Joint Deep Modeling of Users and Items Using Reviews for Recommendation. In Tenth Acm International Conference on Web Search & Data Mining. [24] Li, J., Galley, M., Brockett, C., Spithourakis, G. P., Gao, J., and Dolan, B. (2016). A persona-based neural conversation model. arXiv preprint arXiv:1603.06155. [25] Li, J., Monroe, W., Ritter, A., Galley, M., Gao, J., and Jurafsky, D. (2016). Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541. [26] Li, J., Monroe, W., Shi, T., Jean, S., Ritter, A., and Jurafsky, D. (2017). Adversarial learning for neural dialogue generation. arXiv preprint arXiv:1701.06547. [27] Lipton, Z. C., Vikram, S., and McAuley, J. (2015). Generative concatenative nets jointly learn to write and classify reviews. arXiv preprint arXiv:1511.03683. [28] Liu, B., Xu, Z., Sun, C., Wang, B., Wang, X., Wong, D. F., et al. (2018). Content-Oriented User Modeling for Personalized", "meta": {"paper_id": "1905.01984v1", "chunk_index": 14, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_15", "text": "dialogue generation. arXiv preprint arXiv:1701.06547. [27] Lipton, Z. C., Vikram, S., and McAuley, J. (2015). Generative concatenative nets jointly learn to write and classify reviews. arXiv preprint arXiv:1511.03683. [28] Liu, B., Xu, Z., Sun, C., Wang, B., Wang, X., Wong, D. F., et al. (2018). Content-Oriented User Modeling for Personalized Response Ranking in Chatbots. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 26(1), 122-133. [29] Lowe, R., Pow, N., Serban, I., and Pineau, J. (2015). The ubuntu dialogue corpus: A large dataset for research in unstructured multi- turn dialogue systems. arXiv preprint arXiv:1506.08909. [30] Luan, Y., Brockett, C., Dolan, B., Gao, J., and Galley, M. (2017). Multi-task learning for speaker-role adaptation in neural conversation models. arXiv preprint arXiv:1710.07388. [31] Luo, L., Huang, W., Zeng, Q., Nie, Z., and Sun, X. (2018). Learning personalized end-to-end goal-oriented dialog. arXiv preprint arXiv:1811.04604. [32] Ma, L., Lu, Z., and Li, H. (2016). Learning to answer questions from image using convolutional neural network. In Thirtieth AAAI Conference on Artificial Intelligence. [33] Malinowski, M., Rohrbach, M., and Fritz, M. (2017). Ask your neurons: A deep learning approach to visual question answering. International Journal of Computer Vision, 125(1-3), 110-135. [34] Mao, J., Xu, W., Yang, Y., Wang, J., and Yuille, A. L. (2014). Explain images with multimodal recurrent neural networks. arXiv preprint arXiv:1410.1090. [35] McAuley, J., and Leskovec, J. (2013). Hidden factors and hidden topics: understanding rating dimensions with review text. In Proceedings of the 7th ACM conference on Recommender systems. [36] Mikolov, T., Karafiát, M., Burget, L., Černocký, J., and Khudanpur, S. (2010). Recurrent neural network based language model. In Eleventh annual conference of the international speech communication association. [37] Mitchell, M., Han, X., Dodge, J., Mensch, A., Goyal, A., Berg, A., et al. (2012). Midge: Generating image descriptions from computer vision detections. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics. [38] Mo, K., Zhang, Y., Li, S., Li, J., and Yang, Q. (2018). Personalizing a dialogue system with transfer reinforcement learning. In Thirty- Second AAAI Conference on Artificial Intelligence. [39] Nallapati, R., Zhou, B., Gulcehre, C., and Xiang, B. (2016). Abstractive text summarization using sequence-to-sequence rnns and beyond. arXiv preprint arXiv:1602.06023. [40] Nenkova, A., Vanderwende, L., and McKeown, K. (2006). A compositional context sensitive multi-document summarizer: exploring the factors that influence summarization. In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. [41] Ni, J., and McAuley, J. (2018). Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). [42] Noh, H., Hongsuck Seo, P., and Han, B. (2016). Image question answering using convolutional neural network with dynamic parameter prediction. In Proceedings of the IEEE conference on computer vision and pattern recognition. [43] Oremus, W. (2014). The first news report on the LA earthquake was written by a robot. Slate. In. [44] Pasunuru, R., and Bansal, M. (2018). Multi-reward reinforced summarization with saliency and entailment. arXiv preprint arXiv:1804.06451. [45] Paulus, R., Xiong, C.,", "meta": {"paper_id": "1905.01984v1", "chunk_index": 15, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_16", "text": "the IEEE conference on computer vision and pattern recognition. [43] Oremus, W. (2014). The first news report on the LA earthquake was written by a robot. Slate. In. [44] Pasunuru, R., and Bansal, M. (2018). Multi-reward reinforced summarization with saliency and entailment. arXiv preprint arXiv:1804.06451. [45] Paulus, R., Xiong, C., and Socher, R. (2017). A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304. [46] Qian, Q., Huang, M., Zhao, H., Xu, J., and Zhu, X. (2018). Assigning Personality/Profile to a Chatting Machine for Coherent Conversation Generation. In IJCAI. [47] Radford, A., Jozefowicz, R., and Sutskever, I. (2017). Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444. [48] Ren, M., Kiros, R., and Zemel, R. (2015). Exploring models and data for image question answering. In Advances in neural information processing systems. [49] Ritter, A., Cherry, C., and Dolan, W. B. (2011). Data-driven response generation in social media. In Proceedings of the conference on empirical methods in natural language processing. [50] Rush, A. M., Chopra, S., and Weston, J. (2015). A neural attention model for abstractive sentence summarization. arXiv preprint arXiv:1509.00685. [51] Sharma, V., Sharma, H., Bishnu, A., and Patel, L. (2018). Cyclegen: Cyclic consistency based product review generator from attributes. In Proceedings of the 11th International Conference on Natural Language Generation. [52] Socher, R., Ganjoo, M., Manning, C. D., and Ng, A. (2013). Zero-shot learning through cross-modal transfer. In Advances in neural information processing systems. [53] Sordoni, A., Galley, M., Auli, M., Brockett, C., Ji, Y., Mitchell, M., et al. (2015). A neural network approach to context-sensitive generation of conversational responses. arXiv preprint arXiv:1506.06714. [54] Srivastava, N., and Salakhutdinov, R. R. (2012). Multimodal learning with deep boltzmann machines. In Advances in neural information processing systems. [55] Svore, K., Vanderwende, L., and Burges, C. (2007). Enhancing single- document summarization by combining RankNet and third-party sources. In Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP-CoNLL). [56] Tang, J., Yang, Y., Carton, S., Zhang, M., and Mei, Q. (2016). Context-aware natural language generation with recurrent neural networks. arXiv preprint arXiv:1611.09900. [57] Tian, Z., Yan, R., Mou, L., Song, Y., Feng, Y., and Zhao, D. (2017). How to make context more useful? an empirical study on context- aware neural conversational models. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). [58] Tintarev, N., and Masthoff, J. (2007). Effective explanations of recommendations: user-centered design. In Proceedings of the 2007 ACM conference on Recommender systems. [59] Vinyals, O., and Le, Q. (2015). A neural conversational model. arXiv preprint arXiv:1506.05869. [60] Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2015). Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recognition. [61] Wang, K., and Wan, X. (2018). SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks. In IJCAI. [62] Wen, T.-H., Vandyke, D., Mrksic, N., Gasic, M., Rojas-Barahona, L. M., Su, P.-H., et al. (2016). A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562. [63] Xing, C., Wu,", "meta": {"paper_id": "1905.01984v1", "chunk_index": 16, "filename": "1905.01984v1.pdf", "chunk_length": 512}}
{"id": "1905.01984v1_chunk_17", "text": "pattern recognition. [61] Wang, K., and Wan, X. (2018). SentiGAN: Generating Sentimental Texts via Mixture Adversarial Networks. In IJCAI. [62] Wen, T.-H., Vandyke, D., Mrksic, N., Gasic, M., Rojas-Barahona, L. M., Su, P.-H., et al. (2016). A network-based end-to-end trainable task-oriented dialogue system. arXiv preprint arXiv:1604.04562. [63] Xing, C., Wu, W., Wu, Y., Liu, J., Huang, Y., Zhou, M., et al. (2017). Topic aware neural response generation. In Thirty-First AAAI Conference on Artificial Intelligence. [64] Yan, R., Song, Y., and Wu, H. (2016). Learning to respond with deep neural networks for retrieval-based human-computer conversation system. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval. [65] Yang, M., Zhao, Z., Zhao, W., Chen, X., Zhu, J., Zhou, L., et al. (2017). Personalized response generation via domain adaptation. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval. [66] Yang, Z., He, X., Gao, J., Deng, L., and Smola, A. (2016). Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition. [67] Yu, L., Zhang, W., Wang, J., and Yu, Y. (2017). Seqgan: Sequence generative adversarial nets with policy gradient. In Thirty-First AAAI Conference on Artificial Intelligence. [68] Zang, H., and Wan, X. (2017). Towards automatic generation of product reviews from aspect-sentiment scores. In Proceedings of the 10th International Conference on Natural Language Generation. [69] Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., and Weston, J. (2018). Personalizing Dialogue Agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243. [70] Zhang, Y., Gan, Z., and Carin, L. (2016). Generating text via adversarial training. In NIPS workshop on Adversarial Training. [71] Zhou, X., Dong, D., Wu, H., Zhao, S., Yu, D., Tian, H., et al. (2016). Multi-view response selection for human-computer conversation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. [72] Zhou, X., Li, L., Dong, D., Liu, Y., Chen, Y., Zhao, W. X., et al. (2018). Multi-turn response selection for chatbots with deep attention matching network. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). [73] Zhu, Y., Groth, O., Bernstein, M., and Fei-Fei, L. (2016). Visual7w: Grounded question answering in images. In Proceedings of the IEEE conference on computer vision and pattern recognition.", "meta": {"paper_id": "1905.01984v1", "chunk_index": 17, "filename": "1905.01984v1.pdf", "chunk_length": 392}}
{"id": "1905.01998v1_chunk_0", "text": "A Persona-based Multi-turn Conversation Model in an Adversarial Learning Framework Oluwatobi O. Olabiyi Capital One Conversation Research Vienna, VA Email: oluwatobi.olabiyi@capitalone.com Anish Khazane Capital One Conversation Research San Francisco, CA Email: anish.khazane@capitalone.com Erik T. Mueller Capital One Conversation Research Vienna, VA Email: erik.mueller@capitalone.com Abstract—In this paper, we extend the persona-based sequence- to-sequence (Seq2Seq) neural network conversation model to multi-turn dialogue by modifying the state-of-the-art hredGAN architecture. To achieve this, we introduce an additional input modality into the encoder and decoder of hredGAN to capture other attributes such as speaker identity, location, sub-topics, and other external attributes that might be available from the corpus of human-to-human interactions. The resulting persona hredGAN (phredGAN) shows better performance than both the existing persona-based Seq2Seq and hredGAN models when those external attributes are available in a multi-turn dialogue corpus. This superiority is demonstrated on TV drama series with character consistency (such as Big Bang Theory and Friends) and customer service interaction datasets such as Ubuntu dialogue corpus in terms of perplexity, BLEU, ROUGE, and Distinct n- gram scores. I. INTRODUCTION Recent advances in machine learning especially with deep neural networks has led to tremendous progress in natural language processing and dialogue modeling research [1]–[3]. Nevertheless, developing a good conversation model capable of ﬂuent interactions between a human and a machine is still in its infancy. Most existing work relies on a limited dialogue history to produce responses with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models ﬁnd it difﬁcult to disambiguate characteristics such as speaker personality, location, and sub-topic in the data. Most work in this domain has primarily focused on opti- mizing dialogue consistency. For example, Serban et al. [3]– [5] and Xing et al. [6] introduce a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not support any guarantees on the generator output since the output conditional probability is not calibrated. Olabiyi et al. [7] tackle this problem by training a modiﬁed HRED generator alongside an adversarial discriminator in order to provide a stronger guarantee to the generator’s output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modalities within a dataset and fails to generate persona-speciﬁc responses in datasets with multiple modalities. At the same time, there has been some recent work on introducing persona into dialogue models. For example, Li et al. [8] integrate learnable speaker attributes into a single- turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models: one with Speaker-only representation and another with Speaker and Addressee representations (Speaker-Addressee model), both of which capture aspects of speaker identity and interactions. Nguyen et al. [9] continue the same line of thought by considering a Seq2Seq dialogue model with Responder-only representations. In both cases, the attribute representation is learned during the system", "meta": {"paper_id": "1905.01998v1", "chunk_index": 0, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_1", "text": "representation and another with Speaker and Addressee representations (Speaker-Addressee model), both of which capture aspects of speaker identity and interactions. Nguyen et al. [9] continue the same line of thought by considering a Seq2Seq dialogue model with Responder-only representations. In both cases, the attribute representation is learned during the system training. Zhang et al. [10] propose a slightly different approach. Here, the attributes are a set of sentences describing the proﬁle of the speaker. In this case, the attribute representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona- based models have a limited dialogue history (single-turn), suffer from exposure bias worsening the trade off between personalization and conversation quality, and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses even though they generally capture the persona of the speaker. To overcome these limitations, we propose phredGAN, a multi-modal hredGAN dialogue system which additionally conditions the adversarial framework proposed by Olabiyi et al. [7] on speaker and/or utterance attributes in order to maintain response quality of hredGAN and still capture speaker and other modalities within a conversation. The attributes can be seen as another input modality as the utterance. The attribute representation is an embedding that is learned together with the rest of model parameters, similar to [8]. The introduction of attributes allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring what-if scenarios of model responses. We train and sample the proposed phredGAN similar to the procedure for hredGAN [7]. To demonstrate model capability, we train on customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poster and answerer, and character consistent TV scripts from two popular series, The Big Bang Theory and Friends with quantitative and qualitative analysis. We demonstrate system superiority over hredGAN and the state-of-the-art persona conversational model in terms arXiv:1905.01998v1 [cs.CL] 29 Apr 2019 TABLE I SAMPLE OF phredGAN OUTPUTS WITH DISCRIMINATOR SCORE. Source Speaker D(G(.)) Utterance Big Bang Theory and Friends TV Series Context 0 Penny NA You have to, we all have to be there at the same time to yell surprise!. Context 1 Howard NA Okay, you have to understand something, we’re in a hospital right now. Response 0 Penny 0.1705 Okay, I’m sorry. I’m not really even like that. Response 1 Penny 0.1426 Okay, I’m sorry, I’m sorry. Response 2 Penny 0.0688 Okay. Context 0 Leonard NA Holy crap, Sheldon, did you just ﬁgure out a method for synthesizing a new stable super-heavy element? Context 1 Sheldon NA Did I? Well, that can’t be right. No one’s ever done that before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard 0.3976 Sheldon , what are you doing ? Response 1 Leonard 0.3503 Sheldon ? Context 0 Leonard NA She’s gotten really hooked on Age of Conan, she’s playing non-stop. Response 0 Raj 0.4890 Okay", "meta": {"paper_id": "1905.01998v1", "chunk_index": 1, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_2", "text": "before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard 0.3976 Sheldon , what are you doing ? Response 1 Leonard 0.3503 Sheldon ? Context 0 Leonard NA She’s gotten really hooked on Age of Conan, she’s playing non-stop. Response 0 Raj 0.4890 Okay , so we know , what about you and Penny doing here ? Response 1 Raj 0.3586 Okay , so we would have to say that about people ? Response 2 Raj 0.1113 Okay , let ’ s go . UDC Context 0 asker NA all i need some quick amarok help. i cant hear my music Context 1 helper NA is amarok muted? Context 2 asker NA no Response 0 helper 0.3079 use the UNK drivers , and then run the ” UNK ” command to get the UNK Response 1 helper 0.1283 what is the error message ? Response 2 helper 0.0725 what version of ubuntu ? Context 0 asker NA anyone had problems with the kernel update from today? giving me a kernel panic Context 1 helper NA you can select previous kernels at the bootloader (grub) menu on booth. Response 0 asker 0.3665 it says that it is not installed . . . Response 1 asker 0.3195 i’m not sure what i can ﬁnd . . . Response 2 asker 0.0186 it’s a UNK , I’m not sure of the way . Context 0 asker NA how do I install Ubuntu? Response 0 helper 0.5797 use the alternate cd , it should be in the repos , it’s a good place to get the source of the kernel Response 1 helper 0.1984 use the UNK package , it should work . . . Response 2 helper 0.0131 use the UNK of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE, and distinct n-gram scores. The rest of the paper is organized as follows: In section II, we describe the model architecture. Section III describes model training and evaluation while section IV contains experiments and result discussion. Finally, section V concludes and explores some future work directions. II. MODEL ARCHITECTURE In this section, we brieﬂy introduce the state-of-the-art hredGAN model and subsequently show how we derive the persona version by combining it with the distributed representation of the dialogue speaker and utterance attributes. A. hredGAN: Adversarial Learning Framework The hredGANproposed by Olabiyi et. al [7] contains three major components. Encoder: The encoder consists of three RNNs, the aRNN that encodes an utterance for attention memory, the eRNN that encodes an utterance for dialogue context, and the cRNN that encodes a multi-turn dialogue context from the eRNN outputs. The ﬁnal states of aRNN and cRRN are concatenated to form the ﬁnal encoder state. Generator: The generator contains a single decoder RNN, dRNN, initialized with the ﬁnal encoder state. The dRNN inputs consists of cRNN output, embedding of the ground truth or previously generated tokens, as well as noise samples and the attention over the aRNN outputs. Discriminator: The discriminator also contains a single RNN, DRNN, initialized by the ﬁnal encoder", "meta": {"paper_id": "1905.01998v1", "chunk_index": 2, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_3", "text": "RNN, dRNN, initialized with the ﬁnal encoder state. The dRNN inputs consists of cRNN output, embedding of the ground truth or previously generated tokens, as well as noise samples and the attention over the aRNN outputs. Discriminator: The discriminator also contains a single RNN, DRNN, initialized by the ﬁnal encoder state. In the case of hredGAN [7], it is a bidirectional RNN that discriminates at the word level to capture both the syntactic and semantic difference between the ground truth and the generator output. Problem Formulation: The hredGAN [7] formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances, Xi = \u0000X1, X2, · · · , Xi \u0001 , where each utterance Xi = \u0000X1 i , X2 i , · · · , XMi i \u0001 contains a variable-length sequence of Mi word tokens such that Xi j ∈V for vocabulary V , the dialogue model produces an output Yi = \u0000Y 1 i , Y 2 i , · · · , Y Ti i \u0001 , where Ti is the number of generated tokens. The framework uses a conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens. The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D, akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss LMLE(G) between the ground truth Xi+1, and the generator output Yi. The following objective summarizes both goals: G∗, D∗= arg min G max D \u0000λGLcGAN(G, D)+ λMLMLE(G) \u0001 . (1) where λG and λM are hyperparameters and LcGAN(G, D) and LMLE(G) are deﬁned in Eqs. (5) and (7) of [7] respec- tively. Note that the generator G and discriminator D share the same encoder and embedding representation of the word tokens. B. phredGAN: Persona Adversarial Learning Framework The proposed architecture of phredGAN is very sim- ilar to that of hredGAN summarized above. The only difference is that the dialogue history is now Xi = \u0000(X1, C1), (X2, C2), · · · , (Xi, Ci) \u0001 where Ci is additional input that represents the speaker and/or utterance attributes. Note that Ci can either be a sequence of tokens or a single token such that Ci j ∈V c for vocabulary V c. The embedding for attribute tokens is also learned similar to that of word tokens. The modiﬁed system is as follows: Encoder: In addition to the three RNNs in the encoder of hredGAN, if the attribute Ci is a sequence of tokens, then another RNN, sattRNN is used to summarize the token embeddings; otherwise a single attribute embedding is concatenated with the output of eRNN as shown in Fig. 1. Generator: In addition to the dRNN in the generator of hredGAN, if the attribute Ci+1 is a sequence of tokens, then another RNN, tattRNN is used to summarize the attribute token embeddings; otherwise a single attribute embedding is concatenated with the other inputs of dRNN as shown in Fig.", "meta": {"paper_id": "1905.01998v1", "chunk_index": 3, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_4", "text": "1. Generator: In addition to the dRNN in the generator of hredGAN, if the attribute Ci+1 is a sequence of tokens, then another RNN, tattRNN is used to summarize the attribute token embeddings; otherwise a single attribute embedding is concatenated with the other inputs of dRNN as shown in Fig. 1. Discriminator: In addition to the DRNN in the discriminator of hredGAN, if the attribute Ci+1 is a sequence of tokens, then the same tattRNN is used to summarize the attribute token embeddings; otherwise the single attribute embedding is concatenated with the other inputs of DRNN in Fig. 1 of [7]. Fig. 1. The PHRED generator with local attention - Encoder: Encoder RNN, eRNN, Attention RNN, aRNN and the Context RNN, cRRN. Generator: Decoder RNN, dRNN. The same encoder is shared between the generator and the discriminator (Fig. 1 of [7]). The attributes C allow the generator to condition its response on the utterance attributes such as speaker identity, subtopics, and so on. Noise Injection: Although [7] demonstrated that injecting noise at the word level seems to perform better than at the utterance level for hredGAN, we found that this is dataset- dependent for phredGAN. The phredGAN model with utterance-level noise injection and word-level noise injection are tagged phredGANu and phredGANw respectively. The losses, LcGAN(G, D) and LMLE(G) in eq. (1) are then respectively updated as: LcGAN(G, D) = EXi,Ci+1,Xi+1[log D(Xi, Ci+1, Xi+1)]+ EXi,Ci+1,Zi[1 −log D(Xi, Ci+1, G(Xi, Ci+1, Zi))] (2) LMLE(G) = EXi+1[−log PG \u0000Xi+1|Xi, Ci+1, Zi \u0001 ]. (3) The addition of speaker or utterance attributes allows the dialogue model to exhibit personality traits giving consistent responses across style, gender, location, and so on. III. MODEL TRAINING AND INFERENCE A. Model Training We train both the generator and the discriminator (with a shared encoder) of phredGAN using the same training procedure in Algorithm 1 with λG = λM = 1 [7]. Since the encoder, word embeddings and attribute embeddings are shared, we are able to train the system end-to-end with back- propagation. Encoder: The encoder RNNs, eRNN, and eRNN are bidirectional while cRRN is unidirectional. All RNN units are 3-layer GRU cells with a hidden state size of 512. We use a word vocabulary size, V = 50, 000, with a word embedding size of 512. The number of attributes, V c is dataset-dependent Algorithm 1 Adversarial Learning of phredGAN Require: A generator G with parameters θG. Require: A discriminator D with parameters θD. Require: Training hyperparameters, λG, and λM. for number of training iterations do Initialize cRNN to zero state, h0 Sample a mini-batch of conversations, X = {Xi, Ci}N i=1, Xi = \u0000(X1, C1), (X2, C2), · · · , (Xi, Ci) \u0001 with N utterances. Each utterance mini batch i contains Mi word tokens. for i = 1 to N −1 do Update the context state. hi = cRNN(eRNN(E(Xi)), hi−1, Ci) Compute the generator output similar to Eq. (11) in [7]. PθG \u0000Yi|, Zi, Xi, Ci+1 \u0001 = \b PθG \u0000Y j i |X1:j−1 i+1 , Zj i , Xi, Ci+1 \u0001 Mi+1 j=1 Sample a corresponding", "meta": {"paper_id": "1905.01998v1", "chunk_index": 4, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_5", "text": "1 to N −1 do Update the context state. hi = cRNN(eRNN(E(Xi)), hi−1, Ci) Compute the generator output similar to Eq. (11) in [7]. PθG \u0000Yi|, Zi, Xi, Ci+1 \u0001 = \b PθG \u0000Y j i |X1:j−1 i+1 , Zj i , Xi, Ci+1 \u0001 Mi+1 j=1 Sample a corresponding mini batch of utterance Yi. Yi ∼PθG \u0000Yi|, Zi, Xi, Ci+1 \u0001 end for Compute the adversarial discriminator accuracy Dacc over N −1 utterances {Yi}N−1 i=1 and {Xi+1}N−1 i=1 if Dacc < accDth then Update phredGAN’s θD with gradient of the discriminator loss. P i [∇θD log D(hi, Ci+1, Xi+1) + ∇θD log \u00001 −D(hi, Ci+1, Yi) \u0001 ] end if if Dacc < accGth then Update θG with the generator’s MLE loss only. P i [∇θG −log PθG \u0000Yi|, Zi, Xi, Ci+1 \u0001 ] else Update θG with attribute, adversarial and MLE losses. P i [λG∇θG log D(hi, Ci+1, Yi) + λM∇θG−log PθG \u0000Yi|, Zi, Xi, Ci+1 \u0001 ] end if end for but we use an attribute embedding size of 512. In this study, we only use one attribute per utterance, so there is no need to use RNNs, sattRNN and tattRNN to combine the attribute embeddings. Generator: The generator RNN, dRNN is also a 3-layer GRU cell with a hidden state size of 512. The aRNN outputs are connected to the dRNN input using an additive attention mechanism [11]. Discriminator: The discriminator RNN, DRNN, is a bidirec- tional RNN, each 3-layer GRU cell having a hidden state size of 512. The output of both the forward and the backward cells for each word are concatenated and passed to a fully-connected layer with binary output. The output is the probability that the word is from the ground truth given the past and future words of the sequence as well as the responding speaker’s embedding. Others: All parameters are initialized with Xavier uniform random initialization [12]. Due to the large word vocabulary size, we use sampled softmax loss [13] for MLE loss to expedite the training process. However, we use full softmax for model evaluation. The parameter update is conditioned on the discriminator accuracy performance as in [7] with accDth = 0.99 and accGth = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework. B. Model Inference We use an inference strategy similar to the approach in Olabiyi et. al [7]. The only differences between training and inference are : (i) The generator is run in autoregressive mode with greedy decoding by passing the previously generated word token to the input of the dRNN at the next step. (ii) A modiﬁed noise sample N(0, αI) is passed into the generator input. For the modiﬁed noise sample, we perform a linear search for α with sample size L = 1 based on the average discriminator loss, −logD(G(.)) [7] using trained models run in autore- gressive mode to reﬂect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During", "meta": {"paper_id": "1905.01998v1", "chunk_index": 5, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_6", "text": "noise sample, we perform a linear search for α with sample size L = 1 based on the average discriminator loss, −logD(G(.)) [7] using trained models run in autore- gressive mode to reﬂect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding, and the attribute embedding of the intended responder. With multiple noise samples, L = 64, we rank the generator outputs by the discriminator which is also conditioned on the encoder outputs, and the intended responder’s embedding. The ﬁnal response is the response ranked highest by the discriminator. IV. EXPERIMENTS AND RESULTS In this section, we explore phredGAN’s results on two conversational datasets and compare its performance to the persona system in Li et al. [8] and hredGAN [7] in terms of quantitative and qualitative measures. A. Datasets TV Series Transcripts dataset [3]. We train our model on transcripts from the two popular TV drama series, Big Bang Theory and Friends. Following a preprocessing setup similar to [8], we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue. We split the corpus into training, development, and test sets with 94%, 3%, and 3% proportions, respectively, and TABLE II predGAN VS. SEQ2SEQ PERSONA MODELS [8] PERFORMANCE. METRIC SM1 [8] SAM [8] phredGAN TV SERIES PERPLEXITY 25.0 25.4 25.9 BLEU-4 1.88% 1.90% 3.00% ROGUE-2 - - 0.4044 DISTINCT-1 - - 0.1765 DISTINCT-2 - - 0.2164 pair each set with a corresponding attribute ﬁle that maps speaker IDs to utterances in the combined dataset. Due to the small size of the combined transcripts dataset, we ﬁrst train our model on the larger Movie Triplets Corpus (MTC) by Banchs et al. [14] which consists of 240,000 dialogue triples. We pre-train our model on this dataset to initialize our model parameters to avoid overﬁtting on a relatively small persona TV series dataset. After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization [12] for training on the combined person TV series dataset. Ubuntu Dialogue Corpus (UDC) dataset [4]. We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow the same training, development, and test split as the UDC dataset in [7], with 90%, 5%, and 5% proportions, respectively. While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that are not classiﬁed under either a questioner or helper speaker type. To remain consistent, we assume that there are only two speaker types within this dataset and that the ﬁrst utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into the model’s ability to construct attribute embeddings. However, our experimental results demonstrate that our model is still", "meta": {"paper_id": "1905.01998v1", "chunk_index": 6, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_7", "text": "assume that there are only two speaker types within this dataset and that the ﬁrst utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into the model’s ability to construct attribute embeddings. However, our experimental results demonstrate that our model is still able to differentiate between the larger two speaker types in the dataset. B. Evaluation Metrics We use similar evaluation metrics as in [7] including perplexity, BLEU [15], ROUGE [16], and distinct n-gram [17] scores. C. Baseline We compare our system to [8] which uses a Seq2Seq framework in conjunction with learnable persona embeddings. Their work explores two persona models to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their Seq2Seq model, i.e., Speaker and Speaker- Addressee models. While we compare with both models quantitatively, we mostly compare with the Speaker-Addressee 1SM stands for Speaker Model and SAM stands for Speaker-Addressee Model TABLE III phredGAN VS. hredGAN [7] PERFORMANCE METRIC hredGAN [7] phredGAN UDC PERPLEXITY 48.18 27.3 ROUGE-2 (F1) 0.1252 0.1692 DISTINCT-1 14.05% 20.12% DISTINCT-2 31.24% 24.53% model qualitatively. Our quantitative comparison uses perplexity and BLEU-4 scores as those are the ones reported in [8]. In addition, we also measure our model performance in terms of ROGUE and Distinct n-gram scores for the purpose of completeness. For fair comparison, we use the same TV drama series dataset used in their study. We also compare our system to hredGAN from [7] in terms of perplexity, ROGUE, and distinct n-grams scores. In [7], the authors recommend the version with word-level noise injection, hredGANw, so we use this version in our comparison. Also for fair comparison, we use the same UDC dataset as reported in [7]. The only addition we made is to add the speaker attribute to the utterances of the dataset as described in the Dataset subsection. D. Hyperparameter search Prior to evaluation, we determine the noise injection method and the optimum noise variance α that are suitable for the two datasets. We consider the two variations of phredGAN, that is, phredGANu for utterance-level and phredGANw for word- level noise injections. We notice a partial mode collapse with phredGANw on the combined TV transcripts, likely due to the high variation of word-level perturbation on a very limited dataset. However, this issue was rectiﬁed by phredGANu. Therefore, we use phredGANu for the combined TV series dataset and phredGANw for the UDC dataset. We perform a linear search for optimal noise variance values between 1 and 30 at an increment of 1, with a sample size of L = 1. We obtained an optimal α of 2 with phredGANu for the combined TV series dataset and an optimal α of 5 for phredGANw for the much larger UDC dataset. E. Results After training phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN are listed in Table I. The table shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii)", "meta": {"paper_id": "1905.01998v1", "chunk_index": 7, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_8", "text": "After training phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN are listed in Table I. The table shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii) generates responses condi- tioned on a persona attribute; (iii) generates multiple responses per dialogue context and scores their human likelihood by the discriminator. We observe that the discriminator score is generally reasonable with longer, more informative, and more persona-related responses receiving higher scores. It is worth noting that this behavior, although similar to the behavior of a human judge, is learned without supervision. Furthermore, we observe that phredGAN responses retain contextual consistency, sometimes referencing background information inherent in the conversation between two speakers. For example, in the second sample of the TV series in Table I, phredGAN generator, Leonard refers to Sheldon by name. Also, in the third sample, phredGAN, Raj refers to Penny when responding to Leonard who happens to be Penny’s boy friend. We observe similar persona-based response generation for the UDC dataset with distinct communication styles between the asker and the helper. We will now present performance comparisons of phredGAN against the baselines, hredGAN, and Li et al.’s persona Seq2Seq models. 1) Quantitative Analysis: We ﬁrst report the performance of phredGANu on TV series transcripts in table II. Our system actually performs slightly worse than both variations of the Speaker Model and Speaker-Addressee systems in [8] in terms of the perplexity measure. This is because the entropy of multi-turn dialogue is higher than that of single-turn. Similar observations have been made by Serban et al. [3] about seq2seq and HRED dialogue models. However, phredGANu gives a signiﬁcantly larger BLEU-4 score than both Speaker and Speaker-Addressee models. We attribute this improvement to (i) the multi-turn dialogue context, and (ii) training in an adversarial framework, which forces the model to produce longer, more informative, and diverse responses that have high topic relevance even with a limited dataset. Also, unlike Speaker-Addressee models which suffer from lower response quality due to persona conditioning, conditioning the generator and discriminator of phredGAN on speaker embeddings does not compromise the system’s ability to produce diverse responses. This problem might have been alleviated by the adversarial training too. We also compare phredGAN with the recommended variant of hredGAN that includes word-level noise injection at the decoder on the UDC dataset. The results are summarized in Table III. We note an improvement in a variety of evaluation metrics including perplexity, ROUGE, and distinct n-grams, with the exception of distinct 2-grams. This is expected as phredGAN should be generally less diverse than hredGAN since the number of distinct data distribution modes is more for the phredGAN dataset due to the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN. Also, the better ROUGE(f1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset.", "meta": {"paper_id": "1905.01998v1", "chunk_index": 8, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_9", "text": "the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN. Also, the better ROUGE(f1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset. 2) Qualitative Analysis: A qualitative assessment of these results are in Table IV with responses from several characters in the TV series dataset and the two characters in UDC. We see that for TV drama series, phredGAN responses are comparatively more informative than that of the Speaker- Addressee model of [8]. For example, with Speaker-Addressee model, nearly all the characters in the TV series respond with “Of course I love you.” to the dialogue context, “Do TABLE IV PHREDGAN SAMPLE RESPONSES ON TV SERIES AND UDC. Model Speaker Response Big Bang Theory Series Context Sheldon Do you like me? phredGAN Leonard Yes ? phredGAN Penny No , you don ’ t understand . You ’ re an idiot . phredGAN Howard Well , I don ’ t know what to say . This is the part I ’ ve ever seen . phredGAN Raj Well , you know , we could be a little more than my friends ’ s friends . Context Emily Do you like me? Speaker-Addressee Sheldon Of course, I love you, Emily Speaker-Addressee Penny Of course, I love you, Emily. Speaker-Addressee Rachel Of course I love you. Context 0 Howard We cant go home. Context 1 Leonard Alright, I’ll be right back. phredGAN Howard Okay , well , before you and Penny get out and get married with Penny just get married. Context 0 Amy Spent a lot of my childhood throwing coins into wishing wells hoping for friends Context 1 Penny Whoa, whoa, give me a second to catch up here. Hold on Context 2 Amy That’d be my boyfriend. Happier playing his dopey Star Trek game with his friends than hanging out with me. phredGAN Penny You know , when you ’ re being so nice about Sheldon and I want to take him to like his own girlfriend Context Sheldon I recently read that during World War Two, Joseph Stalin had a research program to create supersoldiers by having women impregnated by gorillas. predGAN Howard You ’ re right . I don ’ t know what you ’ re talking about , Sheldon . Friends TV Series Context Rachel Do you like me? phredGAN Joey Yeah . phredGAN Ross Well , no , no , no . I mean , I just , I don ’ t know . . . phredGAN Monica Well , I do . And I ’ m not gonna tell you another guy . phredGAN Chandler No , I do not . You are a mean little man . . Context Rachel Do you like me? Speaker-Addressee Chandler Of course I love you. Ubuntu Context Asker i need to install ubuntu on a pendrive so i can install software on the pendrive after booting from the usb, is it possible?", "meta": {"paper_id": "1905.01998v1", "chunk_index": 9, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_10", "text": "not . You are a mean little man . . Context Rachel Do you like me? Speaker-Addressee Chandler Of course I love you. Ubuntu Context Asker i need to install ubuntu on a pendrive so i can install software on the pendrive after booting from the usb, is it possible? phredGAN Helper you can just boot into the live cd and install grub. Context Asker anyone had problems with the kernel update from today? giving me a kernel panic there a simple way to make a rollback? anyone if I can go back to my previous kernel? phredGAN Helper what version of ubuntu are you using ? Context Helper did you see my message about looking at other log ﬁles? predGAN Asker yeah , it ’ s not that I ’ m trying to get the UNK to work Context Asker you can select previous kernels at the bootloader (grub) menu on boot phredGAN Helper i ’ m not to be able to get the grub to the grub menu you love me?” despite the fact that some of the responders sometimes have unfriendly relationship with the addressee. Many of the novel situations explored by phredGAN are unachievable with the Speaker-Addressee model due to a lack of informative responses. For example, by conditioning as Sheldon from The Big Bang Theory and asking “Do you like me?”, our model responds with annoyance if conditioned as Penny (“No, you don’t understand. You’re an idiot”), brevity with Leonard (“Yes?”), and sarcasm with Raj (“Well , you know , we could be a little more than my friend’s friends.”) The wide range of responses indicate our model’s ability to construct distinct attribute embeddings for each character even from a limited dataset. The other interesting responses in Table IV indicate our model’s ability to infer not only the context of the conversation but important character information about the Addressee. We also see similar results with our model’s output on UDC. We demonstrate that by conditioning as either a helper or questioner from the UDC dataset, our model is able to respond differently to input utterances as well as stay close to the context of the conversation. V. CONCLUSION AND FUTURE WORK In this article, we improve upon state-of-the-art persona- based response generation models by adding attribute embed- dings to hredGAN in order to capture a representation of speaker identity and style in a multi-turn dialogue. Our model outperforms the existing Speaker-Model and Speaker-Addressee models from [8] with respect to BLEU score and even improves upon hredGAN with respect to ROUGE, distinct n-grams, and perplexity scoring. We also see a qualitative improvement in response quality that more clearly separates speaker identity from even limited datasets. In the future, we hope to carry out a human evaluation to conﬁrm qualitative improvement of our model’s outputs. We also hope to extend this work to Multi-class GAN by optimizing over two separate adversarial losses, one for speaker attribute classiﬁcation and the other for dialogue generation real/fake discrimination. This will allow us to further improve on persona distinctions without the loss of", "meta": {"paper_id": "1905.01998v1", "chunk_index": 10, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_11", "text": "qualitative improvement of our model’s outputs. We also hope to extend this work to Multi-class GAN by optimizing over two separate adversarial losses, one for speaker attribute classiﬁcation and the other for dialogue generation real/fake discrimination. This will allow us to further improve on persona distinctions without the loss of response quality. REFERENCES [1] I. Sutskever, O. Vinyals, and Q. Le, “Sequence to sequence learning with neural networks,” in Proceedings of Advances in Neural Information Processing Systems (NIPS), 2014, pp. 3104–3112. [2] O. Vinyals and Q. Le, “A neural conversational model,” in Proceedings of ICML Deep Learning Workshop, 2015. [3] I. Serban, A. Sordoni, Y. Bengio, A. Courville, and J. Pineau, “Building end-to-end dialogue systems using generative hierarchical neural network models,” in Proceedings of The Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI 2016), 2016, pp. 3776–3784. [4] I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville, and Y. Bengio, “A hierarchical latent variable encoder-decoder model for generating dialogue,” in Proceedings of The Thirty-ﬁrst AAAI Conference on Artiﬁcial Intelligence (AAAI 2017), 2017. [5] I. V. Serban, T. Klinger, G. Tesauro, K. Talamadupula, B. Zhou, Y. Bengio, and A. Courville, “Multiresolution recurrent neural networks: An application to dialogue response generation,” in Proceedings of The Thirty-ﬁrst AAAI Conference on Artiﬁcial Intelligence (AAAI 2017), 2017. [6] C. Xing, W. Wu, Y. Wu, M. Zhou, Y. Huang, and W. Ma, “Hierarchical recurrent attention network for response generation,” in arXiv preprint arXiv:1701.07149, 2017. [7] O. Olabiyi, A. Salimov, A. Khazane, and E. Mueller, “Multi-turn dialogue response generation in an adversarial learning framework,” in arXiv preprint arXiv:1805.11752, 2018. [8] J. Li, M. Galley, C. Brockett, G. Spithourakis, J. Gao, and B. Dolan, “A persona-based neural conversation model,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 2016, pp. 994–1003. [9] H. Nguyen, D. Morales, and T. Chin, “A neural chatbot with personality,” in Stanford NLP Course website: https://web.stanford.edu/class/cs224n/reports/2761115.pdf, 2018. [10] S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, and J. Weston, “Personalizing dialogue agents: I have a dog, do you have pets too?” in arXiv preprint arXiv:1801.07243v3, 2018. [11] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proceedings of International Conference of Learning Representation (ICLR 2015), 2015. [12] X. Glorot and Y. Bengio, “Understanding the difﬁculty of training deep feedforward neural networks,” in International conference on artiﬁcial intelligence and statistics, 2010. [13] S. Jean, K. Cho, R. Memisevic, and Y. Bengio, “On using very large target vocabulary for neural machine translation,” in arXiv preprint arXiv:1412.2007, 2015. [14] R. E. Banchs, “Movie-dic: A movie dialogue corpus for research and development,” in Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, 2012, pp. 203–207. [15] K. Papineni, S. Roukos, T. Ward, and W. Zhu, “Bleu: A method for automatic evalution of machine translation,” in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002, pp. 311–318. [16] C. Y. Lin, “Rouge: a package for automatic evaluation of summaries,” in Proceedings of the Workshop on Text", "meta": {"paper_id": "1905.01998v1", "chunk_index": 11, "filename": "1905.01998v1.pdf", "chunk_length": 512}}
{"id": "1905.01998v1_chunk_12", "text": "Roukos, T. Ward, and W. Zhu, “Bleu: A method for automatic evalution of machine translation,” in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, 2002, pp. 311–318. [16] C. Y. Lin, “Rouge: a package for automatic evaluation of summaries,” in Proceedings of the Workshop on Text Summarization Branches Out, 2014. [17] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity- promoting objective function for neural conversation models,” in Pro- ceedings of NAACL-HLT, 2016.", "meta": {"paper_id": "1905.01998v1", "chunk_index": 12, "filename": "1905.01998v1.pdf", "chunk_length": 81}}
{"id": "1905.02019v1_chunk_0", "text": "Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System Heguang Liu∗ Uber Technologies, Inc Stanford University heguangl@uber.com Abstract Applying neural-networks on Question Answering has gained increasing popu- larity in recent years. In this paper, I implemented a model with Bi-directional attention ﬂow layer, connected with a Multi-layer LSTM encoder, connected with one start-index decoder and one conditioning end-index decoder. I introduce a new end-index decoder layer, conditioning on start-index output. Experiment shows this has increased model performance by 15.16%. For prediction, I pro- posed a new smart-span equation, rewarding both short answer length and high probability in start-index and end-index, which further improved the prediction accuracy. The best single model achieves an F1 score of 73.97% and EM score of 64.95% on test set. 1 Introduction Deep Neural Networks have gained signiﬁcant popularity recently and have been applied to many Natural Language Processing (NLP) systems, such as machine translation, speech recognition, sen- timent analysis, document summarization and reading comprehension. Among these applications, question answering, which is used in chatbots and dialogue agents, has attracted many attentions. The goal of question answering system is to provide an accurate answer to a given question based on a given context paragraph. This requires complex model interactive between question and context. In this paper, I explored a variety of improvements of SQuAD approaches, implement a question answering system using a Bi-direction attention ﬂow layer, connected with a Multi-layer LSTM encoder and decoder for start-index. On top of this, I introduce a new end-index decoder layer, conditioning on start-index output. Experiment shows this has increased model performance by 15.16%. For prediction, I introduce a new smart-span equation, which rewarding both short answer length and high probability in start and end-index, which further improved the prediction accuracy. 2 Background Stanford Question Answering Dataset(SQuAD)[1] (Rajpurkar et al., 2016) is a reading comprehen- sion dataset, consisting of 100,000+ question-answer pairs on 500+ articles. SQuAD has led to many research papers and signiﬁcant breakthroughs in building effective reading comprehension systems. One big breakthrough is Bi-Directional Attention Flow[2] (Seo et al., 2016), which en- ables the system to focus on the ROI in the context paragraph that is most relevant to the question, as well as the ROI in the question that is most relevant to the context. Another trending direction is Conditioning Prediction. ”Match-LSTM and Answer Pointer”[3](Wang et al., 2016) proposed a Pointer Network that outputs a probability distribution over locations in the context. DrQA(Chen ∗Senior Software Engineer at Uber Technologies, Inc and SCPD student at Stanford University. 1 arXiv:1905.02019v1 [cs.CL] 2 May 2019 et al., 2017) explored a test-time smarter span selection approach, based on the fact answer ofter consists less than 15 words. 3 Approach 3.1 Architecture For this project, I propose the following 6-layer model architecture, as showed in Figure 1. Figure 1: Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System Architecture 3.1.1 Embedding Layer Embedding layer maps each word in question and context to a vector space. For this project, I uses pre-trained Stanford GloVe [5] embeddings, during training, the embeddings are constants. My", "meta": {"paper_id": "1905.02019v1", "chunk_index": 0, "filename": "1905.02019v1.pdf", "chunk_length": 512}}
{"id": "1905.02019v1_chunk_1", "text": "showed in Figure 1. Figure 1: Conditioning LSTM Decoder and Bi-directional Attention Based Question Answering System Architecture 3.1.1 Embedding Layer Embedding layer maps each word in question and context to a vector space. For this project, I uses pre-trained Stanford GloVe [5] embeddings, during training, the embeddings are constants. My experiment shows that, increasing embedding dimension from 100 to 300 doesn’t provide much model improvements, but it will make the training 40% slower and more prone to over-ﬁtting. So I use GloVe 100-dimension embeddings for all following models. 3.1.2 Encoder Layer The context and question embedding sequence are feed into a 2-Layer Bi-directional LSTM to en- code to forward and backward hidden states. 2 3.1.3 Attention Layer To enable the model focus on the part of the question that is relevant to the context as well as part of the context that is relevant to the question, I use Bidirectional Attention Flow Layer described at [2]. 3.1.4 Start Decoder and End-Decoder For this project, I introduced two Bi-LSTM decoders to predict start-index and end-index position distribution separately. The end-index decoder will take start-index decoder output combined with post-attention layer hidden states as the input so that the end-index prediction is conditioning on the start-index distribution. Compared with baseline model, this approach adds a Bi-LSTM model layer and the two models are no longer sharing weights due to they are different problems by nature. This changes alone has magically increased the F1 score by 13% when comparing with baseline model. 3.1.5 Output Layer Output Layer is a fully connected layer followed by a ReLU non-linearity. Finally, we apply softmax function to start-index logits and conditioning end-index logits to generate the probability distribu- tion for both start and end position. Loss function is the sum of the cross-entropy loss of start and end indexes. 3.1.6 Smart Span After analyzing failed session, I noticed that if a shorter answer and a longer answer has similar pstart ∗pend probability, the ground-truth answer is often the shorter answer. To capture this in- formation, I introduced an smart-span equation to measure start-end pair-wise probability. This function is rewarding short answer length as well as high start-end-index probability at the same time. p(start, end) = pstart ∗pend log(end −start + 1) + 1 4 Experiments 4.1 Training Data SQuAD[1] provides word sequences of paragraphs sourced from 500+ Wikipedia articles and word sequence of 100K questions and answers, crowdsourced using Amazon Mechanical Turk. The an- swer are always taken directly from the paragraph. To minimize the noise in human-curated ground- truth answer, SQuAD provides three human answers for each question, and ofﬁcial evaluation will take the highest score among the three answers. The data is split into 80% training and 10% devel- opment set and 10% withhold test set. Based on the analysis on 86,318 training data, 98.98% of answers is less than 20 words. 98.34% of context is less than 300 words. In this paper, I use pre-trained word embedding GloVe[5]. GloVe is trained on the 6 Billions words of Wikipedia and Gigaword. It has choice of dimensionality 50/100/200/300,", "meta": {"paper_id": "1905.02019v1", "chunk_index": 1, "filename": "1905.02019v1.pdf", "chunk_length": 512}}
{"id": "1905.02019v1_chunk_2", "text": "on the analysis on 86,318 training data, 98.98% of answers is less than 20 words. 98.34% of context is less than 300 words. In this paper, I use pre-trained word embedding GloVe[5]. GloVe is trained on the 6 Billions words of Wikipedia and Gigaword. It has choice of dimensionality 50/100/200/300, each contains 400k lowercase words. 4.2 Evaluation Metrics Model Performance is measured via F1 and Exact Match(EM) score. F1 is a harmonic mean of precision and recall. This is a less strict metrics. F1 = 2 ∗Precision ∗Recall Precision + Recall 3 (a) Answer Length Distribution (b) Context Length Distribution Figure 2: Training and Dev Loss Tensorboard EM is a binary measure of whether the system output matches the ground truth answer exactly. This is a fairly strict metrics. EM = ExactMatchCount TotalCount 4.3 Training Details The code is implemented using Tensorﬂow v1.4 [6] framework, and written in python. All the models are trained on GPU of Microsoft Azure NV12 standard VM, which has 2 Nvidia Tesla 60M GPU pre-installed. The most complex model takes 2.5 days to converge. I start the training with a baseline model. After analyzing, I found Dev set converge at F1=42% and the training F1 is only 70%. To make the model complex enough to capture the training infor- mation, 2-Layer Bi-direction LSTMs and BiDAF is introduced to improve the F1 score to 53.59%. After analyzing failed predictions, I noticed some answers are abnormally long. To capture this information, instead of predict end-index position independently, its prediction can be conditioning on start-index output. This ﬁnal approach achieves 15.16% increase in F1 score. Table 1: Model Comparison Model F1 EM Parameter Size Training Time Batch Size Baseline 44.05 34.53 521,802 1.2s 100 2-Layer BiLSTMs 53.59 44.63 1,283,802 2.0s 100 2-Layer BiLSTMs with BiDAF 58.16 48.76 2,405,103 2.7s 100 2-Layer BiLSTMs with BiDAF, ConDecoders 73.82 64.54 3,048,603 2.8s 40 4.4 Model Parameters Tuning On top of the 2-Layer BiLSTMs with BiDAF, Conditioning Decoders model architecture, we start model hyper parameters tuning. Because most of the context are less than 300 words. So I truncate the context words after 300 words, to improve training speed. For this project, the model tuning is mainly focused on hidden size, dropout rate and embedding size. The converged tuning result after 50,000 iterations is list in Table 3. Table 3 shows that • When HiddenSize = 150, Dropout = 0.2 is a good choice. Increase will lead to high variance model and decrease will lead to over-ﬁtting in early stage • Increase HiddenSize from 150 to 250 doesn’t improve the performance much. 4 (a) Baseline Model (b) 2-Layer Bi-LSTMs (c) 2-Layer Bi-LSTMs with BiDAF (d) Bi-LSTMs, BiDAF and Conditioning Decoders Figure 3: Training and Dev Loss Tensorboard Table 2: Model Parameters Tuning Model F1 EM Hidden Size Dropout Rate Embedding Size Model 1 73.83 64.54 150 0.20 100 Model 2 71.68 61.97 150 0.25 200 Model 3 72.66 62.16 200 0.30 100 Model 4 73.32 63.59 250 0.20 100 Model 5 73.73 64.51 150 0.15 100 • EmbeddingSize = 100 has a reasonable good", "meta": {"paper_id": "1905.02019v1", "chunk_index": 2, "filename": "1905.02019v1.pdf", "chunk_length": 512}}
{"id": "1905.02019v1_chunk_3", "text": "EM Hidden Size Dropout Rate Embedding Size Model 1 73.83 64.54 150 0.20 100 Model 2 71.68 61.97 150 0.25 200 Model 3 72.66 62.16 200 0.30 100 Model 4 73.32 63.59 250 0.20 100 Model 5 73.73 64.51 150 0.15 100 • EmbeddingSize = 100 has a reasonable good performance. Increase embedding size from 100 to 200 doesn’t affect model performance much. 5 Result and Analysis 5.1 Category Analysis To better understand the result, I did an category analysis on the 10,391 Dev predictions. The result per category analysis in Table 4 shows that questions contain ”When” and ”Who” are clearly better than average, while questions contain ”Why” are performing below average. 5 Table 3: Evaluation by Question Category Category F1 EM Total 73.83 64.54 Who 76.37 70.52 When 83.07 77.50 Where 73.32 63.77 Why 65.71 41.44 What 72.14 62.00 Which 74.36 65.66 How 74.24 65.15 5.2 Error Analysis After analyzing the model prediction on tiny-dev dataset, I ﬁnd there are four common errors in my model prediction. 5.2.1 Wrong Position of Attention in Inverted Sentence Context: A 16-yard reception by Devin Funchess and a 12-yard run by Stewart then set up Gano’s 39-yard ﬁeld goal Question: Who had a 12-yard rush on this drive? Answer: Stewart Prediction: Devin Funchess Reason and Proposal: In this case, the order of subject and verb are reversed. Having part-of- speech tagging feature will help with the inverted sentences. 5.2.2 Mis-understand Number and Special Characters Context: while Jonathan Stewart ﬁnished the drive with a 1-yard touchdown run, cutting the score to 10-7 with 11:28 left in the second quarter. Question: How much time was left in the quarter when Stewart got the touchdown? Answer: 11:28 Prediction: 10-7 Reason and Proposal: Word embedding doesn’t capture the meaning of numbers and special char- acters. We can either introduce a number/special character embedding or introduce number format feature to help improve this case. 5.2.3 Incorrect Entity Detection Context: As opposed to broadcasts of primetime series, CBS broadcast special episodes of its late night talk shows as its lead-out programs for Super Bowl 50, beginning with a special episode of The Late Show with Stephen Colbert following the game. Question: Which late night comedy host show played immediately after Super Bowl 50 ended? Answer: The Late Show with Stephen Colbert Prediction: stephen colbert Reason and Proposal: The model falsely recognize stephen colbert as the name of a show. Include name entity feature will improve this issue. 6 5.2.4 Inaccurate boundary Context: The Super Bowl 50 Host Committee has vowed to be ”the most giving Super Bowl ever”, and will dedicate 25 percent of all money it raises for philanthropic causes in the Bay Area. Question: The Super Bowl 50 Host Committee said it would be the most what ever? Answer: the most giving Super Bowl ever Prediction: giving super bowl ever Reason and Proposal: Due to the pre-processing tokenization logic, we loss the double quote information. We can tokenize ”the most” into [”, the, most, ”], then the model will be quote boundary aware. 6 Conclusion In this paper,", "meta": {"paper_id": "1905.02019v1", "chunk_index": 3, "filename": "1905.02019v1.pdf", "chunk_length": 512}}
{"id": "1905.02019v1_chunk_4", "text": "Answer: the most giving Super Bowl ever Prediction: giving super bowl ever Reason and Proposal: Due to the pre-processing tokenization logic, we loss the double quote information. We can tokenize ”the most” into [”, the, most, ”], then the model will be quote boundary aware. 6 Conclusion In this paper, I implemented a model with Bi-directional attention ﬂow layer, connected with a Multi-layer LSTM encoder, connected with one start-index decoder and one conditioning end-index decoder and achieved great result. The best single model achieves an F1 score of 73.97% and EM score of 64.95% on test set. Based on the error analysis, 1) introducing number and special characters format feature 2) introducing part-of-speech tag 3) introducing name entity feature 4) improving tokenization logic will further improve the model performance. References [1] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, Squad: 100, 000+ questions for machine comprehension of text. CoRR, abs/1606.05250, 2016. [2] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention ﬂow for machine comprehension. arXiv preprint arXiv:1611.01603, 2016. [3] Shuohang Wangand JingJiang. Machine comprehension using match-lstm and answerpointer. arXiv preprint arXiv:1608.07905, 2016. [4] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain questions. arXiv preprint arXiv:1704.00051, 2017. [5] Jeffrey Pennington, Richard Socher, Christopher D. Manning. Glove: Global vectors for word representa- tion. 2014. [6] Tensorﬂow org: https://www.tensorﬂow.org/ 7", "meta": {"paper_id": "1905.02019v1", "chunk_index": 4, "filename": "1905.02019v1.pdf", "chunk_length": 227}}
{"id": "1905.02851v2_chunk_0", "text": "FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance Wataru Sakata LINE Corporation wataru.sakata@linecorp.com Tomohide Shibata∗ Kyoto University CREST, JST shibata@nlp.ist.i.kyoto-u.ac.jp Ribeka Tanaka Kyoto University CREST, JST tanaka@nlp.ist.i.kyoto-u.ac.jp Sadao Kurohashi Kyoto University CREST, JST NII, CRIS kuro@nlp.ist.i.kyoto-u.ac.jp ABSTRACT Frequently Asked Question (FAQ) retrieval is an important task where the objective is to retrieve an appropriate Question-Answer (QA) pair from a database based on a user’s query. We propose a FAQ retrieval system that considers the similarity between a user’s query and a question as well as the relevance between the query and an answer. Although a common approach to FAQ retrieval is to construct labeled data for training, it takes annotation costs. Therefore, we use a traditional unsupervised information retrieval system to calculate the similarity between the query and question. On the other hand, the relevance between the query and answer can be learned by using QA pairs in a FAQ database. The recently- proposed BERT model is used for the relevance calculation. Since the number of QA pairs in FAQ page is not enough to train a model, we cope with this issue by leveraging FAQ sets that are similar to the one in question. We evaluate our approach on two datasets. The first one is localgovFAQ, a dataset we construct in a Japanese administrative municipality domain. The second is StackExchange dataset, which is the public dataset in English. We demonstrate that our proposed method outperforms baseline methods on these datasets. ACM Reference Format: Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, and Sadao Kurohashi. 2019. FAQ Retrieval using Query-Question Similarity and BERT-Based Query-Answer Relevance. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SI- GIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3331184.3331326 ∗currently at Yahoo Japan Corporation Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR ’19, July 21–25, 2019, Paris, France © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6172-9/19/07...$15.00 https://doi.org/10.1145/3331184.3331326 1 INTRODUCTION There are often frequently asked question (FAQ) pages with various information on the web. A FAQ retrieval system, which takes a user’s query and returns relevant QA pairs, is useful for navigating these pages. In FAQ retrieval tasks, it is standard to check similarities of user’s query (q) to a FAQ’s question (Q) or to a question-answer (QA) pair [2]. Many FAQ retrieval models use the dataset with the relevance label between q and a QA pair. However, it costs a lot to construct such labeled data. To cope with this", "meta": {"paper_id": "1905.02851v2", "chunk_index": 0, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_1", "text": "to check similarities of user’s query (q) to a FAQ’s question (Q) or to a question-answer (QA) pair [2]. Many FAQ retrieval models use the dataset with the relevance label between q and a QA pair. However, it costs a lot to construct such labeled data. To cope with this problem, we adopt an unsupervised method for calculating the similarity between a query and a question. Another promising approach is to check the q-A relevance trained by QA pairs, which shows the plausibility of the FAQ answer for the given q. Studies of community QA use a large number of QA pairs for learning the q-A relevance [8, 11]. However, these methods do not apply to FAQ retrieval task, because the size of QA entries in FAQ is not enough to train a model generally. We address this problem by collecting other similar FAQ sets to increase the size of available QA data. In this study, we propose a method that combines the q-Q similar- ity obtained by unsupervised model and the q-A relevance learned from the collected QA pairs. Figure 1 shows the proposed model. Previous studies show that neural methods (e.g., LSTM and CNN) work effectively in learning q-A relevance. Here we use the recently- proposed model, BERT [1]. BERT is a powerful model that applies to a wide range of tasks and obtains the state-of-the-art results on many tasks including GLUE [10] and SQuAD [4]. An unsupervised retrieval system achieves high precision, but it is difficult to deal with a gap between the expressions of q and Q. By contrast, since BERT validates the relevance between q and A, it can retrieve an appropriate QA pair even if there is a lexical gap between q and Q. By combining the characteristics of two models, we achieve a robust and high-performance retrieval system. We conduct experiments on two datasets. The first one is the localgovFAQ dataset, which we construct to evaluate our model in a setting where other similar FAQ sets are available. It consists of QA pairs collected from Japanese local government FAQ pages and an evaluation set constructed via crowdsourcing. The second one is the StackExchange dataset [2], which is the public dataset for FAQ arXiv:1905.02851v2 [cs.IR] 24 May 2019 User’s query：Can you please remove the wasp’s nest? q-Q similarity TSUBAKI q-A relevance BERT ＋ Target FAQ set 呍 呍 呍 Local government FAQ sets Q: Will you remove the honeycomb? A: We will remove it only if it is the nest of wasps… Q: How to obtain tax cer<ﬁcate by mail? A: To receive cer<ﬁcates by mail, … … … Q: How to throw out bulky trash? A: Call the cleaning center. Q: I want you to exterminate the bee’s nest. A: When there are bee nests residen<al buildings… … … Figure 1: An overview of our proposed method. retrieval tasks. We evaluate our model on these datasets and show that the proposed method works effectively in FAQ retrieval. 2 PROPOSED METHOD 2.1 Task Description We begin by formally defining the task of FAQ retrieval. Here, we", "meta": {"paper_id": "1905.02851v2", "chunk_index": 1, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_2", "text": "residen<al buildings… … … Figure 1: An overview of our proposed method. retrieval tasks. We evaluate our model on these datasets and show that the proposed method works effectively in FAQ retrieval. 2 PROPOSED METHOD 2.1 Task Description We begin by formally defining the task of FAQ retrieval. Here, we focus on local government FAQ as an example. Suppose that the number of local government FAQ sets is N. Our target FAQ set, Tt , is one of them. When the number of QA entries in Tt is M, Tt is a collection of QA pairs {(Q1,A1), (Q2,A2), ..., (QM,AM)}. The task is then to find the appropriate QA pair (Qi,Ai) from Tt based on a user’s query q. We use T1,T2, ...,TN as our training data, including the FAQ set Tt of the target local government. 2.2 q-Q similarity by TSUBAKI We use TSUBAKI [7] to compute q-Q similarity. TSUBAKI is an unsupervised retrieval engine based on OKAPI BM25 [5]. TSUBAKI accounts for a dependency structure of a sentence, not just its words, to provide accurate retrieval. For flexible matching, it also uses synonyms automatically extracted from dictionaries and Web corpus. Here we regard Q in each QA as a document and compute Similarity(q,Q) for the q-Q similarity. 2.3 q-A relevance by BERT We use BERT to compute q-A relevance. BERT is based on the Transformer [9] that effectively encodes an input text. It is designed to be pre-trained using a language model objective on a large raw corpus and fine-tuned for each specific task including sentence classification, sentence-pair classification, and question answering. As it is pre-trained on a large corpus, BERT achieves high accuracy even if the data size of the specific task is not large enough. We apply BERT to a sentence-pair classifier for questions and answers. By applying the Transformer to the input question and answer, it captures the relevance between the pair. The training data we use is the collection of QA pairs from FAQ sets (see Sec. 2.1). For each positive example (Q,A), we ran- domly select ¯A and produce negative training data (Q, ¯A). On this data, we train BERT to solve the two-class classification problem: • I’d like you to issue a copy of family register, but how much does it cost? • I’d like you to publish a maternal and child health handbook, but what is required for the procedure? • I’m thinking of purchasing a new housing, so I want to know about the reduction measure. • From which station does the pick-up bus of the Center Pool come out? Figure 2: Examples of queries collected via crowdsourcing. Relevance(Q,A) is 1 andRelevance(Q, ¯A) is 0, whereRelevance(Q,A) stands for the relevance between Q and A. At the search stage, we compute Relevance(q,Ai) (i = 1, · · · , M) for the user’s query q and every QA pair in the target Tt . QA pairs in a higher rank are used as search results. 2.4 Combining TSUBAKI and BERT In order to realize robust and flexible matching, we combine the q-Q similarity", "meta": {"paper_id": "1905.02851v2", "chunk_index": 2, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_3", "text": "= 1, · · · , M) for the user’s query q and every QA pair in the target Tt . QA pairs in a higher rank are used as search results. 2.4 Combining TSUBAKI and BERT In order to realize robust and flexible matching, we combine the q-Q similarity by TSUBAKI and the q-A relevance by BERT. When TSUBAKI’s similarity score is high, this is probably a positive case because the words in q and Q highly overlap with each other. However, it is difficult to cope with the lexical gaps between q and Q. On the other hand, since BERT validates the relevance between q and A, it can retrieve an appropriate QA pair even if there is a lexical gap between q and Q. To make use of these characteristics, we combine two methods as follows. First, we take the ten-highest results of BERT’s output. For QA pairs whose TSUBAKI score gets a higher score than α, we rank them in order of TSUBAKI’s score. For the others, we rank them in order of the score of Similarity(q,Q) × t + Relevance(q,A) where t is a hyper-parameter. TSUBAKI’s score tends to be higher when the given query is longer. Hence, before taking the sum, we normalize TSUBAKI’s score by using the numbers of content words and dependency relations in the query. We divide the original score by the following value.1 Count(ContentWords) × k1 + Count(DependencyRelations) × k2 3 EXPERIMENTS AND EVALUATION We conducted our experiments on two datasets, localgovFAQ and StackExchange. We constructed localgovFAQ dataset, as explained in Sec 3.1. StackExchange dataset is constructed in the paper [2] by extracting QA pairs from the web apps domain of StackExchange and consists of 719 QA pairs. Each Q has paraphrase queries, and the total number of queries is 1,250. All the models were evaluated using five-fold cross validation. In each validation, all the queries were split into training (60%), development (20%) and test (20%). The task is to estimate an appropriate QA pair for each query q among 719 QA pairs. 3.1 LocalgovFAQ Evaluation Set Construction Amagasaki-city, a relatively large city in Japan, was chosen as a target government, whose Web site has 1,786 QA pairs. First, queries to this government were collected using a crowdsourcing. Example queries are shown in Figure 2. We collected 990 queries in total. 1 We do not normalize the BERT’s score because it takes a value between 0 to 1. TSUBAKI and BERT output at most five relevant QA pairs for each query, and each QA pair was manually evaluated assigning one of the following four categories: A Contain correct information. B Contain relevant information. C The topic is same as a query, but do not contain relevant information. D Contain only irrelevant information. In general, information retrieval evaluation based on the pooling method has inherently a biased problem. To alleviate this problem, when there are no relevant QA pairs among the outputs by TSUB- AKI and BERT, a correct QA pair was searched by using appropriate different keywords. If there are no relevant", "meta": {"paper_id": "1905.02851v2", "chunk_index": 3, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_4", "text": "In general, information retrieval evaluation based on the pooling method has inherently a biased problem. To alleviate this problem, when there are no relevant QA pairs among the outputs by TSUB- AKI and BERT, a correct QA pair was searched by using appropriate different keywords. If there are no relevant QA pair found, this query was excluded from our evaluation set. The resultant queries were 784. Since 20% of queries were used for the development set, 627 queries were used for our evaluation. 3.2 Experimental Settings For the localgovFAQ dataset, MAP (Mean Average Precision), MRR (Mean Reciprocal Rank), P@5 (Precision at 5), SR@k (Success Rate)2 and nDCG (normalized Discounted Cumulative Gain) were used as our evaluation measures. The categories A, B and C were regarded as correct for MAP, MRR, P@5 and SR@k, and the evaluation level of categories A, B and C was regarded as 3, 2 and 1 for nDCG, respectively. For the StackExchange dataset, MAP, MRR and P@5 were used, following Karan et al. [2] . For Japanese, the pre-training of BERT was performed using Japanese Wikipedia, which consists of approximately 18M sen- tences, and the fine-tuning was performed using FAQs of 21 Japan- ese local governments, which consists of approximately 20K QA pairs. The morphological analyzer Juman++3 was applied to input texts for word segmentation, and words were broken into sub- words by applying BPE [6]. For English BERT pre-trained model, a publicly-available model was used4. For the fine-tuning for Stack- Exchange dataset, the training set (q,Q,A) was divided into (q,A) and (Q,A). In the localgovFAQ dataset, Bi-LSTM with attention [8] was adopted as our baseline. We also used model BERTtargetOnly, which is fine-tuned only with the target FAQ set, in order to test the effect of using other FAQ sets. In the StackExchange dataset, CNN- rank in q-Q and q-QA settings, which is the neural FAQ retrieval model based on a convolutional neural network, was used, whose scores were from Karan et al. [2]. Furthermore, BERT (w/o query paraphrases) was adopted, where (q,A) pairs were not used for BERT training, to see the performance when no manually-assigned query paraphrases were available. For both datasets, TSUBAKI was applied only in q-Q setting5. For both BERT and Bi-LSTM models, 24 negative samples for one positive sample were used. For the coefficients explained in Sec. 2Success Rate is the fraction of questions for which at least one related question is ranked among the top k. 3http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN++ 4https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_ A-12.zip 5We omit the TSUBAKI’s results in the q-A setting and the q-QA setting as we got the worse scores than the q-Q setting. Model MAP MRR P@5 SR@1 SR@5 NDCG q-Q TSUBAKI 0.558 0.598 0.297 0.504 0.734 0.501 q-A Bi-LSTM 0.451 0.498 0.248 0.379 0.601 0.496 BERTtargetOnly 0.559 0.610 0.285 0.504 0.751 0.526 BERT 0.576 0.631 0.333 0.509 0.810 0.560 q-QA Proposed 0.647 0.705 0.357 0.612 0.841 0.621 Table 1: Evaluation result on the localgovFAQ dataset. Model MAP MRR P@5 q-Q CNN-rank 0.79 0.77 0.63 TSUBAKI 0.698 0.669 0.638 q-A BERT (w/o query paraphrases) 0.631 0.805 0.546 BERT 0.887 0.936 0.770 q-QA", "meta": {"paper_id": "1905.02851v2", "chunk_index": 4, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_5", "text": "0.526 BERT 0.576 0.631 0.333 0.509 0.810 0.560 q-QA Proposed 0.647 0.705 0.357 0.612 0.841 0.621 Table 1: Evaluation result on the localgovFAQ dataset. Model MAP MRR P@5 q-Q CNN-rank 0.79 0.77 0.63 TSUBAKI 0.698 0.669 0.638 q-A BERT (w/o query paraphrases) 0.631 0.805 0.546 BERT 0.887 0.936 0.770 q-QA CNN-rank 0.74 0.84 0.62 Proposed 0.897 0.942 0.776 Table 2: Evaluation result on the StackExchange dataset. 2.4, k1 and k2 were set to 4 and 2, respectively, and α and t were set to 0.3 and 10 respectively using the development set. 3.3 Evaluation Results and Discussions Table 1 shows an experimental result on localgovFAQ dataset. In q-A setting, BERT was better than the Bi-LSTM baseline, which indicates BERT was useful for this task. Although the performances of TSUBAKI and BERT were almost the same in terms of SR@1, the performance of BERT was better than TSUBAKI in terms of SR@5, which indicates BERT could retrieve a variety of QA pairs. The proposed method performed the best. This demonstrated the effectiveness of our proposed method. The score of BERT was better than one of BERTtargetOnly, which indidates that using other FAQ sets is effective. Table 2 shows an experimental result on StackExchange dataset. In the same way as the result on localgovFAQ, BERT performed well, and the proposed method performed the best in terms of all the measures. The performance of BERT was better than one of \"BERT (w/o query paraphrases)\", which indicates that the use of various augmented questions was effective. Figure 3 shows the performance of TSUBAKI and BERT on local- govFAQ according to their TOP1 scores. From this figure, we can find that in the retrieved QA pair whose TSUBAKI score is high, its accuracy is very high. On the otherhand, there is a relatively loose correlation between the accuracy and BERT score. This indi- cates TSUBAKI and BERT have different characteristics, and our proposed combining method is reasonable. Table 3 shows the examples, translated from Japanese, of system outputs and correct QA pairs on localgovFAQ. In the first example, although TSUBAKI retrieved the wrong QA pair since there is a word \"consultation\" and \"counseling\" in the query and Q, BERT and the proposed method could retrieve a correct QA pair. In the second example, the proposed method could retrieve a correct QA pair on the first rank although the first rank of TSUBAKI and BERT was wrong. In the third example, no methods could retrieve a correct QA pair. Although BERT could capture the relevance between a word \"instruments\" in the query and \"music\" in A, the retrieved QA pair was wrong. The correct QA pair consists of Q saying \"Information Query TSUBAKI BERT Proposed method Is there a consultation desk for workplace ha- rassment? 1 × Q: I’d like to have a career counsel- ing. A: Consultation place: Amagasaki- city, ... 1 ✓ Q: I’d like to consult a lawyer for work-related problems. A: On specialized and sophisticated labor issues such as wages, dis- missal, occupational accidents, ... 1 ✓ Q: Can we get a lawyer’s labor", "meta": {"paper_id": "1905.02851v2", "chunk_index": 5, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_6", "text": "I’d like to have a career counsel- ing. A: Consultation place: Amagasaki- city, ... 1 ✓ Q: I’d like to consult a lawyer for work-related problems. A: On specialized and sophisticated labor issues such as wages, dis- missal, occupational accidents, ... 1 ✓ Q: Can we get a lawyer’s labor coun- selor? A: On specialized and sophisticated labor issues such as wages, dis- missal, occupational accidents, ... Where should I renew my license? 1 × Q: Where should I apply for med- ical staff licenses (new, corrected / rewritten, re-issued)? A: License application for doctors, dentists, public health nurses ... 1 × Q: To update a bus ticket, do I have to go myself? A: In principle, please apply for the application by yourself. ... 1 ✓ Q: Please tell me about the proce- dure of updating your driver’s li- cense. A: Regarding the renewal procedure of your driver’s license ... 2 ✓ Q: Please tell me about the proce- dure of updating your driver’s li- cense. A: Regarding the renewal procedure of your driver’s license ... 2 × Q: Can I file an agent application to renew my bus ticket? A: As a general rule, please apply for yourself. ... 2 × Q: Can I file an agent application to renew my bus ticket? A: As a general rule, please apply for yourself. ... Is there a place that we can use for practicing instruments? 1 × Q: Where is the location of the polling place before the election’s due date? A: There are three polling stations before the date in the city. ... 1 × Q: Please tell me about Amagasaki City boys music corps. A: \"Amagasaki City Boys Music Club\" includes a choir corps, a brass band, ... 1 × Q: Please tell me about Amagasaki City boys music corps. A: \"Amagasaki City Boys Music Club\" includes a choir corps, a brass band, ... Table 3: Examples of system outputs and their manual evaluations. (✓and × in the table mean correct and incorrect, respec- tively, where the evaluation categories A, B, and C are regarded as correct.) 0 50 100 150 200 250 300 350 400 450 (0.0,0.2) [0.2,0.4) [0.4,0.6) [0.6,0.8) [0.8,1.0) The number of correct or incorrect TOP1 Score TSUBAKI Correct Incorrect BERT Correct Incorrect Figure 3: The number of queries to which the systems can or cannot output correct QA pair as TOP1 with respect to their scores. on the facility of the youth center, hours of use, and closed day\" and A mentioning a music room as one of the available facilities in the youth center. To retrieve this correct QA pair, the deeper understanding of QA texts is necessary. It takes about 2 seconds to retrieve QA pairs per query on local- govFAQ dataset by using 7 GPUs (TITAN X Pascal), and our model is practical enough. For a larger FAQ set, one can use our method in a telescoping setting [3]. 4 CONCLUSION This paper presented a method for using query-question similarity and BERT-based query-answer relevance in a FAQ retrieval task. By collecting other", "meta": {"paper_id": "1905.02851v2", "chunk_index": 6, "filename": "1905.02851v2.pdf", "chunk_length": 512}}
{"id": "1905.02851v2_chunk_7", "text": "using 7 GPUs (TITAN X Pascal), and our model is practical enough. For a larger FAQ set, one can use our method in a telescoping setting [3]. 4 CONCLUSION This paper presented a method for using query-question similarity and BERT-based query-answer relevance in a FAQ retrieval task. By collecting other similar FAQ sets, we could increase the size of available QA data. BERT, which has been recently proposed, was applied to capture the relevance between queries and answers. This method realized the robust and high-performance retrieval. The experimental results demonstrated that our combined use of query- question similarity and query-answer relevance was effective. We are planning to make the code and constructed dataset localgovFAQ publicly available6. ACKNOWLEDGMENT This work was partly supported by JST CREST Grant Number JPMJCR1301, Japan. REFERENCES [1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805 [2] Mladen Karan and Jan Snajder. 2018. Paraphrase-focused learning to rank for domain-specific frequently asked questions retrieval. Expert Systems with Appli- cations 91 (2018), 418–433. [3] Irina Matveeva, Chris Burges, Timo Burkard, Andy Laucius, and Leon Wong. 2006. High Accuracy Retrieval with Multiple Nested Ranker. In SIGIR. ACM, 437–444. [4] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In EMNLP. Association for Computational Linguistics, 2383–2392. [5] Stephen E. Robertson, Steve Walker, Micheline Hancock-Beaulieu, Aarron Gull, and Marianna Lau. 1992. Okapi at TREC. In TREC. 21–30. [6] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Trans- lation of Rare Words with Subword Units. In ACL. Association for Computational Linguistics, 1715–1725. [7] Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara, Chikara Hashimoto, and Sadao Kurohashi. 2008. TSUBAKI: An Open Search Engine Infrastructure for Developing New Information Access Methodology. In IJCNLP. 189–196. [8] Ming Tan, Bing Xiang, and Bowen Zhou. 2015. LSTM-based Deep Learn- ing Models for Non-factoid Answer Selection. CoRR abs/1511.04108 (2015). arXiv:1511.04108 [9] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NIPS. 5998–6008. [10] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natu- ral Language Understanding. In EMNLP2018 Workshop BlackboxNLP. Association for Computational Linguistics, 353–355. [11] Yu Wu, Wei Wu, Zhoujun Li, and Ming Zhou. 2018. Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots. In ACL. 420–425. 6http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?BERT-Based_FAQ_Retrieval", "meta": {"paper_id": "1905.02851v2", "chunk_index": 7, "filename": "1905.02851v2.pdf", "chunk_length": 416}}
{"id": "1905.02869v1_chunk_0", "text": "JMLR: Workshop and Conference Proceedings 1–11, 2019 LearnAut 2019 Automatic Inference of Minimalist Grammars using an SMT-Solver Sagar Indurkhya INDURKS@MIT.EDU MIT Abstract We introduce (1) a novel parser for Minimalist Grammars (MG), encoded as a system of ﬁrst-order logic formulae that may be evaluated using an SMT-solver, and (2) a novel procedure for inferring Minimalist Grammars using this parser. The input to this procedure is a sequence of sentences that have been annotated with syntactic relations such as semantic role labels (connecting arguments to predicates) and subject-verb agreement. The output of this procedure is a set of minimalist grammars, each of which is able to parse the sentences in the input sequence such that the parse for a sentence has the same syntactic relations as those speciﬁed in the annotation for that sentence. We applied this procedure to a set of sentences annotated with syntactic relations and evaluated the inferred grammars using cost functions inspired by the Minimum Description Length principle and the Subset principle. Inferred grammars that were optimal with respect to certain combinations of these cost functions were found to align with contemporary theories of syntax. Keywords: Minimalist Grammar, Satisﬁability Modulo Theory, Grammatical Inference 1. Introduction Inspired by earlier formulations of grammars using logic (Pereira and Warren (1983); Rayner et al. (1988); Stabler (1993); Rogers (1998); Graf (2013)) and recent, substantive improvements in the performance of SMT-solvers (De Moura and Bjørner (2011); Cadar and Sen (2013)), we have de- veloped a novel procedure, with the form of a model of language acquisition (Chomsky (1965)), for automatically inferring Minimalist Grammars (MG) (Stabler (1996)) from a sequence of sentences that have been annotated with syntactic relations for predicate-argument structure and morphologi- cal agreement .1 In this study, we report preliminary results that demonstrate our inference proce- dures’ capacity to acquire grammars that comport with contemporary theories of minimalist syntax (Chomsky (1995)).2 The remainder of this study is organized as follows: after reviewing the MG formalism and prior work on modeling MGs with logic (Section 2), we present our inference procedure (Section 3) and use it to infer a set of MG lexicons from the sequence of annotated sentences listed in Table 1 (Section 4); we identify members of the inferred set of MG lexicons that are optimal with respect to cost functions inspired by the Minimum Description Length (MDL) principle (Gr¨unwald (2007)) and the Subset principle (Berwick (1985); Wexler (1993)) and present several examples of these optimal MG lexicons, one of which aligns with contemporary minimalist syntax, producing for each sentence in the input sequence a parse tree that matches standard syntactic analysis. Finally, 1. Please contact the authors to obtain an implementation of the inference procedure introduced in this study. 2. For detailed presentations of minimalist syntax, see (Adger (2003); Hornstein et al. (2005); Radford (2009)). c⃝2019 S. Indurkhya. arXiv:1905.02869v1 [cs.CL] 8 May 2019 INDURKHYA in (Section 5) we discuss how our procedure, which takes the form of a computational model of language acquisition, may be applied to the evaluation of the Strong Minimalist Thesis.3 2. Minimalist Grammars The Minimalist Grammar", "meta": {"paper_id": "1905.02869v1", "chunk_index": 0, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_1", "text": "et al. (2005); Radford (2009)). c⃝2019 S. Indurkhya. arXiv:1905.02869v1 [cs.CL] 8 May 2019 INDURKHYA in (Section 5) we discuss how our procedure, which takes the form of a computational model of language acquisition, may be applied to the evaluation of the Strong Minimalist Thesis.3 2. Minimalist Grammars The Minimalist Grammar (MG) formalism, introduced in Stabler (1996), is a well established for- mal model of syntax inspired by Chomsky (1995). We chose to use this formalism because: (i) MGs are mildly context-sensitive (Michaelis (1998)) and can model cross-serial dependencies that arise in natural language (Vijay-Shanker et al. (1987); Stabler (2004)); (ii) MGs can model displacement, a basic fact of natural language that enables a phrase to be interpreted both in its ﬁnal, surfaced position, as well as other positions within a syntactic structure (Chomsky (2013)).4 An MG consists of: (i) a lexicon, consisting of a ﬁnite set of atomic structures, referred to as lexical items, each of which pairs a phonetic form5 with a ﬁnite sequence of (syntactic) features.6 (ii) merge, a recursive structure building operation that combines two structures to produce a new structure.7 Each application of merge consumes the ﬁrst feature from each of its two arguments, requiring that the two features have the same value; one of the two arguments then projects its feature sequence to the structure produced by merged. To parse a sentence, a set of lexical items is drawn from the lexicon and combined together, via the recursive application of merge, into a single structure in which all of the features have been consumed; if the ordering of the phonetic forms in the resulting structure aligns with the order of the words in the sentence being parsed, then the structure is considered to be a valid parse of the sentence. See Figure 2 and Figure 3 for examples of MG parses. Finally, let us consider whether an MG may be modeled with Satisﬁability Modulo Theory (SMT) (Barrett and Tinelli (2018). Rogers (1998) established that the axioms of GB theory are expressible with Monadic Second Order logic (MSO); subsequently, Graf (2013) produced an MSO axiomatization for MGs8, and notes that over ﬁnite domains these constraints may be expressed with ﬁrst order logic. As this study only considers models with ﬁnite domains, we can develop a ﬁnite theory of MGs with an axiomatization based in part on the MSO axiomatization of MGs 3. Chomsky (2008) The Strong Minimalist Thesis asserts that “language is an optimal solution to interface conditions that FL must satisfy; that is, language is an optimal way to link sound and meaning, where these notions are given a technical sense in terms of the interface systems that enter into the use and interpretation of expressions generated by an I-language.” See also: Chomsky (2001). 4. A single phrase satisfying multiple interface conditions often requires that it undergo syntactic movement to establish a discontinuous structure (i.e. a chain) with multiple local relations; by the Principle of Last Resort, movement is driven by morphological considerations – e.g. morphological agreement (Chomsky (1995)). 5. A phonetic form is either overt (e.g.", "meta": {"paper_id": "1905.02869v1", "chunk_index": 1, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_2", "text": "phrase satisfying multiple interface conditions often requires that it undergo syntactic movement to establish a discontinuous structure (i.e. a chain) with multiple local relations; by the Principle of Last Resort, movement is driven by morphological considerations – e.g. morphological agreement (Chomsky (1995)). 5. A phonetic form is either overt (e.g. a word in the sentence) or covert (i.e. unpronounced). 6. A feature has: (i) a value from a ﬁnite set of categories; (ii) a type, which is either selector, selectee, licensor or licensee, indicated by the preﬁx =, ∼, + and −respectively; a < or > preﬁxed before a selector preﬁx indicates that the selector triggers left or right head-movement respectively. There is also a special feature, C, that serves to indicate the completion of a parse. 7. Merge applies to two logically disjoint cases: (i) internal merge, for the case in which one argument is a substructure of the other (i.e. they are not disjoint), requires that the consumed features for the two arguments be a licensor and licensee, with the former projecting. (ii) external merge, for the case in which the two arguments are disjoint, requires that the consumed features for the two arguments be a selector and selectee, with the former projecting. 8. Graf (2013) also shows that constraints may be encoded in an MG lexicon if and only if they are MSO expressible. 2 AUTOMATIC INFERENCE OF MINIMALIST GRAMMARS USING AN SMT-SOLVER developed by Graf (2013).9 We will express the theory with a multi-sort quantiﬁer-free10 ﬁrst-order logic extended with the theory of uninterpreted functions (i.e. the theory of equality), allowing us to model the theory with an SMT-solver and (decidably) identify interpretations of models. 3. Inference Procedure Our inference procedure takes the form of a computational model of language acquisition (Chomsky (1965)) consisting of: (i) an initial state, S0, consisting of a system of ﬁrst-order logical formulae that serve as axioms for deducing the class of minimalist lexicons; (ii) the input, consisting of a sequence of n sentences, denoted I1, I2, . . . , In, each of which is annotated with syntactic relations between pairs of words in the sentence; (iii) a function, Q, that takes as input a state, Si, and an annotated sentence, Ii, and outputs the successor state, Si+1; (iv) a function, R, that maps a state Si to a set of MG lexicons, Gi, with the property that for each sentence Ij in the input sequence, each lexicon L ∈Gi can produce a parse pL j such that the syntactic relations in pL j parse match those speciﬁed in the annotation of sj.11 The procedure consumes the input sequence one annotated sentence at a time, using Q to drive the initial state, S0, to the ﬁnal state, Sn; the function R is then applied to Sn to produce a set of MG lexicons, Gn, that constitutes the output of the inference procedure. (See Table-1 for an example of input to the procedure) We implemented this inference procedure by encoding an MG parser as a system of ﬁrst-order, quantiﬁer-free logical formulas that can be", "meta": {"paper_id": "1905.02869v1", "chunk_index": 2, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_3", "text": "applied to Sn to produce a set of MG lexicons, Gn, that constitutes the output of the inference procedure. (See Table-1 for an example of input to the procedure) We implemented this inference procedure by encoding an MG parser as a system of ﬁrst-order, quantiﬁer-free logical formulas that can be solved with an SMT-solver.12 This system of formulas is composed of formulas for MG parse trees (see Section 3.1) that are connected (by way of shared symbols) to a formula for an MG lexicon (i.e. S0); by imposing constraints on the formulas for parse trees, the set of solutions to the lexicon formula is restricted. Let us now review the role Q and R play in this. When the inference procedure consumes an annotated sentence from the input sequence, the function Q: (1) instantiates a formula for a MG parse; (2) translates the annotations for the sen- tence into (logic) formulas that constrain the parse tree – e.g. predicate-argument relations and morphological agreement are translated into locality constraints13, and each sentence is marked as declarative or interrogative, indicating which of two pre-speciﬁed covert phonetic forms, ϵCDecl or ϵCIntr, must appear in a parse of the sentence (see Fig. 3 for an example); (3) adds these new formulas to the existing system of formulas in Si to produce Si+1. In order to compute the set of lexicons, Gi = R(Si), we used the Z3 SMT-solver to solve for the set of lexicons satisfying the formulae in Si.14 Note that the inferred set Gi is not enumerated; rather, 9. Although we use the concept of slices, presented in Graf (2013), our axiomatization does not utilize the ﬁrst-order theory of ﬁnite trees (originally presented in Backofen et al. (1995)). 10. The axioms in the theory must be quantiﬁer free as the SMT-solver cannot guarantee decidability for problems involving universal quantiﬁers; this is established via explicit quantiﬁcation. 11. In the case of the initial state, S0, since there are no constraints yet imposed by the input, R(S0) will map to the set of all minimalist lexicons. 12. All logical formulas in this study, being used to encode ﬁnite models over bounded domains, are ﬁrst-order and quantiﬁer-free; this has the beneﬁt that these formulas are decidable. 13. The principle of syntactic locality asserts that syntactic relations are established locally by merge (Sportiche et al. (2013)). 14. Z3 is a high-performance solver for Satisﬁability Modulo Theories (SMT) that can solve ﬁrst-order quantiﬁer-free multi-sort logic formulas that may combine symbols from a set of additional logics deﬁned by a number of back- ground theories such as empty theory (i.e. the theory of uninterpreted functions with equality). See De Moura and Bjørner (2008) for further reference. 3 INDURKHYA it exists implicitly in the model produced by the SMT-solver (i.e. the solution to the system of logical formulas), and members of this set may be ﬁltered, searched and sampled by querying this model using Z3; since the number of inferred lexicons is often exponentially large due to symmetries, we do not enumerate the entire set of lexicons; instead we use Z3", "meta": {"paper_id": "1905.02869v1", "chunk_index": 3, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_4", "text": "solution to the system of logical formulas), and members of this set may be ﬁltered, searched and sampled by querying this model using Z3; since the number of inferred lexicons is often exponentially large due to symmetries, we do not enumerate the entire set of lexicons; instead we use Z3 to sample lexicons from Gi. 3.1. Modeling an MG Parse Tree We now provide an overview of a ﬁnite model of a minimalist parse tree, based closely on the MG formalism, that we have developed using a multi-sort ﬁrst-order quantiﬁer-free logic extended with the theory of equality and uninterpreted functions. The model consists of several sorts, uninter- preted functions acting over these sorts, and a set of axioms constraining these functions that every minimalist parse tree must satisfy; additionally, the syntactic relations that annotate a sentence can also be expressed as a set of axioms (ﬁrst order logic formulas) that further constrain the model. An interpretation of the model (consisting of interpretations of the uninterpreted functions) is thus a minimalist parse tree that accords with the speciﬁed syntactic relations for a given sentence. A (minimalist) parse tree15 is modeled as a labeled directed acyclic graph16, which is in turn modeled via (i) a ﬁnite sort, members of which are nodes in the graph and (ii) a set of (unary and binary) uninterpreted functions and predicates (acting over the sorts), that establish labeled edges in the graph.17 Interpretations of these functions and predicates are constrained by a set of axioms that include both: (a) an axiomatization for the MG formalism; (b) axioms that aid in expressing constraints imposed by interface conditions – e.g. axioms for structural conﬁgurations for predicate- argument structure and projection of categories.18 These axioms are derived from properties and principles of natural language syntax that are considered universal in so far as they apply to all natural languages (Chomsky (1995); Collins and Stabler (2016)). Let us now review aspects of linguistic theory from which these axioms are derived and discuss how these axioms constrain the model of the MG parse tree. In accordance with the theory of Bare Phrase Structure (BPS) (Chomsky (1995)), one of the uninterpreted functions is a binary function (over the nodes in the graph) that is constrained by axioms that model the recursive structure building operation, Merge (Chomsky (1995); Collins and Stabler (2016)); another unary functions models the chains produced by the movement of phrases within the parse tree.19 Each node in the parse tree has a head, which is one of the leaf nodes (which correspond to lexical items) in the parse tree20; this mapping is established by a unary uninterpreted function. Each node in the parse tree is labeled with a category. Categories are interpretable properties of lexical items that can project – the category associated with a given 15. This tree corresponds to the derivation tree in the MG formalism. 16. Each node in this graph corresponds to a node in the parse tree and the graph is constrained so as to have a single element with no out-going edges (which corresponds to", "meta": {"paper_id": "1905.02869v1", "chunk_index": 4, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_5", "text": "category associated with a given 15. This tree corresponds to the derivation tree in the MG formalism. 16. Each node in this graph corresponds to a node in the parse tree and the graph is constrained so as to have a single element with no out-going edges (which corresponds to the root of the parse tree); nodes with no incoming edges correspond to atomic syntactic structures. 17. E.g. an uninterpreted binary predicate models dominance relations between nodes in the graph, and the transitive closure of this predicate establishes a binary tree in accordance with the Binary Branching Hypothesis (Radford (2009)). 18. Along with the ﬁnite sort that constitutes the nodes of the derivation tree, a number of additional ﬁnite sorts and functions mapping to and from them are employed to represent phonetic forms, features, categories, etc. 19. An uninterpreted function for modeling head movement is also included, with relevant axioms in accordance with the Head-Movement constraint as given in (Baker (1988); Hale (1993); Stabler (2001) 20. See Radford (2009) for a discussion of the Headedness Principle, according to which “every nonterminal node in a syntactic structure is a projection of a head word.” 4 AUTOMATIC INFERENCE OF MINIMALIST GRAMMARS USING AN SMT-SOLVER Ii Sentence Syntactic Relations I1 “John has eaten pizza.” agree(John, has), arg(John, eaten), arg(pizza, eaten) I2 “Has Sally eaten pizza?” agree(Sally, has), arg(Sally, eaten), arg(pizza, eaten) I3 “What has John eaten?” agree(John, has), arg(John, eaten), arg(What, eaten) I4 “Who has eaten pizza?” agree(Who, has), arg(Who, eaten), arg(pizza, eaten) I5 “Sally was eating pizza.” agree(Sally, was), arg(Sally, eating), arg(pizza, eating) I6 “Was John eating pizza?” agree(John, was), arg(John, eating), arg(pizza, eating) I7 “What was Sally eating?” agree(Sally, was), arg(Sally, eating), arg(What, eating) I8 “Who was eating pizza?” agree(Who, was), arg(Who, eating), arg(pizza, eating) I9 “Pizza was eaten.” agree(pizza, was), arg(pizza, eaten) I10 “Was pizza eaten?” agree(pizza, was), arg(pizza, eaten) I11 “What was eaten?” agree(What, was), arg(What, eaten) Table 1: Input sequence of annotated sentences. The annotation of each sentence includes the syntactic relations listed for morphological agreement (indicated by agree) and predicate-argument structure (indicated by arg); the type of the sentence – i.e. either declarative or interrogative – is also annotated on each sentence (but not listed here). The sentences listed here include passive constructions (I9, I10, I11), yes/no-questions (I2, I6, I10) and wh-questions (I3, I4, I7, I8, I11). phrase is the category associated with the head of that phrase. An additional ﬁnite sort encodes the universal functional and lexical categories {CDeclarative, CQuestion, T, v, V } and {P, D, N} (Adger and Svenonius (2011)) and additional functions and axioms encode the two extended projections C- T-V and P-D-N (Grimshaw (2005); Adger (2003)) that constrain what structural conﬁgurations the functional categories may be arranged in within a derivation that converges. Finally, we consider the axioms imposed on a given sentence by: (a) the linear ordering of the words in a sentence – linearization is modeled by a conjunction of axioms for lifting the derived tree from the derivation tree as presented in Graf (2013)21 and Kayne’s Linear Correspondence Axioms (Kayne (1994));", "meta": {"paper_id": "1905.02869v1", "chunk_index": 5, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_6", "text": "we consider the axioms imposed on a given sentence by: (a) the linear ordering of the words in a sentence – linearization is modeled by a conjunction of axioms for lifting the derived tree from the derivation tree as presented in Graf (2013)21 and Kayne’s Linear Correspondence Axioms (Kayne (1994)); (b) the syntactic relations annotating a sentence – these are each translated into axioms for either morphological agreement or predicate-argument structure, the latter in accordance with the theory of argument structure in (Hale (1993)).22 4. Experiment We used our implementation of the procedure presented in Section 3 to infer a set of minimalist lexicons, denoted here as G∗, from an input sequence with eleven sentences (listed in Table-1), each annotated with predicate-argument relations as well as morphological agreement.23 We validated the lexicons sampled from G∗by using an agenda-based MG parser (Harkema (2001)) to verify that the lexicon can be used to parse each sentence in the input sequence. Manual inspection of lexicons sampled from G∗revealed that many of them had a large number of lexical items and produced parses that do not resemble those found in contemporary theories of syntax – see Lexicon-A in Figure-1 for an example. 21. See also Collins and Stabler (2016). 22. This theory requires that the lexical heads associated with a predicate and its arguments must enter into particular structural conﬁgurations within a derivation. (See also Hale and Keyser (2002). 23. We bounded the acquisition model with the following parameters: a parse may have up to 3 instances of phrasal movement and up to one instance of head movement; lexical items may have at most 3 features. 5 INDURKHYA Lexicon-A Lexicon-B Lexicon-C eaten::<=x2,+l,∼x1 eaten::=x2,∼x2 eaten::=x3,∼x4 eaten::=x2,+l,∼x2 eaten::=x2,+r,∼x2 eaten::=x0,=x0,∼x1 eating::<=x2,+l,∼x1 eating::=x2,∼x2 eating::=x0,=x0,∼x4 eating::=x2,+l,∼x2 has::=x1,+l,∼x2 has::=x1,+l,∼x2 has::∼x0,-l has::=x2,+l,∼x2 john::∼x0,-l has::=x2,+r,∼x1 has::∼x2,-r,-r pizza::∼x0 john::=x0,∼x2 john::=x2,+r,∼x2 pizza::∼x3,-l john::∼x2,-l,-r pizza::=x2,+r,∼x2 sally::∼x0,-l pizza::=x2,∼x2 sally::=x2,+r,∼x2 was::=x4,+l,∼x2 pizza::=x2,+l,∼x2 was::=x2,+l,∼x2 what::∼x0,-r sally::∼x2,-l,-r was::=x2,+l,∼x2 what::∼x3,-l,-r sally::=x2,+l,∼x2 what::∼x2,-r,-l who::∼x0,-l,-r was::∼x2,-l,-l who::=x2,+r,∼x0 ϵCdecl::=x2,C was::=x2,+r,∼x1 ϵCdecl::<=x2,C ϵCintr::<=x2,C what::=x1,∼x1 ϵCintr::=x2,C ϵCintr::<=x2,+r,C what::=x2,+l,∼x1 ϵCintr::<=x0,C who::∼x2,-l,-r ϵCdecl::<=x2,C ϵCdecl::=x1,C ϵCintr::=x1,C ϵCintr::<=x1,C Figure 1: Examples of inferred lexicons that satisfy the conditions imposed by the input sequence in Table-1. Each lexical item consists of a pairing of a phonetic form and a sequence of syntactic features, separated by a double-colon. The phonetic forms ϵCdecl and ϵCintr are covert (unpro- nounced); their presence in a minimalist parse tree indicates whether the parse is of a declarative or interrogative sentence, respectively. We ﬁltered out lexicons such as these by using Z3 to identify lexicons in G∗that were optimal with respect to a cost function that penalizes a lexicon for the number of lexical entries it has.24 This produced a subset of G∗in which every lexicon had exactly 15 lexical items, the minimal number of lexical items required for a lexicon to be able to produce parses that accord with the speciﬁed input sequence. See Lexicon-B in Figure-1 for an example of a lexicon in this subset; see Figure-3 for an example of a parse produced by Lexicon-B. We manually inspected this subset of G∗and found that most of the lexicons produced parses with many instances of internal", "meta": {"paper_id": "1905.02869v1", "chunk_index": 6, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_7", "text": "accord with the speciﬁed input sequence. See Lexicon-B in Figure-1 for an example of a lexicon in this subset; see Figure-3 for an example of a parse produced by Lexicon-B. We manually inspected this subset of G∗and found that most of the lexicons produced parses with many instances of internal merge that could have been eliminated without any side-effects, and that these parses that did not accord with contemporary theories of syntax. Finally, we further reﬁned this subset of G∗by using Z3 to identify lexicons that were optimal with respect to two additional cost functions: 1. (minimize) the total number of selectional and licensing features in the lexicon and the parses; this cost function rewards reduction in the total size of both the lexicon and the derivations;25 24. We did this by encoding this cost function as a logical formula, adding it to the SMT-solver after running the inference procedure, and then re-solving; the resulting set of (inferred) minimalist grammars are optimal with respect to the speciﬁed cost functions. 25. This cost function is based on the MDL principle (Gr¨unwald (2007)) as applied to MGs in (Stabler (1998)). 6 AUTOMATIC INFERENCE OF MINIMALIST GRAMMARS USING AN SMT-SOLVER 2. (maximize) the number of distinct selectional features in the lexicon; this cost function re- wards lexicons that are less inclusive (i.e. they are less likely to overgenerate).26 This produced a subset of G∗in which each lexicon had exactly: 15 lexical items; 33 features in the lexicon (not including the special feature C); 125 features in the parses; at least 4 distinct selectional features. See Lexicon-C in Figure-1 for a representative member of this subset. We found that the lexicons in this subset were all of the same form – i.e. they are only differentiated by permutations of the feature values and other symmetries in the model – and that these lexicons produced parses that agreed with those prescribed by contemporary minimalist theories of syntax. (as presented in Hornstein et al. (2005), Adger (2003), and Radford (1997).) See Figure-2 for a parse produced by Lexicon-C that demonstrates several of the syntactic phenomenon that Lexicon-C models correctly (i.e. as prescribed by minimalist theories of syntax) while respecting the syntactic relations prescribed in sentence I7 of Table-1. Figure 2: An MG parse for the sentence “What was Sally eating?” (see I7 in Table-1 for annotations) derived from Lexicon-C in Figure-1. The parse is assembled in a bottom-up manner via merge: ﬁrst “eating” merges with “what” and then with “Sally”, thus establishing (via locality) predicate- argument relations; once the resulting structure then merges with “was”, “sally” then undergoes subject-raising by (internally) merging with “was”, thus establishing morphological agreement be- tween ‘”sally” and “was”. Next the head of the auxiliary verb “was” undergoes head movement to merge with the covert form, ϵCintr, which indicates that the sentence is an interrogative. Finally, “what” undergoes wh-fronting by (internally) merging with ϵCintr. Wh-fronting and Subject-raising are triggered by different licensor features, the former by +r and the latter by +l. The feature se- quences displayed in internal nodes have an asterisk separating", "meta": {"paper_id": "1905.02869v1", "chunk_index": 7, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_8", "text": "covert form, ϵCintr, which indicates that the sentence is an interrogative. Finally, “what” undergoes wh-fronting by (internally) merging with ϵCintr. Wh-fronting and Subject-raising are triggered by different licensor features, the former by +r and the latter by +l. The feature se- quences displayed in internal nodes have an asterisk separating features that have already been consumed (on the left) from those that have not (on the right). 26. This cost function is based on the Subset Principle (Berwick (1985)), which asserts that a language learner will always choose the least inclusive grammar available at each stage of acquisition; the adaption of this principle is a logical necessity if one assumes that the learner does not make use of (indirect) negative evidence. See also Yang (2015, 2016). 7 INDURKHYA Figure 3: An MG parse for the sentence “What was Sally eating?” derived from Lexicon-B in Figure-1. Although this parse satisﬁes the constraints imposed by the annotations listed for I7 in Table-1, it does not accord with the parse prescribed by contemporary theories of syntax – e.g. nowhere in the parse does the auxiliary verb “was” establish a local relation with the main predicate “eating.” 5. Conclusion In this study we have (i) proposed and implemented a procedure for inferring MGs and (ii) used this procedure to infer an MG that closely aligns with contemporary theories of syntax, thus demonstrat- ing how linguistically-relevant MGs may be identiﬁed within the inferred set of MGs by optimizing cost functions derived from methods of inductive inference that are relevant to cognitively-faithful models of language acquisition. We observe that by enabling and disabling axioms in our model, we can carry out experiments to determine which are redundant, and thereby gain insight into whether the linguistic principles, from which the axioms of the system are largely derived, are justiﬁed or can be discarded, thus aiding in the evaluation of the Strong Minimalist Thesis. Going forward, we plan to: incorporate phase theory (Chomsky (2001, 2008)) into our model of a minimalist parse tree, following the approach taken by Chesi (2007); examine the over-generations produced by the MGs inferred by our procedure and understand how these over-generations relate to the cost functions used by our procedure for identifying optimal grammars; investigate the potential for this procedure to be used for producing MG treebanks27, which may aid treebank based parsing strategies, by extracting sets of (partially) annotated sentences (that may be used as input to the inference procedure) from treebanks such as PropBank (Kingsbury and Palmer (2002)) or the UD treebanks (Nivre et al. (2016)). 27. See Torr (2018) for an alternative approach to developing large-scale MG treebanks. 8 AUTOMATIC INFERENCE OF MINIMALIST GRAMMARS USING AN SMT-SOLVER Acknowledgments The author would like to thank Robert C. Berwick, Sandiway Fong, Beracah Yankama, and Norbert Hornstein for their suggestions, feedback, and inspiration. Additionally, the author is very grateful for the ﬁnancial support provided by Moody’s Investor Services. References David Adger. Core syntax: A minimalist approach, volume 33. Oxford University Press Oxford, 2003. David Adger and Peter Svenonius. Features in minimalist syntax. The Oxford handbook", "meta": {"paper_id": "1905.02869v1", "chunk_index": 8, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_9", "text": "Norbert Hornstein for their suggestions, feedback, and inspiration. Additionally, the author is very grateful for the ﬁnancial support provided by Moody’s Investor Services. References David Adger. Core syntax: A minimalist approach, volume 33. Oxford University Press Oxford, 2003. David Adger and Peter Svenonius. Features in minimalist syntax. The Oxford handbook of linguistic minimalism, pages 27–51, 2011. Rolf Backofen, James Rogers, and Krishnamurti Vijay-Shanker. A ﬁrst-order axiomatization of the theory of ﬁnite trees. Journal of Logic, Language and Information, 4(1):5–39, 1995. Mark C. Baker. Incorporation: A theory of grammatical function changing. University of Chicago Press, 1988. Clark Barrett and Cesare Tinelli. Satisﬁability modulo theories. In Handbook of Model Checking, pages 305–343. Springer, 2018. Robert C. Berwick. The acquisition of syntactic knowledge. MIT press, 1985. Cristian Cadar and Koushik Sen. Symbolic execution for software testing: three decades later. Commun. ACM, 56(2):82–90, 2013. Cristiano Chesi. An introduction to phase-based minimalist grammars: why move is top-down from left-to-right. Studies in linguistics, 2007. Noam Chomsky. Aspects of the theory of syntax. MIT Press, 1965. Noam Chomsky. The Minimalist Program. MIT Press, 1995. Noam Chomsky. Derivation by phase. In Michael Kenstowicz, editor, Ken Hale: A life in language, pages 1–52. MIT press, 2001. Noam Chomsky. On phases. Current Studies in Linguistics Series, 45:133, 2008. Noam Chomsky. Problems of projection. Lingua, 130:33–49, 2013. Chris Collins and Edward Stabler. A formalization of minimalist syntax. Syntax, 19(1):43–78, 2016. Leonardo De Moura and Nikolaj Bjørner. Z3: An efﬁcient smt solver. In Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems, TACAS’08/ETAPS’08, pages 337–340. Springer-Verlag, 2008. 9 INDURKHYA Leonardo De Moura and Nikolaj Bjørner. Satisﬁability modulo theories: introduction and applica- tions. Communications of the ACM, 54(9):69–77, 2011. Thomas Graf. Local and transderivational constraints in syntax and semantics. PhD thesis, Uni- versity of California at Los Angeles, 2013. Jane B Grimshaw. Words and structure. Center for the Study of Language and Information, Stanford University, 2005. Peter D Gr¨unwald. The minimum description length principle. MIT press, 2007. Kenneth Hale. On argument structure and the lexical expression of syntactic relations. In Ken Hale and Samuel J. Keyser, editors, The view from Building 20: Essays in linguistics in honor of Sylvain Bromberger. MIT Press, 1993. Kenneth Locke Hale and Samuel Jay Keyser. Prolegomenon to a theory of argument structure, volume 39. MIT press, 2002. Hendrik Harkema. Parsing minimalist languages. Citeseer, 2001. Norbert Hornstein, Jairo Nunes, and Kleanthes K Grohmann. Understanding minimalism. Cam- bridge University Press, 2005. Richard S Kayne. The antisymmetry of syntax. Number 25. MIT Press, 1994. Paul Kingsbury and Martha Palmer. From treebank to propbank. In LREC, pages 1989–1993. Citeseer, 2002. Jens Michaelis. Derivational minimalism is mildly context-sensitive. In LACL, volume 98, pages 179–198. Springer, 1998. Joakim Nivre, Marie-Catherine De Marneffe, Filip Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan T McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. Universal dependencies v1: A multilingual treebank collection. In LREC, 2016. Fernando C. N. Pereira and David H. D. Warren. Parsing as deduction. In Proceedings of the 21st Annual Meeting on Association", "meta": {"paper_id": "1905.02869v1", "chunk_index": 9, "filename": "1905.02869v1.pdf", "chunk_length": 512}}
{"id": "1905.02869v1_chunk_10", "text": "Ginter, Yoav Goldberg, Jan Hajic, Christopher D Manning, Ryan T McDonald, Slav Petrov, Sampo Pyysalo, Natalia Silveira, et al. Universal dependencies v1: A multilingual treebank collection. In LREC, 2016. Fernando C. N. Pereira and David H. D. Warren. Parsing as deduction. In Proceedings of the 21st Annual Meeting on Association for Computational Linguistics, ACL ’83, pages 137–144. Association for Computational Linguistics, 1983. Andrew Radford. Syntactic theory and the structure of English: A minimalist approach. Cambridge University Press, 1997. Andrew Radford. An introduction to English sentence structure. Cambridge university press, 2009. Manny Rayner, ˚Asa Hugosson, and G¨oran Hagert. Using a logic grammar to learn a lexicon. In Proceedings of the 12th conference on Computational linguistics-Volume 2, pages 524–529. Association for Computational Linguistics, 1988. James Rogers. A descriptive approach to language-theoretic complexity. CSLI Publications, 1998. Dominique Sportiche, Hilda Koopman, and Edward Stabler. An introduction to syntactic analysis and theory. John Wiley & Sons, 2013. 10 AUTOMATIC INFERENCE OF MINIMALIST GRAMMARS USING AN SMT-SOLVER Edward Stabler. Derivational minimalism. In International Conference on Logical Aspects of Com- putational Linguistics, pages 68–95. Springer, 1996. Edward P Stabler. The Logical Approach to Syntax. MIT Press, 1993. Edward P. Stabler. Acquiring languages with movement. Syntax, 1(1):72–97, 1998. Edward P Stabler. Recognizing head movement. In International Conference on Logical Aspects of Computational Linguistics, pages 245–260. Springer, 2001. Edward P Stabler. Varieties of crossing dependencies: structure dependence and mild context sen- sitivity. Cognitive Science, 28(5):699–720, 2004. John Torr. Constraining mgbank: Agreement, l-selection and supertagging in minimalist grammars. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 590–600. Association for Computational Linguistics, 2018. K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. Characterizing structural descriptions pro- duced by various grammatical formalisms. In Proceedings of the 25th Annual Meeting on Asso- ciation for Computational Linguistics, ACL ’87, pages 104–111. Association for Computational Linguistics, 1987. Kenneth Wexler. The subset principle is an intensional principle. In Knowledge and language, pages 217–239. Springer, 1993. Charles Yang. Negative knowledge from positive evidence. Language, 91(4):938–953, 2015. Charles Yang. The price of linguistic productivity: How children learn to break the rules of lan- guage. MIT Press, 2016. 11", "meta": {"paper_id": "1905.02869v1", "chunk_index": 10, "filename": "1905.02869v1.pdf", "chunk_length": 360}}
{"id": "1905.02925v1_chunk_0", "text": "ShapeGlot: Learning Language for Shape Differentiation Panos Achlioptas* Judy Fan Robert Hawkins Noah Goodman Leonidas Guibas Stanford University distractors target thin legs, no holes in back five bars on back, skinny top irregular cube on two legs wide seat with solid back Pragmatic Neural Speaking (novel images) (real-world images) (novel 3D point-clouds) (unseen classes) Attentive Neural Listening (novel images) the chair has lines all up the back (unseen object parts) rounded seat 0.00 0.10 0.90 listener scores: 0.01 0.00 0.99 0.04 0.18 0.78 distractors target distractors target Figure 1: We introduce a large corpus of utterances that refer to the shape of objects and develop neural speakers and listeners with broad generalization capacity. Top row: A neural speaker generates utterances to distinguish a “target” from two “distractors” in unseen: images of synthetic data (left), out-of-distribution (OOD) real-world images (center), and 3D point-clouds of CAD models (right). Bottom row: A neural listener interprets human-generated utterances for unseen (left- to-right): images of synthetic data, OOD object categories (here, lamps), and OOD object parts. Listener scores indicate model interpretation about which object the utterance refers to. Words are color-coded according to the attention placed by the neural listener (warmer color indicates higher attention). Abstract In this work we explore how ﬁne-grained differ- ences between the shapes of common objects are expressed in language, grounded on images and 3D models of the objects. We ﬁrst build a large scale, carefully controlled dataset of human utterances that each refers to a 2D ren- dering of a 3D CAD model so as to distinguish it from a set of shape-wise similar alternatives. Using this dataset, we develop neural language understanding (listening) and production (speaking) models that vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speak- ers that reason about a listener or not), and the neural ar- *Corresponding author: optas@cs.stanford.edu Project webpage: https://www.bit.ly/shapeglot chitecture (e.g. with or without attention). We ﬁnd mod- els that perform well with both synthetic and human part- ners, and with held out utterances and objects. We also ﬁnd that these models are amenable to zero-shot transfer learning to novel object classes (e.g. transfer from train- ing on chairs to testing on lamps), as well as to real-world images drawn from furniture catalogs. Lesion studies indi- cate that the neural listeners depend heavily on part-related words and associate these words correctly with visual parts of objects (without any explicit network training on object parts), and that transfer to novel classes is most successful when known part-words are available. This work illustrates a practical approach to language grounding, and provides a case study in the relationship between object shape and lin- guistic structure when it comes to object differentiation. 1 arXiv:1905.02925v1 [cs.CL] 8 May 2019 1. Introduction Objects are best understood in terms of their structure and function, both of which are built on a foundation of object parts and their relations [10, 9, 55, 8]. Natural lan- guages have been optimized across human history to solve the problem of efﬁciently communicating the", "meta": {"paper_id": "1905.02925v1", "chunk_index": 0, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_1", "text": "May 2019 1. Introduction Objects are best understood in terms of their structure and function, both of which are built on a foundation of object parts and their relations [10, 9, 55, 8]. Natural lan- guages have been optimized across human history to solve the problem of efﬁciently communicating the aspects of the world most relevant to one’s current goals [22, 12]. As such, languages can provide an effective medium to describe the shapes and the parts of different objects, and to express object differences. For example, when we see a chair we can decompose it into semantically meaningful parts, like a back and a seat, and can combine words to create utterances that reﬂect their geometric and topological shape-properties e.g. ‘wide seat with a solid back. Moreover, given a speciﬁc communication context, we can craft references that are not merely true, but which are also relevant: i.e. we can refer to the lines found in a chair’s back to distinguish it among other similar objects (see Fig. 1). In this paper we explore this interplay between natu- ral, referential language, and the shape of common objects. While a great deal of recent work has explored visually- grounded language understanding [20, 32, 52, 27, 26, 51], the resulting models have limited capacity to reﬂect the ge- ometry and topology (i.e. the shape) of the underlying ob- jects. This is because reference in previous studies was possible using properties like color, or properties regard- ing the object and it’s hosting environment (e.g. it’s abso- lute, or relative to other objects, location). Indeed, elicit- ing natural language that refers only to shape properties re- quires carefully controlling the objects, their presentation, and the linguistic task. To address such challenges, we use pure 3D representations of objects (CAD models), which allow for ﬂexible and controlled presentation (i.e. texture- less, uniform-color objects, viewed without obstruction in a ﬁxed pose). We further make use of the 3D form to construct a reference game task in which the referred ob- ject is similar shape-wise to the contrasting objects. The result of this effort is a new multimodal dataset, termed CiC (Chairs in Context), comprised of 4,511 unique chairs from ShapeNet [4] and 78,789 referential utterances. In CiC chairs are organized into 4,054 sets of size 3 (representing contrastive communication contexts) and each utterance is intended to distinguish a chair in context. The visual dif- ferences among the grouped objects require a deep under- standing of very ﬁne-grained shape properties (especially, for Hard contexts, see Section 2); the language that peo- ple use to do so is correspondingly complex, exhibiting rich compositionality. We use CiC to train and analyze a variety of modern neural language understanding (listening) and production (speaking) models. These models vary in their grounding (pure 3D forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not) and the neural architec- ture (e.g. with or without word attention, and with context- free or context-aware object encoders). We evaluate these models on the original reference game", "meta": {"paper_id": "1905.02925v1", "chunk_index": 1, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_2", "text": "forms via point-clouds vs. rendered 2D images), the degree of pragmatic reasoning captured (e.g. speakers that reason about a listener or not) and the neural architec- ture (e.g. with or without word attention, and with context- free or context-aware object encoders). We evaluate these models on the original reference game task with both syn- thetic and human partners, and with held out utterances and objects, ﬁnding strong performance. Since language con- veys abstractions, such as object parts, that are shared be- tween object categories, we hypothesized that our models learn robust representations that are transferable to objects of unseen classes (e.g. training on chairs while testing on lamps). Indeed, we show that these models have strong gen- eralization capacity to novel object categories, as well as to real-world colored images drawn from furniture catalogs. Finally, we explore how our models are succeeding on these communication tasks. We demonstrate that the neu- ral listener learns to prioritize the same abstractions in ob- jects (i.e. properties of chair parts) that humans do in solv- ing the communication task, despite never being provided with an explicit decomposition of these objects into parts. Similarly, we show that transfer learning to novel object classes is most successful when known part-related words are available. Last, we show that a neural speaker that is pragmatic—planing utterances in order to convey the right target object to an imagined listener —produces signiﬁ- cantly more informative utterances than a literal (listener- unaware) speaker, as measured by human performance in identifying the correct object. 2. Dataset and task hard context easy context CiC (Chairs in Context) consists of triplets of chairs coupled with referential utter- ances that aim to distinguish one chair (the “target”) from the remaining two (the “dis- tractors”). To obtain such utterances, we paired partic- ipants from Amazon’s Me- chanical Turk (AMT) to play an online reference game [16]. On each round of the game, the two players were shown the same triplet of chairs. The designated target chair was pri- vately highlighted for one player (the “speaker”) who was asked to send a message through a chat box such that their partner (the “listener”) could successfully select it from the context. To ensure speakers used only shape-related infor- mation, we scrambled the positions of the chairs for each participant independently and used textureless, uniform- color renderings of pre-aligned 3D CAD models, taken from the same viewpoint. To ensure communicative inter- action was natural, no constraints were placed on the chat box: referring expressions from the speaker were occasion- ally followed by clariﬁcation questions from the listener or other discourse. A key decision in building our dataset concerned the construction of contexts that would reliably elicit diverse and potentially very ﬁne-grained contrastive language. To achieve diversity we considered all ∼7,000 chairs from ShapeNet. This object class is geometrically complex, highly diverse, and abundant in the real world. To control the granularity of ﬁne-grained distinctions that were nec- essary in solving the communication task, we constructed two types of contexts: Hard contexts consisted of very similar shape-wise chairs, and Easy contexts", "meta": {"paper_id": "1905.02925v1", "chunk_index": 2, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_3", "text": "chairs from ShapeNet. This object class is geometrically complex, highly diverse, and abundant in the real world. To control the granularity of ﬁne-grained distinctions that were nec- essary in solving the communication task, we constructed two types of contexts: Hard contexts consisted of very similar shape-wise chairs, and Easy contexts consisted of less similar chairs. To measure shape-similarity in an un- supervised manner, we used the latent space derived from an Point Cloud-AutoEncoder (PC-AE) [1]. We note, that point-clouds are an intrinsic representation of a 3D object, oblique to color or texture. After extracting a 3D point- cloud from the surface of each ShapeNet model we com- puted the underlying K-nearest-neighbor graph among all models according to their PC-AE embedding distances. For a chair with sufﬁciently high-in degree on this graph (cor- responding intuitively to a canonical chair) we contrasted it with four distractors: the two closest to it in latent-space, and two that were sufﬁciently far (see inset for a demon- stration and at the Appendix for additional details). Last, to reduce potential data biases we counterbalanced each com- munication context, by considering every chair of a given context as target, in at least four games. Before presenting our neural agents, we identify some distinctive properties of our corpus. Human performance on the reference game was high, but listeners made sig- niﬁcantly more errors in the Hard triplets (accuracy 94.2% vs. 97.2%, z = 13.54, p < 0.001). Also, in Hard triplets longer utterances were used to describe the targets (on aver- age 8.4 words vs. 6.1, t = −35, p < 0.001). A wide spec- trum of descriptions was elicited, ranging from the more holistic/categorical (e.g. “the rocking chair”) common for Easy triplets, to more complex and ﬁne-grained language, (e.g. “thinner legs but without armrests”) common for Hard triplets. Interestingly, 78% of the utterances used at least one part-related word: “back”, “legs”, “seat,” “arms”, or closely related synonyms e.g. “armrests”. 3. Neural listeners Constructing neural listeners that reason effectively about shape properties is a key contribution of our work. Below we conduct a detailed comparison between three distinct architectures, highlight the effect of different reg- ularization techniques, and investigate the merits of dif- ferent representations of 3D objects for the listening task, namely, 2D rendered images and 3D surface point clouds. In what follows, we denote the three objects of a commu- nication context as O = {o1, o2, o3}, the corresponding word-tokenized utterance as U = u1, u2, . . . and as t ∈O the designated target. Our proposed listener is inspired by [31]. It takes as in- put a (latent code) vector that captures shape information for each of the objects in O, and a (latent code) vector for each token of U, and outputs an object–utterance compati- bility score L(oi, U) ∈[0, 1] for each input object. At its core lies a multi-modal LSTM [17] that receives as input (“is grounded” with) the vector of a single object, processes the word-sequence U, and is read out by a ﬁnal MLP to yield a single number (the", "meta": {"paper_id": "1905.02925v1", "chunk_index": 3, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_4", "text": "compati- bility score L(oi, U) ∈[0, 1] for each input object. At its core lies a multi-modal LSTM [17] that receives as input (“is grounded” with) the vector of a single object, processes the word-sequence U, and is read out by a ﬁnal MLP to yield a single number (the compatibility score). This is re- peated for each oi, sharing all network parameters across the objects. The resulting three scores are soft-max normal- ized and compared to the ground-truth indicator vector of the target under the cross-entropy loss.* Object encoders We experimented with three object representations to capture the underlying shapes: (a) the bottleneck vector of a pretrained Point Cloud-AutoEncoder (PC-AE), (b) the embedding provided by a convolutional network operating on single-view images of non-textured 3D objects, or (c) a combination of (a) and (b). Speciﬁ- cally, for (a) we use the PC-AE architecture of [1] trained with single-class point clouds extracted from the surfaces of 3D CAD models, while for (b) we use the activations of the penultimate layer of a VGG-16 [38], pre-trained on Im- ageNet [7], and ﬁne-tuned on an 8-way classiﬁcation task with images of objects from ShapeNet. For each represen- tation we project the corresponding latent code vector to the input space of the LSTM using a fully connected (FC) layer with L2-norm weight regularization. The addition of these projection-like layers improves the training and con- vergence of our system. While there are many ways to simultaneously incorpo- rate the two modalities in the LSTM, we found that the best performance resulted when we ground the LSTM with the image code, concatenate the LSTM’s ﬁnal output (after pro- cessing U) with the point cloud code, and feed the concate- nated result in a shallow MLP to produce the compatibil- ity score. We note that grounding the LSTM with point clouds and using images towards the end of the pipeline, resulted in a signiﬁcant performance drop (∼4.8% on av- erage). Also, proper regularization was critical: adding dropout at the input layer of the LSTM and L2 weight reg- ularization and dropout at and before the FC projecting lay- ers improved performance ∼10%. The token codes of each sentence where initialized with the GloVe embedding [35] and ﬁne-tuned for the listening task. Incorporating context information Our proposed baseline listener architecture (Baseline, just described) ﬁrst scores each object separately then applies softmax normal- *Architecture details, hyper-parameter search strategy, and optimal hyper-parameters for all experiments are described in the Appendix. ization to yield a score distribution over the three objects. We also consider two alternative architectures that explic- itly encode information about the entire context while scor- ing an object. The ﬁrst alternative (Early-Context), is iden- tical to the proposed architecture, except for the codes used to ground the LSTM. Speciﬁcally, if vi is the image code vector of the i-th object (oi ∈O) resulting from VGG, instead of using vi as the grounding vector of oi, a shal- low convolutional network is introduced. This network, of which the output is the grounding code, receives the signal f(vj, vk)||g(vj, vk)||vi, where", "meta": {"paper_id": "1905.02925v1", "chunk_index": 4, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_5", "text": "if vi is the image code vector of the i-th object (oi ∈O) resulting from VGG, instead of using vi as the grounding vector of oi, a shal- low convolutional network is introduced. This network, of which the output is the grounding code, receives the signal f(vj, vk)||g(vj, vk)||vi, where f, g are the symmetric (and norm-normalized), max-pool and mean-pool functions, || denotes feature-wise concatenation and vj, vk the alterna- tive constrastive objects. Here, we use symmetric functions to induce that object-order is irrelevant for our task. The second alternative architecture (Combined-Interpretation) ﬁrst feeds the image vectors for all three objects sequen- tially to the LSTM as inputs and then proceeds to process the tokens of U once, to yield the three scores. Similarly to the Baseline architecture, point clouds are incorporated in both alternatives via a separate MLP after the LSTM. Word attention We hypothesized that a listener forced to prioritize a few words in each utterance would learn to prioritize words that express properties that distin- guish the target from the distractors (and, thus, perform bet- ter). To test this hypothesis, we augment the listener models with a standard bilinear attention mechanism [37]. Specif- ically, to estimate the “importance” of each text-token ui we compare the output of the LSTM at ui (denoted as ri) with the hidden state after the entire utterance has been pro- cessed (denoted as h). The relative importance of each word is ai = ∆rT i × Watt × h, where Watt is a trainable diag- onal matrix. The ﬁnal output of the LSTM uses this at- tention to combine all latent states: P|U| i=1 ri ⊙ˆai, where ˆai = exp(ai) P|U| j exp(aj) and ⊙is the point-wise product. 4. Listener experiments We begin our evaluation of the proposed listeners using two reference tasks based on different data splits. In the language generalization task, we test on target objects that were seen as targets in at least one context during training but ensure that all utterances in the test split are from un- seen speakers. In the more challenging object generaliza- tion task, we restrict the set of objects that appeared as tar- gets in the test set to be disjoint from those in training such that all speakers and objects in the test split are new. For each of these tasks, we evaluate choices of input modality and word attention, using [80%, 10%, 10%] of the data, for training, validating and testing purposes. Baseline listener accuracies are shown in Table 2.† Over- †In all results mean accuracies and standard errors across 5 random seeds are reported, to control for the data-split populations and the initial- all the model achieves good performance. As expected, all listeners have higher accuracy on the language generaliza- tion task (3.2% on average). The attention mechanism on words yields a mild performance boost, as long as images are part of the input. Interestingly, images provide a signiﬁ- cantly better input than point-clouds when only one modal- ity is used. This may be due to the higher-frequency content of images (we", "meta": {"paper_id": "1905.02925v1", "chunk_index": 5, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_6", "text": "(3.2% on average). The attention mechanism on words yields a mild performance boost, as long as images are part of the input. Interestingly, images provide a signiﬁ- cantly better input than point-clouds when only one modal- ity is used. This may be due to the higher-frequency content of images (we use point-clouds with only 2048 points), or the fact that VGG was pre-trained while the PC-AE was not. However, we ﬁnd signiﬁcant gains in accuracy (4.1% on average) from exploiting the two object representations simultaneously, implying a complementarity among them. Next, we evaluate how the different approaches in incor- porating context information described in Section 3 affect listener performance. We focus on the more challenging object generalization task, using listeners that include at- tention and both object modalities. We report the ﬁndings in Table 1. We ﬁnd that the Baseline and Early-Context models perform best overall, outperforming the Combined- Interpretation model, which does not share weights across objects. This pattern held for both hard and easy trial types in our dataset. We further explored the small portion (∼14%) of our test set that use explicitly contrastive lan- guage: superlatives (“skinniest”) and comparatives (“skin- nier”). Somewhat surprisingly we ﬁnd that the Baseline architecture remains competitive against the architectures with more explicit context information. The Baseline model thus achieves high performance and is the most ﬂexible (at test time it can be applied to arbitrary-sized contexts); we focus on this architecture in the explorations below. 4.1. Exploring learned representations Linguistic ablations Which aspects of a sentence are most critical for our listener’s performance? To inspect the properties of words receiving the most attention, we ran a part-of-speech tagger on our corpus. We found that the highest attention weight is placed on nouns, controlling for the length of the utterance. However, adjectives that modify nouns received more attention in hard contexts (controlling for the average occurrence in each context), where nouns are often not sufﬁcient to disambiguate (see Fig. 2A). To more systematically evaluate the role of higher-attention to- kens in listener performance, we conducted an utterance le- sioning experiment. For each utterance in our dataset, we successively replaced words with the <UNK> token accord- ing to three schemes: (1) from highest attention to lowest, (2) from lowest attention to highest, and (3) in random or- der. We then fed these through an equivalent listener trained without attention. We found that up to 50% of words can be removed without much performance degradation, but only if these are low attention words (see Fig. 2B). Our word- attentive listener thus appears to rely on context-appropriate ization of the neural-network. Architecture Subpopulations Overall Hard Easy Sup-Comp Combined-Interpretation 75.9 ± 0.5% 67.4 ± 1.0% 83.8 ± 0.6% 74.4 ± 1.5% Early-Context 79.4 ± 0.8% 70.1 ± 1.3% 88.1 ± 0.6% 75.6 ± 2.2% Baseline 79.6 ± 0.8% 69.9 ± 1.3% 88.8 ± 0.4% 76.3 ± 1.3% Table 1: Comparing different ways to include context. The simplest Baseline model performs as well as more complex alternatives. Subpopulations are the subset of test data containing: hard trials (shape-wise similar distractors),", "meta": {"paper_id": "1905.02925v1", "chunk_index": 6, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_7", "text": "± 0.6% 75.6 ± 2.2% Baseline 79.6 ± 0.8% 69.9 ± 1.3% 88.8 ± 0.4% 76.3 ± 1.3% Table 1: Comparing different ways to include context. The simplest Baseline model performs as well as more complex alternatives. Subpopulations are the subset of test data containing: hard trials (shape-wise similar distractors), easy trials, superlatives or comparatives. Input Modality Language Task Object Task No Attention Point Cloud 67.6 ± 0.3% 66.4 ± 0.7% Image 81.2 ± 0.5% 77.4 ± 0.7% Both 83.1 ± 0.4% 78.9 ± 1.0% With Attention Point Cloud 67.4 ± 0.3% 65.6 ± 1.4% Image 81.7 ± 0.5% 77.6 ± 0.8% Both 83.7 ± 0.3% 79.6 ± 0.8% Table 2: Performance of the Baseline listener architec- ture using different object representations and with/without word level attention, in two generalization tasks. content words to successfully disambiguate the referent. A B 0.4 0.6 0.8 0.00 0.25 0.50 0.75 1.00 % words lesioned from utterance listener accuracy smallest to largest largest to smallest random part of speech normalized attention score easy hard 0.0 ADJ NOUN Figure 2: (A) The listener places more attention on adjec- tives in hard (orange) triplets than easy (blue) ones. The histogram’s heights depict mean attention scores normal- ized by the length of the underlying utterances; the error bars are bootstapped 95% conﬁdence intervals. (B) Lesion- ing highest attention words to lowest worsens performance more than lesioning random words or lesioning lowest at- tention words. Visual ablations To test the extent to which our listener is relying on the same semantic parts of the object as humans, we next conducted a lesion experiment on the visual input. We took the subset of our test set where (1) all chairs had complete part annotations available [50] and (2) the corre- sponding utterance mentioned a single part (17% of our test set). We then created lesioned versions of all three objects on each trial by removing pixels of images (and/or points when point-clouds are used), corresponding to parts accord- ing to two schemes: removing a single part or keeping a sin- gle part. We did this either for the mentioned one, or another part, chosen at random. We report listener accuracies on these lesioned objects in Table 3. We found that removing random parts hurts the accuracy by 10.4% on average, but removing the mentioned part dropped accuracy more than three times as much, nearly to chance. Conversely, keeping only the mentioned part while lesioning the rest of the im- age merely drops accuracy by 10.6% while keeping a non- mentioned (random) part alone brings accuracy down close to chance. In other words, on trials when participants de- pended on information about a part to communicate the ob- ject to their partner, we found that visual information about that part was both necessary and sufﬁcient for the perfor- mance of our listener model. Single Part Lesioned Single Part Present Mentioned Part 42.8% ± 2.3 66.8% ± 1.4 Random Part 67.0% ± 2.9 38.8% ± 2.0 Table 3: Evaluating the part-awareness of neural listeners by lesioning object parts. Results shown are for image-only", "meta": {"paper_id": "1905.02925v1", "chunk_index": 7, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_8", "text": "necessary and sufﬁcient for the perfor- mance of our listener model. Single Part Lesioned Single Part Present Mentioned Part 42.8% ± 2.3 66.8% ± 1.4 Random Part 67.0% ± 2.9 38.8% ± 2.0 Table 3: Evaluating the part-awareness of neural listeners by lesioning object parts. Results shown are for image-only listeners, with average accuracy of 77.4% when intact ob- jects are used. Similar ﬁndings regarding point-cloud-based lesioning are provided in the Appendix. 5. Neural speakers Architecture We next explore models that learn to gen- erate an utterance that refers to the target and which distin- guishes it from the distractors. Similarly to a neural listener the heart of these models is an LSTM which encodes the ob- jects of a communication context, and then decodes an ut- terance. Speciﬁcally, for an image-based model, on the ﬁrst three time steps, the LSTM input is the VGG code of each object. Correspondingly, for a point-cloud-based model, the LSTM input is the object codes extracted from a PC-AE. During training and after the objects are encoded, the re- maining input to the LSTM is the ‘current’ utterance token, while the output of the LSTM is compared with the ‘next’ utterance token, under the cross-entropy loss [46]. The tar- get object is always presented last, eliminating the need to represent the index of the target separately. To ﬁnd the best model hyper-parameters (e.g. L2-weights, dropout-rate and # of LSTM neurons) and the optimal stopping epoch, we sample synthetic utterances from the model during train- ing and use a pretrained listener to select the result with the highest listener accuracy. We found this approach to produce models and parameters that yield better quality ut- terances than evaluating with listening-unaware metrics like BLEU [34]. Variations The above (literal) speakers can learn to gen- erate language that discriminates targets from distractors. To test the degree to which distractor objects are used for generation, we experiment with context-unaware speakers that are provided the encoding of the target object only, and are otherwise identical to the above models. Motivated by the recursive social reasoning characteristic of human prag- matic language use (as formalized in the Rational Speech Act framework [13]), we create pragmatic speakers that choose utterances according to their capacity to be discrim- inative, as judged by a pretrained “internal” listener. In this case, we sample utterances from the (literal) speakers, but score (i.e. re-rank) them with: β log(PL(t|U, O)) + (1 −β) |U|α log(PS(U|O, t)), (1) where PL is the listener’s probability to predict the target (t) and PS is the likelihood of the literal speaker to gener- ate U. The parameter α controls a length-penalty term to discourage short sentences [48], while β controls the rela- tive importance of the speaker’s vs. the listener’s opinions. 6. Speaker experiments Qualitatively, our speakers produce good object descrip- tions, see Fig. 3 for examples, with the pragmatic speak- ers yielding more discriminating utterances.‡ To quanti- tatively evaluate the speakers we measure their success in reference games with two different kinds of partners: with an independently-trained listener model and with human listeners.", "meta": {"paper_id": "1905.02925v1", "chunk_index": 8, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_9", "text": "Qualitatively, our speakers produce good object descrip- tions, see Fig. 3 for examples, with the pragmatic speak- ers yielding more discriminating utterances.‡ To quanti- tatively evaluate the speakers we measure their success in reference games with two different kinds of partners: with an independently-trained listener model and with human listeners. To conduct a fair study when we used a neu- ral listener, we split the training data in half. The evalu- ating listener was trained using one half, while the scor- ing (or “internal”) listener used by the pragmatic speaker was trained on the remaining half. For our human evalu- ation, we used the literal and pragmatic variants to gener- ate referring expressions on the test set (we use all train- ing data to train the internal listeners here). We then showed these referring expressions to participants recruited ‡The project’s webpage contains additional qualitative results. on AMT and asked them to select the object from context that the speaker was referring to. We collected approxi- mately 2.2 responses for each triplet (we use 1200 unique triplets from the object-generalization test-split, annotated separately by each speaker model). The synthetic utterances used were the highest scoring ones (Eq. 1) for each model with optimal (per-validation) α and a β = 1.0. We note that while the point-based speakers operate solely with point- cloud representations, we present their produced utterances to AMT participants accompanied by CAD rendered im- ages, to keep the human-side presentation identical across experiments. Table 4: Evaluating neural speakers operating with Point Cloud or Image object representations. Speaker Architecture Modality Neural Listener Human Listener Context Unaware Point Cloud Image 59.1 ± 2.0% 64.0 ± 1.7% - - Literal Point Cloud Image 71.5 ±1.3% 76.6 ± 1.0% 66.2 68.3 Pragmatic Point Cloud Image 90.3 ±1.3% 92.2 ±0.5% 69.4 78.7 We found (see Table 4) that our pragmatic speakers per- form best with both synthetic and human partners. While their success with the synthetic listener model may be un- surprising, given the architectural similarity of the internal listener and the evaluating listener, human listeners were 10.4 percentage points better at picking out the target on utterances produced by the pragmatic vs. literal speaker for the best-performing (image-based) variant. We also found an asymmetry between the listening and speaking tasks: while context-unaware listeners achieved high per- formance, we found that context-unaware speakers fare sig- niﬁcantly worse than context-aware ones. Last, we note that both literal and pragmatic speakers produce succinct descriptions (average sentence length 4.21 vs. 4.97) but the pragmatic speakers use a much richer vocabulary (14% more unique nouns and 33% more unique adjectives, after controlling for average length discrepancy). 7. Out-of-distribution transfer learning Language is abstract and compositional. These prop- erties make language use generalizable to new situations (e.g. using concrete language in novel scientiﬁc domains) and robust to low-level perceptual variation (e.g. lighting). In our ﬁnal set of experiments we examine the degree to which our neural listeners and speakers learn representa- tions that are correspondingly robust: that capture associa- tions between the visual and the linguistic domains permit generalization out of", "meta": {"paper_id": "1905.02925v1", "chunk_index": 9, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_10", "text": "novel scientiﬁc domains) and robust to low-level perceptual variation (e.g. lighting). In our ﬁnal set of experiments we examine the degree to which our neural listeners and speakers learn representa- tions that are correspondingly robust: that capture associa- tions between the visual and the linguistic domains permit generalization out of the training domain. the one with the circle on the bottom it has rollers on the feet the chair with the thin-est legs square back, straight legs pragmatic speaker distractors 0.55 0.16 0.29 0.29 0.20 0.51 0.19 0.32 0.49 0.00 0.14 0.86 target literal speaker listener scores listener scores distractors target distractors target the one with the thick-est legs 0.05 0.85 0.10 thin-est seat 0.19 0.24 0.57 Figure 3: Pragmatic vs. literal speakers in unseen (‘hard’) contexts. The pragmatic generations successfully discern the target, even in cases where the literal ones fail. The two left-most examples are based on image-based speakers/listeners, the right-most with point-cloud-based. The utterances are color coded according to the attention placed by an evaluating neural listener whose classiﬁcation scores are shown above each corresponding utterance. Understanding out-of-class reference To test the gener- alization of listeners to novel stimuli, we collected refer- ring expressions in communication contexts made of ob- jects in ShapeNet drawn from new classes: beds, lamps, sofas and tables. These classes are distinct from chairs, but share some parts and properties, making transfer possible for a sufﬁciently compositional model. For each of these classes we created 200 contexts made of random triplets of objects; we collected 2 referring expressions for each target in each context (from participants on AMT). Exam- ples of visual stimuli and collected utterances are shown in Fig. 4 (bottom-row). To this data, we applied an (image- only, with/without-attention) listener trained on the CiC (i.e. chairs) data. We avoid using point-clouds since unlike VGG which was ﬁnetuned with multiple ShapeNet classes, the PC-AE was pre-trained on a single-class. As shown in Table 5, the average accuracy is well above chance in all transfer categories (56% on average). More- over, constraining the evaluation to utterances that contain only words that are in the CiC training vocabulary (75% of all utterances, column: known) only slightly improves the results. This is likely because utterances with unknown words still contain enough known vocabulary for the model to determine meaning. We further dissect the known popu- lation into utterances that contain part-related words (with- part) and their complement (without-part). For the train- ing domain of chairs without-part utterances yield slightly higher accuracy. However the useful subcategories that sup- port this performance (e.g. “recliner”) do not support trans- fer to new categories. Indeed, we observe that for transfer classes (except sofa) the listener performs better when part- related words are present. Furthermore, the performance gap between the two populations appears to become larger as the perceptual distance between the transfer and training domains increases (compare sofas to lamps). Table 5: Transfer-learning of neural listeners in novel object classes, and in different subpopulations of utterances. The subpopulations are: entire: all utterances, known: with all tokens in the chair training-vocabulary, with-part:", "meta": {"paper_id": "1905.02925v1", "chunk_index": 10, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_11", "text": "appears to become larger as the perceptual distance between the transfer and training domains increases (compare sofas to lamps). Table 5: Transfer-learning of neural listeners in novel object classes, and in different subpopulations of utterances. The subpopulations are: entire: all utterances, known: with all tokens in the chair training-vocabulary, with-part: subset of known that contain at least one part-related word, without- part subset of known and complement of with-part. For reference the test-chair statistics are shown (ﬁrst row), but are not included in the reported average (last row). The ac- curacies are averages of ﬁve listeners trained on different data splits. Further details are provided in the Appendix. Population Class entire known with part without part chair 77.4 77.8 77.0 80.5 bed 56.4 55.8 63.8 51.5 lamp 50.1 51.9 60.3 47.1 sofa 53.6 55.0 55.1 54.7 table 63.7 65.5 68.3 62.7 average 56.0 57.1 61.9 54.9 Describing real images Transfer from synthetic data to real data is often difﬁcult for modern machine learning models, that are attuned to subtle statistics of the data. We explored the ability of our models to transfer to real chair images (rather than the training images which were ren- dered without color or texture from CAD models) by cu- rating a modest-sized (300) collection of chair images from online furniture catalogs. These images were taken from a similar view-point to that of the training renderings and have rich color and texture content. We applied the (image- only) pragmatic speaker to these images, after subtracting the average ImageNet RGB values (i.e. before passing the images to VGG). Examples of the speaker’s productions are shown in Figure 4. For each chair, we randomly selected distractors target gap between the back and the seat distractors target distractors target circular arm rests two legs connected the bed has a fancy metal headboard and two pillows 0.06 0.02 0.92 very narrow and tall rectangular table with four tapered legs 0.06 0.07 0.87 listener scores: target distractors target distractors target distractors this lamp is wire mesh 0.01 0.02 0.97 Figure 4: Examples of out-of-distribution neural speaking and listening. Top row: model generations for real-world cat- alogue images. The speaker successfully describes ﬁne grained shape differences on images with rich color and texture content, not present in the training data. Bottom row: results of applying a word-attentive listener on renderings of CAD objects from unseen classes with human-produced utterances. The listener can detect the (often localized) visual cues that humans refer to, despite the large visual discrepancy of these objects from training-domain of chairs. (The utterances are color coded according to the listener’s attention.) two distractors and asked 2 AMT participants to guess the target given the utterance produced by our speaker. Hu- man listeners correctly guessed the target chair 70.1% of the time. Our speaker appears to transfer successfully to real images, which contain color, texture, pose variation, and likely other differences from our training data. 8. Related work Image labeling and captioning Our work builds on recent progress in the development of vision models that in- volve some amount of language data,", "meta": {"paper_id": "1905.02925v1", "chunk_index": 11, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_12", "text": "Our speaker appears to transfer successfully to real images, which contain color, texture, pose variation, and likely other differences from our training data. 8. Related work Image labeling and captioning Our work builds on recent progress in the development of vision models that in- volve some amount of language data, including object cate- gorization [38, 54] and image captioning [19, 45, 49]. Un- like object categorization, which pre-speciﬁes a ﬁxed set of class labels to which all images must project, our systems use open-ended, referential language. Similarly to other re- cent works in image captioning [29, 32, 52, 43, 27, 26, 51], instead of captioning a single image (or entity therein), in isolation, our systems learn how to communicate across di- verse communications contexts. Reference games In our work we use reference games [20] in order to operationalize the demand to be relevant in context. The basic arrangement of such games can be traced back to the language games explored by Wittgenstein [47] and Lewis [25]. For decades, such games have been a valuable tool in cognitive science to quantitatively measure inferences about language use and the behavioral conse- quences of those inferences [36, 23, 5, 42]. Recently, these approaches have also been adopted as a benchmark for dis- criminative or context-aware NLP [33, 2, 40, 44, 31, 6, 24]. Rational speech acts framework Our models draw on recent formalization of human language use in the Ra- tional Speech Acts (RSA) framework [13]. At the core of RSA is the Gricean proposal [15] that speakers are agents who select utterances that are parsimonious yet informative about the state of the world. RSA formalizes this notion of informativity as the expected reduction in the uncertainty of an (internally simulated) listener, as our pragmatic speaker does. The literal listener in RSA uses semantics that mea- sure compatibility between an utterance and a situation, as our baseline listener does. Previous work has shown that RSA models account for context sensitivity in speakers and listeners [14, 31, 53, 11]. Our results add evidence for the effectiveness of this approach in complex domains. 9. Conclusion In this paper, we explored models of natural language grounded in the shape of common objects. The geome- try and topology of objects can be complex and the lan- guage we have for referring to them is correspondingly ab- stract and compositional. This makes the shape of objects an ideal domain for exploring grounded language learning, while making language an especially intriguing source of evidence for shape variations. We introduced the Chairs-in- Context corpus of highly descriptive referring expressions for shapes in context. Using this data we explored a vari- ety of neural listener and speaker models, ﬁnding that the best variants exhibited strong performance. These models draw on both 2D and 3D object representations and appear to reﬂect human-like part decomposition, though they were never explicitly trained with object parts. Finally, we found that the learned models are surprisingly robust, transferring to real images and to new classes of objects. Future work will be required to understand the transfer abilities of", "meta": {"paper_id": "1905.02925v1", "chunk_index": 12, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_13", "text": "3D object representations and appear to reﬂect human-like part decomposition, though they were never explicitly trained with object parts. Finally, we found that the learned models are surprisingly robust, transferring to real images and to new classes of objects. Future work will be required to understand the transfer abilities of these models and how this depends on the compositional structure they have learned. Acknowledgements The authors wish to acknowledge the support of a Sony Stanford Graduate Fellowship, a NSF grant CHS-1528025, a Vannevar Bush Faculty Fellowship and gifts from Autodesk and Amazon Web Services for Ma- chine Learning Research. References [1] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. J. Guibas. Learning representations and generative models for 3d point clouds. Proceedings of the 35th International Conference on Machine Learning, 2018. 3, 11 [2] J. Andreas and D. Klein. Reasoning about pragmatics with neural listeners and speakers. CoRR, 2016. 8 [3] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. CoRR, abs/1607.06450, 2016. 11 [4] A. X. Chang, T. A. Funkhouser, L. J. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, J. Xiao, L. Yi, and F. Yu. Shapenet: An information-rich 3d model repository. CoRR, abs/1512.03012, 2015. 2, 11 [5] H. H. Clark and D. Wilkes-Gibbs. Referring as a collabora- tive process. Cognition, 22(1):1–39, 1986. 8 [6] R. Cohn-Gordon, N. Goodman, and C. Potts. Pragmatically informative image captioning with character-level reference. CoRR, abs/1804.05417, 2018. 8 [7] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 3 [8] A. Dubrovina, F. Xia, P. Achlioptas, M. Shalah, and G. J. Leonidas. Composite shape modeling via latent space fac- torization. CoRR, abs/1901.02968, 2019. 2 [9] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra- manan. Object detection with discriminatively trained part- based models. TPAMI, 2010. 2 [10] A. M. Fischler and E. A. Robert. The representation and matching of pictorial structures. IEEE Trans. on Computers., 1973. 2 [11] D. Fried, J. Andreas, and D. Klein. Uniﬁed pragmatic models for generating and following instructions. CoRR, abs/1711.04987, 2017. 8 [12] E. Gibson, R. Futrell, J. Jara-Ettinger, K. Mahowald, L. Bergen, S. Ratnasingam, M. Gibson, S. T. Piantadosi, and B. R. Conway. Color naming across languages reﬂects color use. Proceedings of the National Academy of Sciences, 114(40):10785–10790, 2017. 2 [13] N. D. Goodman and M. C. Frank. Pragmatic language in- terpretation as probabilistic inference. Trends in Cognitive Sciences, 20(11):818 – 829, 2016. 6, 8 [14] C. Graf, J. Degen, R. X. D. Hawkins, and N. D. Goodman. Animal, dog, or dalmatian? level of abstraction in nomi- nal referring expressions. In Proceedings of the 38th Annual Conference of the Cognitive Science Society, 2016. 8 [15] H. P. Grice. Logic and conversation. In P. Cole and J. Mor- gan, editors, Syntax and Semantics, pages 43–58. Academic Press, New York, 1975. 8 [16] R. X. D. Hawkins. Conducting real-time multiplayer exper- iments on the web. Behavior Research Methods, 47(4):966– 976, 2015. 2 [17] S. Hochreiter and", "meta": {"paper_id": "1905.02925v1", "chunk_index": 13, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_14", "text": "[15] H. P. Grice. Logic and conversation. In P. Cole and J. Mor- gan, editors, Syntax and Semantics, pages 43–58. Academic Press, New York, 1975. 8 [16] R. X. D. Hawkins. Conducting real-time multiplayer exper- iments on the web. Behavior Research Methods, 47(4):966– 976, 2015. 2 [17] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. 3 [18] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015. 11 [19] A. Karpathy and L. Fei-Fei. Deep visual-semantic align- ments for generating image descriptions. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 3128–3137, 2015. 8, 13 [20] S. Kazemzadeh, V. Ordonez, M. Mark, and B. L. Tamara. Referitgame: Referring to objects in photographs of natural scenes. In EMNLP, 2014. 2, 8 [21] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. 12 [22] S. Kirby, M. Tamariz, H. Cornish, and K. Smith. Compres- sion and communication in the cultural evolution of linguis- tic structure. Cognition, 141:87–102, 2015. 2 [23] R. M. Krauss and S. Weinheimer. Changes in reference phrases as a function of frequency of usage in social inter- action: A preliminary study. Psychonomic Science, 1964. 8 [24] A. Lazaridou, K. M. Hermann, K. Tuyls, and S. Clark. Emer- gence of linguistic communication from referential games with symbolic and pixel input. CoRR, abs/1804.03984, 2018. 8 [25] D. Lewis. Convention: A philosophical study. Harvard Uni- versity Press, 1969. 8 [26] J. Lu, J. Yang, D. Batra, and D. Parikh. Neural baby talk. CVPR, 2018. 2, 8 [27] R. Luo and G. Shakhnarovich. Comprehension-guided refer- ring expressions. In Computer Vision and Pattern Recogni- tion (CVPR), volume 2, 2017. 2, 8 [28] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer non- linearities improve neural network acoustic models. In Pro- ceedings of the 30th International Conference on Machine Learning (ICML-13), 2013. 11 [29] J. Mao, J. Huang, A. Toshev, O. Camburu, A. Yuille, and M. Kevin. Generation and comprehension of unambiguous object descriptions. CoRR, abs/1511.02283, 2016. 8 [30] T. Miyato, D. M. Andrew, and G. Ian. Adversarial train- ing methods for semi-supervised text classiﬁcation. Interna- tional Conference on Learning, 2017. 11 [31] W. Monroe, R. X. Hawkins, N. D. Goodman, and C. Potts. Colors in context: A pragmatic neural model for grounded language understanding. CoRR, abs/1703.10186, 2017. 3, 8 [32] K. V. Nagaraja, I. V. Morariu, and D. S. Larry. Modeling context between objects for referring expression understand- ing. ECCV, 2016. 2, 8 [33] M. Paetzel, D. N. Racca, and D. DeVault. A multimodal corpus of rapid dialogue games. In Language Resources and Evaluation Conference (LREC), 2014. 8 [34] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. 6 [35] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the", "meta": {"paper_id": "1905.02925v1", "chunk_index": 14, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_15", "text": "method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA, 2002. Association for Computational Linguistics. 6 [35] J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language pro- cessing (EMNLP), pages 1532–1543, 2014. 3 [36] S. Rosenberg and B. D. Cohen. Speakers’ and listeners’ pro- cesses in a word-communication task. Science, 1964. 8 [37] S. Shen and H. Lee. Neural attention models for sequence classiﬁcation: Analysis and application to key term extrac- tion and dialogue act detection. CoRR, abs/1604.00077, 2016. 4 [38] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 3, 8 [39] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. J. Mach. Learn. Res., 15(1), 2014. 11 [40] J.-C. Su, C. Wu, H. Jiang, and S. Maji. Reasoning about ﬁne-grained attribute phrases using reference games. CoRR, abs/1708.08874, 2017. 8 [41] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015. 11 [42] K. van Deemter. Computational models of referring: a study in cognitive science. MIT Press, 2016. 8 [43] R. Vedanta, S. Bengio, K. Murphy, D. Parikh, and G. Chechik. Context-aware captions from context-agnostic supervision. CoRR, abs/1701.02870, 2017. 8 [44] R. Vedantam, S. Bengio, K. Murphy, D. Parikh, and G. Chechik. Context-aware captions from context-agnostic supervision. In Computer Vision and Pattern Recognition (CVPR), volume 3, 2017. 8 [45] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. CoRR, abs/1411.4555, 2015. 8 [46] R. J. Williams and D. Zipser. A learning algorithm for con- tinually running fully recurrent neural networks. Neural Comput., 1989. 6 [47] L. Wittgenstein. Philosophical investigations. Macmillan, 1953. 8 [48] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, L. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. S. Corrado, M. Hughes, and J. Dean. Google’s neural machine translation system: Bridg- ing the gap between human and machine translation. CoRR, abs/1609.08144, 2016. 6 [49] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhutdi- nov, R. Zemel, and Y. Bengio. Show, attend and tell: Neu- ral image caption generation with visual attention. CoRR, abs/1502.03044, 2016. 8 [50] L. Yi, H. Su, X. Guo, and L. J. Guibas. Syncspeccnn: Syn- chronized spectral CNN for 3d shape segmentation. CoRR, abs/1612.00606, 2016. 5 [51] L. Yu, Z. Lin, X. Shen, Y. Jimei, X. Lu, M. Bansal, and L. T. Berg. Mattnet: Modular attention network for referring ex- pression comprehension. CVPR, 2018. 2, 8 [52] L. Yu, P. Poirson, S. Yang, C. A. Berg, and L. T. Berg. Mod-", "meta": {"paper_id": "1905.02925v1", "chunk_index": 15, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_16", "text": "segmentation. CoRR, abs/1612.00606, 2016. 5 [51] L. Yu, Z. Lin, X. Shen, Y. Jimei, X. Lu, M. Bansal, and L. T. Berg. Mattnet: Modular attention network for referring ex- pression comprehension. CVPR, 2018. 2, 8 [52] L. Yu, P. Poirson, S. Yang, C. A. Berg, and L. T. Berg. Mod- eling context in referring expressions. ECCV, 2016. 2, 8 [53] L. Yu, H. Tan, M. Bansal, and T. L. Berg. A joint speaker- listener-reinforcer model for referring expressions. CoRR, abs/1612.09542, 2017. 8 [54] N. Zhang, J. Donahue, R. Girshick, and T. Darrell. Part- based r-cnns for ﬁne-grained category detection. In Eu- ropean conference on computer vision, pages 834–849. Springer, 2014. 8 [55] L. Zhu, Y. Chen, A. Yuille, and W. Freeman. Latent hierar- chical structural learning for object detection. CVPR, 2010. 2 A. Appendix A.1. CiC details To build the triplets comprising the communication con- texts of CiC, we exploited the latent (bottleneck-derived) vector space of a Point-Cloud based AutoEncoder (PC-AE) [1], trained with chair-only objects of ShapeNet [4]. Con- cretely, we used a PC-AE with small bottleneck (64D) to promote meaningful euclidean distances and after embed- ding all ∼7000 ShapeNet chairs in the resulting space, we computed their underlying 2-(euclidean)-nearest-neighbor graph. On this graph, we selected the 1K chairs with the highest in-degree to ‘seed’ the triplet generation. For each of the 1K (seed) chairs, we considered it together with its two nearest neighbors from the entire shape collection to form a Hard triplet. Also, we considered it together with the two chairs that were closest to it but which were also more distant from it than the median of all pairwise distances, to form an Easy triplet. The above procedure gives rise to 2000 communication contexts when target vs. distractor infor- mation is ignored. However, to counterbalance the dataset while annotating these contexts in AMT, we ensured that each chair of a context was considered as a distractor and as a target, and that each resulting combination was anno- tated by at least 4 humans. Last, we note that when build- ing the Hard triplets, we applied a manually tuned distance- threshold, to reject triplets that contained objects that were ‘too’ close: we found that about ∼3% of chairs had a geo- metric duplicate that could vary only wrt. its texture. A.2. Image and point-cloud pre-training For the listeners and speakers we trained a PC-AE un- der the Chamfer loss [1] with a 128D bottleneck and point clouds with 2048 points extracted from 3D CAD models, uniformly area-wise. We also ﬁne-tuned a VGG-16 pre- trained on ImageNet on a 8-way classiﬁcation, with 36,632 rendered images of textureless 3D CAD models, taken from a single view-point. Concretely, we used images of the 8 largest object classes of Shape-Net (car, airplane, vessel, sofa, chair, table, lamp, rifﬂe) and a uniformly random i.i.d. split of [90%, 5%, 5%] for train/test/val purposes. We ﬁne- tuned the network for 30 epochs. During the ﬁrst 15 epochs we optimized only the weights of the last (fc8) layer and during the last 15 epochs the weights of", "meta": {"paper_id": "1905.02925v1", "chunk_index": 16, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_17", "text": "sofa, chair, table, lamp, rifﬂe) and a uniformly random i.i.d. split of [90%, 5%, 5%] for train/test/val purposes. We ﬁne- tuned the network for 30 epochs. During the ﬁrst 15 epochs we optimized only the weights of the last (fc8) layer and during the last 15 epochs the weights of all layers. The at- tained test classiﬁcation accuracy was 96.9%. Last, to em- bed an image for the downstream listening/speaking tasks, we used the 4096D output activations of the penultimate (fc7) fully-connected layer. A.3. Pre-processing utterances We preprocessed the collected human utterances by i) lowercasing, ii) tokenizing by splitting off punctuation, iii) tokenizing by splitting superlative or comparative adjectives ending in -er, -est to their stem word, e.g. ‘thinner:’ → [‘thin’, ’er’] and, iv) replacing tokens that appear once or not at all in a training split with a special symbol mark- ing an unknown token (<UNK>). Furthermore, we ignored the utterances comprised by more than 33 tokens (99th per- centile) and those for which the human listener in the un- derlying trial did not guess correctly the target. Last, we concatenated listener and speaker utterances from the same trial (in their order of formulation) by adding in the end of each but the last utterance a special symbol marking a dia- logue: (<DIA>), e.g. [‘the’, ‘thin’, ‘chair’, <DIA>, ‘yes’]. A.4. Listeners details For the listeners we used a uni-directional LSTM cell with 100 hidden units, the output of which was passed into a 3-layer MLP with [100, 50, 3] neurons that predicted the triplet’s classiﬁcation logits. To the output of each hidden layer of the MLP, batch normalization [18] and a ReLU [28] non-linearity was applied. The listeners’ word- embedding was initialized with a 100D GloVe embedding pre-trained on the 6B Wikipedia 2014 corpus, and which was further ﬁne-tuned during training. The PC-AE (128D) and VGG (4096D) latent vectors, that encoded each object, were passed as input to the LSTM when only one geometric modality was used. When the two modalities used together, the PC-AE codes were concatenated with the output of the LSTM, and the concatenated result was processed by the ﬁnal MLP. In either case, we ﬁrst re-embedded these geo- metric codes (100D) with 2 separate/single FC-ReLU layers (referred as ‘projection’ layers in the Main Paper Section 3). An overview of the proposed listener reﬂecting the overall design choices is given in Fig.5. We used dropout with 0.5 keep probability before the ‘projection’ layers with a drop- out mask that was the same for the objects of a given triplet. Separate dropout with 0.5 keep probability was applied in all input vectors of the LSTM (i.e. on the language tokens or the grounding geometric codes). Last, the ground-truth indicator vectors of each triplet were label-smoothed [41] by assigning 0.933 probability mass to the target and 0.0333 to the distractors (i.e. smoothing of 0.9). Discussion Label smoothing yielded a mild performance boost of ∼2% across all ablated listener architectures, in accordance with previous work [41]. We note that we did not manage to improve the best attained accuracies by ap-", "meta": {"paper_id": "1905.02925v1", "chunk_index": 17, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_18", "text": "probability mass to the target and 0.0333 to the distractors (i.e. smoothing of 0.9). Discussion Label smoothing yielded a mild performance boost of ∼2% across all ablated listener architectures, in accordance with previous work [41]. We note that we did not manage to improve the best attained accuracies by ap- plying layer normalization [3] in the LSTM, or adversarial regularization [30] on the word-embedding. Dropout [39] was by far the most effective form of regularization for our listeners (∼[8-9]%), following by L2 weight-regularization of the projection layers (∼[2-3]%). Finally, using a sepa- rate MLP to process the PC-AE codes, was slightly better than feeding them directly in the LSTM (after the tokens of each utterance were processed). However, grounding the “five slats straight back top” image features GloVe embedding LSTM VGG five slats top straight back WORD ATTN distractors target point-cloud features MLP FC FC word features PC - AE L(oi, u) ∈R1 Figure 5: Baseline listener architecture combining 2D images, 3D point-clouds and linguistic utterances. XXXXXXXXXXXXXX Hyper Parameters Architecture Baseline Early-Context Combined-Interpretation Learning rate 0.0005 0.001 0.001 Label-smoothing 0.9 0.9 0.9 L2 regularization 0.3 0.05 0.09 LSTM-input-dropout 0.5 0.7 0.45 Table 6: Optimal hyper-parameters for ablated neural listener architectures, using both geometric modalities and word- attention and various degrees of context. Dropout numbers reﬂect the keep probability. LSTM with the PC-AE codes, and using the VGG codes in the end of the pipeline (either via pre-MLP concatenation or by feeding the latter in the LSTM) deteriorate signiﬁcantly all attained results. Context Ablations We ablated three architectures that used simultaneously images and point-clouds, word atten- tion and different degrees of context (See Main Paper Sec- tion 3). The optimal Hyper-Parameters (HP) for each archi- tecture are shown in Table 6. We did a grid search over the space of HP associated with each architecture separately. To circumvent the exponential growth of this space, we search it into two phases. First, we optimized the learn- ing rate (in the regime of [0.0001, 0.0005, 0.001, 0.002, 0.004, 0.005]) in conjunction with the drop-out (keep prob- ability) applied at the LSTM’s input, in the range [0.4-0.7] with increments of 0.05. Given the acquired optimal val- ues, we searched for the optimal L2 weight-regularization (in the range of [0.005, 0.01, 0.05, 0.1, 0.3, 0.9]) applied at the two projection layers, and label-smoothing ([0.8, 0.9, 1.0]). For these experiments we used a single random seed to control for the data splits with the object-generalization task. We note that for the Early-Context listener, using a single 1D convolutional layer to extract the grounding vec- tor of each object, appeared to produce better results than using a single FC layer (or deeper alternatives). This sin- gle convolutional layer we used, converted the input sig- nal [f(vj, vk)||g(vj, vk)||vi] ∈R100×3 to a R100×1 LSTM- grounding vector for each object vi, with an 8×3×1 kernel and stride 1. Training We trained the Baseline and the Combined- Interpretation for 500 epochs and the Early-Context for 350. This was sufﬁcient, as more training increased overﬁt- ting without improving the attained test/val accuracies. We", "meta": {"paper_id": "1905.02925v1", "chunk_index": 18, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_19", "text": "to a R100×1 LSTM- grounding vector for each object vi, with an 8×3×1 kernel and stride 1. Training We trained the Baseline and the Combined- Interpretation for 500 epochs and the Early-Context for 350. This was sufﬁcient, as more training increased overﬁt- ting without improving the attained test/val accuracies. We halved the learning every 50 epochs, if the validation error was not improved in any of them. Namely, every 5 epochs we evaluated the model on the validation split in order to se- lect the epoch/weights with the best accuracy. Because the Combined-Interpretation is sensitive in the input order of the object codes, we randomly permute them during train- ing. We use the ADAM [21] (β1 = 0.9) optimizer for all experiments. A.5. Speaker details Image-based speaker To ﬁnd good model parameters for an image-based speaker, we considered a hyper-parameter search on a literal variant. Similarly, to what we did in the ablations of listener variants we conducted a two-stage grid search given a single random seed and the object general- ization task. At the ﬁrst stage, we searched models varying: a) the hidden neurons of the LSTM (100 or 200), b) the initial learning rate ([0.0005, 0.001, 0.003]), c) the drop- out keep probability applied on the word-embeddings ([0.8, 0.9, 1.0]) and d) the dropout keep probability applied at the LSTM’s output ([0.8, 0.9, 1.0]). The two best performing models were further optimized by considering L2-weight regularization applied at the FC-projection layer (with val- ues in [0, 0.005, 0.01]) and the dropout keep-probability applied before the FC-projection layer ([0.5, 0.7, 0.9 1.0]). The resulting optimal parameters are reported in Table 7. Point-cloud-based speaker For the point-based speaker, we did a similar but more constrained hyper-parameter search as we did for the image-based speaker, by also con- sidering its literal variant. Here, we ﬁxed the drop-out ap- plied the word-embeddings and to the LSTM’s output (0.8 and 0.9 keep-probability respectively) and ablated the re- maining hyper-parameters as we did for the image-based speaker. We found the same conﬁguration of parameters (Table 7) to be optimal for point-based models as well. Ex- ception to this was the the dropout applied to the PC-AE codes before the FC-projection (no dropout at all was best in this case). Also, the point-based speakers needed more training to converge than the image-based ones (maximally 400 epochs vs. 300). Model selection To do model selection for a training speaker, we used a pre-trained listener (with the same train/test/val splits) which evaluated the synthetic utterances produced by the speaker during training. To this purpose the speaker generated 1 utterance for each unique triplet in the validation set via greedy (arg-max) sampling every 10 epochs of training and the listener reported the accuracy of predicting the target given the synthetic utterance. In the end of training (300 epochs for image-based speakers vs. 400 for point-based ones), the epoch/model with the high- est accuracy was selected. Other details We initially used GloVe to provide our speakers pre-trained word embeddings, as in the listener, but found that it was sufﬁcient to train", "meta": {"paper_id": "1905.02925v1", "chunk_index": 19, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_20", "text": "utterance. In the end of training (300 epochs for image-based speakers vs. 400 for point-based ones), the epoch/model with the high- est accuracy was selected. Other details We initially used GloVe to provide our speakers pre-trained word embeddings, as in the listener, but found that it was sufﬁcient to train the word embedding from uniformly random initialized weights (we used the range [-0.1, 0.1]). We also initialized the bias terms of the linear word-encoding layer with the log probability of the frequency of each word in the training data [19], which pro- vided faster convergence. We train with SGD and ADAM (β1 = 0.9) and apply norm-wise gradient clipping with a cut-off threshold of 5.0. The training utterances have a max- imal length of 33 tokens (99th percentile of the dataset). For any speaker we sampled utterances of the maximum train- ing length. For the pragmatic speaker we sample and score 50 utterances per triplet at test time (following Eq. 1 of Main Paper). Point-cloud & image-based speaker In preliminary ex- periments, we attempted to incorporate both geometric modalities: point-clouds and images in a speaker net- work, similarly to what we did for the best-performing lis- tener. While, this resulted in a (literal) speaker model that could achieve higher neural-listener evaluation-accuracy than when either modality was used in isolation, we did not observe any improvement against the image-based speaker in AMT human-listener experiments. We attempted three ways of ‘mixing’ the two modalities in a speaker. Namely, for each object of a communication context: a) providing the LSTM with the concatenation of its projected VGG code and its projected PC-AE code, b) same as a) but instead of concatenation, using the sum op- erator, c) ﬁrst providing its PC-AE projected code followed at the next time step by its VGG one. We compared these approaches by using the optimal hyper-parameters for an image-based speaker and only vary the amount of dropout applied to the point-cloud before the projection layer ([1.0 0.8, 0.6] keep probability). In all cases, avoiding dropout was best. The ﬁnal results for a single random-seed and the object-generalization task are reported in Table 8. We note that while the optimal speaker that used two modalities performed slightly better than the image-based speaker, per neural-listener evaluation, it did not improve the attained performance in preliminary experiments with of human lis- teners in AMT. A.6. Further quantitative results A.6.1 Listeners: context incorporation In Table 9 we complement the results presented in the Main Paper at Table 1, by including two more sub-populations (’Negative’ and ’Split’). In Table 10, we repeat this study for listeners trained and tested on the language generaliza- tion task. ’Negative’ is a subpopulation of utterances that contain at least one word of negative content e.g. ’not’, ’but’ etc. and is comprised by ∼15.0% of all test utterances. ’Split’ is smaller subpopulation (∼3.2% of test data) that includes language the explicitly contrasts the target with the distractors e.g. ‘from the two that have thin legs, the one...’. We used an ad hoc set of search queries to ﬁnd such", "meta": {"paper_id": "1905.02925v1", "chunk_index": 20, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_21", "text": "and is comprised by ∼15.0% of all test utterances. ’Split’ is smaller subpopulation (∼3.2% of test data) that includes language the explicitly contrasts the target with the distractors e.g. ‘from the two that have thin legs, the one...’. We used an ad hoc set of search queries to ﬁnd such utter- ances among the test set and found that the Early-Context architecture does perform noticeably better on these utter- ances. However, given the low occurrence of such cases, the resulting effects were not signiﬁcant and we decided the gains of Early-Context architecture were not worth the in- crease in model complexity and rigidity with respect to con- LSTM Size Learning rate L2-reg. Word-Dropout Image-Dropout LSTM-out Dropout 200 0.003 0.005 0.8 0.5 0.9 Table 7: Optimal hyper parameters for literal image-based neural-speaker. The dropout numbers reﬂect keep probabilities and the Image-Dropout refers to the dropout applied at the VGG-image codes, before the FC-projection layer. Approach Listener’s Accuracy Concat (100D) 65.1 ± 0.51% Concat (200D) 78.2 ± 0.95% Sum 77.9 ± 0.38% Serial 79.0 ± 0.32% Table 8: Ablating approaches for incorporating simultane- ously point-clouds with images in a literal neural-speakers. Sum: Summing the two latent codes for each object. Con- cat: Concatenating the codes. Serial: Feeding them one af- ter the other in the LSTM. Concatenation naturally doubles the input-dimensions of the LSTM (Concat 200D). To keep them the same as with all other experiments (100D) we also tested reducing the VGG/PC-AE projection layers to 50 di- mensions for each modality (Concat 100D). Results are av- erages of 5 samples of utterances for a ﬁxed test dataset. text size. A.6.2 Listeners: part-lesion We complement Table 3 of the Main Paper, with a similar study (Table 11) where we ablate our neural listeners with regards to their sensitivity in referential utterances based on object parts, when both geometric modalities are used. We have observed that the PC-AE attempts to reconstruct (de- code) noisy but complete models, even when the input is a partial, which could explain the gains seen in Table 11 compared to Table 3 when lesioning parts. A.6.3 Speakers: length penalty and listener awareness To ﬁnd the optimal length-penalty value (α, Main Pa- per Eq.1) for image-based literal and a context-unaware speaker variants, we used our best-performing listener to simultaneously score and evaluate the utterances produced by the speakers for different values of α (Fig. 6a). The best performing length penalty for a context-unaware speaker is 0.7, and for a literal 0.6. Given the optimal α values, for these models we show the effect of using different degrees of listener-awareness (β) in Fig. 6b. It is interesting to ob- serve that even the context-unaware speaker can generate utterances that an evaluating listener can ﬁnd them very dis- criminative, as long as is allows to rank them. In Fig. 7 we demonstrate the effect that the relative (training) size of the evaluating listener vs. the ’internal’ listener used by a pragmatic speaker has for the evaluating accuracy, for two values of β. In either case we observe a slow decline in evaluating accuracy", "meta": {"paper_id": "1905.02925v1", "chunk_index": 21, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_22", "text": "allows to rank them. In Fig. 7 we demonstrate the effect that the relative (training) size of the evaluating listener vs. the ’internal’ listener used by a pragmatic speaker has for the evaluating accuracy, for two values of β. In either case we observe a slow decline in evaluating accuracy as the training size for the evaluating listener increases (from 0.5 to 0.9) and conse- quently the training size for the ’internal’ listener decreases (from 0.5 to 0.1). A.6.4 Understanding out-of-class reference We complement the Table 5 with the standard-deviations of the underlying accuracies in Table 13. We also report simple statistics regarding the underlying transfer classes in Table 12. We note that the transfer learning accuracies ac- quired by listeners operating with both point-clouds and im- ages for these experiments were signiﬁcantly lower (∼7% on average). We hypothesize that this is due to the fact that our (chair-trained) listener models that utilize point-clouds, rely on a pre-trained single-class PC-AE, unlike the pre- trained VGG (image encoder) which was ﬁne-tuned with multiple ShapeNet classes. Also, for these experiments, [~1% ~7%] (depending on the transfer class) of the tokens were not in the chair-vocabulary, and we chose to ignore them i.e. treat them as white-space. Last, per Table 12 in all transfer classes the with-part population contains quite larger utterances than the without-part (9.3 vs. 5.5 on aver- age) and that even in the case of lamps, arguably the most dissimilar category from chairs, 20 + 37 = 57% of the col- lected utterances are in the known population. Architecture Subpopulations Overall Hard Easy Sup-Comp Negative Split Combined-Interpretation 75.9 ± 0.5% 67.4 ± 1.0% 83.8 ± 0.6% 74.4 ± 1.5% 77.3 ± 1.5% 65.8 ± 5.2% Early-Context 79.4 ± 0.8% 70.1 ± 1.3% 88.1 ± 0.6% 75.6 ± 2.2% 78.9 ± 1.4% 67.4 ± 3.6% Baseline 79.6 ± 0.8% 69.9 ± 1.3% 88.8 ± 0.4% 76.3 ± 1.3% 77.5 ± 1.2% 62.5 ± 3.7% Table 9: Comparing the effect of context inspection for listening on various (test) subpopulations of the object generalization task. The listeners use images, point-clouds and word-attention. Reporting averages of ﬁve random seeds controlling the split populations and the network’s initialization. Architecture Subpopulations Overall Hard Easy Sup-Comp Negative Split Combined-Interpretation 78.4 ± 0.2% 71.5 ± 0.6% 85.2 ± 0.3% 75.8 ± 0.9% 77.6 ± 0.8% 61.8 ± 3.0% Early-Context 84.4 ± 0.5% 78.5 ± 0.8% 90.2 ± 0.7% 80.9 ± 0.6% 82.6 ± 1.1% 68.9 ± 2.3% Baseline 83.7 ± 0.2% 77.0 ± 0.8% 90.3 ± 0.3% 80.8 ± 0.8% 80.5 ± 1.0% 64.6 ± 3.7% Table 10: Comparing the effect of context inspection for listening on various (test) subpopulations of the language general- ization task. The listeners use images, point-clouds and word-attention. Reporting averages of ﬁve random seeds controlling the split populations and the network’s initialization. Single Part Lesioned Single Part Present Mentioned Part 44.9% ± 1.2 67.2% ± 1.1 Random Part 68.9% ± 1.3 42.3% ± 1.3 Table 11: Evaluating the part-awareness of neural listeners by lesioning object parts. Results shown are for listeners using both point-clouds and", "meta": {"paper_id": "1905.02925v1", "chunk_index": 22, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_23", "text": "the split populations and the network’s initialization. Single Part Lesioned Single Part Present Mentioned Part 44.9% ± 1.2 67.2% ± 1.1 Random Part 68.9% ± 1.3 42.3% ± 1.3 Table 11: Evaluating the part-awareness of neural listeners by lesioning object parts. Results shown are for listeners using both point-clouds and images, with average accuracy of 78.8% when intact objects are used. Population Class entire with part without part chair 7.1 8.0 (77%) 4.7 (21%) bed 6.4 7.0 (26%) 5.3 (48%) lamp 7.3 11.0 (20%) 5.9 (37%) sofa 10.1 11.0 (72%) 5.9 (15%) table 6.6 8.0 (40%) 4.9 (42%) average 7.6 9.3 (39.5%) 5.5 (35.5%) Table 12: Average length of utterances for various trans- fer classes (complementing Table 5, Main Paper). Between parentheses is reported the percentage of the entire popu- lation that is captured by its speciﬁc sub-population. The average (last row) is wrt. the transfer classes only; the chair- category is displayed for reference. 0.0 0.2 0.4 0.6 0.8 1.0 α 62 64 66 68 70 72 74 76 78 Listener’s Accuracy Literal Context Unaware (a) Effect of the length-penalty. 0.0 0.2 0.4 0.6 0.8 1.0 β 65 70 75 80 85 90 95 100 Listener’s Accuracy Literal Context Unaware (b) Effect of increasing listener’s opinion (β). Figure 6: Left: Measuring the effect of using different length-penalty (α) values to select the top-1 scoring utterance for context-unaware and pragmatic speakers for contexts of the object generalization validation split (left). Right, measuring the the effect of various β-values used in turning the context-unaware and literal speakers (β = 0.0) to pragmatic speakers, under the optimal α of the left ﬁgure. In both plots, the y-axis reﬂects the performance of a listener who is used to rank and evaluate the utterances. Averages are with respect to 5 random seeds controlling the data splits and the initializations of the neural-networks. 0.5 0.6 0.7 0.8 0.9 0.5 0.6 0.7 0.8 0.9 Evaluating-listener |data|-fraction 74 76 78 80 82 84 86 Listener’s Accuracy Literal Context Unaware (a) Speakers using a modest β = 0.5 value. 0.5 0.6 0.7 0.8 0.9 0.5 0.6 0.7 0.8 0.9 Evaluating-listener |data|-fraction 84 86 88 90 92 Listener’s Accuracy Literal Context Unaware (b) Speakers using the most aggressive β = 1.0 value. Figure 7: Effect of partitioning the training data for the evaluating and ‘internal’ listeners. Here, we turn context-unaware and literal speakers into pragmatic ones under two β values. The x-axis shows the fraction (f) of the training data that was used to train the evaluating listener (the remaining 100 −f% is used to train the internal listener) of the resulting pragmatic speaker. On the y-axis we display the performance of the evaluating listener for the top-scoring model-generated utterance. XXXXXXXXXX Population Class bed chair lamp sofa table entire 56.4 ±2.0% 77.4 ±0.9% 50.1 ±1.3% 53.6 ±2.0% 63.7 ±1.2% known 55.8 ±1.5% 77.8 ±0.8% 51.9 ±1.8% 55.0 ±2.0% 65.5 ±0.9% with part 63.8 ±4.2% 77.0 ±0.8% 60.3 ±4.4% 55.1 ±2.5% 68.3 ±2.6% without part 51.5 ±3.0% 80.5 ±1.2% 47.1 ±2.8% 54.7 ±5.5% 62.7 ±0.9% Table 13: Transfer-learning of neural listeners in", "meta": {"paper_id": "1905.02925v1", "chunk_index": 23, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_24", "text": "77.4 ±0.9% 50.1 ±1.3% 53.6 ±2.0% 63.7 ±1.2% known 55.8 ±1.5% 77.8 ±0.8% 51.9 ±1.8% 55.0 ±2.0% 65.5 ±0.9% with part 63.8 ±4.2% 77.0 ±0.8% 60.3 ±4.4% 55.1 ±2.5% 68.3 ±2.6% without part 51.5 ±3.0% 80.5 ±1.2% 47.1 ±2.8% 54.7 ±5.5% 62.7 ±0.9% Table 13: Transfer-learning of neural listeners in novel object classes: average accuracies with standard deviations (comple- menting Table 13, Main Paper). The sub-populations denote entire: all collected utterances, known: utterances containing only chair-training-vocabulary words, with-part: subset of known, with utterances containing at least one part-related word, without-part subset of known and complement of with-part. For reference the test-chair statistics are shown (ﬁrst row) but not included in the reported average (last row). Figure 8: Examples of attention weights on human utterances. The listener’s LSTM appears to learn attention weights that emphasize the more informative words disambiguating the referent. For these results the Baseline listener is used and the attention-scores are extracted when the target object is grounding the LSTM. 0.01 0.00 0.99 adirondack chair <DIA> flat arms human utterance listener scores 0.00 0.10 0.90 the chair has lines all up the back 0.29 0.02 0.69 fancy shape back with armrest back and seat have slats 0.01 0.07 0.92 listener scores listener scores listener scores 0.09 0.11 0.80 0.05 0.04 0.91 0.21 0.02 0.77 0.31 0.02 0.67 attention weight low high the chair with many horizontal openings in the back the one that almost has a solid back chair has triangles on back support has four vertical spaces in the back and multiple pieces for the seat distractors target distractors target human utterance human utterance human utterance Figure 9: Examples of lesioning all but the mentioned part. Here, we show the response of a Baseline listener tested with visual representations of entire objects (left column, three chairs) vs. its response when it receives only the visual features corresponding to the referred semantic-part (right column). The corresponding utterance is shown left-most of each row. In these examples the listener assigns higher conﬁdence to the actual target when the isolated parts are considered instead of the entire objects, implying that further performance gains can occur with an explicit part-aware visual attention mechanism. solid, square backing <DIA> hole in back? <DIA> no distractors target Human Utterance 0.48 listener scores 0.01 0.51 0.32 0.08 0.60 sleek rounded arms, expensive 0.30 listener scores 0.11 0.59 0.14 0.05 0.81 0.04 listener scores 0.84 0.12 0.07 0.30 0.63 the seat of the chair has a curve the one with the fattest legs 0.38 listener scores 0.43 0.19 0.07 0.13 0.80 distractors target Figure 10: Pragmatic vs. literal speakers for two modalities. More examples of pragmatic vs. literal generations in Hard contexts. Tor-row includes examples from image-based speakers. Bottom-row from point-based ones. the one with the thick-est legs knobby legs pragmatic speaker distractors target literal speaker distractors target distractors target square arms with the tall-est back and seat no arm rests the one with high-est back image-based speakers thin-est seat most square back pragmatic speaker distractors target literal speaker distractors target distractors target thick-est legs square rack at", "meta": {"paper_id": "1905.02925v1", "chunk_index": 24, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_25", "text": "knobby legs pragmatic speaker distractors target literal speaker distractors target distractors target square arms with the tall-est back and seat no arm rests the one with high-est back image-based speakers thin-est seat most square back pragmatic speaker distractors target literal speaker distractors target distractors target thick-est legs square rack at bottom of chair tall-est back has arms point-cloud based speakers (a) Model generations with real images. The top-scoring utterance of a pragmatic model is displayed under each context. no arms queen ann legs gap between the back and seat circular arm rests the only one with a slotted back two legs connected office chair with wheels four legs, tallest back wide seat with solid back distractors target distractors target distractors target (b) Human-utterance comprehension with unseen object classes. The human utterance is color-coded according to the attention placed by a chair-trained listener who also evaluates the object-utterance compatibility (scores shown under its context). the bed is square shaped and the headboard is the shape of a rectangle the bed has a fancy metal headboard and two pillows three legged lamp short square table with four square legs very narrow and tall rectangular table with four tapered legs the target has three sections for the back this sofa has rounded back and corners distractors target 0.04 0.06 0.90 listener scores 0.02 0.04 0.94 listener scores 0.06 0.07 0.87 listener scores irregular cube on two legs 0.01 0.00 0.99 listener scores distractors target listener scores listener scores listener scores listener scores 0.01 0.06 0.93 0.06 0.02 0.92 0.01 0.19 0.80 0.02 0.01 0.97 Figure 12: Effect of context on production: Synthetic utterances generated by a literal and pragmatic image-based speaker. The top and bottom rows show utterances produced for the same target in a Easy and Hard context, respectively. The Baseline (with point-clouds and images and attention) listener is used to predict the target and its conﬁdence is displayed above each utterance. While both speaker models produce similarly effective utterances in Easy contexts, the literal speaker fails to produce effective utterances in Hard contexts. looks like a recliner pragmatic 0.09 0.01 0.90 0.01 0.00 0.99 0.61 0.03 0.36 0.06 0.07 0.87 target literal listener scores listener scores square chair with cushion downward sloping front of the arm rest the one with the thin est legs four legs 0.03 0.18 0.79 0.01 0.01 0.98 0.03 0.72 0.25 0.09 0.05 0.86 tallest back the one with the thickest legs the chair with the curved back holes in back 0.01 0.01 0.98 0.00 0.00 1.00 0.80 0.04 0.16 0.28 0.27 0.45 the chair with the vertical bars on the back five bars on back, skinny top the one with the thick est legs office chair 0.00 0.01 0.99 0.00 0.01 0.99 0.39 0.27 0.34 0.01 0.02 0.97 office chair no arm rest the one with the most square back target easy context hard context target target pragmatic literal listener scores listener scores (a) Neural-listener failure cases. Our top-performing listener model appears to struggle to interpret referential language that relies on metaphors, precisely counting parts, or (to a", "meta": {"paper_id": "1905.02925v1", "chunk_index": 25, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_26", "text": "office chair no arm rest the one with the most square back target easy context hard context target target pragmatic literal listener scores listener scores (a) Neural-listener failure cases. Our top-performing listener model appears to struggle to interpret referential language that relies on metaphors, precisely counting parts, or (to a less degree) negations. All examples are drawn from the test set and were correctly classiﬁed by human listeners in the original task. mickey mouse ears six bars on back open arms, no rails on back metaphors counting negation if it were a man’s suit it would have tails distractors target distractors target 0.63 0.02 0.35 0.57 0.13 0.30 0.39 0.27 0.34 0.48 0.13 0.39 listener scores human utterance listener scores human utterance (b) Neural-speaker failure cases. Sometimes even the pragmatic speaker produces insufﬁciently speciﬁc utterances that mention only undiagnostic features, or produces utterances that are literally false of the target (e.g. there technically is a hole in the back) while still succeeding in distinguishing the objects. not specific enough the one with the thicker cushion pragmatic 0.32 0.21 0.47 0.22 0.48 0.30 literal listener scores listener scores the one with the square base literally inaccurate but relatively true solid 0.05 0.09 0.86 0.02 0.06 0.92 no holes in back distractors target distractors target A.7. Miscellaneous Easy word ofﬁce sofa regular folding wooden stool wheels metal normal rocking pmi -1.70 -0.94 -0.88 -0.84 -0.83 -0.79 -0.78 -0.71 -0.67 -0.66 Hard word alike identical thickness texture darker skinnier thicker perfect similar larger pmi 0.69 0.67 0.67 0.66 0.65 0.64 0.63 0.62 0.62 0.61 Table 14: Most distinctive words in each context type ac- cording to point-wise mutual information (excluding tokens that appeared fewer than 30 times in the dataset). Lower numbers are more distinctive of Easy and higher numbers are more distinctive of Hard. Each game consisted of 69 trials (unique triplets) and participants swapped speaker and listener roles with the conclusion of each trial. The game’s interface is depicted in Figure 14. Participants were allowed to play multiple games, but most participants in our dataset played exactly one game (81% of participants). The most distinctive words in each triplet type (as measured by point-wise mutual in- formation) are shown in Table 14). You are the speaker. You the back rest and the bottom parts look the same You top part of the back is wider than in the seat Partner ok, got it send Figure 14: Reference game interface. Communication was natural without any system constraints being imposed. 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Training-data fraction 66 68 70 72 74 76 78 80 Listener’s Accuracy Figure 15: Listener’s accuracy for different sizes of training data, under the object generalization task. The original split includes [80%, 10%, 10%] for training/test/val purposes, thus the maximum size of training data is 0.8 of the entire dataset corresponding to the value (fraction) 1.0 in the x- axis. The listener model uses the Baseline architecture with word attention, images and point-clouds and its accuracy is measured on the original (10%) test split. Results", "meta": {"paper_id": "1905.02925v1", "chunk_index": 26, "filename": "1905.02925v1.pdf", "chunk_length": 512}}
{"id": "1905.02925v1_chunk_27", "text": "training/test/val purposes, thus the maximum size of training data is 0.8 of the entire dataset corresponding to the value (fraction) 1.0 in the x- axis. The listener model uses the Baseline architecture with word attention, images and point-clouds and its accuracy is measured on the original (10%) test split. Results are av- erages of 5 random seeds controlling the original data split and the neural-net’s initialization.", "meta": {"paper_id": "1905.02925v1", "chunk_index": 27, "filename": "1905.02925v1.pdf", "chunk_length": 66}}
{"id": "1906.00579v3_chunk_0", "text": "LISTENING WHILE SPEAKING AND VISUALIZING: IMPROVING ASR THROUGH MULTIMODAL CHAIN Johanes Effendi1,2, Andros Tjandra1, Sakriani Sakti1,2, Satoshi Nakamura1,2 1Nara Institute of Science and Technology, Japan 2RIKEN, Center for Advanced Intelligence Project AIP, Japan ABSTRACT Previously, a machine speech chain, which is based on sequence-to-sequence deep learning, was proposed to mimic speech perception and production behavior. Such chains sep- arately processed listening and speaking by automatic speech recognition (ASR) and text-to-speech synthesis (TTS) and simultaneously enabled them to teach each other in semi- supervised learning when they received unpaired data. Un- fortunately, this speech chain study is limited to speech and textual modalities. In fact, natural communication is actually multimodal and involves both auditory and visual sensory systems. Although the said speech chain reduces the require- ment of having a full amount of paired data, in this case we still need a large amount of unpaired data. In this research, we take a further step and construct a multimodal chain and design a closely knit chain architecture that combines ASR, TTS, image captioning, and image production models into a single framework. The framework allows the training of each component without requiring a large number of parallel multimodal data. Our experimental results also show that an ASR can be further trained without speech and text data and cross-modal data augmentation remains possible through our proposed chain, which improves the ASR performance. Index Terms— speech recognition, semi-supervised, multimodal 1. INTRODUCTION Researchers have been working in speech recognition tech- nology for many decades. State-of-the-art ASR systems are currently based on end-to-end deep learning frameworks. Traditionally, they are usually trained by applying supervised learning techniques that rely on the availability of speech data and corresponding transcriptions. To improve the per- formance in the presence of unexpected acoustic variability, we usually collect more data to train more detailed models. Although some systems have successfully reached parity with humans [1, 2], such a learning style can only be perfectly ﬁt for recognizing the speech of about 10-20 of the world’s most common languages. For many others, the problem is the size of the required speech and corresponding transcriptions are usually unavailable. Recently, approaches that utilize learning from source-to- target and vice-versa as well as feedback links have gained attention and provide the possibility of training models with unpaired datasets. He et al. [3] and Cheng et al. [4], re- cently published work that addressed a mechanism called dual learning in neural machine translation (NMT). Their system has a dual task: source-to-target language translation (primal) versus target-to-source language translation (dual). It can leverage monolingual data to improve neural machine translation. In image processing, several methods have also been proposed to achieve unsupervised joint distribution matching without any paired data, including DiscoGAN [5], CycleGAN [6], and DualGAN [7]. The framework provides learning to translate an image from a source domain to a target domain without paired examples based on a cycle-consistent adversarial network. Implementation on voice conversion applications has also been investigated [8]. However, most only work with the same domain between the source and the target. In speech-language processing, the", "meta": {"paper_id": "1906.00579v3", "chunk_index": 0, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_1", "text": "learning to translate an image from a source domain to a target domain without paired examples based on a cycle-consistent adversarial network. Implementation on voice conversion applications has also been investigated [8]. However, most only work with the same domain between the source and the target. In speech-language processing, the speech chain frame- work [9, 10, 11] was proposed to integrate human speech perception and production behaviors that utilize the primal model (ASR) that transcribes a text, given the speech versus the dual model (TTS) that synthesizes the speech given the text. Perhaps this is the ﬁrst framework that was constructed on a different domain (speech versus text). The approach pro- vides freedom from needing a large amount of speech-text paired data and possibilities to improve ASR performance in semi-supervised learning by allowing ASR and TTS to teach each other, given only text or only speech data. Unfortu- nately, although this approach reduces the requirement of a large amount of paired data, we still need a large amount of unpaired data. Furthermore, this study is limited to speech and textual modalities. In fact, natural communication is ac- tually multimodal that involves both auditory and visual sen- sory systems. In this research, we constructed the ﬁrst framework that accommodates triangle modality (speech, text, and image) and addressed the problems of speech-to-text, text-to-speech, text-to-image, and image-to-text. Our new framework mim- ics the mechanism of the entire human communication system with auditory and visual sensors. Similar to a machine speech chain, it allows each component to be trained without need- arXiv:1906.00579v3 [cs.CL] 14 Nov 2019 ASR TTS IC IR ŷ = (caption) ŷ = (caption) ẑ = z = x = x̂ = ASR TTS ŷ = \"text\" ŷ = \"text\" y = \"text\" x̂ = x = x̂ = (a) (b) Fig. 1. Architecture: (a) speech chain framework [9], and (b) our proposed multimodal chain mechanism. ing a large number of parallel multimodal data. In addition to the above advantages, through closely knit chain architec- ture that combines ASR, TTS, IC, and IR models into a single framework, it further frees us from needing a large amount of unpaired data. For example, speciﬁcally to ASR tasks, even when no more speech and text data are available, it is still possible to improve ASR by cross-modal data augmentation through our proposed chain. 2. MULTIMODAL CHAIN FRAMEWORK Figure 1 illustrates (a) the original speech chain framework [9] and (b) our proposed multimodal chain mechanism. In this extension, we included image captioning (IC) and im- age retrieval (IR) models to incorporate visual modality into the chain. The framework consists of dual loop mechanisms between the speech and visual chains that involve quadruple learning components: ASR, TTS, IC, and IR. In the speech chain, sequence-to-sequence ASR and sequence-to-sequence TTS models are jointly trained in a loop connection, and in the visual chain, neural image captioning and neural embedding- based image retrieval models are also jointly trained in a loop connection. Both chains (speech and visual components) are allowed to collaborate by text modality. The sequence-to-sequence model in closed-loop", "meta": {"paper_id": "1906.00579v3", "chunk_index": 1, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_2", "text": "TTS models are jointly trained in a loop connection, and in the visual chain, neural image captioning and neural embedding- based image retrieval models are also jointly trained in a loop connection. Both chains (speech and visual components) are allowed to collaborate by text modality. The sequence-to-sequence model in closed-loop architec- ture allows us to train our entire model in a semi-supervised fashion by concatenating both the labeled and unlabeled data. To further clarify the learning process, we describe the mech- anism based on the availability condition of the training data: 1. Paired speech(x)-text(y)-image(z) data exist: sep- arately train ASR, TTS, IC and IR (supervised learning) Given complete multimodal dataset DPxyz, we can set-up speech utterances x and corresponding text tran- scriptions y as dataset DPxyz xy to separately train both the ASR and TTS models in a supervised manner. ASR losses LP ASR and LP T T S are calculated directly with teacher-forcing, where the ground truth for each time step is used as input when decoding. We can also set-up images z and captions y as dataset DPxyz yz to separately train the IC and IR models with supervised learning. The IC model is trained with teacher-forcing on reference caption y, and the IR model is trained with pairwise rank loss [12] on reference image z and its contrastive sample. 2. Unpaired speech (x), text (y), images (z) data ex- ist: jointly Train ASR&TTS in the speech chain and IC&IR in the visual chain (unsupervised learning) In this case, although speech, text, and image data are available, they are unpaired. (a) Only using speech data: unrolled process ASR→TTS in a speech chain Here we only use speech utterances x of dataset DUxyz, and ASR generates transcription ˆy for TTS to reconstruct. Reconstructed transcriptions ˆx calculate loss LU T T S between x and ˆx and up- date the model parameter. (b) Only using image data: unrolled process IC→IR in a visual chain Using only image z in dataset DUxyz, image cap- tions ˆy are generated with the IC model. These captions are then used by the IR model to update its multimodal space using pairwise rank loss, which resulted in loss LU IR. (c) Only using text data: unrolled process TTS→ASR in the speech chain and IR→IC in the visual chain Given only the text in dataset DUxyz, TTS gener- ates speech utterance ˆx for the ASR, which then reconstructs the speech utterances into text ˆy in which reconstruction loss LU ASR between y and ˆy can be calculated. On the other hand, image cap- tions y retrieve images ˆz, which are reconstructed into text ˆy using the IC model in which losses LU IC are calculated between y and ˆy. 3. Single data (either speech (x), text (y), or images (z)) exist: train ASR & TTS jointly in the speech chain and IC & IR in the visual chain (unsupervised learn- ing) In this case, only a single modality data (either speech, text, or image) is available, and the others are empty. (a) Only text data exist: train the", "meta": {"paper_id": "1906.00579v3", "chunk_index": 2, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_3", "text": "images (z)) exist: train ASR & TTS jointly in the speech chain and IC & IR in the visual chain (unsupervised learn- ing) In this case, only a single modality data (either speech, text, or image) is available, and the others are empty. (a) Only text data exist: train the speech and vi- sual chains, as in 2(c) If only text data are available in dataset DUy, we can separately perform unrolled process TTS→ASR in the speech chain and IR→IC in the visual chain (b) Only speech data exist: speech chain →visual chain If only speech data are available in dataset DUx, ﬁrst we perform unrolled process ASR→TTS in the speech chain (See 2(a)). The generated text transcription ˆy is then used to performs IR→IC in the visual chain (See 2(c)). (c) Only image data exist: visual chain →speech chain If only image data are available in dataset DUz, ﬁrst we perform unrolled process IC→IR in the visual chain (See 2(b)). The generated image cap- tion ˆy is then used to perform unrolled process TTS→ASR in the speech chain (See 2(c)). Here our main concern is the last point (3(c)). We are interested to learn whether in the situation where only image data exist (no more speech and text data are available) we can still improve the ASR performance through a learning process from a visual chain to a speech chain by leveraging cross- modal data augmentation. We combine all of the losses and update both the ASR and TTS models as well as the IC and IR models: Lsc =αASRLP ASR + αT T SLP T T S+ βASRLU ASR + βT T SLU T T S (1) θASR =Optim(θASR, ▽θASRL) (2) θT T S =Optim(θT T S, ▽θT T S L) (3) Lvc = γICLP IC + γIRLP IR + δICLU IC + δIRLU IR (4) θIC = Optim(θIC, ▽θIC L) (5) θIR = Optim(θIR, ▽θIRL) (6) which results in losses Lsc and Lvc for the speech and visual chains. Parameters α, β, γ, and δ are hyper-parameters for scaling the loss between the supervised (paired) and unsuper- vised (unpaired) losses in each chain. 3. MULTIMODAL CHAIN COMPONENTS In this section, we brieﬂy describe all of the components in- side the multimodal chain framework. 3.1. Sequence-to-sequence ASR We use the sequence-to-sequence ASR model with attention, whose architecture resembles a previous one used in [9] that is also based on LAS framework [13]. It directly models con- ditional probability P(y|x) of transcription y given speech feature x. For the speech feature, the MFCC or the mel- spectogram are usually encoded by a bidirectional LSTM en- coder. The hidden representations are then attended by a LSTM or GRU decoder that decodes a sequence of charac- ters or phonemes. 3.2. Sequence-to-sequence TTS A sequence-to-sequence TTS is a parametric TTS that gen- erates sequence of speech feature x from transcription y. We also used similar architecture as a previous one used in [9] which is based on a slight modiﬁcation to Tacotron [14]. Tacotron produces a mel-spectogram given the text utter- ances, and is", "meta": {"paper_id": "1906.00579v3", "chunk_index": 3, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_4", "text": "sequence-to-sequence TTS is a parametric TTS that gen- erates sequence of speech feature x from transcription y. We also used similar architecture as a previous one used in [9] which is based on a slight modiﬁcation to Tacotron [14]. Tacotron produces a mel-spectogram given the text utter- ances, and is further transformed into a linear spectogram so that the speech signal can be reconstructed using the Grifﬁn- Lim algorithm [15]. 3.3. Image Captioning Image encoder: ResNet h0 <s> ŷ0 h1 y0 ŷ1 ... ht yn-1 </s> Text decoder: LSTM MLP attention z [r0..rn] Fig. 2. Caption model An image captioning model receives image z and pro- duces caption prediction ˆy = [ˆy0..ˆyn] by minimizing softmax cross entropy loss against original caption y = [y0..yn]. We utilized similar architecture as [16], where image z are encoded through a series of convolutional neural network encimg resulting in a high level feature representation within a certain number of region encimg(z) = [r0..rn] that rep- resent parts of the image. During decoding the [ˆy0..ˆyn], linear attention grounds each decoded word into corre- lated image region rn by calculating alignment probability at(encimg(z)) = Align([r0..rn], ht) over decoder states ht. Unlike Xu et al’s model, we use ResNet [17] instead of VGG [18]. 3.4. Image Retrieval Neural IR models [19, 20, 21, 22] are implemented by re- alizing a multimodal embedding between image z and its caption y. Image embedding ze is usually extracted from a series of pretrained convolutional neural networks followed by linear transformation. Recurrent neural network encoder are used to generate caption embedding ye, which is trans- formed from the last encoder state hn that encodes sequence of words [y0..yn]. These two embedding representations are then trained using pairwise rank loss function LIR to join them into a unique multimodal embedding space. As shown in Eq. 7, this procedure reduces mean squared distance d between each image embedding ze with related LIR Image encoder: ResNet LSTM y0 h0 LSTM LSTM y1 h1 hn ... yn Text encoder bidirectional LSTM ye ze Fig. 3. Retrieval model caption represented with text embedding ye, and increases its distance with unrelated caption ˆye. Margin M is used to dis- tance the already similar pairs, providing space to optimize the hard-positive examples: LIR = X |ye| X |ˆze| max{0, M −d(ye, ze) + d(ye, ˆze)}+ X |ze| X |ˆye| max{0, M −d(ze, ye) + d(ze, ˆye)} (7) 4. EXPERIMENTAL SET-UP 4.1. Corpus Dataset Table 1. Training data partition for Flickr30k with three con- ditions: (1) available paired data are denoted as ⃝, (2) avail- able data but unpaired are denoted as ▲, and unavailable data are denoted as ×. Dataset Speech Text Image # Data* Training x y z Type DPxyz ⃝ ⃝ ⃝ 2000 1 (paired) DUxyz ▲ ▲ ▲ 7000 2 (unpaired) DUx ▲ × × 10000 3 (unpaired) DUz × × ▲ 10000 3 (unpaired) We ran our experiment with the Flickr30k dataset [23] that has 31,014 photos of everyday activities, events, and scenes. Similar to other image captioning datasets, each image has ﬁve captions with a", "meta": {"paper_id": "1906.00579v3", "chunk_index": 4, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_5", "text": "▲ 7000 2 (unpaired) DUx ▲ × × 10000 3 (unpaired) DUz × × ▲ 10000 3 (unpaired) We ran our experiment with the Flickr30k dataset [23] that has 31,014 photos of everyday activities, events, and scenes. Similar to other image captioning datasets, each image has ﬁve captions with a vocabulary of 18k words. However, since we use this dataset not only for captioning but also for re- trieval, we need to maintain a balance between source and target. For image captioning, we use one caption per im- age to make the learning target consistent by avoiding one-to- many confusion. Conversely, in image retrieval we used all ﬁve captions because the learning target is already consistent. To train the speech counterpart of our proposed architecture, we generated speech from the Flickr30k captions using single speaker Google TTS. The transcription resulted in 145k ut- terances, all with the same speaker, with the total duration of *Different modality has different unit (speech=utterance, text=sentence, image=picture). In paired data, one image is at least associated with one text sentence and one speech utterance. 178.8 hours. Each caption typically consists of 12.32 words in the form of a sentence. In our experiment, we used the dev and test set of Flickr30k for validation and testing, respectively. Similar to the training data, Google TTS is also used to generate the corresponding speech (single speaker voice) of these two sets. To show the capability of our model for semi-supervised learning, we formulated the training part of the dataset into four parts. For more details, see the speciﬁcations in Section 2. The selection of which data belongs to which part was done randomly since all the training data are shufﬂed before being split into four parts. The ﬁrst part, DPxyz, was used to supervisedly train each model (Type 1), because all the data are paired (⃝). Next, the DUxyz dataset, which has all the modalities available but unpaired (▲) was used to separately train the speech and visual chains (Type 2). DUx and DUz are assumed to be a single modality corpus (×), which only has speech and images without any transcription or captioning. By decoding the DUx dataset into image captions, and DUz into utterance transcriptions, we can use the generated data to further semi-supervisedly improve each model (Type 3). Without our proposed architecture, these monomodal data DUx and DUz cannot be used because their modality is com- pletely unrelated to the chain in the other modality pair. As mentioned above, our main concern is to know whether it is still possible to improve ASR performance in the Type 3 sit- uation, where only image data is available on DUz. 4.2. Model Details We used a standard sequence-to-sequence model with an MLP attention module for ASR as mentioned in Section 3.1. For the TTS, we used Tacotron 3.2. The features and archi- tecture of the ASR and TTS models are similar with Tjandra et al. (2017) single speaker speech chain models [9]. We used an Adam optimizer with a learning rate of 1e-3 for the ASR model, 2.5e-4 for", "meta": {"paper_id": "1906.00579v3", "chunk_index": 5, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_6", "text": "Section 3.1. For the TTS, we used Tacotron 3.2. The features and archi- tecture of the ASR and TTS models are similar with Tjandra et al. (2017) single speaker speech chain models [9]. We used an Adam optimizer with a learning rate of 1e-3 for the ASR model, 2.5e-4 for the TTS model, and 1e-4 for the IC model. For the IR model, we used a stochastic gradient descent with a 0.1 learning rate. In the visual chain, we implemented the IC and IR mod- els as previously described in Sections 3.3 and 3.4. For the convolutional part that extracts the image features, we used ResNet [17] as an image encoder in IC and IR. In the IC model, we removed the last two layers of the ResNet, which yields a 14x14 latent representation of the image region in which the decoder could attend to. Then, for the IR model, we removed the last layer, giving us a 2048-dimensional hidden representation that can be regarded as an image representa- tion. These representations are then linearly transformed into 300-dimensional image embeddings. On the other hand, we generated text embeddings using a single-layer bidirectional LSTM with 256 hidden sizes in each direction. We decoded the transcription in the speech chain using beam-search decoding with a size of three. Similarly, during the visual chain operation, the IC model produced its hypoth- esis using beam-search decoding of the same size. The granu- larity difference between each chain (i.e., char and word) was ﬁxed during training between the chains. To simulate sam- pling in the IR hypothesis, we randomly sampled one hypoth- esis from ﬁve candidates. 5. EXPERIMENT RESULTS 5.1. A Large Amount of Paired Data Exists - Topline Case Table 2. Our ASR and TTS performance in comparison with the existing published results Data ASR TTS CER(%) L2-norm2 Kim et al. [24] 11.08 - Tjandra et al. [25] 6.60 0.682 Ours 6.87 0.653 Table 3. Our IC and IR performance in comparison with the existing published results Data IC IR BLEU1 R@10↑ med r↓ Xu et al. (2015) [16] 67.00 - - Vilalta et al. (2017) [20] - 59.8 6 Ours 66.27 62.42 5 In this subsection, we assumed that a large amount of paired data exists. Therefore, we can train each model inde- pendently using supervised training. Here we compared the performance of all the system components with the existing published results on a well-known dataset. For the ASR and TTS tasks, we evaluated the performance of our models on the Wall Street Journal dataset [26], which is a natural mul- tispeaker speech corpus. Our settings for the training, devel- opment, and test sets are identical as the Kaldi s5 recipe [27]. We trained our model with the WSJ-SI284 data. Our valida- tion set was dev93, and our test set was eval92. For the IC and IR tasks, we evaluated the performance of our models on a full set of Flickr30k. Tables 2 and 3 show a comparable evaluation for the ASR- TTS and IC-IR tasks. As seen in the table, our ASR performs better", "meta": {"paper_id": "1906.00579v3", "chunk_index": 6, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_7", "text": "dev93, and our test set was eval92. For the IC and IR tasks, we evaluated the performance of our models on a full set of Flickr30k. Tables 2 and 3 show a comparable evaluation for the ASR- TTS and IC-IR tasks. As seen in the table, our ASR performs better than Kim et al. [24] and provides a similar performance to Tjandra et al. [10]. Our TTS model also performs on par with Tjandra et al. [10]. Our IC model also performed on par on the Flickr30k dataset with the work by Xu et al. by a 0.7 BLEU1 margin. Finally, we compared our IR model with Vilalta et al. (2017) who proposed a full network embedding model in which our model performed slightly better. These results reveal that in a fully supervised scenario, our model works as well as previously published papers. *We trained our baseline model with the only 2k paired data to simulate a real-condition where an only small amount of paired dataset is available to show that our chain can improve the initial model that was only trained with a small amount of data using semi-supervised learning fashion. Table 4. ASR and TTS performance using Multimodal Chain Data ASR TTS WER(%) L2-norm2 Baseline: ASR & TTS (Supervised learning - Type 1) DPxyz xy 2k* 81.31 0.874 Proposed: speech chain ASR→TTS and TTS→ASR (Semi-supervised learning - Type 2(a)&2(c)) +DUxyz xy 7k 10.60 0.714 Proposed: visual chain →speech chain Semi-supervised learning - Type 3(b)&3(a)) +DUz10k 7.97 0.645 Topline: ASR & TTS separately (Supervised learning - Full Data) DPxyz xy 29k 2.37 0.398 Table 5. IC and IR performance using Multimodal Chain Data IC IR BLEU1 R@10↑ med r↓ Baseline: IC & IR (Supervised learning - Type 1) DPxyz yz 2k* 33.91 26.88 34 Proposed: visual chain IC→IR and IR→IC (Semi-supervised learning - Type 2(b)&2(c)) +DUxyz yz 7k 42.11 28.14 31 Proposed: speech chain →visual chain Semi-supervised learning - Type 3(c)&3(a)) +DUx10k 43.08 28.44 30 Topline: IC & IR separately (Supervised learning - Full data) DPxyz yz 29k 66.27 62.42 5 5.2. Only a Small Amount of Paired Data Exists - Utilizing Multimodal Chain As explained in Sec. 2, in this section we demonstrate how our proposed multimodal chain mechanism deal with the data composition written in Table 4.1 where there are only 2k speech-text-image paired data, 7k speech-text-image un- paired data and 10k single modality data. Note that in this research, the idea is not to show that the multimodal chain can outperform a baseline that was only trained with a small dataset. Instead, we want to identify how much we can improve performance when only unpaired data are available or even how much more improvement is possi- ble when the required data are no longer available. We are interested to see how much we can improve the ASR perfor- mance when no more speech and text are available to train the model. Table 4 shows the ASR and TTS results from the scenar- ios in Section 2. First, we trained them on 2k paired data DPxyz xy as shown", "meta": {"paper_id": "1906.00579v3", "chunk_index": 7, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_8", "text": "to see how much we can improve the ASR perfor- mance when no more speech and text are available to train the model. Table 4 shows the ASR and TTS results from the scenar- ios in Section 2. First, we trained them on 2k paired data DPxyz xy as shown in the ﬁrst block using the supervised train- ing method. This initial model is then used in the next step as the speech chain component. We continued the training into the speech chain using DUxyz xy 7k data and achieved 10.60% WER and 0.714 L2-norm2. Finally, using the IC model that was trained semi-supervisedly through Type 2(a)&2(c), we decoded the image-only DUz dataset which enables it to be used in speech chain. By this way, we achieved about 2.6% WER improvement over the original speech chain [9] that was only trained using the speech and text datasets. This result proved that the cross-modal data augmentation from the im- age modality into this speech chain is correlated positively with model quality. Our proposed strategies makes improve- ment of ASR and TTS possible, even without any speech or text data, with the help of a visual chain. Table 5 shows the IC and IR results from similar scenar- ios with the improvement from the speech chain. First, we did training using paired 2k data and achieved the baseline score shown in the ﬁrst block. Next, we semi-supervisedly trained the IC and IR models in the visual chain mechanism, and produced over 8.2 BLEU1 improvement, 1.26 recall at 10 (R@10) improvement and 3 point improvement for the me- dian r metrics. Finally, in the third block we show that the visual chain can also be improved using speech data, by the help of speech chain. There was about 1 point improvement in terms of BLEU for IC (high is good) and median r for IR (low is good). This result also implies that using our pro- posed learning strategy, the IC and IR model can be improved even without image and text datasets available. Therefore, we showed that it also works not only from image-to-speech modality, but also reversely. 6. RELATED WORKS Human communication is multisensory and involves several communication channels, including auditory and visual chan- nels. Such multiple signals in different form are processed based on their characteristics, but later can be used for mu- tual augmentation. Moreover, the sensory inputs from several modalities share complementary behavior to ensure a robust perception of the overall information. Over the past decades, several studies have integrated audio and visual cues to improve speech recognition perfor- mance. Within recent deep learning frameworks, Petridis et al. [28] proposed one of the ﬁrst end-to-end audiovisual speech recognition schemes. Another approach is the “Watch, Listen, Attend, and Spell (WLAS)” framework [29], which is an extension of the LAS framework [13] for speech recog- nition tasks that utilize a dual attention mechanism that can operate in three ways: over visual input only, audio input only, or both. Afouras et al. [30] also proposed a deep audio- visual speech recognition to recognize", "meta": {"paper_id": "1906.00579v3", "chunk_index": 8, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_9", "text": "[29], which is an extension of the LAS framework [13] for speech recog- nition tasks that utilize a dual attention mechanism that can operate in three ways: over visual input only, audio input only, or both. Afouras et al. [30] also proposed a deep audio- visual speech recognition to recognize phrases and sentences spoken by a talking face with or without audio. Therefore, although the idea of incorporating visual in- formation for automatic speech recognition (ASR) is basi- cally not new, most approaches are usually done by simply concatenating the information that is inefﬁcient to effectively fuse information from various modalities. Furthermore, these methods require that all of the information from these modal- ities be available altogether, which is often difﬁcult. In con- trast, humans process different modalities by different organs (i.e., ears for listening, mouths for speaking, eyes for seeing, etc.), which enables independent processing of such informa- tion while simultaneously opening room for augmenting each other. Previously, a machine speech chain, based on sequence- to-sequence deep learning, was proposed to mimic speech perception and production behavior. It separately processed listening and speaking by ASR and a text-to-speech synthe- sis (TTS), but also enabled semi-supervised learning to teach each other when they received unpaired data. Unfortunately, our study is limited to speech and textual modalities. In fact, natural communication is actually multimodal because it in- volves both auditory and visual sensory systems In this research, we take a further step to construct a multi- modal chain and design a closely knit chain architecture that combines ASR, TTS, image captioning, and image produc- tion (retrieval or generation) models into a single framework. The framework allows each component to be trained without a large number of parallel multimodal data. Our experimental results show that further training of an ASR without available speech and text data remains possible by cross-modal data augmentation through our proposed multimodal chain, which improves ASR performance. 7. CONCLUSION We described a novel approach for cross-modal data augmen- tation that upgrades a speech chain into a multimodal chain. We proposed a visual chain by jointly training IC and IR models in a loop connection that can learn semi-supervisedly over an unpaired image-text dataset. Then we improved the speech chain using an image-only dataset, bridged by our vi- sual chain, and vice-versa. Therefore, we conclude that it is still possible to improve ASR, even without speech and text data available, with our proposed multimodal chain. We showed that each model in both chains can assist each other given an incomplete dataset by leveraging the data augmenta- tion among modalities. In the future, we will jointly train both the speech and visual chain so that both can also be updated together. Furthermore, following the previous speech chain [10] that can synthesize multi-speaker speech, we will inves- tigate our multimodal chain on natural multispeaker speech dataset as well. 8. ACKNOWLEDGEMENTS Part of this work is supported by JSPS KAKENHI Grant Numbers JP17H06101 and JP17K00237 as well as NII CRIS Contract Research 2019 and Google AI Focused Research Awards Program. 9. REFERENCES [1] W.", "meta": {"paper_id": "1906.00579v3", "chunk_index": 9, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_10", "text": "speech, we will inves- tigate our multimodal chain on natural multispeaker speech dataset as well. 8. ACKNOWLEDGEMENTS Part of this work is supported by JSPS KAKENHI Grant Numbers JP17H06101 and JP17K00237 as well as NII CRIS Contract Research 2019 and Google AI Focused Research Awards Program. 9. REFERENCES [1] W. Xiong, L. Wu, F. Alleva, J. Droppo, X. Huang, and A. Stolcke, “The microsoft 2017 conversational speech recognition system,” in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP), April 2018, pp. 5934–5938. [2] G. Saon, G. Kurata, T. Sercu, K. Audhkhasi, S. Thomas, D. Dimitriadis, X. Cui, B. Ramabhadran, M. Picheny, L.-L. Lim, B. Roomi, and P. Hall, “English conversa- tional telephone speech recognition by humans and ma- chines,” in Proc. Interspeech 2017, 2017, pp. 132–136. [3] D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T.-Y. Liu, and W.- Y. Ma, “Dual learning for machine translation,” in Ad- vances in Neural Information Processing Systems, 2016, pp. 820–828. [4] Y. Cheng, Z. Tu, F. Meng, J. Zhai, and Y. Liu, “Towards robust neural machine translation,” in Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers). Mel- bourne, Australia: Association for Computational Lin- guistics, Jul. 2018, pp. 1756–1766. [5] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim, “Learn- ing to discover cross-domain relations with generative adversarial networks,” in Proceedings of the 34th In- ternational Conference on Machine Learning, ser. Pro- ceedings of Machine Learning Research, D. Precup and Y. W. Teh, Eds., vol. 70. International Convention Cen- tre, Sydney, Australia: PMLR, 06–11 Aug 2017, pp. 1857–1865. [6] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-image translation using cycle-consistent ad- versarial networks,” in Computer Vision (ICCV), 2017 IEEE International Conference on, 2017. [7] Z. Yi, H. R. Zhang, P. Tan, and M. Gong, “Dualgan: Unsupervised dual learning for image-to-image transla- tion,” in IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017, 2017, pp. 2868–2876. [8] K. Tanaka, T. Kaneko, N. Hojo, and H. Kameoka, “Synthetic-to-natural speech waveform conversion us- ing cycle-consistent adversarial networks,” in 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018, 2018, pp. 632–639. [9] A. Tjandra, S. Sakti, and S. Nakamura, “Listening while speaking: Speech chain by deep learning,” CoRR, vol. abs/1707.04879, 2017. [Online]. Available: http://arxiv.org/abs/1707.04879 [10] ——, “Machine speech chain with one-shot speaker adaptation,” in Interspeech 2018, 19th Annual Con- ference of the International Speech Communica- tion Association, Hyderabad, India, 2-6 Septem- ber 2018., 2018, pp. 887–891. [Online]. Available: https://doi.org/10.21437/Interspeech.2018-1558 [11] ——, “End-to-end feedback loss in speech chain framework via straight-through estimator,” CoRR, vol. abs/1810.13107, 2018. [12] J. Weston, S. Bengio, and N. Usunier, “Wsabie: Scaling up to large vocabulary image annotation,” in Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence - Volume Volume Three, ser. IJCAI’11. AAAI Press, 2011, pp. 2764– 2770. [Online]. Available: http://dx.doi.org/10.5591/ 978-1-57735-516-8/IJCAI11-460 [13] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, at- tend and spell: A neural network for large vocabulary conversational", "meta": {"paper_id": "1906.00579v3", "chunk_index": 10, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_11", "text": "annotation,” in Proceedings of the Twenty-Second International Joint Conference on Artiﬁcial Intelligence - Volume Volume Three, ser. IJCAI’11. AAAI Press, 2011, pp. 2764– 2770. [Online]. Available: http://dx.doi.org/10.5591/ 978-1-57735-516-8/IJCAI11-460 [13] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, at- tend and spell: A neural network for large vocabulary conversational speech recognition,” in 2016 IEEE In- ternational Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964. [14] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Ben- gio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron: Towards end-to-end speech synthe- sis,” 2017. [15] D. Grifﬁn and J. Lim, “Signal estimation from modi- ﬁed short-time fourier transform,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 2, pp. 236–243, April 1984. [16] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio, “Show, at- tend and tell: Neural image caption generation with vi- sual attention,” in International conference on machine learning, 2015, pp. 2048–2057. [17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recog- nition, 2016, pp. 770–778. [18] K. Simonyan and A. Zisserman, “Very deep convo- lutional networks for large-scale image recognition,” CoRR, vol. abs/1409.1556, 2014. [19] J. Dong, X. Li, and C. G. M. Snoek, “Word2visualvec: Cross-media retrieval by visual feature prediction,” CoRR, vol. abs/1604.06838, 2016. [Online]. Available: http://arxiv.org/abs/1604.06838 [20] A. Vilalta, D. Garcia-Gasulla, F. Par´es, E. Ayguad´e, J. Labarta, E. U. Moya-S´anchez, and U. Cort´es, “Study- ing the impact of the full-network embedding on multi- modal pipelines,” Semantic Web, no. Preprint, pp. 1–15. [21] L. Ma, Z. Lu, L. Shang, and H. Li, “Multimodal con- volutional neural networks for matching image and sen- tence,” 2015 IEEE International Conference on Com- puter Vision (ICCV), pp. 2623–2631, 2015. [22] I. Calixto and Q. Liu, “Sentence-level multilingual multi-modal embedding for natural language process- ing,” in Proceedings of the International Conference Re- cent Advances in Natural Language Processing, RANLP 2017. Varna, Bulgaria: INCOMA Ltd., Sep. 2017, pp. 139–148. [23] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik, “Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models,” in 2015 IEEE International Conference on Computer Vision (ICCV), Dec 2015, pp. 2641–2649. [24] S. Kim, T. Hori, and S. Watanabe, “Joint ctc-attention based end-to-end speech recognition using multi-task learning,” in 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2017, pp. 4835–4839. [25] A. Tjandra, S. Sakti, and S. Nakamura, “Multi-scale alignment and contextual history for attention mecha- nism in sequence-to-sequence model,” in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 648–655. [26] D. B. Paul and J. M. Baker, “The design for the wall street journal-based csr corpus,” in Proceedings of the workshop on Speech and Natural Language. Associa- tion for Computational Linguistics, 1992, pp. 357–362. [27] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel,", "meta": {"paper_id": "1906.00579v3", "chunk_index": 11, "filename": "1906.00579v3.pdf", "chunk_length": 512}}
{"id": "1906.00579v3_chunk_12", "text": "pp. 648–655. [26] D. B. Paul and J. M. Baker, “The design for the wall street journal-based csr corpus,” in Proceedings of the workshop on Speech and Natural Language. Associa- tion for Computational Linguistics, 1992, pp. 357–362. [27] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., “The kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Sig- nal Processing Society, 2011. [28] S. Petridis, Y. Wang, Z. Li, and M. Pantic, “End-to- end audiovisual fusion with lstms,” in Auditory-Visual Speech Processing, AVSP 2017, Stockholm, Sweden, 25- 26 August 2017., 2017, pp. 36–40. [29] J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Lip reading sentences in the wild,” in 2017 IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2017, pp. 3444–3453. [30] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, “Deep audio-visual speech recognition,” IEEE transactions on pattern analysis and machine in- telligence, 2018.", "meta": {"paper_id": "1906.00579v3", "chunk_index": 12, "filename": "1906.00579v3.pdf", "chunk_length": 174}}
{"id": "1906.00744v5_chunk_0", "text": "Hierarchical Decision Making by Generating and Following Natural Language Instructions Hengyuan Hu∗ Facebook AI Research hengyuan@fb.com Denis Yarats∗ New York University & Facebook AI Research denisyarats@cs.nyu.edu Qucheng Gong Facebook AI Research qucheng@fb.com Yuandong Tian Facebook AI Research yuandong@fb.com Mike Lewis Facebook AI Research mikelewis@fb.com Abstract We explore using latent natural language instructions as an expressive and compo- sitional representation of complex actions for hierarchical decision making. Rather than directly selecting micro-actions, our agent ﬁrst generates a latent plan in natural language, which is then executed by a separate model. We introduce a challenging real-time strategy game environment in which the actions of a large number of units must be coordinated across long time scales. We gather a dataset of 76 thousand pairs of instructions and executions from human play, and train instruc- tor and executor models. Experiments show that models using natural language as a latent variable signiﬁcantly outperform models that directly imitate human actions. The compositional structure of language proves crucial to its effectiveness for action representation. We also release our code, models and data23. 1 Introduction Many complex problems can be naturally decomposed into steps of high level planning and low level control. However, plan representation is challenging—manually specifying macro-actions requires signiﬁcant domain expertise, limiting generality and scalability [18, 22], but learning composite actions from only end-task supervision can result in the hierarchy collapsing to a single action [3]. We explore representing complex actions as natural language instructions. Language can express arbitrary goals, and has compositional structure that allows generalization across commands [1, 14]. Our agent has a two-level hierarchy, where a high-level instructor model communicates a sub-goal in natural language to a low-level executor model, which then interacts with the environment (Fig. 1). Both models are trained to imitate humans playing the roles. This approach decomposes decision making into planning and execution modules, with a natural language interface between them. We gather example instructions and executions from two humans collaborating in a complex game. Both players have access to the same partial information about the game state. One player acts as the instructor, and periodically issues instructions to the other player (the executor), but has no direct control on the environment. The executor acts to complete the instruction. This setup forces the instructor to focus on high-level planning, while the executor concentrates on low-level control. ∗Equal Contribution. 2A demo is available at www.minirts.net 3Our code is open-sourced at www.github.com/facebookresearch/minirts 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1906.00744v5 [cs.AI] 2 Oct 2019 Screen Features Result of Actions Instructor Non-spatial Features Instruction Observation Actions our_army_attr enemy_army_attr resourses_attr select_unit(peasant) build_building(tower) game_state Executor build a tower next to the base Figure 1: Two agents, designated instructor and executor collaboratively play a real-time strategy game (§2). The instructor iteratively formulates plans and issues instructions in natural language to the executor, who then executes them as a sequence of actions. We ﬁrst gather a dataset of humans playing each role (§3). We then train models to imitate humans actions in each role (§4). To test our approach, we introduce a real-time", "meta": {"paper_id": "1906.00744v5", "chunk_index": 0, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_1", "text": "plans and issues instructions in natural language to the executor, who then executes them as a sequence of actions. We ﬁrst gather a dataset of humans playing each role (§3). We then train models to imitate humans actions in each role (§4). To test our approach, we introduce a real-time strategy (RTS) game, developing an environment based on [23]. A key property of our game is the rock-paper-scissors unit attack dynamic, which emphasises strategic planning over micro control. Our game environment is a challenging decision making task, because of exponentially large state-action spaces, partial observability, and the variety of effective strategies. However, it is relatively intuitive for humans, easing data collection. Using this framework, we gather a dataset of 5392 games, where two humans (the instructor and executor) control an agent against rule-based opponents. The dataset contains 76 thousand pairs of human instructions and executions, spanning a wide range of strategies. This dataset poses challenges for both instruction generation and execution, as instructions may apply to different subsets of units, and multiple instructions may apply at a given time. We design models for both problems, and extensive experiments show that using latent language signiﬁcantly improves performance. In summary, we introduce a challenging RTS environment for sequential decision making, and a corresponding dataset of instruction-execution mappings. We develop novel model architectures with planning and control components, connected with a natural language interface. Our agent with latent language signiﬁcantly outperforms agents that directly imitate human actions, and we show that exploiting the compositional structure of language improves performance by allowing generalization across a large instruction set. We also release our code, models and data. 2 Task Environment We implement our approach for an RTS game, which has several attractive properties compared to traditional reinforcement learning environments, such as Atari [13] or grid worlds [19]. The large state and action spaces mean that planning at different levels of abstraction is beneﬁcial for both humans and machines. However, manually designed macro-actions typically do not match strong human performance, because of the unbounded space of possible strategies [21, 25]. Even with simple rules, adversarial games have the scope for complex emergent behaviour. We introduce a new RTS game environment, which distills the key features of more complex games while being faster to simulate and more tractable to learn. Current RTS environments, such as StarCraft, have dozens of unit types, adding large overheads for new players to learn the game. Our new environment is based on MiniRTS [23]. It has a set of 7 unit types, designed with a rock-paper- scissors dynamic such that each has some units it is effective against and vulnerable to. Maps are randomly generated each game to force models to adapt to their environment as well as their opponent. The game is designed to be intuitive for new players (for example, catapults have long range and are effective against buildings). Numerous strategies are viable, and the game presents players with dilemmas such as whether to attack early or focus on resource gathering, or whether to commit to a 2 strategy or to attempt", "meta": {"paper_id": "1906.00744v5", "chunk_index": 1, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_2", "text": "to be intuitive for new players (for example, catapults have long range and are effective against buildings). Numerous strategies are viable, and the game presents players with dilemmas such as whether to attack early or focus on resource gathering, or whether to commit to a 2 strategy or to attempt to scout for the opponent’s strategy ﬁrst. Overall, the game is easy for humans to learn, but challenging for machines due to the large action space, imperfect information, and need to adapt strategies to both the map and opponent. See the Appendix for more details. 3 Dataset To learn to describe actions with natural language, we gather a dataset of two humans playing collaboratively against a rule-based opponent. Both players have access to the same information about the game state, but have different roles. One is designated the instructor, and is responsible for designing strategies and describing them in natural language, but has no direct control. The other player, the executor, must ground the instructions into low level control. The executor’s goal is to carry out commands, not to try to win the game. This setup causes humans to focus on either planning or control, and provides supervision for both generating and executing instructions. Statistic Value Total games 5392 Win rate 58.6% Total instructions 76045 Unique instructions 50669 Total words 483650 Unique words 5007 # words per instruction 9.54 # instructions per game 14.1 Table 1: We gather a large language dataset for instruction generation and following. Major chal- lenges include the wide range of unique instruc- tions and the large number of low-level actions required to execute each instruction. We collect 5392 games of human teams against our bots.4 Qualitatively, we observe a wide vari- ety of different strategies. An average game con- tains 14 natural language instructions and lasts for 16 minutes. Each instruction corresponds to roughly 7 low-level actions, giving a challeng- ing grounding problem (Table 1). The dataset contains over 76 thousand instructions, most of which are unique, and their executions. The diver- sity of instructions shows the wide range of useful strategies. The instructions contain a number of challenging linguistic phenomena, particularly in terms of reference to locations and units in the game, which often requires pragmatic inference. Instruction execution is typically highly depen- dent on context. Our dataset is publicly available. For more details, refer to the Appendix. Analysing the list of instructions (see Appendix), we see that the head of the distribution is dominated by straightforward commands to perform the most frequent actions. However, samples from the complete instruction list reveal many complex compositional instructions, such as Send one catapult to attack the northern guard tower [and] send a dragon for protection. We see examples of challenging quantiﬁers (Send all but 1 peasant to mine), anaphora (Make 2 more cavalry and send them over with the other ones), spatial references (Build a new town hall between the two west minerals patches) and conditionals (If attacked retreat south). 4 Model We factorize agent into an executor model (§4.2), which maps instructions and the game states into", "meta": {"paper_id": "1906.00744v5", "chunk_index": 2, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_3", "text": "anaphora (Make 2 more cavalry and send them over with the other ones), spatial references (Build a new town hall between the two west minerals patches) and conditionals (If attacked retreat south). 4 Model We factorize agent into an executor model (§4.2), which maps instructions and the game states into unit-level actions of the environment, and an instructor model (§4.3), which generates language instructions given the game states. We train both models with human supervision (§4.4). 4.1 Game Observation Encoder We condition both the instructor and executor models on a ﬁxed-sized representation of the current game state, which we construct from a spatial map observation, internal states of visible units, and several previous natural language instructions. (Fig. 2). We detail each individual encoder below. 4.1.1 Spatial Inputs Encoder We encode the spatial information of the game map using a convolutional network. We discretize the map into a 32 × 32 grid and extract different bits of information from it using separate channels. For example, three of those channels provide binary indication of a particular cell visibility, which 4Using ParlAI [12] 3 { } dim { dim { dim { dim # our units } resources features map cells features enemy units features our units features # resources } (x,y) { dim } (x,y) { dim instructions features } last K instructions { dim } } # enemy units observation encoder instruction encoder spatial inputs non-spatial inputs unit_1{type, hp} unit_2{type, hp} ... enemy_1{type, hp} ... resource_1:{cap} last K instructions instruction_1 instruction_2 ... instruction_K { dim extra features auxililary encoder extra inputs amount of resources Figure 2: At each time step of the environment we encode spatial observations (e.g. the game map) and non-spatial internal states for each game object (e.g. units, buildings, or resources) via the observation encoder, which produces separate feature vectors for each unit, resource, or discrete map locations. We also embed each of the last K natural language instructions into individual instruction feature vectors. Lastly, we learn features for all the other global game attributes by employing the auxiliary encoder. We then use these features for both the executor and instructor networks. indicates INVISIBLE, SEEN, and VISIBLE states. We also have a separate channel per unit type to record the number of units in each spatial position for both our and enemy units separately. Note that due to \"fog-of-war\", not all enemy units are visible to the player. See the Appendix for more details. We apply several 3 × 3 convolutional layers that preserve the spatial dimensions to the input tensor. Then we use 4 sets of different weights to project the shared convolutional features onto different 2D features spaces, namely OUR UNITS, ENEMY UNITS, RESOURCES, and MAP CELLS. We then use (x, y) locations for units, resources, or map cells to extract their features vectors from corresponding 2D features spaces. 4.1.2 Non-spatial Inputs Encoder We also take advantage of non-spatial attributes and internal state for game objects. Speciﬁcally, we improve features vectors for OUR UNITS and ENEMY UNITS by adding encodings of units health points, previous and current actions.", "meta": {"paper_id": "1906.00744v5", "chunk_index": 3, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_4", "text": "cells to extract their features vectors from corresponding 2D features spaces. 4.1.2 Non-spatial Inputs Encoder We also take advantage of non-spatial attributes and internal state for game objects. Speciﬁcally, we improve features vectors for OUR UNITS and ENEMY UNITS by adding encodings of units health points, previous and current actions. If an enemy unit goes out the players visibility, we respect this by using the state of the unit’s attributes from the last moment we saw it. We project attribute features onto the same dimensionality of the spatial features and do a element-wise multiplication to get the ﬁnal set of OUR UNITS and ENEMY UNITS features vectors. 4.1.3 Instruction Encoders The state also contains a ﬁxed-size representation of the current instruction. We experiment with: • An instruction-independent model (EXECUTORONLY), that directly mimics human actions. • A non-compositional encoder (ONEHOT) which embeds each instruction with no parameter sharing across instructions (rare instructions are represented with an unknown embedding). • A bag-of-words encoder (BOW), where an instruction encoding is a sum of word embed- dings. This model tests if the compositionality of language improves generalization. • An RNN encoder (RNN), which is order-aware. Unlike BOW, this approach can differentiate instructions such as attack the dragon with the archer and attack the archer with the dragon. 4.1.4 Auxiliary Encoder Finally, we encode additional game context, such as the amount of money the player has, through a simple MLP to get the EXTRA features vector. 4.2 Executor Model The executor predicts an action for every unit controlled by the agent based on the global summary of the current observation. We predict an action for each of the player’s units by choosing over an 4 ACTION TYPE ﬁrst, and then selecting the ACTION OUTPUT. There are 7 action types available: IDLE, CONTINUE, GATHER, ATTACK, TRAIN UNIT, BUILD BUILDING, MOVE. ACTION OUTPUT speciﬁes the target output for the action, such as a target location for the MOVE action, or the unit type for TRAIN UNIT. Fig. 3 gives an overview of the executor design, also refer to the Appendix. } resource features enemy units features enemy units summary our units summary } } resources features resources summary } our units features instructions features unit dependent instructions features unit features ... ... extra features softmax over unit actions global summary MOVE ~ ~ map cells features attention over each cell (x,y) action output action type (x,y) (x,y) Figure 3: Modeling an action for an unit requires predicting an action type based on the global sum- mary of current observation, and then, depending on the predicted action type, computing a probabil- ity distribution over a set of the action targets. In this case, the MOVE action is sampled, which uses the map cells features as the action targets. For each unit, we consider a history of recent N instructions (N = 5 in all our experiments), because some units may still be focusing on a previous instruction that has long term ef- fect like keep scouting or build 3 peasants. To encode the N instructions, we ﬁrst embed them in", "meta": {"paper_id": "1906.00744v5", "chunk_index": 4, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_5", "text": "unit, we consider a history of recent N instructions (N = 5 in all our experiments), because some units may still be focusing on a previous instruction that has long term ef- fect like keep scouting or build 3 peasants. To encode the N instructions, we ﬁrst embed them in isolation with the 4.1.3. We take K that represents how many frames have passed since that instruction gets issued and compute H = max(Hmax, K/B) where Hmax, B are constants deﬁning the number of bins and bin size. We also take O = 1, 2, ..., N that repre- sents the temporal ordering of those instructions. We embed O and H and concatenate the embed- dings with language embedding. Dot product attention is used to compute an attention score between a unit and recent instructions and then a unit dependent instruction representation is obtained through a weighted sum of history in- struction embeddings using attention score as weight. We use the same observation encoder (§4.1) to obtain the features mentioned above. To form a global summary, we sum our unit features, enemy unit features, and resource features re- spectively and then concatenate together with EXTRA features. To decide the action for each unit, we ﬁrst feed the concatenation of the unit feature, unit de- pending instruction feature and the global sum- mary into a multi-layer neural classiﬁer to sample an ACTION TYPE. Depending on the action type, we then feed inputs into different action-speciﬁc classiﬁers to sample ACTION OUTPUT. In the action argument classiﬁer, the unit is represented by the concatenation of unit feature and instruction feature, and the targets are represented by different target embeddings. For ATTACK, the target embeddings are enemy features; for GATHER; the target embeddings are resource features; for MOVE, the target embeddings are map features; for TRAIN UNIT, the target embeddings are embeddings of unit types; for BUILD BUILDING, the target embeddings are embeddings of unit types and map features, and we sample type and location independently. The distribution over targets for the action is computed by taking the dot product between the unit representation and each target, followed by a softmax. We add an additional binary classiﬁer, GLOBAL CONTINUE, that takes the global summary and current instruction embedding as an input to predict whether all the agent’s units should continue working on their previous action. 4.3 Instructor Model The instructor maps the game state to instructions. It uses the game observation encoder (§4.1) to compute a global summary and current instruction embedding similar to the executor. We experiment with two model types: Discriminative Models These models operate on a ﬁxed set of instructions. Each instruction is encoded as a ﬁxed-size vector, and the dot product of this encoding and the game state encoding is fed into a softmax classiﬁer over the set of instructions. As in §4.1.3, we consider non-compositional (ONEHOT), bag-of-words (BOW) and RNN DISCRIMINATIVE encoders. 5 Generative Model The discriminative models can only choose between a ﬁxed set of instructions. We also train a generative model, RNN GENERATIVE, which generates instructions autoregressively. To compare", "meta": {"paper_id": "1906.00744v5", "chunk_index": 5, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_6", "text": "a softmax classiﬁer over the set of instructions. As in §4.1.3, we consider non-compositional (ONEHOT), bag-of-words (BOW) and RNN DISCRIMINATIVE encoders. 5 Generative Model The discriminative models can only choose between a ﬁxed set of instructions. We also train a generative model, RNN GENERATIVE, which generates instructions autoregressively. To compare likelihoods with the discriminative models, which consider a ﬁxed set of instructions, we re-normalize the probability of an instruction over the space of instructions in the set. The instructor model must also decide at each time-step whether to issue a new command, or leave the executor to follow previous instructions. We add a simple binary classiﬁer that conditions on the global feature, and only sample a new instruction if the result is positive. Because the game is only partially observable, it is important to consider historical information when deciding an instruction. For simplicity, we add a running average of the number of enemy units of each type that have appeared in the visible region as an extra input to the model. To make the instructor model aware of how long the current instruction has been executed, we add an extra input representing number of time-step passed since the issuance of current instruction. As mentioned above, these extra inputs are fed into separate MLPs and become part of the EXTRA feature. 4.4 Training Since one game may last for tens of thousands of frames, it is not feasible nor necessary to use all frames for training. Instead, we take one frame every K frames to form the supervised learning dataset. To preserve unit level actions for the executor training, we put all actions that happen in [tK, (t + 1)K) frames onto the tKth frame if possible. For actions that cannot happen on the tKth frame, such as actions for new units built after tKth frame, we simply discard them. Humans players sometimes did not execute instructions immediately. To ensure our executor acts promptly, we ﬁlter out action-less frames between a new instruction and ﬁrst new actions. 4.4.1 Executor Model The executor is trained to minimize the following negative log-likelihood loss: L = −log Pcont(c|s) −(1 −c) · |u| X i=1 log PA(aui|s) where s represents game state and instruction, Pcont(·|s) is the executor GLOBAL CONTINUE classiﬁer (see §4.2), c is a binary label that is 1 if all units should continue their previous action, PA(aui|s) is the likelihood of unit i doing the correct action aui. 4.4.2 Instructor Model The loss for the instructor model is the sum of a loss for deciding whether to issue a new instruction, and the loss for issuing the correct instruction: L = −log Pcont(c|s) −(1 −c) · Llang where s represents game state and current instruction, Pcont(·|s) is the continue classiﬁer, and c is a binary label with c = 1 indicating that no new instruction is issued. The language loss Llang is the loss for choosing the correct instruction, and is deﬁned separately for each model. For ONEHOT instructor, Llang is simply negative log-likelihood of a categorical classiﬁer over a pool of N instructions.", "meta": {"paper_id": "1906.00744v5", "chunk_index": 6, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_7", "text": "binary label with c = 1 indicating that no new instruction is issued. The language loss Llang is the loss for choosing the correct instruction, and is deﬁned separately for each model. For ONEHOT instructor, Llang is simply negative log-likelihood of a categorical classiﬁer over a pool of N instructions. If the true target is not in the candidate pool Llang is 0. Because BOW and RNN DISCRIMINATIVE can compositionally encode any instruction (in contrast to ONEHOT), we can additionally train on instructions from outside the candidate pool. To do this, we encode the true instruction, and discriminate against the N instructions in the candidate pool and another M randomly sampled instructions. The true target is forced to appear in the M + N candidates. We then use the NLL of the true target as language loss. This approach approximates the expensive softmax over all 40K unique instructions. For RNN GENERATIVE, the language loss is the standard autoregressive loss. 6 Executor Model Negative Log Likelihood Win/Lose/Draw Rate (%) EXECUTORONLY 3.15 ± 0.0024 41.2/40.7/18.1 ONEHOT 3.05 ± 0.0015 49.6/37.9/12.5 BOW 2.89 ± 0.0028 54.2/33.9/11.9 RNN 2.88 ± 0.0006 57.9/30.5/11.7 Table 2: Negative log-likelihoods of human actions for executor models, and win-rates against EXECUTORONLY, which does not use latent language. We use the RNN DISCRIMINATIVE instructor with 500 instructions. Modelling instructions compositionally improves performance, showing linguistic structure enables generalization. Instructor Model Negative Log Likelihood Win/Lose/Draw rate (%) (with N instructions) N=50 N=250 N=500 N=50 N=250 N=500 ONEHOT 0.662 ± 0.005 0.831 ± 0.001 0.911 ± 0.005 44.6 / 43.4 / 12.0 49.7 / 35.9 / 14.3 43.1 / 41.1 / 15.7 BOW 0.638 ± 0.004 0.792 ± 0.001 0.869 ± 0.002 41.3 / 41.2 / 17.5 51.5 / 33.3 / 15.3 50.5 / 37.1 / 12.5 RNN DISCRIMINATIVE 0.618 ± 0.005 0.764 ± 0.002 0.826 ± 0.002 47.8 / 36.5 / 15.7 55.4 / 33.1 / 11.5 57.9 / 30.5 / 11.7 RNN GENERATIVE 0.638 ± 0.006 0.794 ± 0.006 0.857 ± 0.002 47.3 / 38.1 / 14.6 51.1 / 33.7 / 15.2 54.8 / 33.8 / 11.4 Table 3: Win-rates and likelihoods for different instructor models, with the N most frequent instructions. Win-rates are against a non-hierarchical executor model, and use the RNN executor. Better results are achieved with larger instruction sets and more compositional instruction encoders. 5 Experiments We compare different executor (§5.1) and instructor (§5.2) models in terms of both likelihoods and end-task performance. We show that hierarchical models perform better, and that the compositional structure of language improves results by allowing parameter sharing across many instructions. 5.1 Executor Model The executor model learns to ground pairs of states and instructions onto actions. With over 76 thousand examples, a large action space, and multiple sentences of context, this problem in isolation is one of the largest and most challenging tasks currently available for grounding language in action. We evaluate executor performance with different instruction encoding models (§4.1.3). Results are shown in Table 2, and show that modelling instructions compositionally—by encoding words (BOW) and word order (RNN)—improves both the likelihoods of human", "meta": {"paper_id": "1906.00744v5", "chunk_index": 7, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_8", "text": "is one of the largest and most challenging tasks currently available for grounding language in action. We evaluate executor performance with different instruction encoding models (§4.1.3). Results are shown in Table 2, and show that modelling instructions compositionally—by encoding words (BOW) and word order (RNN)—improves both the likelihoods of human actions, and win-rates over non-compositional instructor models (ONEHOT). The gain increases with larger instruction sets, demonstrating that a wide range of instructions are helpful, and that exploiting the compositional structure of language is crucial for generalization across large instruction sets. We additionally ablate the importance of considering multiple recent instructions during execution (our model performs attention over the most recent 5 commands §4.2). When considering only the current instruction with the RNN executor, we ﬁnd performance drops to a win-rate of 52.9 (from 57.9) and negative log likelihood worsens from 2.88 to 2.93. 5.2 Instructor Model We compare different instructor models for mapping game states to instructions. As in §5.1, we experiment with non-compositional, bag-of-words and RNN models for instruction generation. For the RNNs, we train both a discriminative model (which maps complete instructions onto vectors, and then chooses between them) and a generative model that outputs words auto-regressively. Evaluating language generation quality is challenging, as many instructions may be reasonable in a given situation, and they may have little word overlap. We therefore compare the likelihood of the human instructions. Our models choose from a ﬁxed set of instructions, so we measure the likelihood of choosing the correct instruction, normalized over all instructions in the set. Likelihoods across different instructions sets are not comparable. Table 3 shows that, as §5.1, more structured instruction models give better likelihoods—particularly for larger instruction sets, which are harder to model non-compositionally. 7 We compare the win-rate of our models against a baseline which directly imitates human actions (without latent language). All latent instruction models outperform this baseline. More compositional instruction encoders improve performance, and can use more instructions effectively. These results demonstrate the potential of language for compositionally representing large spaces of complex plans. 5.3 Qualitative Analysis Observing games played by our model, we ﬁnd that most instructions are both generated and executed as humans plausibly would. The executor is often able to correctly count the number of units it should create in commands such as build 3 dragons. There are several limitations. The executor sometimes acts without instructions—partly due to mimicking some humans behaviour, but also indicating a failure to learn dependencies between instructions and actions. The instructor sometimes issues commands which are impossible in its state (e.g. to attack with a unit that the it does not have)—causing weak behaviour from executor model. 6 Related work Previous work has used language to specify exponentially many policies [1, 14, 27], allowing zero-shot generalization across tasks. We develop this work by generating instructions as well as executing them. We also show how complex tasks can be decomposed into a series of instructions and executions. Executing natural language instructions has seen much attention. The task of grounding language into an executable representation is sometimes called semantic", "meta": {"paper_id": "1906.00744v5", "chunk_index": 8, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_9", "text": "tasks. We develop this work by generating instructions as well as executing them. We also show how complex tasks can be decomposed into a series of instructions and executions. Executing natural language instructions has seen much attention. The task of grounding language into an executable representation is sometimes called semantic parsing [29], and has been applied to navigational instruction following, e.g. [2]. More recently, neural models instruction following have been developed for a variety of domains, for example [11] and [10]. Our dataset offers a challenging new problem for instruction following, as different instructions will apply to different subsets of available units, and multiple instructions may be apply at a given time. Instruction generation has been studied as a separate task. [7] map navigational paths onto instructions. [8] generate instructions for complex tasks that humans can follow, and [9] train a model for instruction generation, which is used both for data augmentation and for pragmatic inference when following human-generated instructions. We build on this work by also generating instructions at test time, and showing that latent language improves performance. Learning to play a complete real-time strategy game, including unit building, resources gathering, defence, invasion, scouting, and expansion, remains a challenging problem [15], in particular due to the complexity and variations of commercially successful games (e.g., StarCraft I/II), and its demand of computational resources. Traditional approaches focus on sub-tasks with hand-crafted features and value functions (e.g., building orders [5], spatial placement of building [4], attack tactics between two groups of units [6], etc). Inspired by the recent success of deep reinforcement learning, more works focus on training a neural network to ﬁnish sub-tasks [17, 24], some with strong computational requirement [28]. For full games, [23] shows that it is possible to train an end-to-end agent on a small-scaled RTS game with predeﬁned macro actions, and TStarBot [20] applies this idea to StarCraft II and shows that the resulting agent can beat carefully-designed, and even cheating rule-based AI. By using human demonstrations, we hand crafting macro-actions. Learning an end-to-end agent that plays RTS games with unit-level actions is even harder. Progress is reported for MOBA games, a sub-genre of RTS games with fewer units—for example, [16] shows that achieving professional level of playing DoTA2 is possible with massive computation, and [26] shows that with supervised pre-training on unit actions, and hierarchical macro strategies, a learned agent on Honor of Kings is on par with a top 1% human player. 7 Conclusion We introduced a framework for decomposing complex tasks into steps of planning and execution, connected with a natural language interface. We experimented with this approach on a new strategy game which is simple to learn but features challenging strategic decision making. We collected a large dataset of human instruction generations and executions, and trained models to imitate each role. 8 Results show that exploiting the compositional structure of natural language improves generalization for both the instructor and executor model, signiﬁcantly outperforming agents without latent language. Future work should use reinforcement learning to further improve the planning and execution models, and explore generating novel", "meta": {"paper_id": "1906.00744v5", "chunk_index": 9, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_10", "text": "trained models to imitate each role. 8 Results show that exploiting the compositional structure of natural language improves generalization for both the instructor and executor model, signiﬁcantly outperforming agents without latent language. Future work should use reinforcement learning to further improve the planning and execution models, and explore generating novel instructions. References [1] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with policy sketches. In Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 166–175, 2017. [2] Yoav Artzi and Luke Zettlemoyer. Weakly supervised learning of semantic parsers for mapping instructions to actions. Transactions of the Association of Computational Linguistics, 1:49–62, 2013. [3] Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option-critic architecture. CoRR, abs/1609.05140, 2016. [4] Michal Certicky. Implementing a wall-in building placement in starcraft with declarative programming. arXiv preprint arXiv:1306.4460, 2013. [5] David Churchill and Michael Buro. Build order optimization in starcraft. In AIIDE, pages 14–19, 2011. [6] David Churchill, Abdallah Safﬁdine, and Michael Buro. Fast heuristic search for rts game combat scenarios. In AIIDE, pages 112–117, 2012. [7] Andrea F Daniele, Mohit Bansal, and Matthew R Walter. Navigational instruction generation as inverse reinforcement learning with neural machine translation. In Proceedings of the 2017 ACM/IEEE International Conference on Human-Robot Interaction, pages 109–118. ACM, 2017. [8] Daniel Fried, Jacob Andreas, and Dan Klein. Uniﬁed pragmatic models for generating and following instructions. arXiv preprint arXiv:1711.04987, 2017. [9] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker- follower models for vision-and-language navigation. arXiv preprint arXiv:1806.02724, 2018. [10] Karl Moritz Hermann, Felix Hill, Simon Green, Fumin Wang, Ryan Faulkner, Hubert Soyer, David Szepesvari, Wojciech Marian Czarnecki, Max Jaderberg, Denis Teplyashin, et al. Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551, 2017. [11] Hongyuan Mei, Mohit Bansal, and Matthew R Walter. Listen, attend, and walk: Neural mapping of navigational instructions to action sequences. In AAAI, volume 1, page 2, 2016. [12] A. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bordes, D. Parikh, and J. Weston. Parlai: A dialog research software platform. arXiv preprint arXiv:1705.06476, 2017. [13] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. Riedmiller. Playing atari with deep reinforcement learning. CoRR, 2013. [14] Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli. Zero-shot task generalization with multi-task deep reinforcement learning. ICML, 2017. [15] Santiago Ontanón, Gabriel Synnaeve, Alberto Uriarte, Florian Richoux, David Churchill, and Mike Preuss. A survey of real-time strategy game ai research and competition in starcraft. IEEE Transactions on Computational Intelligence and AI in games, 5(4):293–311, 2013. [16] OpenAI. Openai ﬁve. https://blog.openai.com/openai-five/, 2018. [17] Peng Peng, Quan Yuan, Ying Wen, Yaodong Yang, Zhenkun Tang, Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. CoRR, abs/1703.10069, 2017. 9 [18] Sutton Richard, Precup Doina, and Singh Satinder. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2), 1999. [19] Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve,", "meta": {"paper_id": "1906.00744v5", "chunk_index": 10, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_11", "text": "Haitao Long, and Jun Wang. Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. CoRR, abs/1703.10069, 2017. 9 [18] Sutton Richard, Precup Doina, and Singh Satinder. Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning. Artif. Intell., 112(1-2), 1999. [19] Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith Chintala, and Rob Fergus. Mazebase: A sandbox for learning from games. CoRR, abs/1511.07401, 2015. [20] Peng Sun, Xinghai Sun, Lei Han, Jiechao Xiong, Qing Wang, Bo Li, Yang Zheng, Ji Liu, Yongsheng Liu, Han Liu, et al. Tstarbots: Defeating the cheating level builtin ai in starcraft ii in the full game. arXiv preprint arXiv:1809.07193, 2018. [21] Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala, Timothée Lacroix, Zeming Lin, Florian Richoux, and Nicolas Usunier. Torchcraft: a library for machine learning research on real-time strategy games. CoRR, abs/1611.00625, 2016. [22] Chen Tessler, Shahar Givony, Tom Zahavy, Daniel J. Mankowitz, and Shie Mannor. A deep hierarchical approach to lifelong learning in minecraft. CoRR, abs/1604.07255, 2016. [23] Yuandong Tian, Qucheng Gong, Wenling Shang, Yuxin Wu, and C Lawrence Zitnick. Elf: An extensive, lightweight and ﬂexible research platform for real-time strategy games. In Advances in Neural Information Processing Systems, pages 2659–2669, 2017. [24] Nicolas Usunier, Gabriel Synnaeve, Zeming Lin, and Soumith Chintala. Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks. ICLR, 2017. [25] Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich Küttler, John Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen, Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy P. Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence, Anders Ekermo, Jacob Repp, and Rodney Tsing. Starcraft II: A new challenge for reinforcement learning. CoRR, abs/1708.04782, 2017. [26] Bin Wu, Qiang Fu, Jing Liang, Peng Qu, Xiaoqian Li, Liang Wang, Wei Liu, Wei Yang, and Yongsheng Liu. Hierarchical macro strategy model for moba game ai. arXiv preprint arXiv:1812.07887, 2018. [27] Denis Yarats and Mike Lewis. Hierarchical text generation and planning for strategic dia- logue. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, pages 5587–5595, 2018. [28] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl Tuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforcement learning. arXiv preprint arXiv:1806.01830, 2018. [29] Luke Zettlemoyer and Michael Collins. Online learning of relaxed ccg grammars for parsing to logical form. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), 2007. 10 Appendix A Detailed game design We develop an RTS game based on the MiniRTS framework, aspiring to make it intuitive for humans, while still providing a signiﬁcant challenge to machines due to extremely high-dimensional observation and actions spaces, partial observability, and non-stationary environment dynamics imposed by the opponent. Below we describe the key game concepts. A.1 Game units speciﬁcations Building units Our game supports 6 different building types, each implementing a particular function in the game. Any building unit can be constructed by the PEASANT unit", "meta": {"paper_id": "1906.00744v5", "chunk_index": 11, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_12", "text": "actions spaces, partial observability, and non-stationary environment dynamics imposed by the opponent. Below we describe the key game concepts. A.1 Game units speciﬁcations Building units Our game supports 6 different building types, each implementing a particular function in the game. Any building unit can be constructed by the PEASANT unit type at any available map location by spending a speciﬁed amount of resources. Later, the constructed building can be used to construct units. Most of the building types can produce up to one different unit type, except of WORKSHOP, which can produce 3 different unit types. This property of the WORKSHOP building allows various strategies involving blufﬁng. A full list of available building units can be found in Table 4. Figure 4: Our game implements the rock-paper- scissors attack graph, where each unit has some units it is effective against and vulnerable to. Army units The game provides a player with 7 army unit types, each having different strengths and weaknesses. PEASANT is the only unit type that can construct building units and mine re- sources, so it is essential for advancing to the later stages of the game. We design the attack relation- ships between each unit type with a rock-paper- scissors dynamic—meaning that each unit type has another unit type that it is effective against or vulnerable to. This property means that effective agents must be reactive to their opponent’s strat- egy. See Fig. 4 for a visualization. Descriptions of army units can be found in Table 5. Resource unit RESOURCE is a stationary and neutral unit type, it cannot be constructed by any- one, and is only created during the map genera- tion phase. PEASANTs of both teams are allowed to mine the same RESOURCE unit, until it is ex- hausted. Initial capacity is set to 500, and one mine action subtracts 10 points from the RESOURCE. Several RESOURCE units are placed randomly on the map, which gives raise to many strategies around RESOURCE domination. A.2 Game map We represent the game map as a discrete grid of 32x32. Each cell of the grid can either be grass or water, where the grass cell is passable for any army units, while the water cell prevents all units Building name Description TOWN HALL The main building of the game, it allows a player to train PEASANTs and serves as a storage for mined resources. BARRACK Produces SPEARMEN. BLACKSMITH Produces SWORDMEN. STABLE Produces CAVALRY. WORKSHOP Produces CATAPULT, DRAGON and ARCHER. The only building that can produce multiple unit types. GUARD TOWER A building that can attack enemies, but cannot move. Table 4: The list of the building units available in the game. 11 except of DRAGON to go through. Having water cells around one’s main base can be leveraged as a natural protection. We generate maps randomly for each new game, we ﬁrst place one TOWN HALL for each player randomly. We then add some water cells onto the map, making sure that there is at least one path between two opposing TOWN HALLs, but otherwise aiming to create bottlenecks. Finally,", "meta": {"paper_id": "1906.00744v5", "chunk_index": 12, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_13", "text": "natural protection. We generate maps randomly for each new game, we ﬁrst place one TOWN HALL for each player randomly. We then add some water cells onto the map, making sure that there is at least one path between two opposing TOWN HALLs, but otherwise aiming to create bottlenecks. Finally, we randomly locate several RESOURCE units onto the map such that they are approximately equidistant from the players TOWN HALLs. Unit name Description PEASANT Gathers minerals and constructs buildings, not good at ﬁghting. SPEARMAN Effective against cavalry. SWORDMAN Effective against spearmen. CAVALRY Effective gainst swordmen. DRAGON Can ﬂy over obstacles, can only be attacked by archers and towers. ARCHER Great counter unit against dragons. CATAPULT Easily demolishes buildings. Table 5: The list of the army units available in the game. B RTS game as an Reinforcement Learning environment Our platform can be also used as an RL environment. In our code base we implement a framework that allows a straightforward interaction with the game environment in a canonical RL training loop. Below we detail the environment properties. B.1 Observation space We leverage both spatial representation of the map, as well as internal state of the game engine (e.g. units health points and attacking cool downs, the amount of resources, etc.) to construct an observation. We carefully address the fog of war, by masking out the regions of the map that have not been visited. In addition, we remove any unseen enemy units attributes from the observation. The partial observability of the environment makes it especially challenging to apply RL due to highly non-stationary state distribution. B.2 Action space At each timestep of the environment we predict an action for each of our units, both buildings and army. The action space is consequently large—for example, any unit can go to any location at each timestep. Prediction of an unit action proceeds in steps, we ﬁrst predict an action type (e.g. MOVE or ATTACK), then, based on the action type, we predict the action outputs. For example, for the BUILD BUILDING action type the outputs will be the type of the future building and its location on the game map. We summarize all available action types and their structure in Table 6. B.3 Reward structure We support a sparse reward structure, e.g. the reward of 1 is issued to an agent at the end if the game is won, all the other timesteps receive the reward of 0. Such reward structure makes exploration an especially challenging given the large dimensionality of the action space and the planning horizon. C Data collection We design a data collection task based on ParlAI, a transparent framework to interact with human workers. We develop separate game control interfaces for both the instructor and the executor players, and ask two humans to play the game collaboratively against a rule-based AI opponent. Both player have the same access to the game observation, but different control abilities. The instructor control interface allows the human player to perform the following actions: 12 Action Type Action Output Input Features IDLE NULL NULL CONTINUE", "meta": {"paper_id": "1906.00744v5", "chunk_index": 13, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_14", "text": "two humans to play the game collaboratively against a rule-based AI opponent. Both player have the same access to the game observation, but different control abilities. The instructor control interface allows the human player to perform the following actions: 12 Action Type Action Output Input Features IDLE NULL NULL CONTINUE NULL NULL GATHER resource_id resources_features ATTACK enemy_unit_id enemy_units_features TRAIN UNIT unit_type unit_type_features BUILD BUILDING unit_type, (x,y) unit_type_features, map_cells_features MOVE (x,y) map_cells_features Table 6: We implement a separate action classiﬁer per action type, because each action type needs to model a probability distribution over different objects (Action Output). For example, for the ATTACK action we need estimate a probability distribution over all visible enemy units and predict an enemy unit id, or BUILD BUILDING action needs to model two probability distributions, one over building type to be constructed, and another over all possible (x, y) discrete location on the map where the future building will be placed. • Issue a natural language instruction to the executor at any time of the game. We allow any free-form language instruction. • Pause the game ﬂow at any time. Pausing allows the instructor to analyze the game state more thoroughly and plan strategically. • Warn the executor player in case they do not follow issued instructions precisely. This option allows us to improve data quality, by ﬁltering executors who do not follow instructions. On the other hand, the executor player gets to: • Control the game units by direct manipulation using computer’s input devices (e.g. mouse). The executor is tasked to complete the current instruction, rather than to win the game. • Ask the instructor for either a new instruction, or a clariﬁcation. Each human workers is assigned with either the instructor or the executor role randomly, thus the same person can experience the game on both ends over multiple attempts. C.1 Quality control To make sure that we collect data of high quality we take the following steps: Game manual We provide a detailed list of instructions to a human worker at the beginning of each game and during the game’s duration. This manual aims to narrate a comprehensive overview various game elements, such as player roles, army and building units, control mechanics, etc. We also record several game replays that serve as an introductory guideline to the players. Onboarding We implement an onboarding process to make sure that novice players are comfortable with the game mechanics, so that they can play with other players effectively. For this, we ask a novice player to perform the executor’s duties and pair them with a bot that issues a pre-deﬁned set of natural language instructions that implements a simple walkthrough strategy. We allocate enough time for the human player to work on the current instruction, and to also get comfortable with the game ﬂow. We let the novice player play several games until we verify that they pass the required quality bar. We assess the performance of the player by running a set of pattern-matching scripts that verify if the performed control actions correspond to the issued", "meta": {"paper_id": "1906.00744v5", "chunk_index": 14, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_15", "text": "also get comfortable with the game ﬂow. We let the novice player play several games until we verify that they pass the required quality bar. We assess the performance of the player by running a set of pattern-matching scripts that verify if the performed control actions correspond to the issued instructions (for example, if an instruction says \"build a barrack\", we make sure that the player executes the corresponding low-level action). If the human player doesn’t pass our qualiﬁcation requirements within 5 games, we prevent them from participating in our data collection going forward and ﬁlter their games from the dataset. Player proﬁle We track performance of each player, breaking it down by a particular role (e.g. instructor or executor). We gather various statistics about each player and build a comprehensive player proﬁle. For example, for the instructor role we gather data such as overall win rate, the number of instructions issued per game, diversity of issued instructions; for the executor role we monitor 13 Strategy Name Description SIMPLE This strategy ﬁrst sends all 3 initially available PEASANTs to mine to the closest resource, then it chooses one army unit type from SPEARMAN, SWORDMAN, CAVALRY, ARCHER, or DRAGON, then it constructs a corre- sponding building, and ﬁnally trains 3 units of the selected type and sends them to attack. The strategy then continuously maintains the army size of 3, in case an army unit dies. MEDIUM Same as SIMPLE strategy, only the size of the army is randomly selected between 3 and 7. STRONG This strategy is adaptive, and it reacts to the opponent’s army. In particular, this strategy constantly scouts the map using one PEASANT and to lean the opponent’s behaviour. Once it sees the opponent’s army it immediately trains a counter army based on the attack graph (see Fig. 4). Then it clones the MEDIUM strategy. SECOND BASE This strategy aims to build a second TOWN HALL near the second closest resource and then it uses the double income to build a large army of a particular unit type. The other behaviours is the same as in the MEDIUM strategy. TOWER RUSH A non-standard strategy, that ﬁrst scouts the map in order to ﬁnd the opponent using a spare PEASANT. Once it ﬁnds it, it starts building GUARD TOWERs close to the opponent’s TOWN HALL so they can attack the opponent’s units. PEASANT RUSH This strategy sends ﬁrst 3 PEASANTs to mine, then it keeps producing more PEASANTs and sending them to attack the opponent. The hope of this strategy is to beat the opponent by surprise. Table 7: The rule-based strategies we use as an opponent to the human players during data collection. how well they perform on the issued instruction (using a pattern matching algorithm), the number of warnings they receive from the instructor, and many more. We then use this proﬁle to decide whether to upgrade a particular player to playing against stronger opponents (see Appendix C.2) in case they are performing well, or prevent them from participating in our data collection at all otherwise. Feedback We", "meta": {"paper_id": "1906.00744v5", "chunk_index": 15, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_16", "text": "warnings they receive from the instructor, and many more. We then use this proﬁle to decide whether to upgrade a particular player to playing against stronger opponents (see Appendix C.2) in case they are performing well, or prevent them from participating in our data collection at all otherwise. Feedback We use several initial round of data collection as a source of feedback from the human players. The received feedback helps us to improve the game quality. Importantly, after we ﬁnalize the game conﬁguration, we disregard all the previously collected data in our ﬁnal dataset. Final ﬁltering Lastly, we take another ﬁltering pass against all the collected game replays and eliminate those replays that don’t meet the following requirements: • A game should have at least 3 natural language instructions issued by the instructor. • A game should have at least 25 low-level control actions issued by the executor. By implementing all the aforementioned safe guards we are able to gather a high quality dataset. C.2 Rule-based bots We design a set of diverse game strategies that are implemented by our rule-based bots ( Table 7). Our handcrafted strategies explore much of the possibilities that the game can offer, which in turn allows us to gather a multitude of emergent human behaviours in our dataset. Additionally, we employ a resource scaling hyperparameter, which controls the amount of resources a bot gets during mining. This hypermarameter offers a ﬁner control over the bot’s strength, which we ﬁnd beneﬁcial for onboarding novice human players. We pair a team of two human players (the instructor and executor) with a randomly sampled instance of a rule-based strategy and the resource scaling hyperparameter during our data collection, so the human player doesn’t know in advance who is their opponent. This property rewards reactive players. We later observe that our models are able to learn the scouting mechanics from the data, which is a crucial skill to be successful in our game. 14 (a) Top 500 most frequent instructions (b) Top 500 most frequent words Figure 5: Frequency histograms for the dataset instructions and words. D Model architecture D.1 Convolutional channels of Spatial Encoder We use the following set of convolutional channels to extract different bits of information from spatial representation of the current observation. 1. Visibility: 3 binary channels for each state of visibility of a cell (VISIBLE, SEEN, and INVISIBLE). 2. Terrain: 2 binary channels for each terrain type of a cell (grass or water). 3. Our Units: 13 channels for each unit type of our units. Here, a cell contains the number of our units of the same type located in it. 4. Enemy Units: similarly 13 channels for visible enemy units. 5. Resources: 1 channel for resource units. D.2 Action Classiﬁers At each step of the game we predict actions for each of the player’s units, we do this by performing a separate forward pass for ofv the following network for each unit. Firstly, we run an MLP (Fig. 6) based action classiﬁer to sample the unit’s ACTION TYPE. We feed the unit’s global summary", "meta": {"paper_id": "1906.00744v5", "chunk_index": 16, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_17", "text": "the game we predict actions for each of the player’s units, we do this by performing a separate forward pass for ofv the following network for each unit. Firstly, we run an MLP (Fig. 6) based action classiﬁer to sample the unit’s ACTION TYPE. We feed the unit’s global summary features (see Fig. 3 of the main paper) into the classiﬁer and sample an action type (see Table 6 for the full list of possible actions). Then, given the sampled action type we predict the ACTION OUTPUT based on the unit’s features, unit dependent instructions features, and the action input features. We provide an overview of ACTION OUTPUTs and INPUT FEATURES for each actions in Table 6. In addition, you can refer to the diagram Fig. 7. global summary softmax over action types {IDLE, CONTINUE, GATHER, ATTACK, TRAIN_UNIT, BUILD_BUILDING, MOVE} Figure 6: The ACTION TYPE classiﬁer is parameterized as an MLP network to model a softmax distribution over action types based on the unit’s global summary features vector. 15 Linguistic Phenomena Example Counting Build 3 dragons. Spatial Reference Send him to the choke point behind the tower. Locations Build one to the left of that tower. Composed Actions Attack archers, then peasants. Cross-instruction anaphora Use it as a lure to kill them. Table 8: Complex linguistic phenomena emerge as humans instruct others how to play the game. E Dataset details Through our data collection we gather a dataset of over 76 thousand of instructions and corresponding executions. We observe a wide variety of different strategies and their realizations in natural language. For example, we observe emergence of complicated linguistic constructions (Table 8). We also study the distribution of collected instructions. While we notice that some instructions are more frequent than others, we still observe a good coverage of strategies realizations, which serve as a ground for generalization. In Table 10 we provide a list of most frequently used instructions, and in Fig. 5 shows the overall frequency distribution for instructions and words in our dataset. Finally, we provide a random sample of 50 instructions from our dataset in Table 9, where showing the diversity and complexity of the collected instructions. 16 Instruction Build 1 more cavalry. Attack peaons. Build barrack in between south pass at new town. Have all peasants gather minerals next to town hall. Have all peasants mine ore. Fight u peaas. Stop the peasants from mining. Build a new town hall between the two west minerals patches. Build 2 more swords. Use cavalry to attack enemey. Explore and ﬁnd miners. If you see any idle peasants please have them build. Okay that doesn’t work then build them on your side of the wall then. Create 4 more archers. Make a new town hall in the middle of all 3. Attack tower with catas. Kill cavalry and peasants then their townhall. Attack enemy peasants with cavalry as well. Send all peasants to collect minerals. Attack enemy peasant. Keep creating peasants and sending them to mine. Send one catapult to attack the northern guard tower send a dragon for protection. Send", "meta": {"paper_id": "1906.00744v5", "chunk_index": 17, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_18", "text": "Attack tower with catas. Kill cavalry and peasants then their townhall. Attack enemy peasants with cavalry as well. Send all peasants to collect minerals. Attack enemy peasant. Keep creating peasants and sending them to mine. Send one catapult to attack the northern guard tower send a dragon for protection. Send all but 1 peasant to mine. Mine with the three peasants. Use that one to scout and don’t stop. Bring scout back to base to mine. You’ll need to attack them with more peasants to kill them. Build a barracks. Send all peasants to ﬁnd a mine and mine it. Start mining there with your 3. Make four peasants. Move archers west then north. Attack with cavalry. Make two more workers. Make 2 more calvary and send them over with the other ones. Return to base with scout. Build 2 peasants at the new mine. If attacked retreat south. Make the rest gather minerals too. All peasants ﬂee the enemy. Attack the peasants in the area. Attack the last archer with all peasants on the map. Table 9: Examples of randomly sampled instructions. 17 Instruction Frequency Instruction Frequency Attack. 527 Send idle peasants to mine. 68 Send all peasants to mine. 471 Attack that peasant. 68 Build a workshop. 414 Send all peasants to mine min- erals. 65 Retreat. 323 Build a barracks. 64 Build a stable. 278 Build barrack. 62 Send peasants to mine. 267 Return to mine. 62 All peasants mine. 266 Build peasant. 61 Send idle peasant to mine. 211 Build catapult. 61 Build workshop. 191 Create a dragon. 61 Build a dragon. 168 Mine with peasants. 60 Kill peasants. 168 Build 3 peasants. 59 Attack enemy. 166 Defend. 58 Attack peasants. 159 Build cavalry. 58 Build a guard tower. 146 Make an archer. 58 Attack the enemy. 142 Attack dragon. 58 Stop. 141 Send all peasants to collect min- erals. 57 Attack peasant. 139 Defend base. 57 Kill that peasant. 132 Build 2 more peasants. 56 Mine. 119 Build 2 peasants. 55 Build another dragon. 113 Make 2 archers. 55 Make another peasant. 113 Make dragon. 54 Build stable. 112 Build 2 dragons. 54 Make a dragon. 110 Attack dragons. 54 Build a blacksmith. 108 Make a stable. 53 Build a catapult. 108 Make a catapult. 53 Back to mining. 106 Build 6 peasants. 52 Build another peasant. 104 Attack archers. 50 Make a peasant. 98 Kill all peasants. 50 Build a barrack. 97 Build 2 catapults. 50 Build 4 peasants. 93 Idle peasant mine. 49 Have all peasants mine. 92 Make peasant. 48 Build 2 archers. 90 Attack enemy peasant. 48 Build dragon. 87 Attack archer. 48 Attack with peasants. 87 Build another archer. 47 Return to mining. 87 Make 4 peasants. 47 Build a peasant. 86 Make 3 peasants. 47 Idle peasant to mine. 85 Build 2 more archers. 46 Make a workshop. 83 Send idle peasant back to mine. 46 Create a workshop. 81 Make more peasants. 46 Mine with all peasants. 80 Make 2 more peasants. 46 Build 3 more peasants. 79 Build blacksmith.", "meta": {"paper_id": "1906.00744v5", "chunk_index": 18, "filename": "1906.00744v5.pdf", "chunk_length": 512}}
{"id": "1906.00744v5_chunk_19", "text": "Make 3 peasants. 47 Idle peasant to mine. 85 Build 2 more archers. 46 Make a workshop. 83 Send idle peasant back to mine. 46 Create a workshop. 81 Make more peasants. 46 Mine with all peasants. 80 Make 2 more peasants. 46 Build 3 more peasants. 79 Build blacksmith. 46 Create another peasant. 79 Collect minerals. 45 Send all idle peasants to mine. 77 Kill. 45 Build 3 archers. 77 Build an archer. 45 Kill peasant. 77 Keep mining. 45 Make another dragon. 76 Keep attacking. 43 Kill him. 72 Attack dragons with archers. 43 Build guard tower. 70 Create a stable. 42 Attack town hall. 70 Make 3 more peasants. 42 Start mining. 69 Attack the peasant. 41 Table 10: The top 100 instructions sorted by their usage frequency. 18 IDLE ~ action type global summary softmax over action types CONTINUE ~ action type global summary softmax over action types ... GATHER ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (resource units) resource_unit_id global summary softmax over action types ... ATTACK ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (enemy units) enemy_unit_id global summary softmax over action types TRAIN_UNIT ~ dot-product attention unit/instruction features action type input features (unit types) ~ softmax over action output action output unit type ... global summary softmax over action types ... BUILD_BUILDING ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (map cells) input features (unit types) x,y ~ softmax over action output action output unit type ... global summary softmax over action types ... MOVE ~ ~ softmax over action output action output dot-product attention unit/instruction features action type input features (map cells) x,y global summary softmax over action types Figure 7: Separate classiﬁers for each of the available action types. 19", "meta": {"paper_id": "1906.00744v5", "chunk_index": 19, "filename": "1906.00744v5.pdf", "chunk_length": 314}}
{"id": "1906.00790v2_chunk_0", "text": "Multi-task Pairwise Neural Ranking for Hashtag Segmentation Mounica Maddela1, Wei Xu1, Daniel Preot¸iuc-Pietro2 1 Department of Computer Science and Engineering, The Ohio State University 2 Bloomberg LP {maddela.4, xu.1265}@osu.edu dpreotiucpie@bloomberg.net Abstract Hashtags are often employed on social me- dia and beyond to add metadata to a tex- tual utterance with the goal of increasing dis- coverability, aiding search, or providing addi- tional semantics. However, the semantic con- tent of hashtags is not straightforward to infer as these represent ad-hoc conventions which frequently include multiple words joined to- gether and can include abbreviations and un- orthodox spellings. We build a dataset of 12,594 hashtags split into individual segments and propose a set of approaches for hash- tag segmentation by framing it as a pairwise ranking problem between candidate segmen- tations.1 Our novel neural approaches demon- strate 24.6% error reduction in hashtag seg- mentation accuracy compared to the current state-of-the-art method. Finally, we demon- strate that a deeper understanding of hash- tag semantics obtained through segmentation is useful for downstream applications such as sentiment analysis, for which we achieved a 2.6% increase in average recall on the Se- mEval 2017 sentiment analysis dataset. 1 Introduction A hashtag is a keyphrase represented as a sequence of alphanumeric characters plus underscore, pre- ceded by the # symbol. Hashtags play a cen- tral role in online communication by providing a tool to categorize the millions of posts generated daily on Twitter, Instagram, etc. They are useful in search, tracking content about a certain topic (Berardi et al., 2011; Ozdikis et al., 2012), or dis- covering emerging trends (Sampson et al., 2016). Hashtags often carry very important informa- tion, such as emotion (Abdul-Mageed and Ungar, 1Our toolkit along with the code and data are pub- licly available at https://github.com/mounicam/ hashtag_master Type Single-token Multi-token Named-entity (33.0%) #lionhead #toyotaprius Events (14.8%) #oscars #ipv6summit Standard (43.6%) #snowfall #epicfall Non-standard (11.2%) #sayin #iloveu4eva Table 1: Examples of single- (47.1%) and multi-word hashtags (52.9%) and their categorizations based on a sample of our data. 2017), sentiment (Mohammad et al., 2013), sar- casm (Bamman and Smith, 2015), and named en- tities (Finin et al., 2010; Ritter et al., 2011). How- ever, inferring the semantics of hashtags is non- trivial since many hashtags contain multiple to- kens joined together, which frequently leads to multiple potential interpretations (e.g., lion head vs. lionhead). Table 1 shows several exam- ples of single- and multi-token hashtags. While most hashtags represent a mix of standard to- kens, named entities and event names are preva- lent and pose challenges to both human and auto- matic comprehension, as these are more likely to be rare tokens. Hashtags also tend to be shorter to allow fast typing, to attract attention or to sat- isfy length limitations imposed by some social me- dia platforms. Thus, they tend to contain a large number of abbreviations or non-standard spelling variations (e.g., #iloveu4eva) (Han and Baldwin, 2011; Eisenstein, 2013), which hinders their un- derstanding. The goal of our study is to build efﬁcient meth- ods for automatically splitting a hashtag into a meaningful word sequence. Our contributions", "meta": {"paper_id": "1906.00790v2", "chunk_index": 0, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_1", "text": "Thus, they tend to contain a large number of abbreviations or non-standard spelling variations (e.g., #iloveu4eva) (Han and Baldwin, 2011; Eisenstein, 2013), which hinders their un- derstanding. The goal of our study is to build efﬁcient meth- ods for automatically splitting a hashtag into a meaningful word sequence. Our contributions are: • A larger and better curated dataset for this task; • Framing the problem as pairwise ranking using novel neural approaches, in contrast to previous work which ignored the relative order of candi- date segmentations; • A multi-task learning method that uses differ- ent sets of features to handle different types of arXiv:1906.00790v2 [cs.CL] 13 Jun 2019 hashtags; • Experiments demonstrating that hashtag seg- mentation improves sentiment analysis on a benchmark dataset. Our new dataset includes segmentation for 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher qual- ity than the previous dataset of 1,108 hash- tags (Bansal et al., 2015). We frame the segmenta- tion task as a pairwise ranking problem, given a set of candidate segmentations. We build several neu- ral architectures using this problem formulation which use corpus-based, linguistic and thesaurus based features. We further propose a multi-task learning approach which jointly learns segment ranking and single- vs. multi-token hashtag clas- siﬁcation. The latter leads to an error reduction of 24.6% over the current state-of-the-art. Finally, we demonstrate the utility of our method by us- ing hashtag segmentation in the downstream task of sentiment analysis. Feeding the automatically segmented hashtags to a state-of-the-art sentiment analysis method on the SemEval 2017 benchmark dataset results in a 2.6% increase in the ofﬁcial metric for the task. 2 Background and Preliminaries Current approaches for hashtag segmentation can be broadly divided into three categories: (a) gaze- teer and rule based (Maynard and Greenwood, 2014; Declerck and Lendvai, 2015; Billal et al., 2016), (b) word boundary detection (C¸ elebi and ¨Ozg¨ur, 2017, 2016), and (c) ranking with lan- guage model and other features (Wang et al., 2011; Bansal et al., 2015; Berardi et al., 2011; Reuter et al., 2016; Simeon et al., 2016). Hashtag seg- mentation approaches draw upon work on com- pound splitting for languages such as German or Finnish (Koehn and Knight, 2003) and word seg- mentation (Peng and Schuurmans, 2001) for lan- guages with no spaces between words such as Chi- nese (Sproat and Shih, 1990; Xue and Shen, 2003). Similar to our work, Bansal et al. (2015) extract an initial set of candidate segmentations using a sliding window, then rerank them using a linear regression model trained on lexical, bigram and other corpus-based features. The current state-of- the-art approach (C¸ elebi and ¨Ozg¨ur, 2017, 2016) uses maximum entropy and CRF models with a combination of language model and hand-crafted features to predict if each character in the hashtag is the beginning of a new word. Generating Candidate Segmentations. Mi- crosoft Word Breaker (Wang et al., 2011) is, among the existing methods, a strong baseline for hashtag segmentation, as reported in C¸ elebi and ¨Ozg¨ur (2017) and Bansal et al. (2015). It employs a beam", "meta": {"paper_id": "1906.00790v2", "chunk_index": 1, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_2", "text": "character in the hashtag is the beginning of a new word. Generating Candidate Segmentations. Mi- crosoft Word Breaker (Wang et al., 2011) is, among the existing methods, a strong baseline for hashtag segmentation, as reported in C¸ elebi and ¨Ozg¨ur (2017) and Bansal et al. (2015). It employs a beam search algorithm to extract k best segmen- tations as ranked by the n-gram language model probability: ScoreLM(s) = n X i=1 log P(wi|wi−N+1 . . . wi−1) where [w1, w2 . . . wn] is the word sequence of seg- mentation s and N is the window size. More sophisticated ranking strategies, such as Bino- mial and word length distribution based ranking, did not lead to a further improvement in perfor- mance (Wang et al., 2011). The original Word Breaker was designed for segmenting URLs using language models trained on web data. In this pa- per, we reimplemented2 and tailored this approach to segmenting hashtags by using a language model speciﬁcally trained on Twitter data (implementa- tion details in §3.6). The performance of this method itself is competitive with state-of-the-art methods (evaluation results in §5.3). Our proposed pairwise ranking method will effectively take the top k segmentations generated by this baseline as candidates for reranking. However, in prior work, the ranking scores of each segmentation were calculated independently, ignoring the relative order among the top k can- didate segmentations. To address this limitation, we utilize a pairwise ranking strategy for the ﬁrst time for this task and propose neural architectures to model this. 3 Multi-task Pairwise Neural Ranking We propose a multi-task pairwise neural ranking approach to better incorporate and distinguish the relative order between the candidate segmenta- tions of a given hashtag. Our model adapts to ad- dress single- and multi-token hashtags differently via a multi-task learning strategy without requir- ing additional annotations. In this section, we de- scribe the task setup and three variants of pairwise neural ranking models (Figure 1). 2To the best of our knowledge, Microsoft discontinued its Word Breaker and Web Ngram API services in early 2018. hashtag (h) #songsonghaddaﬁsitunes segmentation (s∗) songs on ghaddaﬁs itunes (i.e. songs on Ghaddaﬁ’s iTunes) candidate segmentations (s ∈S) songs on ghaddaﬁs itunes songs on ghaddaﬁsi tunes songs on ghaddaf is itunes song song haddaﬁs i tunes songsong haddaﬁsitunes (and . . . ) Table 2: Example hashtag along with its gold and pos- sible candidate segmentations. 3.1 Segmentation as Pairwise Ranking The goal of hashtag segmentation is to divide a given hashtag h into a sequence of meaningful words s∗= [w1, w2, . . . , wn]. For a hashtag of r characters, there are a total of 2r−1 possible seg- mentations but only one, or occasionally two, of them (s∗) are considered correct (Table 2). We transform this task into a pairwise rank- ing problem: given k candidate segmentations {s1, s2, . . . , sk}, we rank them by comparing each with the rest in a pairwise manner. More speciﬁ- cally, we train a model to predict a real number g(sa, sb) for any two candidate segmentations sa", "meta": {"paper_id": "1906.00790v2", "chunk_index": 2, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_3", "text": "into a pairwise rank- ing problem: given k candidate segmentations {s1, s2, . . . , sk}, we rank them by comparing each with the rest in a pairwise manner. More speciﬁ- cally, we train a model to predict a real number g(sa, sb) for any two candidate segmentations sa and sb of hashtag h, which indicates sa is a better segmentation than sb if positive, and vice versa. To quantify the quality of a segmentation in training, we deﬁne a gold scoring function g∗based on the similarities with the ground-truth segmentation s∗: g∗(sa, sb) = sim(sa, s∗) −sim(sb, s∗). We use the Levenshtein distance (minimum num- ber of single-character edits) in this paper, al- though it is possible to use other similarity mea- surements as alternatives. We use the top k seg- mentations generated by Microsoft Word Breaker (§2) as initial candidates. 3.2 Pairwise Neural Ranking Model For an input candidate segmentation pair ⟨sa, sb⟩, we concatenate their feature vectors sa and sb, and feed them into a feedforward network which emits a comparison score g(sa, sb). The feature vector sa or sb consists of language model probabilities using Good-Turing (Good, 1953) and modiﬁed Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999), lexical and linguistic features (more details in §3.5). For training, we use all the possible pairs ⟨sa, sb⟩of the k candi- dates as the input and their gold scores g∗(sa, sb) as the target. The training objective is to minimize the Mean Squared Error (MSE): LMSE = 1 m m X i=1 (g∗(i)(sa, sb) −ˆg(i)(sa, sb))2 (1) where m is the number of training examples. To aggregate the pairwise comparisons, we fol- low a greedy algorithm proposed by Cohen et al. (1998) and used for preference ranking (Parakhin and Haluptzok, 2009). For each segmentation s in the candidate set S = {s1, s2, . . . , sk}, we calculate a single score ScorePNR(s) = P s̸=sj∈S g(s, sj), and ﬁnd the segmentation smax corresponding to the highest score. We repeat the same procedure after removing smax from S, and continue until S reduces to an empty set. Fig- ure 1(a) shows the architecture of this model. 3.3 Margin Ranking (MR) Loss As an alternative to the pairwise ranker (§3.2), we propose a pairwise model which learns from can- didate pairs ⟨sa, sb⟩but ranks each individual can- didate directly rather than relatively. We deﬁne a new scoring function g′ which assigns a higher score to the better candidate, i.e., g′(sa) > g′(sb), if sa is a better candidate than sb and vice-versa. Instead of concatenating the features vectors sa and sb, we feed them separately into two identi- cal feedforward networks with shared parameters. During testing, we use only one of the networks to rank the candidates based on the g′ scores. For training, we add a ranking layer on top of the net- works to measure the violations in the ranking or- der and minimize the Margin Ranking Loss (MR): LMR = 1 m m X i=1 max(0, 1 −l(i) ab p(i) ab ) p(i) ab", "meta": {"paper_id": "1906.00790v2", "chunk_index": 3, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_4", "text": "based on the g′ scores. For training, we add a ranking layer on top of the net- works to measure the violations in the ranking or- der and minimize the Margin Ranking Loss (MR): LMR = 1 m m X i=1 max(0, 1 −l(i) ab p(i) ab ) p(i) ab = (ˆg′(i)(sa) −ˆg′(i)(sb)) lab =    1 g∗(sa, sb) > 0 −1 g∗(sa, sb) < 0 0 otherwise (2) where m is the number of training samples. The architecture of this model is presented in Fig- ure 1(b). 3.4 Adaptive Multi-task Learning Both models in §3.2 and §3.3 treat all the hashtags uniformly. However, different features address different types of hashtags. By design, the lin- guistic features capture named entities and multi- word hashtags that exhibit word shape patterns, (a) Pairwise Ranking Model (MSE §3.2) (b) Margin Ranking Loss w/ shared parameters (MR §3.3) (c) Adaptive Multi-task Learning for Pairwise ranking (MSE+Multitask §3.4) Figure 1: Pairwise neural ranking models for hashtag segmentation. Given two candidate segmentations sa and sb of hashtag h, the goal is to predict the segmentation’s goodness relative score (g) or absolute (g′) score. such as camel case. The ngram probabilities with Good-Turing smoothing gravitate towards multi- word segmentations with known words, as its es- timate for unseen ngrams depends on the frac- tion of ngrams seen once which can be very low (Heaﬁeld, 2013). The modiﬁed Kneser-Ney smoothing is more likely to favor segmentations that contain rare words, and single-word segmen- tations in particular. Please refer to §5.3 for a more detailed quantitative and qualitative analysis. To leverage this intuition, we introduce a binary classiﬁcation task to help the model differentiate single-word from multi-word hashtags. The bi- nary classiﬁer takes hashtag features h as the in- put and outputs wh, which represents the prob- ability of h being a multi-word hashtag. wh is used as an adaptive gating value in our multi- task learning setup. The gold labels for this task are obtained at no extra cost by simply verifying whether the ground-truth segmentation has mul- tiple words. We train the pairwise segmentation ranker and the binary single- vs. multi-token hash- tag classiﬁer jointly, by minimizing LMSE for the pairwise ranker and the Binary Cross Entropy Er- ror (LBCE) for the classiﬁer: Lmultitask = λ1LMSE + λ2LBCE LBCE = −1 m m X i=1 \u0002 l(i) ∗log(w(i) h )+ (1 −l(i)) ∗log(1 −w(i) h ) \u0003 (3) where wh is the adaptive gating value, l ∈{0, 1} indicates if h is actually a multi-word hashtag and m is the number of training examples. λ1 and λ2 are the weights for each loss. For our experiments, we apply equal weights. More speciﬁcally, we divide the segmentation feature vector sa into two subsets: (a) sKN a with modiﬁed Kneser-Ney smoothing features, and (b) sGL a with Good-Turing smoothing and linguistic features. For an input candidate segmentation pair ⟨sa, sb⟩, we construct two pairwise vectors sKN ab = [sKN a ; sKN b ] and sGL ab = [sGL a ; sGL b ] by concate- nation, then combine", "meta": {"paper_id": "1906.00790v2", "chunk_index": 4, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_5", "text": "Kneser-Ney smoothing features, and (b) sGL a with Good-Turing smoothing and linguistic features. For an input candidate segmentation pair ⟨sa, sb⟩, we construct two pairwise vectors sKN ab = [sKN a ; sKN b ] and sGL ab = [sGL a ; sGL b ] by concate- nation, then combine them based on the adaptive gating value wh before feeding them into the feed- forward network G for pairwise ranking: ˆg(sa, sb) = G \u0000whsGL ab + (1 −wh)sKN ab \u0001 (4) We use summation with padding, as we ﬁnd this simple ensemble method achieves similar perfor- mance in our experiments as the more complex multi-column networks (Ciresan et al., 2012). Fig- ure 1(c) shows the architecture of this model. An analogue multi-task formulation can also be used for the Margin Ranking loss as: Lmultitask = λ1LMR + λ2LBCE. (5) 3.5 Features We use a combination of corpus-based and lin- guistic features to rank the segmentations. For a candidate segmentation s, its feature vector s in- cludes the number of words in the candidate, the length of each word, the proportion of words in an English dictionary3 or Urban Dictionary4 (Nguyen et al., 2018), ngram counts from Google Web 1TB corpus (Brants and Franz, 2006), and ngram prob- abilities from trigram language models trained on the Gigaword corpus (Graff and Cieri, 2003) and 3https://pypi.org/project/pyenchant 4https://www.urbandictionary.com 1.1 billion English tweets from 2010, respectively. We train two language models on each corpus: one with Good-Turing smoothing using SRILM (Stol- cke, 2002) and the other with modiﬁed Kneser- Ney smoothing using KenLM (Heaﬁeld, 2011). We also add boolean features, such as if the can- didate is a named-entity present in the list of Wikipedia titles, and if the candidate segmentation s and its corresponding hashtag h satisfy certain word-shapes (more details in appendix A.1). Similarly, for hashtag h, we extract the feature vector h consisting of hashtag length, ngram count of the hashtag in Google 1TB corpus (Brants and Franz, 2006), and boolean features indicating if the hashtag is in an English dictionary or Urban Dictionary, is a named-entity, is in camel case, ends with a number, and has all the letters as con- sonants. We also include features of the best- ranked candidate by the Word Breaker model. 3.6 Implementation Details We use the PyTorch framework to implement our multi-task pairwise ranking model. The pairwise ranker consists of an input layer, three hidden lay- ers with eight nodes in each layer and hyperbolic tangent (tanh) activation, and a single linear out- put node. The auxiliary classiﬁer consists of an input layer, one hidden layer with eight nodes and one output node with sigmoid activation. We use the Adam algorithm (Kingma and Ba, 2014) for optimization and apply a dropout of 0.5 to prevent overﬁtting. We set the learning rate to 0.01 and 0.05 for the pairwise ranker and auxiliary classi- ﬁer respectively. For each experiment, we report results obtained after 100 epochs. For the baseline model used to extract the k initial candidates, we reimplementated the Word Breaker (Wang et al., 2011) as described", "meta": {"paper_id": "1906.00790v2", "chunk_index": 5, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_6", "text": "We set the learning rate to 0.01 and 0.05 for the pairwise ranker and auxiliary classi- ﬁer respectively. For each experiment, we report results obtained after 100 epochs. For the baseline model used to extract the k initial candidates, we reimplementated the Word Breaker (Wang et al., 2011) as described in §2 and adapted it to use a language model trained on 1.1 billion tweets with Good-Turing smoothing using SRILM (Stolcke, 2002) to give a better perfor- mance in segmenting hashtags (§5.3). For all our experiments, we set k = 10. 4 Hashtag Segmentation Data We use two datasets for experiments (Table 3): (a) STANsmall, created by Bansal et al. (2015), which consists of 1,108 unique English hashtags from 1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset (Go and Huang, 2009) along with their crowdsourced segmentations and Data num. of Hashtags avg. avg. (multi-token%) #char #word Train 2518 (51.9%) 8.5 1.8 STANlarge Dev 629 (52.3%) 8.4 1.7 Test 9447 (53.0%) 8.6 1.8 STANsmall Test 1108 (60.5%) 9.0 1.9 Table 3: Statistics of the STANsmall and STANlarge datasets – number of unique hashtags, percentage of multi-token hashtags, average length of hashtags in characters and words. our additional corrections; and (b) STANlarge, our new expert curated dataset, which includes all 12,594 unique English hashtags and their associ- ated tweets from the same Stanford dataset. Dataset Analysis. STANsmall is the most com- monly used dataset in previous work. However, after reexamination, we found annotation errors in 6.8%5 of the hashtags in this dataset, which is sig- niﬁcant given that the error rate of the state-of-the- art models is only around 10%. Most of the er- rors were related to named entities. For example, #lionhead, which refers to the “Lionhead” video game company, was labeled as “lion head”. Our Dataset. We therefore constructed the STANlarge dataset of 12,594 hashtags with addi- tional quality control for human annotations. We displayed a tweet with one highlighted hashtag on the Figure-Eight6 (previously known as Crowd- Flower) crowdsourcing platform and asked two workers to list all the possible segmentations. For quality control on the platform, we displayed a test hashtag in every page along with the other hash- tags. If any annotator missed more than 20% of the test hashtags, then they were not allowed to con- tinue work on the task. For 93.1% of the hashtags, out of which 46.6% were single-token, the work- ers agreed on the same segmentation. We further asked three in-house annotators (not authors) to cross-check the crowdsourced annotations using a two-step procedure: ﬁrst, verify if the hashtag is a named entity based on the context of the tweet; then search on Google to ﬁnd the correct segmen- tation(s). We also asked the same annotators to ﬁx the errors in STANsmall. The human upperbound of the task is estimated at ∼98% accuracy, where we consider the crowdsourced segmentations (two workers merged) as correct if at least one of them matches with our expert’s segmentations. 5More speciﬁcally, 4.8% hashtags is missing one of the two acceptable segmentations and another 2.0% is incorrect segmentation. 6https://figure-eight.com", "meta": {"paper_id": "1906.00790v2", "chunk_index": 6, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_7", "text": "upperbound of the task is estimated at ∼98% accuracy, where we consider the crowdsourced segmentations (two workers merged) as correct if at least one of them matches with our expert’s segmentations. 5More speciﬁcally, 4.8% hashtags is missing one of the two acceptable segmentations and another 2.0% is incorrect segmentation. 6https://figure-eight.com All Hashtags Multi-token Single-token A@1 F1@1 A@2 MRR A@1 F1@1 A@2 MRR A@1 A@2 MRR Original hashtag 51.0 51.0 – – 19.1 19.1 – – 100.0 – – Rule-based (Billal et al., 2016) 58.1 63.5 – – 57.6 66.5 – – 58.8 – – GATE Hashtag Tokenizer (M&G, 2014) 73.2 77.2 – – 71.4 78.0 – – 76.0 – – Viterbi (Berardi et al., 2011) 73.4 78.5 – – 74.5 83.1 – – 71.6 – – MaxEnt (C¸ elebi and ¨Ozg¨ur, 2017) 92.4 93.4 – – 91.9 93.6 – – 93.1 – – Word Breaker w/ Twitter LM 90.8 91.7 97.4 94.5 88.5 90.0 97.8 93.7 94.3 96.8 95.7 Pairwise linear ranker 88.1 89.9 97.2 93.1 83.8 86.8 97.3 91.3 94.7 97.0 95.9 Pairwise neural ranker (MR) 92.3 93.5 98.2 95.4 90.9 92.8 99.0 95.2 94.5 96.9 95.8 Pairwise neural ranker (MSE) 92.5 93.7 98.2 95.5 91.2 93.1 99.0 95.4 94.5 97.0 95.8 Pairwise neural ranker (MR+multitask) 93.0 94.3 97.8 95.7 91.5 93.7 98.7 95.4 95.2 96.6 96.0 Pairwise neural ranker (MSE+multitask) 94.5 95.2 98.4 96.6 93.9 95.1 99.4 96.8 95.4 96.8 96.2 Human Upperbound 98.0 98.3 – – 97.8 98.2 – – 98.4 – – Table 4: Evaluation results on the corrected version of STANsmall. For reference, on the original version of STANsmall, the Microsoft Word Breaker API reported an 84.6% F1 score and an 83.6% accuracy for the top one output (C¸ elebi and ¨Ozg¨ur, 2017), while our best model (MSE+multitask) reported 89.8% F1 and 91.0% accuracy. All Hashtags Multi-token Single-token A@1 F1@1 A@2 MRR A@1 F1@1 A@2 MRR A@1 A@2 MRR Original hashtag 55.5 55.5 – – 16.2 16.2 – – 100.0 – – Rule-based (Billal et al., 2016) 56.1 61.5 – – 56.0 65.8 – – 56.3 – – Viterbi (Berardi et al., 2011) 68.4 73.8 – – 71.2 81.5 – – 65.0 – – GATE Hashtag Tokenizer (M&G, 2014) 72.4 76.1 – – 70.0 76.8 – – 75.3 – – MaxEnt (C¸ elebi and ¨Ozg¨ur, 2017) 91.2 92.3 – – 90.2 92.4 – – 92.3 – – Word Breaker w/ Twitter LM 90.1 91.0 96.6 93.9 88.5 90.0 97.0 93.4 91.9 96.2 94.4 Pairwise linear ranker 89.2 91.1 96.3 93.3 84.2 87.8 95.6 91.0 94.8 97.0 95.9 Pairwise neural ranker (MR) 91.3 92.6 97.2 94.6 89.9 92.4 97.5 94.3 92.8 96.8 94.9 Pairwise neural ranker (MSE) 91.3 92.6 97.0 94.5 91.0 93.6 97.7 94.9 91.5 96.2 94.1 Pairwise neural ranker (MR+multitask) 91.4 92.7 97.2 94.6 90.0 92.6 97.7 94.4 92.9 96.6 94.9 Pairwise neural ranker (MSE+multitask) 92.4 93.6 97.3 95.2 91.9 94.1 98.0 95.4 93.0 96.5 94.9 Human Upperbound 98.6 98.8 – – 98.0 98.4 – – 99.2 – – Table 5: Evaluation results on our STANlarge test dataset. For single-token hashtags, the token-level F1@1 is equivalent", "meta": {"paper_id": "1906.00790v2", "chunk_index": 7, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_8", "text": "97.7 94.4 92.9 96.6 94.9 Pairwise neural ranker (MSE+multitask) 92.4 93.6 97.3 95.2 91.9 94.1 98.0 95.4 93.0 96.5 94.9 Human Upperbound 98.6 98.8 – – 98.0 98.4 – – 99.2 – – Table 5: Evaluation results on our STANlarge test dataset. For single-token hashtags, the token-level F1@1 is equivalent to segmentation-level A@1. For multi-token cases, A@1 and F1@1 for the original hashtag base- line are non-zero because 11.4% of the hashtags have more than one acceptable segmentations. Our best model (MSE+multitask) shows a statistically signiﬁcant improvement (p < 0.05) over the state-of-the-art approach (C¸ elebi and ¨Ozg¨ur, 2017) based on the paired bootstrap test (Berg-Kirkpatrick et al., 2012). 5 Experiments In this section, we present experimental results that compare our proposed method with the other state-of-the-art approaches on hashtag segmenta- tion datasets. The next section will show exper- iments of applying hashtag segmentation to the popular task of sentiment analysis. 5.1 Existing Methods We compare our pairwise neural ranker with the following baseline and state-of-the-art ap- proaches: (a) The original hashtag as a single token; (b) A rule-based segmenter, which employs a set of word-shape rules with an English dictionary (Billal et al., 2016); (c) A Viterbi model which uses word frequencies from a book corpus7 (Berardi et al., 2011); 7Project Gutenberg http://norvig.com/big.txt (d) The specially developed GATE Hashtag To- kenizer from the open source toolkit,8 which combines dictionaries and gazetteers in a Viterbi-like algorithm (Maynard and Green- wood, 2014); (e) A maximum entropy classiﬁer (MaxEnt) trained on the STANlarge training dataset. It predicts whether a space should be inserted at each position in the hashtag and is the current state-of-the-art (C¸ elebi and ¨Ozg¨ur, 2017); (f) Our reimplementation of the Word Breaker algorithm which uses beam search and a Twit- ter ngram language model (Wang et al., 2011); (g) A pairwise linear ranker which we im- plemented for comparison purposes with the same features as our neural model, but using perceptron as the underlying classiﬁer (Hop- kins and May, 2011) and minimizing the hinge 8https://gate.ac.uk/ Single Multi All A MRR A MRR A MRR Kneser-Ney 95.4 95.7 56.0 75.3 74.9 85.1 Good-Turing (GT) 91.4 93.5 85.9 91.8 88.6 92.6 Linguistic (Ling) 89.4 91.7 71.6 82.6 80.1 87.0 GT + Ling 92.4 93.9 86.2 92.3 88.9 92.7 All Features 91.1 93.1 89.0 93.7 90.0 93.4 Table 6: Evaluation of automatic hashtag segmentation (MSE) with different features on the STANlarge dev set. A denotes accuracy@1. While Kneser-Ney features perform well on single-token hashtags, GT+Ling fea- tures perform better on multi-token hashtags. loss between g∗and a scoring function similar to g′. It is trained on the STANlarge dataset. 5.2 Evaluation Metrics We evaluate the performance by the top k (k = 1, 2) accuracy (A@1, A@2), average token-level F1 score (F1@1), and mean reciprocal rank (MRR). In particular, the accuracy and MRR are calculated at the segmentation-level, which means that an output segmentation is considered correct if and only if it fully matches the human segmen- tation. Average token-level F1 score accounts for partially correct segmentation in the multi-token hashtag cases. 5.3 Results", "meta": {"paper_id": "1906.00790v2", "chunk_index": 8, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_9", "text": "reciprocal rank (MRR). In particular, the accuracy and MRR are calculated at the segmentation-level, which means that an output segmentation is considered correct if and only if it fully matches the human segmen- tation. Average token-level F1 score accounts for partially correct segmentation in the multi-token hashtag cases. 5.3 Results Tables 4 and 5 show the results on the STANsmall and STANlarge datasets, respectively. All of our pairwise neural rankers are trained on the 2,518 manually segmented hashtags in the train- ing set of STANlarge and perform favorably against other state-of-the-art approaches. Our best model (MSE+multitask) that utilizes different features adaptively via a multi-task learning procedure is shown to perform better than simply combining all the features together (MR and MSE). We high- light the 24.6% error reduction on STANsmall and 16.5% on STANlarge of our approach over the previous SOTA (C¸ elebi and ¨Ozg¨ur, 2017) on the Multi-token hashtags, and the importance of hav- ing a separate evaluation of multi-word cases as it is trivial to obtain 100% accuracy for Single- token hashtags. While our hashtag segmentation model is achieving a very high accuracy@2, to be practically useful, it remains a challenge to get the top one predication exactly correct. Some hash- tags are very difﬁcult to interpret, e.g., #BTVSMB refers to the Social Media Breakfast (SMB) in Burlington, Vermont (BTV). The improved Word Breaker with our addition of a Twitter-speciﬁc language model is a very strong Kneser-Ney Good-Turing Linguistic count Example Hashtags ◦ ◦ ◦ 31 #omnomnom #BTVSMB • ◦ ◦ 13 #commbank #mamapedia ◦ • ◦ 38 #wewantmcﬂy #winebarsf ◦ ◦ • 24 #cfp09 #TechLunchSouth • • ◦ 44 #twittographers #bringback • ◦ • 16 #iccw #ecom09 ◦ • • 53 #LetsGoPens #epicwin • • • 420 #prototype #newyork Table 7: Error (◦) and correct (•) segmentation anal- ysis of three pairwise ranking models (MSE) trained with different feature sets Each row corresponds to one area in the Venn diagram; for example, ◦◦◦is the set of hashtags that all three models failed in the STANlarge dev data and •◦◦is the set of hashtags that only the model with Kneser-Ney language model features (but not the other two models) segmented correctly. baseline, which echos the ﬁndings of the origi- nal Word Breaker paper (Wang et al., 2011) that having a large in-domain language model is ex- tremely helpful for word segmentation tasks. It is worth noting that the other state-of-the-art system (C¸ elebi and ¨Ozg¨ur, 2017) also utilized a 4-gram language model trained on 476 million tweets from 2009. 5.4 Analysis and Discussion Feature Analysis. To empirically illustrate the effectiveness of different features on different types of hashtags, we show the results for mod- els using individual feature sets in pairwise rank- ing models (MSE) in Table 6. Language mod- els with modiﬁed Kneser-Ney smoothing perform best on single-token hashtags, while Good-Turing and Linguistic features work best on multi-token hashtags, conﬁrming our intuition about their use- fulness in a multi-task learning approach. Table 7 shows a qualitative analysis with the ﬁrst column (◦◦◦) indicating which features lead to correct or", "meta": {"paper_id": "1906.00790v2", "chunk_index": 9, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_10", "text": "els with modiﬁed Kneser-Ney smoothing perform best on single-token hashtags, while Good-Turing and Linguistic features work best on multi-token hashtags, conﬁrming our intuition about their use- fulness in a multi-task learning approach. Table 7 shows a qualitative analysis with the ﬁrst column (◦◦◦) indicating which features lead to correct or wrong segmentations, their count in our data and illustrative examples with human segmentation. Length of Hashtags. As expected, longer hashtags with more than three tokens pose greater challenges and the segmentation-level accuracy of our best model (MSE+multitask) drops to 82.1%. For many error cases, our model predicts a close-to-correct seg- mentation, e.g., #youknowyouupttooearly, #iseelondoniseefrance, which is also reﬂected by Type num. of Hashtags single 4426 (47.1%) 2 tokens 3436 (36.2%) 3 tokens 1085 (11.2%) 4 tokens 279 (2.9%) 5+ tokens 221 (2.6%) Figure 2: Token-level F1 scores (MSE+multitask) on hashtags of different lengths in the STANlarge test set. Figure 3: Token-level F1 scores of our pairwise ranker (MSE+multitask) and Word Breaker on the STANlarge test set, using language models trained with varying amounts of data. the higher token-level F1 scores across hashtags with different lengths (Figure 2). Size of the Language Model. Since our ap- proach heavily relies on building a Twitter lan- guage model, we experimented with its sizes and show the results in Figure 3. Our approach can perform well even with access to a smaller amount of tweets. The drop in F1 score for our pairwise neural ranker is only 1.4% and 3.9% when using the language models trained on 10% and 1% of the total 1.1 billion tweets, respectively. Time Sensitivity. Language use in Twitter changes with time (Eisenstein, 2013). Our pairwise ranker uses language models trained on the tweets from the year 2010. We tested our approach on a set of 500 random English hashtags posted in tweets from the year 2019 and show the results in Table 8. With a segmentation-level accuracy of 94.6% and average token-level F1 score of 95.6%, our approach performs favorably on 2019 hashtags. A@1 F1@1 MRR Word Breaker w/ Twitter LM 92.1 93.9 94.7 Pairwise neural ranker (MSE+multitask) 94.6 95.6 96.7 Table 8: Evaluation results on 500 random hashtags from the year 2019. 6 Extrinsic Evaluation: Twitter Sentiment Analysis We attempt to demonstrate the effectiveness of our hashtag segmentation system by studying its impact on the task of sentiment analysis in Twitter (Pang et al., 2002; Nakov et al., 2016; Rosenthal et al., 2017). We use our best model (MSE+multitask), under the name HashtagMas- ter, in the following experiments. 6.1 Experimental Setup We compare the performance of the BiL- STM+Lex (Teng et al., 2016) sentiment analysis model under three conﬁgurations: (a) tweets with hashtags removed, (b) tweets with hashtags as sin- gle tokens excluding the # symbol, and (c) tweets with hashtags as segmented by our system, Hash- tagMaster. BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment (Tay et al., 2018). It learns a context-sensitive sentiment intensity score by leveraging a Twitter- based sentiment lexicon (Tang et al., 2014). We use the same settings as described by Teng", "meta": {"paper_id": "1906.00790v2", "chunk_index": 10, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_11", "text": "hashtags as segmented by our system, Hash- tagMaster. BiLSTM+Lex is a state-of-the-art open source system for predicting tweet-level sentiment (Tay et al., 2018). It learns a context-sensitive sentiment intensity score by leveraging a Twitter- based sentiment lexicon (Tang et al., 2014). We use the same settings as described by Teng et al. (2016) to train the model. We use the dataset from the Sentiment Analy- sis in Twitter shared task (subtask A) at SemEval 2017 (Rosenthal et al., 2017). 9 Given a tweet, the goal is to predict whether it expresses POSITIVE, NEGATIVE or NEUTRAL sentiment. The training and development sets consist of 49,669 tweets and we use 40,000 for training and the rest for devel- opment. There are a total of 12,284 tweets con- taining 12,128 hashtags in the SemEval 2017 test set, and our hashtag segmenter ended up splitting 6,975 of those hashtags present in 3,384 tweets. 6.2 Results and Analysis In Table 9, we report the results based on the 3,384 tweets where HashtagMaster predicted a split, as for the rest of tweets in the test set, the hashtag segmenter would neither improve nor worsen the sentiment prediction. Our hashtag seg- menter successfully improved the sentiment anal- ysis performance by 2% on average recall and FPN 1 comparing to having hashtags unsegmented. This improvement is seemingly small but decid- edly important for tweets where sentiment-related information is embedded in multi-word hashtags 9We did not use the Stanford Sentiment Analysis Dataset (Go and Huang, 2009), which was used to construct the STANsmall and STANlarge hashtag datasets, because of its noisy sentiment labels obtained using distant supervision. AvgR FP N 1 Acc Original tweets 61.7 60.0 58.7 −No Hashtags 60.2 58.8 54.2 + Single-word 62.3 60.3 58.6 + HashtagMaster 64.3 62.4 58.6 Table 9: Sentiment analysis evaluation on the 3384 tweets from SemEval 2017 test set using the BiL- STM+Lex method (Tang et al., 2014). Average re- call (AvgR) is the ofﬁcial metric of the SemEval task and is more reliable than accuracy (Acc). FP N 1 is the average F1 of positive and negative classes. Having the hashtags segmented by our system HashtagMaster (i.e., MSE+multitask) signiﬁcantly improves the senti- ment prediction than not (p < 0.05 for AvgR and FP N 1 against the single-word setup). and sentiment prediction would be incorrect based only on the text (see Table 10 for examples). In fact, 2,605 out of the 3,384 tweets have multi- word hashtags that contain words in the Twitter- based sentiment lexicon (Tang et al., 2014) and 125 tweets contain sentiment words only in the hashtags but not in the rest of the tweet. On the entire test set of 12,284 tweets, the increase in the average recall is 0.5%. 7 Other Related Work Automatic hashtag segmentation can improve the performance of many applications besides senti- ment analysis, such as text classiﬁcation (Billal et al., 2016), named entity linking (Bansal et al., 2015) and modeling user interests for recommen- dations (Chen et al., 2016). It can also help in col- lecting data of higher volume and quality by pro-", "meta": {"paper_id": "1906.00790v2", "chunk_index": 11, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_12", "text": "the performance of many applications besides senti- ment analysis, such as text classiﬁcation (Billal et al., 2016), named entity linking (Bansal et al., 2015) and modeling user interests for recommen- dations (Chen et al., 2016). It can also help in col- lecting data of higher volume and quality by pro- viding a more nuanced interpretation of its con- tent, as shown for emotion analysis (Qadir and Riloff, 2014), sarcasm and irony detection (May- nard and Greenwood, 2014; Huang et al., 2018). Better semantic analysis of hashtags can also po- tentially be applied to hashtag annotation (Wang et al., 2019), to improve distant supervision la- bels in training classiﬁers for tasks such as sar- casm (Bamman and Smith, 2015), sentiment (Mo- hammad et al., 2013), emotions (Abdul-Mageed and Ungar, 2017); and, more generally, as labels for pre-training representations of words (Weston et al., 2014), sentences (Dhingra et al., 2016), and images (Mahajan et al., 2018). 8 Conclusion We proposed a new pairwise neural ranking model for hashtag segmention and showed signiﬁcant performance improvements over the state-of-the- Ofcourse #clownshoes #altright #IllinoisNazis #FinallyAtpeaceWith people calling me “Kim Fatty the Third” Leslie Odom Jr. sang that. #ThankYouObama After some 4 months of vegetarianism .. it’s all the same industry. #cutoutthecrap Table 10: Sentiment analysis examples where our HashtagMaster segmentation tool helped. Red and blue words are negative and positive entries in the Twitter sentiment lexicon (Tang et al., 2014), respectively. art. We also constructed a larger and more curated dataset for analyzing and benchmarking hashtag segmentation methods. We demonstrated that hashtag segmentation helps with downstream tasks such as sentiment analysis. Although we fo- cused on English hashtags, our pairwise ranking approach is language-independent and we intend to extend our toolkit to languages other than En- glish as future work. Acknowledgments We thank Ohio Supercomputer Center (Center, 2012) for computing resources and the NVIDIA for providing GPU hardware. We thank Alan Rit- ter, Quanze Chen, Wang Ling, Pravar Mahajan, and Dushyanta Dhyani for valuable discussions. We also thank the annotators: Sarah Flanagan, Kaushik Mani, and Aswathnarayan Radhakrish- nan. This material is based in part on research sponsored by the NSF under grants IIS-1822754 and IIS-1755898, DARPA through the ARO under agreement number W911NF-17-C-0095, through a Figure-Eight (CrowdFlower) AI for Everyone Award and a Criteo Faculty Research Award to Wei Xu. The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing ofﬁcial policies or endorsements of the U.S. Government. References Muhammad Abdul-Mageed and Lyle Ungar. 2017. Emonet: Fine-grained emotion detection with gated recurrent neural networks. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics, ACL, pages 718–728. David Bamman and Noah A Smith. 2015. Contextu- alized Sarcasm Detection on Twitter. In Ninth Inter- national AAAI Conference on Web and Social Media, ICWSM, pages 574–577. Piyush Bansal, Romil Bansal, and Vasudeva Varma. 2015. Towards Deep Semantic Analysis of Hashtags. In Proceedings of the 37th European Conference on Information Retrieval, ECIR, pages 453–464. Giacomo Berardi, Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2011. ISTI@TREC Mi-", "meta": {"paper_id": "1906.00790v2", "chunk_index": 12, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_13", "text": "national AAAI Conference on Web and Social Media, ICWSM, pages 574–577. Piyush Bansal, Romil Bansal, and Vasudeva Varma. 2015. Towards Deep Semantic Analysis of Hashtags. In Proceedings of the 37th European Conference on Information Retrieval, ECIR, pages 453–464. Giacomo Berardi, Andrea Esuli, Diego Marcheggiani, and Fabrizio Sebastiani. 2011. ISTI@TREC Mi- croblog Track 2011: Exploring the Use of Hashtag Segmentation and Text Quality Ranking. In Text RE- trieval Conference (TREC). Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein. 2012. An Empirical Investigation of Statisti- cal Signiﬁcance in NLP. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natu- ral Language Processing and Computational Natural Language Learning, EMNLP-CoNLL, pages 995– 1005. Belainine Billal, Alexsandro Fonseca, and Fatiha Sa- dat. 2016. Named Entity Recognition and Hash- tag Decomposition to Improve the Classiﬁcation of Tweets. In Proceedings of the 2nd Workshop on Noisy User-generated Text (WNUT), COLING, pages 102–111. Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram Version 1. Linguistic Data Consortium (LDC). Arda C¸ elebi and Arzucan ¨Ozg¨ur. 2016. Segment- ing Hashtags using Automatically Created Training Data. In Proceedings of the Tenth International Conference on Language Resources and Evaluation, LREC, pages 2981–2985. Arda C¸ elebi and Arzucan ¨Ozg¨ur. 2017. Segmenting Hashtags and Analyzing Their Grammatical Struc- ture. Journal of Association For Information Science and Technology (JASIST), 69(5):675–686. Ohio Supercomputer Center. 2012. Oakley super- computer. http://osc.edu/ark:/19495/ hpc0cvqn. Stanley F Chen and Joshua Goodman. 1999. An empirical study of smoothing techniques for lan- guage modeling. Computer Speech & Language, 13(4):359–394. Tao Chen, Xiangnan He, and Min-Yen Kan. 2016. Context-aware Image Tweet Modelling and Recom- mendation. In Proceedings of the 24th ACM Interna- tional Conference on Multimedia, MM, pages 1018– 1027. Dan Ciresan, Ueli Meier, and J¨urgen Schmidhuber. 2012. Multi-column Deep Neural Networks for Im- age Classiﬁcation. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recog- nition, CVPR, pages 3642–3649. William W Cohen, Robert E Schapire, and Yoram Singer. 1998. Learning to Order Things. In Ad- vances in Neural Information Processing Systems, NIPS, pages 451–457. Thierry Declerck and Piroska Lendvai. 2015. Process- ing and normalizing hashtags. In Proceedings of the International Conference Recent Advances in Natu- ral Language Processing, RANLP, pages 104–109. Bhuwan Dhingra, Zhong Zhou, Dylan Fitzpatrick, Michael Muehl, and William Cohen. 2016. Tweet2Vec: Character-Based Distributed Rep- resentations for Social Media. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL, pages 269–274. Jacob Eisenstein. 2013. What to do about bad language on the Internet. In Proceedings of the 2013 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics, NAACL, pages 359–369. Tim Finin, Will Murnane, Anand Karandikar, Nicholas Keller, Justin Martineau, and Mark Dredze. 2010. Annotating named entities in Twitter data with crowdsourcing. In Proceedings of the Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, NAACL, pages 80–88. Bhayani R. Go, A. and L. Huang. 2009. Twitter Sentiment Classiﬁcation using Distant Supervision. CS224N Project Report, Stanford. Irving J Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264. David Graff and Christopher Cieri.", "meta": {"paper_id": "1906.00790v2", "chunk_index": 13, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_14", "text": "and Language Data with Amazon’s Mechanical Turk, NAACL, pages 80–88. Bhayani R. Go, A. and L. Huang. 2009. Twitter Sentiment Classiﬁcation using Distant Supervision. CS224N Project Report, Stanford. Irving J Good. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3-4):237–264. David Graff and Christopher Cieri. 2003. English Gi- gaword LDC2003T05. Linguistic Data Consortium (LDC). Bo Han and Timothy Baldwin. 2011. Lexical Normali- sation of Short Text Messages: Makn Sens a# twitter. In Proceedings of the 49th Annual Meeting of the As- sociation for Computational Linguistics, ACL, pages 368–378. Kenneth Heaﬁeld. 2011. KenLM: Faster and Smaller Language Model Queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, WMT, pages 187–197. Kenneth Heaﬁeld. 2013. Efﬁcient Language Modeling Algorithms with Applications to Statistical Machine Translation. Ph.D. thesis, Carnegie Mellon Univer- sity. Mark Hopkins and Jonathan May. 2011. Tuning as ranking. In Proceedings of the Conference on Em- pirical Methods in Natural Language Processing, EMNLP. Hen-Hsen Huang, Chiao-Chen Chen, and Hsin-Hsi Chen. 2018. Disambiguating false-alarm hashtag us- ages in tweets for irony detection. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics, ACL, pages 771–777. Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. In Proceed- ings of the 3rd International Conference for Learning Representations, ICLR. Reinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In Pro- ceedings of the 1995 International Conference on Acoustics, Speech, and Signal Processing, ICASSP, pages 181–184. Philipp Koehn and Kevin Knight. 2003. Empirical methods for compound splitting. In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics, EACL, pages 187–194. Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. 2018. Ex- ploring the Limits of Weakly Supervised Pretraining. In Tech Report. Diana Maynard and Mark A Greenwood. 2014. Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis. In Proceedings of the 9th International Conference on Language Re- sources and Evaluation, LREC, pages 4238–4243. Saif Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. NRC-Canada: Building the state-of-the- art in sentiment analysis of tweets. In Proceedings of the Seventh International Workshop on Semantic Evaluation, SemEval, pages 321–327. Preslav Nakov, Sara Rosenthal, Svetlana Kiritchenko, Saif M. Mohammad, Zornitsa Kozareva, Alan Ritter, Veselin Stoyanov, and Xiaodan Zhu. 2016. Develop- ing a successful SemEval task in sentiment analysis of Twitter and other social media texts. Language Resources and Evaluation, 50(1):35–65. Dong Nguyen, Barbara McGillivray, and Taha Yasseri. 2018. Emo, love and god: making sense of urban dictionary, a crowd-sourced online dictionary. Royal Society Open Science, 5(5):172320. Ozer Ozdikis, Pinar Senkul, and Halit Oguztuzun. 2012. Semantic Expansion of Hashtags for En- hanced Event Detection in Twitter. In Proceedings of the 1st international Workshop on Online Social Systems. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classiﬁcation using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing, EMNLP, pages 79–86. M. Parakhin and P. Haluptzok. 2009. Finding the Most Probable", "meta": {"paper_id": "1906.00790v2", "chunk_index": 14, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_15", "text": "the 1st international Workshop on Online Social Systems. Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment Classiﬁcation using Machine Learning Techniques. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing, EMNLP, pages 79–86. M. Parakhin and P. Haluptzok. 2009. Finding the Most Probable Rranking of Objects with Probabilistic Pair- wise Preferences. In Proceedings of the 10th In- ternational Conference on Document Analysis and Recognition, ICDAR, pages 616–620. Fuchun Peng and Dale Schuurmans. 2001. A hierarchi- cal em approach to word segmentation. In NLPRS, pages 475–480. Ashequl Qadir and Ellen Riloff. 2014. Learning emo- tion indicators from tweets: Hashtags, hashtag pat- terns, and phrases. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing, EMNLP, pages 1203–1209. Jack Reuter, Jhonata Pereira-Martins, and Jugal Kalita. 2016. Segmenting twitter hashtags. International Journal on Natural Language Computing, 5:23–36. Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named Entity Recognition in Tweets: An Experi- mental Study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1524–1534. Sara Rosenthal, Noura Farra, and Preslav Nakov. 2017. SemEval-2017 task 4: Sentiment Analysis in Twitter. In Proceedings of the 11th International Workshop on Semantic Evaluation, SemEval, pages 502–518. Justin Sampson, Fred Morstatter, Liang Wu, and Huan Liu. 2016. Leveraging the implicit structure within social media for emergent rumor detection. In Pro- ceedings of the 25th ACM International on Confer- ence on Information and Knowledge Management, CIKM, pages 2377–2382. C. Simeon, H. J. Hamilton, and R. J. Hilderman. 2016. Word segmentation algorithms with lexical resources for hashtag classiﬁcation. In Proceedings of the 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA), pages 743–751. Richard Sproat and Chilin Shih. 1990. A statistical method for ﬁnding word boundaries in chinese text. Computer Processing of Chinese and Oriental Lan- guages, 4(4):336–351. Andreas Stolcke. 2002. SRILM – An Extensible Lan- guage Modeling Toolkit. In Proceedings of the 7th International Conference on Spoken Language Pro- cessing, ICSLP, pages 901–904. Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and Ting Liu. 2014. Building Large-Scale Twitter-Speciﬁc Sentiment Lexicon : A Representation Learning Ap- proach. In Proceedings of the 25th International Conference on Computational Linguistics, COLING, pages 172–182. Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. 2018. Attentive gated lexicon reader with contrastive contextual co-attention for sentiment classiﬁcation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 3443–3453. Zhiyang Teng, Duy Tin Vo, and Yue Zhang. 2016. Context-Sensitive Lexicon Features for Neural Sen- timent Analysis. In Proceedings of the 2016 Con- ference on Empirical Methods in Natural Language Processing, EMNLP, pages 1629–1638. Kuansan Wang, Christopher Thrasher, and Bo- June Paul Hsu. 2011. Web Scale NLP: A Case Study on URL Word Breaking. In Proceedings of the 20th International Conference on World Wide Web, WWW, pages 357–366. Yue Wang, Jing Li, Irwin King, Michael R. Lyu, and Shuming Shi. 2019. Microblog Hashtag Generation via Encoding Conversation Contexts. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL). Jason Weston,", "meta": {"paper_id": "1906.00790v2", "chunk_index": 15, "filename": "1906.00790v2.pdf", "chunk_length": 512}}
{"id": "1906.00790v2_chunk_16", "text": "In Proceedings of the 20th International Conference on World Wide Web, WWW, pages 357–366. Yue Wang, Jing Li, Irwin King, Michael R. Lyu, and Shuming Shi. 2019. Microblog Hashtag Generation via Encoding Conversation Contexts. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL). Jason Weston, Sumit Chopra, and Keith Adams. 2014. # tagspace: Semantic embeddings from hashtags. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP, pages 1822–1827. Nianwen Xue and Libin Shen. 2003. Chinese word segmentation as LMR tagging. In Proceedings of the second SIGHAN workshop on Chinese Language Processing, SIGHAN, pages 176–179. A Appendix A.1 Word-shape rules Our model uses the following word shape rules as boolean features. If the candidate segmentation s and its corresponding hashtag h satisﬁes a word shape rule, then the boolean feature is set to True. Rule Hashtag →Segmentation Camel Case XxxXxx →Xxx+Xxx Consonants cccc →cccc Digits as preﬁx ddwwww →dd+wwww Digits as sufﬁx wwwwdd →wwww+dd Underscore www www →www + + www Table 11: Word-shape rule features used to identify good segmentations. Here, X and x represent capital- ized and non-capitalized alphabetic characters respec- tively, c denotes consonant, d denotes number and w denotes any alphabet or number.", "meta": {"paper_id": "1906.00790v2", "chunk_index": 16, "filename": "1906.00790v2.pdf", "chunk_length": 206}}
{"id": "1906.00872v1_chunk_0", "text": "From Words to Sentences: A Progressive Learning Approach for Zero-resource Machine Translation with Visual Pivots Shizhe Chen1∗, Qin Jin1† and Jianlong Fu2 1Renmin University of China, Beijing, P.R. China 2Microsoft Research Asia, Beijing, P.R. China {cszhe1, qjin}@ruc.edu.cn, jianf@microsoft.com Abstract The neural machine translation model has suffered from the lack of large-scale parallel corpora. In contrast, we humans can learn multi-lingual trans- lations even without parallel texts by referring our languages to the external world. To mimic such hu- man learning behavior, we employ images as pivots to enable zero-resource translation learning. How- ever, a picture tells a thousand words, which makes multi-lingual sentences pivoted by the same image noisy as mutual translations and thus hinders the translation model learning. In this work, we pro- pose a progressive learning approach for image- pivoted zero-resource machine translation. Since words are less diverse when grounded in the image, we ﬁrst learn word-level translation with image piv- ots, and then progress to learn the sentence-level translation by utilizing the learned word translation to suppress noises in image-pivoted multi-lingual sentences. Experimental results on two widely used image-pivot translation datasets, IAPR-TC12 and Multi30k, show that the proposed approach signiﬁ- cantly outperforms other state-of-the-art methods. 1 Introduction The recent success of neural machine translation (NMT) [Bahdanau et al., 2015] has greatly beneﬁted from large-scale high-quality parallel corpora. However, such NMT models are data-hungry and perform poorly without sufﬁcient par- allel data [Zoph et al., 2016]. Due to the high expense of collecting parallel texts, more and more researchers are pay- ing attention to develop NMT models under the zero-resource condition where no parallel source-target texts are available. Inspired by how we humans learn a novel language when no parallel texts are available, for example connecting sen- tences in two languages that describe the same image, re- searchers have proposed to employ images as pivots for zero-resource machine translation, which can beneﬁt from ∗This work was performed when Shizhe Chen was visiting Mi- crosoft Research Asia as a research intern. †Qin Jin is the corresponding author. Figure 1: A picture tells a thousand words - illustration of the chal- lenge in image-pivoted zero-resource machine translation. Only few words in red are correct translations even in groundtruth captions of the same image. Captioning mistakes in blue can further make such multi-lingual captions noisy as mutual translation. We translate Ger- man captions in brackets for non-German readers. abundant images with mono-lingual descriptions on the In- ternet [Sharma et al., 2018]. The shared principle in pre- vious image-pivoted works [Nakayama and Nishida, 2017; Lee et al., 2018; Chen et al., 2018] assumes that the source sentence and the target sentence are semantically equivalent as they are describing the same image. However, due to the one-to-many relationship between im- ages and captions (as known as “a picture tells a thousand words”), multi-lingual sentences describing the same image are not necessarily good mutual translations. As shown in Figure 1, although the captions in English and German both accurately describe the image, they capture different aspects in the image and only few words in the captions", "meta": {"paper_id": "1906.00872v1", "chunk_index": 0, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_1", "text": "as “a picture tells a thousand words”), multi-lingual sentences describing the same image are not necessarily good mutual translations. As shown in Figure 1, although the captions in English and German both accurately describe the image, they capture different aspects in the image and only few words in the captions are cor- rect mutual translations. Moreover, since images with multi- lingual captions are hard to obtain in realistic settings, an image caption model is usually adopted to generate multi- lingual caption sentences for the image. However, due to the imperfection of caption models, the semantic discrepancy of generated multi-lingual captions could be even larger. In this work, we propose a progressive learning approach to overcome above challenges in the image-pivoted zero- resource machine translation. We propose to learn the transla- tion in an easy-to-advanced progressive way, by ﬁrstly grasp- ing the word-level translation with the help of image pivots and then progressing to learn more challenging sentence-level translation with the assistance of word translation and im- age pivots. To be speciﬁc, since words grounded in certain regions of the image are less diverse than sentences, word- level translation can be more effectively learned based on im- age pivots. The multi-lingual word representations learned arXiv:1906.00872v1 [cs.CL] 3 Jun 2019 from the word translation and the image pivots altogether are used to enhance the sentence translation from two as- pects: i) suppressing noises in image-pivoted multi-lingual sentences via re-weighting sentences at ﬁne-grained token- level; ii) supporting the learning of language-agnostic sen- tence representation for cross language decoding via auto- encoding. The two aspects are complementary to train the NMT model. We carry out extensive experiments on two benchmark image-pivot machine translation datasets: IAPR- TC12 and Multi30k. Our proposed approach signiﬁcantly outperforms other state-of-the-art image-pivot methods. 2 Related Work The encoder-decoder based neural machine translation (NMT) model [Cho et al., 2014; Bahdanau et al., 2015] has achieved great success in recent years. However, it requires large-scale parallel texts for training, which performs poorly without sufﬁcient data [Zoph et al., 2016; Castilho et al., 2017]. There are mainly three types of methods to avoid the reliance on source-target parallel data, namely third-language pivot, mono-lingual based and visual pivot methods. The third-language pivot methods [Johnson et al., 2017; Chen et al., 2017] demand the source-to-pivot and pivot-to- target parallel corpus to enable zero-resource translation from source to target. [Johnson et al., 2017] trains a universal encoder-decoder with multiple language pairs, which can per- form well in novel language combinations. However, it is not trivial to obtain the pivot language parallel data. The recent mono-lingual based methods [Artetxe et al., 2018; Lample et al., 2018a] only utilize large-scale mono- lingual corpora for translation. [Lample et al., 2018b] sum- marizes three key elements for mono-lingual based meth- ods: careful initialization, strong language model, and back- translation, which achieved promising results for both phrase- based and NMT models. The reason we use image-pivot for translation is that images can help reduce ambiguity of texts especially for visual-related sentences such as commercial product descriptions [Zhou et al., 2018; Calixto et al.,", "meta": {"paper_id": "1906.00872v1", "chunk_index": 1, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_2", "text": "initialization, strong language model, and back- translation, which achieved promising results for both phrase- based and NMT models. The reason we use image-pivot for translation is that images can help reduce ambiguity of texts especially for visual-related sentences such as commercial product descriptions [Zhou et al., 2018; Calixto et al., 2017]. The image-pivot approaches leverage images to connect unpaired source and target languages. [Kiela et al., 2015; Chen et al., 2019] have shown the effectiveness of image piv- ots for bilingual lexicon induction. [Su et al., 2018] follow mono-lingual based methods but utilize images to enhance decoding performance. The 3-way model [Nakayama and Nishida, 2017] maps source sentences and images into com- mon space and employs an image-to-target caption model for translation. However, it cannot embrace attention mech- anism and results in noisy translations since information in images and sentences is not equal. [Chen et al., 2018; Lee et al., 2018] propose a multi-agent communication game with a captioner and translator. The captioner generates source sentence to describe an image, and the translator is trained to maximize rewards from relevancy between image and translated target sentence. [Chen et al., 2018] utilizes log-likelihood as the reward while [Lee et al., 2018] utilizes image retrieval performance. However, since captions related to images are not necessarily good mutual translations, such learning approaches also suffer from noisy rewards. In this Figure 2: The overall progressive learning framework for image- pivoted zero-resource machine translation. We ﬁrstly learn word translations from image pivot in the green module and then we ad- vance to more challenging sentence translation in blue modules. work, we propose to suppress such noises and progressively learn the translation in an easy-to-advanced way. 3 The Proposed Approach The goal of zero-resource machine translation is to learn source to target translation without any source-target sentence pairs. We propose to utilize images as pivots to enable zero- resource machine translation. Assume we have two mono- lingual image caption datasets: Dx = {(I(i) x , X(i))}Nx i=1 in the source language and Dy = {(I(i) y , Y (i))}Ny i=1 in the target language, where I denotes the image, and captions X and Y consist of word sequences {x1, · · · , xTx} and {y1, · · · , yTy} respectively. We omit the superscript i for simplicity here- inafter. The image sets Ix and Iy do not overlap, which means that an image has only one caption, either in source language or target language. The images are used as pivots during the training stage, but are not involved during the test stage. Figure 2 illustrates the proposed progressive learning ap- proach for image-pivoted zero-resource machine translation. Firstly, from the mono-lingual image caption datasets Dx and Dy, we can build image caption models fi→x and fi→y to translate an image into sentence descriptions in source and target languages respectively. Therefore, for each image Ix ∈Dx, we can obtain triplet captions (X, ˜X, ˜Y ) where X refers to the groundtruth caption, ˜X and ˜Y refer to the gener- ated captions from the image caption models fi→x and", "meta": {"paper_id": "1906.00872v1", "chunk_index": 2, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_3", "text": "translate an image into sentence descriptions in source and target languages respectively. Therefore, for each image Ix ∈Dx, we can obtain triplet captions (X, ˜X, ˜Y ) where X refers to the groundtruth caption, ˜X and ˜Y refer to the gener- ated captions from the image caption models fi→x and fi→y in source and target languages respectively. Similarly, for im- age Iy ∈Dy, we can obtain triplet captions (Y, ˜X, ˜Y ). We then induce (X, ˜Y ), ( ˜X, Y ) and ( ˜X, ˜Y ) from above triplets as image-pivoted source-target pseudo sentence pairs, which can be utilized to train NMT models. However, since the diversity of descriptive sentences is quite large, such pseudo pair may not be precisely seman- tic matched, which would greatly hinder the translation per- formance. In order to suppress noises in pseudo pairs, we propose a progressive learning approach which ﬁrstly learns word translation from image pivots. Since the diversity of words is better constrained when the word is grounded in im- age regions, word translation can be more effectively learned with the image-pivot approach. The word translation encodes multi-lingual words into a common semantic space, which Figure 3: The structure of the NMT model. The encoder and decoder both contain source and target embedding matrices Wx and Wy, so we can encode and decode source and target sentences in one model. is then utilized to assist image-pivoted sentence translation from two aspects: 1) to re-weight image-pivoted pseudo pairs and 2) to learn language-agnostic sentence representations. In such progressive manner, the NMT model is able to alleviate noises from image-pivot learning. In the following, we will ﬁrstly describe the structure of NMT model in Section 3.1, and then introduce the progres- sive learning strategy to train the NMT model with image piv- ots in details in Section 3.2. 3.1 Neural Machine Translation Model We utilize a shared multi-lingual encoder-decoder architec- ture as our NMT model, which can perform source-to-target and target-to-source translation in one model. Given the source sentence X = {x1, ..., xTx} and the target sentence Y = {y1, ..., yTy}, the encoder converts the source sentence into a sequence of vectors, and then the decoder sequentially predicts target word yt conditioning on the encoded vectors and previously generated words y<t. Speciﬁcally, our encoder is a bi-directional LSTM [Hochreiter and Schmidhuber, 1997] with a source word em- bedding matrix Wx and a target word embedding matrix Wy. The source and target sentences share the same encoder pa- rameters except the word embedding matrix as follows: zx t = biLSTM(Wxxt, zx t−1, zx t+1; Θe) (1) zy t = biLSTM(Wyyt, zy t−1, zy t+1; Θe) (2) where Θe are parameters of the bi-directional LSTM. The decoder is a LSTM with both source word embed- ding Wx and target word embedding Wy. Suppose z = {z1, · · · , zT } is the encoded input sentence, the decoder can translate z into the source sentence X by: p(xt|x<t, z) = softmax(Wxht) (3) ht = LSTM([Wxxt−1, ct], ht−1; Θd) (4) where Θd are parameters in", "meta": {"paper_id": "1906.00872v1", "chunk_index": 3, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_4", "text": "ding Wx and target word embedding Wy. Suppose z = {z1, · · · , zT } is the encoded input sentence, the decoder can translate z into the source sentence X by: p(xt|x<t, z) = softmax(Wxht) (3) ht = LSTM([Wxxt−1, ct], ht−1; Θd) (4) where Θd are parameters in the decoder LSTM, h0 is initial- ized as zT , [·] is the vector concatenation operation and ct is a context vector to employ relevant input vector zi to predict the target word xt via attention mechanism [Bahdanau et al., 2015]. The computation of ct is formulated as: ct = T X i=1 ai,tzi (5) ai,t = exp(fa([ht−1, zi])) P j exp(fa([ht−1, zj])), (6) where fa is a feed forward neural network to compute the attention weight ai for each hi. Therefore, the probability of generating X conditioning on z is: p(X|z) = Tx Y t=1 p(xt|x<t, z) (7) Similarly, the decoder can also translate the input z into the target sentence Y by replacing Wx in Eq (3) and Eq (4) with Wy. Figure 3 presents the structure of the NMT model. 3.2 Progressive Learning In this section, we describe in details the progressive learning procedure, from learning the word-level translation to more challenging sentence-level translation. Learning Word-level Translation In order to translate multi-lingual words, similar to work in [Chen et al., 2019], we build a multi-lingual image caption model to encode the source and target words into a joint semantic space. The multi-lingual image caption model is based on the encoder-decoder framework, where the encoder converts an image into a set of visual features, and the de- coder generates sentences conditioning on the visual features with attention mechanism. For image captioning in the source and target languages, the encoder and decoder are shared ex- cept the word embedding matrices. Therefore, the word em- bedding matrices of the source and target languages are en- forced to be in a common space constrained by the image piv- ots. We denote the learned word embedding matrices for the source and target words as Wx and Wy, which are employed in our NMT model and remain ﬁxed. Re-weighting Image-pivoted Pseudo Sentence Pairs We employ above learned Wx, Wy to assess the qualities of image-pivoted pseudo sentence pairs. We formulate the semantic distance of the sentence pair as a special case of Earth Mover’s Distance (EMD) [Rubner et al., 2000; Kus- ner et al., 2015], which is to ﬁnd the minimal transportation solution from one sentence to another. For source sentence X = {x1, · · · , xTx} and target sentence Y = {y1, · · · , yTy}, their EMD distance d is: d = min A≥0 X i,j Ai,j Di,j subject to: X j Ai,j = 1/Tx, i = 1, . . . , Tx X i Ai,j = 1/Ty, j = 1, . . . , Ty (8) where Di,j is the cosine distance of xi and yj based on Wx and Wy. The optimal matrix A can be solved efﬁciently via an off-the-shelf toolkit proposed by [Pele and Werman, 2009],", "meta": {"paper_id": "1906.00872v1", "chunk_index": 4, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_5", "text": ". , Tx X i Ai,j = 1/Ty, j = 1, . . . , Ty (8) where Di,j is the cosine distance of xi and yj based on Wx and Wy. The optimal matrix A can be solved efﬁciently via an off-the-shelf toolkit proposed by [Pele and Werman, 2009], which can be viewed as words alignment in the pseudo pair. Besides the sentence distance d, we could also derive token-level distances dxi and dyj for xi and yj respectively based on the optimal A, where dxi = P j Ai,j Di,j and dyj = P i Ai,j Di,j. Based on the sentence-level and token- level distance, we propose to re-weight the i-th pseudo pair (a) Re-weighting image-pivoted pseudo sentence pairs. (b) Language-agnostic auto-encoding. Figure 4: Training of the NMT model. The red solid lines denote translation in training; red dashed lines denote implicitly learned cross-language translation; yellow dashes lines denote tied weights. at both sentence-level and ﬁne-grained token-level via the in- verse distance weight as follows: α(i) xt = 1 (1 + d(i) xt −minkd(i) xk)λtoken (9) α(i) yt = 1 (1 + d(i) yt −minkd(i) yk )λtoken (10) β(i) = 1 (1 + d(i) −minkd(k))λsent (11) where λtoken, λsent are hyper-parameters to punish noisy to- kens and sentences. So α(i) xt and α(i) yt represent the relative importance of token xt and yt in the source and target sen- tence to form a good mutual translation pair, and β(i) is the relative quality of pair i among all pseudo pairs. Figure 4(a) illustrates the re-weighting training process. Therefore, the objective function for image-pivoted sentence pairs is: Lpivot = − N X i=1 β(i) Ty X t=1 α(i) yt log p(y(i) t |y(i) <t, zx(i)) − N X i=1 β(i) Tx X t=1 α(i) xt log p(x(i) t |x(i) <t, zy(i)) (12) Language-agnostic Auto-encoding We propose to employ auto-encoding to learn a language- agnostic sentence representation for cross language transla- tion as illustrated in Figure 4(b). We ﬁx source and target word embedding matrices in our NMT model with Wx and Wy which are in the common space, so that the sentence rep- resentation encoded by the shared encoder are constrained in a common latent space. Since it is trivial to auto-encode a Train Val Test Pairs I-En I-De I-En I-De En-De IAPR-TC12 9k 9k 500 500 1k Multi30k 14.5k 14.5k 507 507 1k Table 1: Data splits for the IAPR-TC12 and Multi30k datasets. sentence, we apply the corruption operation C(·) on the orig- inal sentence as [Lample et al., 2018a], which includes word order jitter, word insertion and deletion. The corrupted sen- tence is used as the input and the NMT model is trained to reconstruct the original sentence from the corrupted one. We apply the denoising auto-encoding for both source and target languages, so the objective is to minimize: Lae = − Nx X i=1 log p(X(i)|C(X(i))) − Ny X i=1 log p(Y (i)|C(Y (i))) (13) The full objective function to train the NMT model is: L = Lpivot + λLae (14) where λ is the weight to balance", "meta": {"paper_id": "1906.00872v1", "chunk_index": 5, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_6", "text": "source and target languages, so the objective is to minimize: Lae = − Nx X i=1 log p(X(i)|C(X(i))) − Ny X i=1 log p(Y (i)|C(Y (i))) (13) The full objective function to train the NMT model is: L = Lpivot + λLae (14) where λ is the weight to balance the two losses. 4 Experiments 4.1 Experimental Setup We utilize two benchmark image-pivot translation datasets IAPR-TC12 and Multi30k. The IAPR-TC12 dataset [Grub- inger et al., 2006] contains 20K images and each image is annotated with multi-sentences in English and its transla- tion in German. We follow [Chen et al., 2018] to use the ﬁrst sentence since it describes the most salient image con- tent. We randomly select 18K images for training, 1K for validation and 1K for testing. Since in the realistic image- pivot setting, images in different languages are mostly non- overlapped, we randomly split the training and validation set into two parts of equal size. One part is constructed only with image-English pairs and the other only with image-German pairs. The Multi30k dataset [Elliott et al., 2016] contains 30K images with two task annotations, one for machine trans- lation and the other for multi-lingual captioning. We utilize the former task annotation, where each image is annotated with one English description and its German translation. We follow the standard split in [Nakayama and Nishida, 2017] with 29K, 1,014 and 1K in the training, validation and test sets respectively. We also apply the similar non-overlapping split operation as in IAPR-TC12 to simulate the non-overlap setting. Table 1 presents the data split in our experiments. We use Moses SMT Toolkit [Koehn et al., 2007] to nor- malize and tokenize descriptions. For IAPR-TC12 dataset, we keep words appeared more than 3 times, which results in 1,621 words for English and 2,102 words for German. For Multi30K dataset, we employ a joint byte pair (BPE) [Sen- nrich et al., 2016] with 10k merge operations, which results in 5,202 tokens for English and 7,065 tokens for German. IAPR-TC12 Multi30k De-En En-De De-En En-De 3-way model 13.9 8.6 8.4 8.0 Multi-agents1 18.6 14.2 - - Emergent model2 - - 6.5 7.4 S-txt-img2 - - 7.5 7.7 ours Lpivot 38.0 30.3 9.9 8.4 Lae 58.9 39.8 21.9 17.6 Lpivot + Lae 61.3 47.1 23.0 18.3 Table 2: The BLEU4 performance of different methods for image- pivoted zero-resource machine translation. IAPR-TC12 Multi30k De-En En-De De-En En-De Lpivot w/o 31.0 25.5 8.5 7.7 with 38.0 30.3 9.9 8.4 Lae w/o 39.6 26.8 7.9 6.8 with 58.9 39.8 21.9 17.6 Table 3: Translation performance comparison of without and with the progressive learning approach for different sentence losses. For the multi-lingual image caption model, we leverage the Resnet152 pretrained on the ImageNet [He et al., 2016] as the encoder and a single-layer LSTM with 512 hidden units as the decoder. We utilize beam search with beam width of 5 to generate one description for each image. For the NMT model, the encoder is a one-layer bidirectional LSTM with 256 hid- den units and the decoder is a one-layer LSTM with 512 hid-", "meta": {"paper_id": "1906.00872v1", "chunk_index": 6, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_7", "text": "LSTM with 512 hidden units as the decoder. We utilize beam search with beam width of 5 to generate one description for each image. For the NMT model, the encoder is a one-layer bidirectional LSTM with 256 hid- den units and the decoder is a one-layer LSTM with 512 hid- den units. We set hyper-parameters λtoken = 10, λsent = 5 and λ = 1 based on validation performance. We utilize the Adam algorithm to train models with learning rate of 0.0005 and batch size of 64. The best model is selected by the loss on validation set. We evaluate the machine translation perfor- mance with BLEU4 metric [Papineni et al., 2002]. 4.2 Comparison with State-of-the-Art Methods We compare the proposed progressively learned NMT model with state-of-the-art image-pivoted models as follows: 1. 3-way model [Nakayama and Nishida, 2017]: It uti- lizes an target image caption model to translate a learned modality-agnostic feature of the source sentence. 2. Multi-agents [Chen et al., 2018]: The NMT model is trained to translate a generated source caption of Iy with image-relevance rewards from the log probability of pre- dicting groundtruth Y . 3. Emergent model [Lee et al., 2018]: Similar to [Chen et al., 2018], but the model utilizes an image retrieval task to calculate the image-caption relevance as rewards. 1Multi-agents method [Chen et al., 2018] utilizes different train- ing and evaluation setup on Multi30k dataset compared with others. 2The two works did not report results on the IAPR-TC12 dataset. 4. S-txt-img [Su et al., 2018]: The model is similar to mono-lingual based methods with auto-encoding and cycle-consistency loss, but employs an additional image encoder in training which can be absent in testing. Table 2 presents the performance of different methods for zero-resource machine translation with image pivots. Since our approach utilizes word translation learned in the ﬁrst pro- gressive step to beneﬁt sentence translation from two losses, we compare each of the loss and their combination with pre- vious methods. The Lpivot achieves superior performance to previous image-pivoted methods on both datasets and trans- lation directions. It demonstrates the effectiveness of the re- weighting approach to suppress noises in image-pivoted sen- tence pairs, while such noises are ignored in previous meth- ods. The auto-encoding loss Lae brings more signiﬁcant per- formance gains, which shows that re-using word embedding matrices from the image-pivoted word translation is effec- tive for the NMT model to encode language-agnostic sen- tence representation. The two losses are also complementary with each other and the combination of them achieves the best translation performance on the two datasets. To be noted, it might be unfair to directly compare our re- sults with the best model in the recent mono-lingual based method [Lample et al., 2018a]. The best model in [Lam- ple et al., 2018a] utilizes word vectors pretrained on a large- scale mono-lingual corpus and multiple iterations of back- translation to achieve good performance on Multi30k dataset (De-En 26.26 and En-De 22.74). Without pretraining and back-translation, it only achieves translation performance of 7.52 for De-En and 6.24 for En-De on Multi30k dataset. However,", "meta": {"paper_id": "1906.00872v1", "chunk_index": 7, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_8", "text": "2018a] utilizes word vectors pretrained on a large- scale mono-lingual corpus and multiple iterations of back- translation to achieve good performance on Multi30k dataset (De-En 26.26 and En-De 22.74). Without pretraining and back-translation, it only achieves translation performance of 7.52 for De-En and 6.24 for En-De on Multi30k dataset. However, our approach which only employs very limited mono-lingual image caption data and single round training without back-translation can still achieve comparable perfor- mance of the best model in [Lample et al., 2018a]. It sug- gests that image-pivoted approaches could be more effective to translate visually relevant sentences. What is more, our approach is orthogonal to mono-lingual based methods. 4.3 Ablation Study In Table 3, we evaluate improvements from progressive learn- ing for the two proposed losses. The loss Lpivot without pro- gressive learning denotes utilizing all image-pivoted pseudo pairs for training without re-weighting by the learned word translation. We can see that it suffers from the noisy pairs and is inferior to our re-weighting model on both datasets. To gain an intuition on the effects of re-weighting strategy, we il- lustrate some image-pivoted pseudo sentence pairs in Table 5 as well as their sentence and token weights. The clean pseudo sentence pairs are ranked higher than the noisy ones, and the token-level re-weighting provides a more ﬁne-grained super- vision from noisy pairs. For the loss Lae, the lack of pro- gressive learning means we do not employ the learned multi- lingual word representation in the ﬁrst word translation step. As shown in Table 3, the image-pivoted word embedding plays an important role for the effectiveness of auto-encoding loss. It demonstrates that performance boost from Lae mainly contributes to the progressive learning strategy. Since the proposed translation learning relies on results from image caption models, we investigate the relation be- Image Source and groundtruth target sentence Translated sentences ein schwarz-weißer hund l¨auft zu einem kaputten ball im schnee. a black-and-white dog goes for a ﬂattened ball on the snow. ours: a black and white dog runs to a catch a ball in the snow. supervised: a black and white dog runs towards a broken ball in the snow. eine ¨altere person ¨uberquert die straße mit einem regenschirm in der hand. an elderly person is crossing a street with an umbrella in their hands. ours: a elderly man crossing the street with a umbrella in the hand. supervised: an elderly person is walking across the street with an umbrella. Table 4: Translation examples from our progressive learning approach and the fully supervised NMT model. Images are only for visualization. βi Image-pivoted Pseudo En-De Sentence Pairs 0.97 a man in a blue shirt and blue shorts playing tennis . ein mann in einem blauen oberteil und blauen shorts spielt tennis . 0.70 a brown dog jumps over a fence . ein weißer hund springt ¨uber eine h¨urde . 0.35 a man carving a pumpkin in his boxers . ein mann in einem blauen hemd h¨alt einen hammer. 0.13 a woman in a white shirt is preparing a meal . peperoni kochen im winter ! Table", "meta": {"paper_id": "1906.00872v1", "chunk_index": 8, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_9", "text": "jumps over a fence . ein weißer hund springt ¨uber eine h¨urde . 0.35 a man carving a pumpkin in his boxers . ein mann in einem blauen hemd h¨alt einen hammer. 0.13 a woman in a white shirt is preparing a meal . peperoni kochen im winter ! Table 5: Image-pivoted pseudo sentence pairs ranked by sentence- level weights. The tokens in the German sentence with high token- level weights are colored in blue. Best viewed in color. IAPR-TC12 Multi30k I/De-En I/En-De I/De-En I/En-De Caption 23.0 18.9 6.6 5.6 Translation 61.3 47.1 23.0 18.3 Table 6: The BLEU4 scores of image caption model and image- pivoted translation model on IAPR-TC12 and Multi30k datasets. tween image caption performance and image-pivoted zero- resource translation performance in Table 6. Firstly, the trans- lation performance is much higher than the image captioning performance. This is mainly because the translation output is more constrained by the input while the caption output is more diverse. So the image caption model directly used in previous image-pivot translation methods might be a main bottleneck. Secondly, the translation performance is propor- tional to the image captioning performance, which indicates that better image captioning can further improve our zero- resource machine translation. Finally, we compare our progressively learned zero- resource NMT model with fully supervised NMT model with respect to variable amount of training data in Figure 5. Our proposed model with zero-resource reaches 75% of the best performance from the supervised NMT model with 18k train- ing pairs for De-to-En, 60% for En-to-De on IAPR-TC12 dataset. Similarly, it reaches 58% of the best performance (a) IAPR-TC12 dataset. (b) Multi30k dataset. Figure 5: Comparison with the fully supervised NMT model trained on variable amount of parallel texts. The x-axis denotes number of training pairs and y-axis denotes the BLEU4 score. from the supervised NMT model with 29k training pairs for De-to-En and 51% for En-to-De on Multi30k dataset. Table 4 presents some randomly selected examples. The proposed approach can generate promising translation results simply based on image pivots without parallel texts. 5 Conclusion In this paper, we address the zero-resource machine transla- tion problem by exploiting visual images as pivots. Due to the nature that a picture tells a thousand words, description sentences for an image may be semantically nonequivalent, which leads to noisy supervisions to train the NMT model in previous works. In this work, we propose a progressive learn- ing approach which consists of progressive easy-to-advanced steps towards learning effective NMT models under zero re- source settings. The learning starts with word-level transla- tion with image pivots and then progresses to sentence-level translation assisted by the word translation and image pivots. Experiments on IAPR-TC12 and Multi30k datasets prove the effectiveness of the proposed approach, which signiﬁcantly outperforms previous image-pivot methods. Acknowledgments This work was supported by National Natural Science Foun- dation of China (No. 61772535), Beijing Natural Science Foundation (No. 4192028), National Key Research and De- velopment Plan (No. 2016YFB1001202) and Research Foun- dation of Beijing Municipal Science & Technology Commis- sion (No. Z181100008918002). References [Artetxe et al., 2018]", "meta": {"paper_id": "1906.00872v1", "chunk_index": 9, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_10", "text": "methods. Acknowledgments This work was supported by National Natural Science Foun- dation of China (No. 61772535), Beijing Natural Science Foundation (No. 4192028), National Key Research and De- velopment Plan (No. 2016YFB1001202) and Research Foun- dation of Beijing Municipal Science & Technology Commis- sion (No. Z181100008918002). References [Artetxe et al., 2018] Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho. Unsupervised neural ma- chine translation. ICLR, 2018. [Bahdanau et al., 2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. ICLR, 2015. [Calixto et al., 2017] Iacer Calixto, Daniel Stein, Evgeny Matusov, Pintu Lohar, Sheila Castilho, and Andy Way. Using images to improve machine-translating e-commerce product listings. In EACL, pages 637–643, 2017. [Castilho et al., 2017] Sheila Castilho, Joss Moorkens, Fed- erico Gaspari, Iacer Calixto, John Tinsley, and Andy Way. Is neural machine translation the new state of the art? The Prague Bulletin of Mathematical Linguistics, 108(1):109– 120, 2017. [Chen et al., 2017] Yun Chen, Yang Liu, Yong Cheng, and Victor OK Li. A teacher-student framework for zero- resource neural machine translation. In ACL, pages 1925– 1935, 2017. [Chen et al., 2018] Yun Chen, Yang Liu, and Victor OK Li. Zero-resource neural machine translation with multi-agent communication game. AAAI, 2018. [Chen et al., 2019] Shizhe Chen, Jin Qin, and Alexander Hauptmann. Unsupervised bilingual lexicon induction with mono-lingual multimodal data. AAAI, 2019. [Cho et al., 2014] Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau, and Yoshua Bengio. On the proper- ties of neural machine translation: Encoder-decoder ap- proaches. arXiv preprint arXiv:1409.1259, 2014. [Elliott et al., 2016] Desmond Elliott, Stella Frank, Khalil Sima’an, and Lucia Specia. Multi30k: Multilin- gual english-german image descriptions. arXiv preprint arXiv:1605.00459, 2016. [Grubinger et al., 2006] Michael Grubinger, Paul Clough, Henning M¨uller, and Thomas Deselaers. The iapr tc-12 benchmark: A new evaluation resource for visual infor- mation systems. In Int. Workshop OntoImage, volume 5, 2006. [He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog- nition. In CVPR, pages 770–778, 2016. [Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. [Johnson et al., 2017] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi´egas, Martin Wattenberg, Greg Corrado, et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation. TACL, 5(1):339–351, 2017. [Kiela et al., 2015] Douwe Kiela, Ivan Vuli´c, and Stephen Clark. Visual bilingual lexicon induction with transferred convnet features. In EMNLP, pages 148–158, 2015. [Koehn et al., 2007] Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, et al. Moses: Open source toolkit for statis- tical machine translation. In ACL, pages 177–180, 2007. [Kusner et al., 2015] Matt Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to docu- ment distances. In ICML, pages 957–966, 2015. [Lample et al., 2018a] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsuper- vised machine translation using monolingual corpora only. ICLR, 2018. [Lample et al., 2018b] Guillaume Lample, Myle", "meta": {"paper_id": "1906.00872v1", "chunk_index": 10, "filename": "1906.00872v1.pdf", "chunk_length": 512}}
{"id": "1906.00872v1_chunk_11", "text": "Kusner, Yu Sun, Nicholas Kolkin, and Kilian Weinberger. From word embeddings to docu- ment distances. In ICML, pages 957–966, 2015. [Lample et al., 2018a] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio Ranzato. Unsuper- vised machine translation using monolingual corpora only. ICLR, 2018. [Lample et al., 2018b] Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, et al. Phrase-based & neu- ral unsupervised machine translation. In EMNLP, pages 5039–5049, 2018. [Lee et al., 2018] Jason Lee, Kyunghyun Cho, Jason We- ston, and Douwe Kiela. Emergent translation in multi- agent communication. ICLR, 2018. [Nakayama and Nishida, 2017] Hideki Nakayama and Noriki Nishida. Zero-resource machine translation by multimodal encoder–decoder network with multimedia pivot. Machine Translation, 31(1-2):49–64, 2017. [Papineni et al., 2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for au- tomatic evaluation of machine translation. In ACL, pages 311–318, 2002. [Pele and Werman, 2009] Oﬁr Pele and Michael Werman. Fast and robust earth mover’s distances. In ICCV, pages 460–467. IEEE, September 2009. [Rubner et al., 2000] Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for image retrieval. ICCV, 40(2):99–121, 2000. [Sennrich et al., 2016] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. In ACL, volume 1, pages 1715–1725, 2016. [Sharma et al., 2018] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, volume 1, pages 2556–2565, 2018. [Su et al., 2018] Yuanhang Su, Kai Fan, Nguyen Bach, C- C Jay Kuo, and Fei Huang. Unsupervised multi- modal neural machine translation. arXiv preprint arXiv:1811.11365, 2018. [Zhou et al., 2018] Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu. A visual attention grounding neural model for multimodal machine translation. In EMNLP, pages 3643–3653, 2018. [Zoph et al., 2016] Barret Zoph, Deniz Yuret, Jonathan May, and Kevin Knight. Transfer learning for low-resource neu- ral machine translation. In EMNLP, pages 1568–1575, 2016.", "meta": {"paper_id": "1906.00872v1", "chunk_index": 11, "filename": "1906.00872v1.pdf", "chunk_length": 330}}
{"id": "1906.01040v3_chunk_0", "text": "A Surprising Density of Illusionable Natural Speech Melody Y. Guan Department of Computer Science Stanford University Stanford, CA 94305 mguan@stanford.edu Gregory Valiant Department of Computer Science Stanford University Stanford, CA 94305 gvaliant@stanford.edu Abstract Recent work on adversarial examples has demonstrated that most natural inputs can be perturbed to fool even state-of-the-art machine learning systems. But does this happen for humans as well? In this work, we investigate what fraction of natural instances of speech can be turned into “illusions” which either alter humans’ perception or result in different people having signiﬁcantly different perceptions? We ﬁrst consider the McGurk effect, the phenomenon by which adding a carefully chosen video clip to the audio channel affects the viewer’s perception of what is said [17]. We obtain empirical estimates that a signiﬁcant fraction of both words and sentences occurring in natural speech have some susceptibility to this effect. We also learn models for predicting McGurk illusionability. Finally we demonstrate that the Yanny or Laurel auditory illusion [24] is not an isolated occurrence by generating several very different new instances. We believe that the surprising density of illusionable natural speech warrants further investigation, from the perspectives of both security and cognitive science. 1 Introduction A growing body of work on adversarial examples has identiﬁed that for machine-learning (ML) systems that operate on high-dimensional data, for nearly every natural input there exists a small perturbation of the point that will be misclassiﬁed by the system, posing a threat to its deployment in certain critical settings [3, 7, 10, 13, 16, 23, 22]. More broadly, the susceptibility of ML systems to adversarial examples has prompted a re-examination of whether current ML systems are truly learning or if they are assemblages of tricks that are effective yet brittle and easily fooled [25]. Implicit in this line of reasoning is the assumption that instances of ”real\" learning, such as human cognition, yield extremely robust systems. Indeed, at least in computer vision, human perception is regarded as the gold-standard for robustness to adversarial examples. Evidently, humans can be fooled by a variety of illusions, whether they be optical, auditory, or other; and there is a long line of research from the cognitive science and psychology communities investigating these [12]. In general, however, these illusions are viewed as isolated examples that do not arise frequently, and which are far from the instances encountered in everyday life. In this work, we attempt to understand how susceptible humans’ perceptual systems for natural speech are to carefully designed “adversarial attacks.” We investigate the density of certain classes of illusion, that is, the fraction of natural language utterances whose comprehension can be affected by the illusion. Our study centers around the McGurk effect, which is the well-studied phenomenon by which the perception of what we hear can be inﬂuenced by what we see [17]. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1). This effect persists even when the subject is aware of the setup, though the", "meta": {"paper_id": "1906.01040v3", "chunk_index": 0, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_1", "text": "can be inﬂuenced by what we see [17]. A prototypical example is that the audio of the phoneme “baa,” accompanied by a video of someone mouthing “vaa”, can be perceived as “vaa” or “gaa” (Figure 1). This effect persists even when the subject is aware of the setup, though the strength of the effect varies signiﬁcantly across people and languages and with factors such as age, gender, and disorders [2, 8, 14, 18, 20, 21, 27, 28, 31]. Figure 1: Illustration of the McGurk effect. For some phoneme pairs, when the speaker visibly mouths phoneme A but the auditory stimulus is actually the phoneme B, listeners tend to perceive a phoneme C ̸= B. A signiﬁcant density of illusionable instances for humans might present similar types of security risks as adversarial examples do for ML systems. Auditory signals such as public service announcements, instructions sent to ﬁrst responders, etc., could be targeted by a malicious agent. Given only access to a screen within eyesight of the intended victims, the agent might be able to signiﬁcantly obfuscate or alter the message perceived by those who see the screen (even peripherally). 1.1 Related work Illusionable instances for humans are similar to adversarial examples for ML systems. Strictly speaking, however, our investigation of the density of natural language for which McGurk illusions can be created, is not the human analog of adversarial examples. The adversarial examples for ML systems are datapoints that are misclassiﬁed, despite being extremely similar to a typical datapoint (that is correctly classiﬁed). Our illusions of misdubbed audio are not extremely close to any typically encountered input, since our McGurk samples have auditory signals corresponding to one phoneme/word and visual signals corresponding to another. Also, there is a compelling argument for why the McGurk confusion occurs, namely that human speech perception is bimodal (audio-visual) in nature when lip reading is available [4, 29]. To the best of our knowledge, prior to our work, there has been little systematic investigation of the extent to which the McGurk effect, or other types of illusions, can be made dense in the set of instances encountered in everyday life. The closest work is Elsayed et al. [9], where the authors demonstrate that some adversarial examples for computer vision systems also fool humans when humans were given less than a tenth of second to view the image. However, some of these examples seem less satisfying as the perturbation acts as a pixel-space interpolation between the original image and the “incorrect” class. This results in images that are visually borderline between two classes, and as such, do not provide a sense of illusion to the viewer. In general, researchers have not probed the robustness of human perception with the same tools, intent, or perspective, with which the security community is currently interrogating the robustness of ML systems. 2 Problem setup For the McGurk effect, we attempt an illusion for a language token (e.g. phoneme, word, sentence) x by creating a video where an audio stream of x is visually dubbed over by a person saying x′ ̸= x.", "meta": {"paper_id": "1906.01040v3", "chunk_index": 1, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_2", "text": "community is currently interrogating the robustness of ML systems. 2 Problem setup For the McGurk effect, we attempt an illusion for a language token (e.g. phoneme, word, sentence) x by creating a video where an audio stream of x is visually dubbed over by a person saying x′ ̸= x. We stress that the audio portion of the illusion is not modiﬁed and corresponds to a person saying x. The illusion f(x′, x) affects a listener if they perceive what is being said to be y ̸= x if they watched the illusory video whereas they perceive x if they had either listened to the audio stream without watching the video or had watched the original unaltered video, depending on speciﬁcation. We call a token illusionable if an illusion can be made for the token that affects the perception of a signiﬁcant fraction of people. In Section 3, we analyze the extent to which the McGurk effect can be used to create illusions for phonemes, words, and sentences, and analyze the fraction of natural language that is susceptible to such illusionability. We thereby obtain a lower bound on the density of illusionable natural speech. 2 Table 1: Illusionable phonemes and effects based on preliminary phoneme-pair testing. Where a number of lip movements were available to affect a phoneme, the most effective one is listed. Phoneme Lip Movement Perceived Sound b/ /w/ /v/, /f/, /p/ D/ /b/ /b/ f/ /z/ /T/, /t/, /b/ m/ /D/ /nT/, /n/, /ml/ p/ /t/ /t/, /k/ v/ /b/ /b/ d/ /v/ /v/, /t/ l/ /v/ /v/ T/ /v/ /d/, /k/, /t/, /f/ w/ /l/ /l/ We ﬁnd that 1) a signiﬁcant fraction of words that occur in everyday speech can be turned into McGurk-style illusions, 2) such illusions persist when embedded within the context of natural sentences, and in fact affect a signiﬁcant fraction of natural sentences, and 3) the illusionability of words and sentences can be predicted using features from natural language modeling. 3 McGurk experiments 3.1 Phoneme-level experiments We began by determining which phoneme sounds can be paired with video dubs of other phonemes to effect a perceived phoneme that is different from the actual sound. We created McGurk videos for all vowel pairs preceded with the consonant /n/ as well as for all consonant pairs followed by the vowel /a/ spoken by a speaker. There are 20 vowel phonemes and 24 consonant phonemes in American English although /Ã/ and /Z/ are redundant for our purposes.1 Based on labels provided by 10 individuals we found that although vowels were not easily confused, there are a number of illusionable consonants. We note that the illusionable phoneme pairs depend both on the speaker and listener identities. Given Table 1 of illusionable phonemes, the goal was then to understand whether these could be leveraged within words or sentences; and if so, the fraction of natural speech that is susceptible. 3.2 Word-level experiments We sampled 200 unique words (listed in Table 2) from the 10,000 most common words in the Project Gutenberg novels in proportion to their frequency in", "meta": {"paper_id": "1906.01040v3", "chunk_index": 2, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_3", "text": "understand whether these could be leveraged within words or sentences; and if so, the fraction of natural speech that is susceptible. 3.2 Word-level experiments We sampled 200 unique words (listed in Table 2) from the 10,000 most common words in the Project Gutenberg novels in proportion to their frequency in the corpus. The 10k words collectively have a prevalence of 80.6% in the corpus. Of the 200 sampled words, 147 (73.5%) contained phonemes that our preliminary phoneme study suggested might be illusionable. For these 147 words, we paired audio clips spoken by the speaker with illusory video dubs of the speaker saying the words with appropriately switched out phonemes. We tested these videos on 20 naive test subjects who did not participate in the preliminary study. Each subject watched half of the words and listened without video to the other half of the words, and were given the instructions: \"Write down what you hear. What you hear may or may not be sensical. Also write down if a clip sounds unclear to you. Not that a clip may sound nonsensical but clear.\" Subjects were allowed up to three plays of each clip. We found that watching the illusory videos led to an average miscomprehension rate of 24.8%, a relative 148% increase from the baseline of listening to the audio alone (Table 3). The illusory videos made people less conﬁdent about their correct answers, with an additional 5.1% of words being heard correctly but unclearly, compared to 2.1% for audio only. For 17% of the 200 words, the illusory videos increased the error rates by more than 30% above the audio-only baseline. 1Refer to www.macmillanenglish.com/pronunciation/phonemic-chart-in-american-english/ for a list of the phoneme symbols of the International Phonetic Alphabet that are used in American English. 3 Table 2: The 200 unique words sampled from the Project Gutenberg novel corpus. The 147 of those for which an illusory video was created are listed on top. Ordering is otherwise alphabetical. Illusion Attempted about addressed all also and anyone arms away bad be been before behind besides blind bought box brothers but by call called calling came child close coming could days dead did die direction done else end even everything far features fell few ﬁghting ﬂy for formed from game gathered gave general generally god good half hands happened hath have him himself idea information july large let letter life like list made many mass may me meet men months more Mrs my myself never nothing of off old one open opinion ordinary other outside passion perhaps please plenty point possessed present put questions roof said save seized shall sharp ship should slow some speech still successful summer terms than that the their them themselves there they things though time top upon used very waited was water we went what when which will wisdom with working world would wounded No Attempt a act air an any are as at change city country eyes go going hair has he heart her higher his house i in into is it its king know nature new no not now on", "meta": {"paper_id": "1906.01040v3", "chunk_index": 3, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_4", "text": "we went what when which will wisdom with working world would wounded No Attempt a act air an any are as at change city country eyes go going hair has he heart her higher his house i in into is it its king know nature new no not now on or our out rest saw see seen she sorrow strange take talking to turn who writing your Table 3: Test results for word-level McGurk illusions among the 147 words predicted to be illusionable. Shown are average error rates for watching the illusory video vs listening to the audio only, as well as the percentage of words that are correctly identiﬁed but sound ambiguous to the listener. Audio Only Illusory Video Relative Increase Words Incorrectly Identiﬁed 10.0 24.8 +148% Words Correct with Low Conﬁdence 2.1 5.1 +144% 3.2.1 Prediction model for word-level illusionability To create a predictive model for word-level illusionability, we used illusionable phonemes enriched with positional information as features. Explicitly, for each of the 10 illusionable phonemes, we created three features from the phoneme being in an initial position (being the ﬁrst phoneme of the word), a medial position (having phonemes come before and after), or a ﬁnal position (being the last phoneme of the word). We then represented each word with a binary bag-of-words model [15], giving each of the 30 phonemes-with-phonetic-context features a value of 1 if present in the word and 0 otherwise. We performed ridge regression on these features with a constant term. We searched for the optimal l2 regularization constant among the values [0.1, 1, 10, 100] and picked the optimal one based on training set performance. The train:test split was in the proportion 85%:15% and was randomly chosen for each trial. Across 10k randomized trials, we obtain average training and test set correlations of 91.1 ± 0.6% and 44.6 ± 28.9% respectively. Our ﬁnal model achieves an out-of-sample correlation of 57% between predicted and observed illusionabilites. Here, the observed illusionability of the words is calculated as the difference between the accuracy of watchers of the illusory videos and the accuracy of listeners, where “accuracy” is deﬁned as the fraction of respondents who were correct. For each word, the predicted illusionability is calculated from doing inference on that word using the averaged regression coefﬁcients of the regression trials where the word is not in the training set. 4 Our predicted illusionability is also calibrated, in the sense that for the words predicted to have an illusionability <0.1, the mean empirical illusionability is 0.04; for words with predicted illusionability in the interval [0.1, 0.2] the mean empirical illusionability is 0.14; for predicted illusionability between [0.2, 0.3] the mean observed is 0.27; and for predicted illusionability >0.3, the mean observed is 0.50. Figure 2 visually depicts the match between the observed and predicted word illusionabilities. Figure 2: Predicted word illusionability closely matches observed word illusionability, with out-of- sample correlation of 57%. The words are sorted by increasing predicted word illusionability (and the observed illusionability of each word was not used in calculating the prediction of that word). 3.3", "meta": {"paper_id": "1906.01040v3", "chunk_index": 4, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_5", "text": "between the observed and predicted word illusionabilities. Figure 2: Predicted word illusionability closely matches observed word illusionability, with out-of- sample correlation of 57%. The words are sorted by increasing predicted word illusionability (and the observed illusionability of each word was not used in calculating the prediction of that word). 3.3 Sentence-level experiments We set up the following experiment on naturally occurring sentences. We randomly sampled 300 sentences of lengths 4-8 words inclusive from the novel Little Women [1] from the Project Gutenberg corpus. From this reduced sample, we selected and perturbed 32 sentences that we expected to be illusionable (listed in Table 4). With the speaker, we prepared two formats of each sentence: original video (with original audio), and illusory video (with original audio).We then evaluated the perception of these on 1306 naive test subjects on Amazon Mechanical Turk. The Turkers were shown videos for six randomly selected sentences, three illusory and three original, and were given the prompt: \"Press any key to begin video [index #] of 6. Watch the whole video, and then you will be prompted to write down what the speaker said.\" Only one viewing of any clip was allowed, to simulate the natural setting of observing a live audio/video stream. Each Turker was limited to six videos to reduce respondent fatigue. Turkers were also asked to report their level of conﬁdence in what they heard on a scale from no uncertainty (0%) to complete uncertainty (100%). One hundred and twelve Turkers (8.6%) did not adhere to the prompt, writing unrelated responses, and their results were omitted from analysis. We found that watching the illusory videos led to an average miscomprehension rate of 32.8%, a relative 145% increase from the baseline of watching the original videos (Table 5). The illusory videos made people less conﬁdent about their correct answers. Turkers who correctly identiﬁed the audio message in an illusory video self-reported an average uncertainty of 42.9%, which is a relative 123% higher than the average reported by the Turkers who correctly understood the original videos. Examples of mistakes made by listeners of the illusory videos are shown in Table 6. Overall we found that for 11.5% of the 200 sampled sentences (23 out of the 30 videos we created), the illusory videos increased the error rates by more than 10%. 3.3.1 Comparing word-level and sentence-level illusionabilities We obtained a sentence-level illusionability prediction model with an out-of-sample correlation of 33% between predicted and observed illusionabilities. Here, the observed illusionability of the sentences was calculated as the difference between the accuracy of watchers of the illusory videos and the accuracy of watchers of original videos, where “accuracy” is deﬁned as the fraction of respondents who were correct. We obtained predicted illusionabilities by simply using the maximum word 5 Table 4: The sentences randomly sampled from Little Women for which we made illusory videos. Those that had observed illusionability >0.1 are listed on top. Illusionable I’m not too proud to beg for Father. No need of that. I’d like to wear them, Mother can I? It’s no harm, Jo Now do", "meta": {"paper_id": "1906.01040v3", "chunk_index": 5, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_6", "text": "The sentences randomly sampled from Little Women for which we made illusory videos. Those that had observed illusionability >0.1 are listed on top. Illusionable I’m not too proud to beg for Father. No need of that. I’d like to wear them, Mother can I? It’s no harm, Jo Now do be still, and stop bothering. Well, I like that! How many did you have out? Open the door, Amy! Nonsense, that’s of no use. I am glad of that! Of course that settled it. I can’t bear saints. Serves me right for trying to be ﬁne. Of course I am! There’s gratitude for you! That’s my good girl. Capital boys, aren’t they? You’ve got me, anyhow. Brown, that is, sometimes. I’ll tell you some day. What do you know about him? He won’t do that. The plan of going over was not forgotten. Not Illusionable On the way get these things. I’m glad of it. Aren’t you going with him? Then I’ll tell you. That was the question. That’s why I do it. I hate to have odd gloves! We don’t know him! What good times they had, to be sure Table 5: Test results for sentence-level McGurk illusions, showing average error rates for watching the illusory video vs watching the original video, and average self-reported uncertainty for correctly identiﬁed words. Original Video Illusory Video Relative Increase Sentences Incorrectly Identiﬁed 13.4% 32.8% +145% Uncertainty of Correct Sentences 19.4% 42.9% +121% Table 6: Sample verbatim mistakes for sentence-level McGurk illusions. Spelling mistakes were ignored for accuracy calculations. Sample illusory videos are provided in the supplementary ﬁles. Sentence Sample Listener Perceptions Serves me right for trying to be ﬁne. Serves thee right for trying to be thine Serve you right for trying to be kind serbes ye right shine by rhine Of course that settled it. up course bat saddle it Of course Max settled it. I can’t bear saints. I can’t wear skates I can’t hear saints. I can’t spare sink How many did you have out? How many did you knock out? How many did you help out I’ll tell you some day. I’ll tell you Sunday. Ill tell you something I’m not too proud to beg for Father. I’m not too proud to beg or bother I want the crowd to beg for father Ine not too proud to beg for father Well, I like that! Well I like pets Well, I like baths Now do be still, and stop bothering. now do we still end stop bothering And do we still end sauce watering? Of course I am! Of course I an Of course I can Of course I anth. You’ve got me, anyhow. you got knee anyhow You got the anyhow. There’s gratitude for you! Bears gratitude for you. Pairs gratitudes or you. Bears gratitude lore you 6 illusionability prediction amongst the words in each sentence, with word predictions obtained from the word-level model. We attempted to improve our sentence-level predictive model by incorporating how likely the words appear under a natural language distribution, considering three classes of words:", "meta": {"paper_id": "1906.01040v3", "chunk_index": 6, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_7", "text": "Pairs gratitudes or you. Bears gratitude lore you 6 illusionability prediction amongst the words in each sentence, with word predictions obtained from the word-level model. We attempted to improve our sentence-level predictive model by incorporating how likely the words appear under a natural language distribution, considering three classes of words: words in the sentence for which no illusion was attempted, the words for which an illusion was attempted, and the potentially perceived words for words for which an illusion was attempted. We used log word frequencies obtained from the the top 36.7k most common words from the Project Gutenberg corpus.2 This approach could not attain better out-of-sample correlations than the naive method. This implies that context is important for sentence-level illusionability, and more complex language models should be used. Finally, comparing word-level and sentence-level McGurk illusionabilities in natural speech, we observe that that the former is signiﬁcantly higher. A greater McGurk effect at the word level is to be expected–sentences provide context with which the viewer could ﬁll in confusions and misunderstandings. Furthermore, when watching a sentence video compared to a short word video, the viewer’s attention is more likely to stray, both from the visual component of the video, which evidently reduces the McGurk effect, as well as the from the audio component, which likely prompts the viewer to rely even more heavily on context. Nevertheless, there remains a signiﬁcant amount of illusionability at the sentence-level. 4 Future Directions This work is an initial step towards exploring the density of illusionable phenomena for humans. There are many natural directions for future work. In the vein of further understanding McGurk-style illusions, it seems worth building more accurate predictive models for sentence-level effects, and further investigating the security risks posed by McGurk illusions. For example, one concrete next step in understanding McGurk-style illusions would be to actually implement a system which takes an audio input, and outputs a video dub resulting in signiﬁcant misunderstanding. Such a system would need to combine a high-quality speech-to-video-synthesis system [30, 32], with a ﬂeshed-out language model and McGurk prediction model. There is also the question of how to guard against “attacks” on human perception. For example, in the case of the McGurk effect, how can one rephrase a passage of text in such a way that the meaning is unchanged, but the rephrased text is signiﬁcantly more robust to McGurk style manipulations? The central question in this direction is what fraction of natural language can be made robust without signiﬁcantly changing the semantics. A better understanding of when and why certain human perception systems are nonrobust can also be applied to make ML systems more robust. In particular, neural networks have been found to be susceptible to adversarial examples in automatic speech recognition [26, 5] and to the McGurk effect [19], and a rudimentary approach to making language robust to the latter problem would be to use a reduced vocabulary that avoids words that score highly in our word-level illusionability prediction model. Relatedly, at the interface of cognitive science and adversarial examples, there has been work suggesting", "meta": {"paper_id": "1906.01040v3", "chunk_index": 7, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_8", "text": "the McGurk effect [19], and a rudimentary approach to making language robust to the latter problem would be to use a reduced vocabulary that avoids words that score highly in our word-level illusionability prediction model. Relatedly, at the interface of cognitive science and adversarial examples, there has been work suggesting that humans can anticipate when or how machines will misclassify, including for adversarial examples [6, 11, 33]. More broadly, as the tools for probing the weaknesses of ML systems develop further, it seems like a natural time to reexamine the supposed robustness of human perception. We anticipate unexpected ﬁndings. To provide one example, we summarize some preliminary results on audio-only illusions. 4.1 Auditory Illusions An audio clip of the word “Laurel\" gained widespread attention in 2018, with coverage by notable news outlets such as The New York Times and Time. Roughly half of listeners perceive “Laurel” and the other half perceive “Yanny” or similar-sounding words, with high conﬁdence on both sides [24]. One of the reasons the public was intrigued is because examples of such phenomena are viewed as rare, isolated instances. In a preliminary attempt to investigate the density of such phenomena, we identiﬁed ﬁve additional distinct examples (Table 7). The supplementary ﬁles include 10 versions of one of these examples, where listeners tend to perceive either “worlds” or “yikes.” Across the 2https://en.wiktionary.org/wiki/Wiktionary:Frequency_lists#Project_Gutenberg 7 different audio clips, one should be able to hear both interpretations. The threshold for switching from one interpretation to another differs from person to person. Table 7: Test results for Yanny/Laurel style illusions. Each row displays a cluster of similar-sounding reported words, with counts. Emdashes indicate unintelligible. Word Slowdown Perceived Sound N worlds 1.5x worlds 5 yikes/yites 4 nights/lights 6 bologna 1.7x bologna 2 alarming 2 alarmy/ayarmy/ignore me 3 uoomi/ayomi/wyoming 3 anomi/anolli/amomi 3 – 2 growing 1.3x growing 8 pearling 3 curling 3 crowing 1 potent 1.7x potent/poatin/poden 4 pogie/bowie/po-ee 3 pone/paam/paan 3 power/poder/pair 3 tana 1 – 2 prologue 1.9x prologue 2 prelude/pro-why/pinelog 3 kayak/kayank/kayan 6 turnip/tienap/tarzan 3 – 1 These examples were generated by examining 5000 words, and selecting the 50 whose spectrograms contain a balance of high and low frequency components that most closely matched those for the word “Laurel\". Each audio ﬁle corresponded to the Google Cloud Text-to-Speech API synthesis of a word, after low frequencies were damped and the audio was slowed 1.3-1.9x. After listening to these top 50 candidates, we evaluated the most promising ﬁve on a set of 15 individuals (3 female, 12 male, age range 22-33). We found multiple distributional modes of perceptions for all ﬁve audio clips. For example, a clip of “worlds” with the high frequencies damped and slowed down 1.5x was perceived by ﬁve listeners as “worlds”, four as “yikes/yites” and six as “nights/lights”. While these experiments do not demonstrate a density of such examples with respect to the set of all words—and it is unlikely that illusory audio tracks in this style can be created for the majority of words—they illustrate that even the surprising Yanny or Laurel phenomenon is not an isolated occurrence. It", "meta": {"paper_id": "1906.01040v3", "chunk_index": 8, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_9", "text": "experiments do not demonstrate a density of such examples with respect to the set of all words—and it is unlikely that illusory audio tracks in this style can be created for the majority of words—they illustrate that even the surprising Yanny or Laurel phenomenon is not an isolated occurrence. It remains to be seen how dense such phenomena can be, given the right sort of subtle audio manipulation. ‘ 5 Conclusion Our work suggests that for a signiﬁcant fraction of natural speech, human perception can be altered by using subtle, learnable perturbations. This is an initial step towards exploring the density of illusionable phenomenon for humans, and examining the extent to which human perception may be vulnerable to security risks like those that adversarial examples present for ML systems. We hope our work inspires future investigations into the discovery, generation, and quantiﬁcation of multimodal and unimodal audiovisual and auditory illusions for humans. There exist many open research questions on when and why humans are susceptible to various types of illusions, how to model the illusionability of natural language, and how natural language can be made more robust to illusory perturbations. Additionally, we hope such investigations inform our interpretations of the strengths and weaknesses of current ML systems. Finally, there is the possibility that some vulnerability to carefully crafted adversarial examples may be inherent to all complex learning systems that interact with high-dimensional inputs in an environment with limited data; any thorough investigation of this question must also probe the human cognitive system. 8 6 Acknowledgements This research was supported in part by NSF award AF:1813049, an ONR Young Investigator award (N00014-18-1-2295), and a grant from Stanford’s Institute for Human-Centered AI. The authors would like to thank Jean Betterton, Shivam Garg, Noah Goodman, Kelvin Guu, Michelle Lee, Percy Liang, Aleksander Makelov, Jacob Plachta, Jacob Steinhardt, Jim Terry, and Alexander Whatley for useful feedback on the work. The research was done under Stanford IRB Protocol 46430. References [1] Louisa May Alcott. Little Women (1868-9). Roberts Brothers, 1994. [2] Mireille Bastien-Toniazzo, Aurelie Stroumza, and Christian Cave. Audio-visual perception and integration in developmental dyslexia: An exploratory study using the mcgurk effect. Current psychology letters. Behaviour, brain & cognition, 25(3), 2010. [3] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction attacks. In International Conference on Machine Learning and Data Mining in Pattern Recognition, pages 262–275. Springer, 2017. [4] Julien Besle, Alexandra Fort, Claude Delpuech, and Marie-H Giard. Bimodal speech: early suppressive visual effects in human auditory cortex. [5] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW), pages 1–7. IEEE, 2018. [6] Arjun Chandrasekaran, Deshraj Yadav, Prithvijit Chattopadhyay, Viraj Prabhu, and Devi Parikh. It takes two to tango: Towards theory of ai’s mind. arXiv preprint arXiv:1704.00717, 2017. [7] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. Thirty-second AAAI conference on artiﬁcial intelligence, 2018. [8] Xavier Delbeuck, Fabienne Collette, and Martial Van der Linden. Is alzheimer’s disease a disconnection", "meta": {"paper_id": "1906.01040v3", "chunk_index": 9, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_10", "text": "ai’s mind. arXiv preprint arXiv:1704.00717, 2017. [7] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. Ead: elastic-net attacks to deep neural networks via adversarial examples. Thirty-second AAAI conference on artiﬁcial intelligence, 2018. [8] Xavier Delbeuck, Fabienne Collette, and Martial Van der Linden. Is alzheimer’s disease a disconnection syndrome?: Evidence from a crossmodal audio-visual illusory experiment. Neuropsychologia, 45(14): 3315–3323, 2007. [9] Gamaleldin F Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alex Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein. Adversarial examples that fool both human and computer vision. Advances in Neural Information Processing Systems, 2018. [10] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. Robust physical-world attacks on machine learning models. IEEE Conference on Computer Vision and Pattern Recognition, 2018. [11] Samuel M Harding, Prashanth Rajivan, Bennett I Bertenthal, and Cleotilde Gonzalez. Human decisions on targeted and non-targeted adversarial samples. Proc. of the 40th Annual Conference of the Cognitive Science Society, pages 451–456, 2018. [12] James M Hillis, Marc O Ernst, Martin S Banks, and Michael S Landy. Combining sensory information: mandatory fusion within, but not between, senses. Science, 298(5598):1627–1630, 2002. [13] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks on neural network policies. International Conference on Learning Representations, 2017. [14] Julia R Irwin, DH Whalen, and Carol A Fowler. A sex difference in visual inﬂuence on heard speech. Perception & Psychophysics, 68(4):582–592, 2006. [15] Thorsten Joachims. Text categorization with support vector machines: Learning with many relevant features. In European conference on machine learning, pages 137–142. Springer, 1998. [16] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint arXiv:1607.02533, 2016. [17] Harry McGurk and John MacDonald. Hearing lips and seeing voices. Nature, 264(5588):746, 1976. [18] Elizabeth A Mongillo, Julia R Irwin, DH Whalen, Cheryl Klaiman, Alice S Carter, and Robert T Schultz. Audiovisual processing in children with and without autism spectrum disorders. Journal of autism and developmental disorders, 38(7):1349–1358, 2008. 9 [19] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y Ng. Multimodal deep learning. In Proceedings of the 28th international conference on machine learning, pages 689–696, 2011. [20] Linda W Norrix, Elena Plante, and Rebecca Vance. Auditory–visual speech integration by adults with and without language-learning disabilities. Journal of Communication Disorders, 39(1):22–36, 2006. [21] Linda W Norrix, Elena Plante, Rebecca Vance, and Carol A Boliek. Auditory-visual integration for speech by children with and without speciﬁc language impairment. Journal of Speech, Language, and Hearing Research, 50(6):1639–1651, 2007. [22] Nicolas Papernot, Patrick McDaniel, Arunesh Sinha, and Michael Wellman. Towards the science of security and privacy in machine learning. arXiv preprint arXiv:1611.03814, 2016. [23] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pages 506–519, 2017. [24] Daniel Pressnitzer, Jackson Graves, Claire Chambers, Vincent De Gardelle, and Paul Egré. Auditory perception: Laurel and yanny together at last. Current Biology, 28(13):R739–R741, 2018. [25] Laasya Samhita and Hans", "meta": {"paper_id": "1906.01040v3", "chunk_index": 10, "filename": "1906.01040v3.pdf", "chunk_length": 512}}
{"id": "1906.01040v3_chunk_11", "text": "against machine learning. In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, pages 506–519, 2017. [24] Daniel Pressnitzer, Jackson Graves, Claire Chambers, Vincent De Gardelle, and Paul Egré. Auditory perception: Laurel and yanny together at last. Current Biology, 28(13):R739–R741, 2018. [25] Laasya Samhita and Hans J Gross. The “clever hans phenomenon” revisited. Communicative & integrative biology, 6(6):e27122, 2013. [26] Lea Schönherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa. Adversarial attacks against automatic speech recognition systems via psychoacoustic hiding. arXiv preprint arXiv:1808.05665, 2018. [27] Kaoru Sekiyama. Cultural and linguistic factors in audiovisual speech processing: The mcgurk effect in chinese subjects. Perception & Psychophysics, 59(1):73–80, 1997. [28] Kaoru Sekiyama, Takahiro Soshi, and Shinichi Sakamoto. Enhanced audiovisual integration with aging in speech perception: a heightened mcgurk effect in older adults. Frontiers in Psychology, 5:323, 2014. [29] Quentin Summerﬁeld. Lipreading and audio-visual speech perception. Phil. Trans. R. Soc. Lond. B, 335 (1273):71–78, 1992. [30] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics, 36(4):95, 2017. [31] Kathleen M Youse, Kathleen M Cienkowski, and Carl A Coelho. Auditory-visual speech perception in an adult with aphasia. Brain injury, 18(8):825–834, 2004. [32] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head models. arXiv preprint arXiv:1905.08233, 2019. [33] Zhenglong Zhou and Chaz Firestone. Humans can decipher adversarial images. Nature communications, 10(1):1334, 2019. 10", "meta": {"paper_id": "1906.01040v3", "chunk_index": 11, "filename": "1906.01040v3.pdf", "chunk_length": 240}}
{"id": "1906.01157v2_chunk_0", "text": "IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 1 A Review of Automated Speech and Language Features for Assessment of Cognitive and Thought Disorders Rohit Voleti, Student Member, IEEE, Julie M. Liss, Visar Berisha, Member, IEEE, Abstract—It is widely accepted that information derived from analyzing speech (the acoustic signal) and language production (words and sentences) serves as a useful window into the health of an individual’s cognitive ability. In fact, most neuropsychological testing batteries have a component related to speech and language where clinicians elicit speech from patients for subjective evalu- ation across a broad set of dimensions. With advances in speech signal processing and natural language processing, there has been recent interest in developing tools to detect more subtle changes in cognitive-linguistic function. This work relies on extracting a set of features from recorded and transcribed speech for objective assessments of speech and language, early diagnosis of neurological disease, and tracking of disease after diagnosis. With an emphasis on cognitive and thought disorders, in this paper we provide a review of existing speech and language features used in this domain, discuss their clinical application, and highlight their advantages and disadvantages. Broadly speaking, the review is split into two categories: language features based on natural language processing and speech features based on speech signal processing. Within each category, we consider features that aim to measure complementary dimensions of cognitive- linguistics, including language diversity, syntactic complexity, semantic coherence, and timing. We conclude the review with a proposal of new research directions to further advance the ﬁeld. Index Terms—cognitive linguistics, vocal biomarkers, Alzheimer’s disease, schizophrenia, thought disorders, natural language processing I. INTRODUCTION E ARLY detection of neurodegenerative disease and episodes of mental illness that impact cognitive function (henceforth, cognitive disorder and thought disorder, respec- tively) is a major goal of current research trends in speech and language processing. These afﬂictions have both signiﬁcant societal and economic impacts on affected individuals. It is estimated that approximately one in six adults in the United States lives with some form of mental illness, according to the National Institute of Mental Health (NIMH), totaling 44.6 million people in 2016 [1]. In the United States alone, some estimate that the economic burden of mental illness is approximately $1 trillion annually [2]. The World Health Organization estimates that, in 2015, the global burden of Alzheimer’s disease and dementia equaled 1.1% of the global R. Voleti is with the School of Electrical, Computer, & Energy Engineering, Arizona State University, Tempe, AZ, 85281 USA e-mail: rnvoleti@asu.edu Manuscript received 16 May 2019; revised 23 Aug 2019; revised 24 Oct 2019; Accepted 26 Oct 2019 Initial Prompt ASR or Manual Transcription Machine Learning Model Clincal Decision Semantic Coherence Syntactic Complexity Lexical Complexity Natural Language Processing Record Subject Response Articulation Prosody & Rate Pause Rate Speech Signal Processing Fig. 1: Overview of the process of using natural language processing and speech signal processing for extraction of speech and language features for clinical decision-making. Example language features include lexical complexity, syntac- tic complexity, semantic coherence, etc. Example of acoustic speech features include pause", "meta": {"paper_id": "1906.01157v2", "chunk_index": 0, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_1", "text": "Rate Pause Rate Speech Signal Processing Fig. 1: Overview of the process of using natural language processing and speech signal processing for extraction of speech and language features for clinical decision-making. Example language features include lexical complexity, syntac- tic complexity, semantic coherence, etc. Example of acoustic speech features include pause rate, prosody, articulation, etc. gross domestic product [3]. The ability to identify early signs and symptoms is critical for the development of interventions to impact progression or episodes. Many aspects of cognitive and thought disorders are man- ifest in the way speech is produced and what is said. Irre- spective of the underlying disease or condition, the analysis of speech and language can provide insight to the underlying neural function. This has motivated current research trends in quantitative speech and language analytics, with the hope of eventually developing clinically-validated algorithms for better diagnosis, prediction, and characterization of these conditions. This has both long-term and nearer-term potential for impact. J-STSP-AAHD-00183-2019 © 2019 IEEE arXiv:1906.01157v2 [cs.CL] 5 Nov 2019 IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 2 Category Subcategory Features or Methods Used Cognitive & Thought Disorder(s) Assessed Text-based Lexical features Bag of words vocabulary analysis Semantic dementia (SD) [4]; Alzheimer’s disease (AD) [5] Linguistic Inquiry & Word Count (LIWC) [6] Mild cognitive impairment (MCI) [7], schizophrenia (Sz/Sza) [8]; AD [9] Lexical Diversity (TTR, MATTR, BI, HS, etc.) AD [5], [10], [11]; primary progressive aphasia (PPA) [12], Sz/Sza [13], bipolar disorder (BPD) [13] Lexical Density (content density, idea density, P-density) MCI [14]; AD [5], [11], [9]; PPA [12]; chronic traumatic encephalopathy (CTE) [15]; Sz/Sza [13], BPD [13] Part-of-speech (POS) tagging MCI [14]; PPA [12]; AD [5], [9]; Sz/Sza [16], [17], [18] Syntactical features Constituency-based parse tree scores (Yngve [19], Frazier [20]) AD [5], [9]; MCI [14]; PPA [12] Dependency-based parse tree scores MCI [14] Speech graphs and attributes Sz/Sza [21], [22]; BPD [21], [22]; MCI [23] Semantic features Word & sentence embeddings: - LSA [24] Sz/Sza [25], [16], [17] - Neural word embeddings (word2vec [26], GloVe [27], etc.) Sz/Sza [28], [18], [13]; BPD [13] - Neural sentence embeddings (SIF [29], InferSent [30], etc.) Sz/Sza [18], [13]; BPD [13] Topic modeling: - LDA [31] Sz/Sza [28], [8] - Vector-space topic modeling with neural networks AD [32], [33]; MCI [33] Semantic role labeling [34] Sz/Sza [28] Pragmatics Sentiment analysis Sz/Sza [28] Acoustic Prosodic features Temporal (pause rate, phonation rate, voiced durations, etc.) MCI [14], [35], [36], [37]; AD [35], [38]; Sz/Sza [39] Frontotemporal lobal degeneration (FTLD) [40] Fundamental frequency (F0) and trajectory AD [38]; BPD [41] Loudness and energy AD [38] Emotional content AD [38] Spectral features Formant trajectories (F1, F2, F3, etc.) PPA [42]; AD [5] Spectral centroid [43] AD [38] MFCC statistics [44] PPA [42]; AD [5] Vocal quality Jitter, shimmer, harmonic-to-noise ratio (HNR) PPA [42]; AD [5], [38] ASR-related Phone-level detection of ﬁlled pauses & temporal features MCI [36], [37] Improving WER for clinical data AD [45], [9]; neurodegenerative dementia (ND) [46], [47], [48] TABLE I: Summary of work covered in this review, including textual and acoustic features to", "meta": {"paper_id": "1906.01157v2", "chunk_index": 1, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_2", "text": "shimmer, harmonic-to-noise ratio (HNR) PPA [42]; AD [5], [38] ASR-related Phone-level detection of ﬁlled pauses & temporal features MCI [36], [37] Improving WER for clinical data AD [45], [9]; neurodegenerative dementia (ND) [46], [47], [48] TABLE I: Summary of work covered in this review, including textual and acoustic features to assess cognitive and thought disorders that are automatically extracted from speech and language samples using NLP and speech signal processing. Note that in the abbreviations in the table, “Sz” refers to schizophrenia and “Sza” refers to the related schizoaffective disorder, which are considered together for simplicity in this summary. In the long term, there is the potential for new diagnostics and early intervention for improved treatment outcomes and reduced economic burden. In the nearer term, there is the potential for improving the efﬁciency of clinical trials evalu- ating new drugs. It is generally accepted that early enrollment in clinical trials for evaluation of new drugs maximizes the chances of showing that a drug is successful [49], [50]. In addition, adopting endpoints that are more sensitive to change means that these studies can be powered with fewer partic- ipants. Digital endpoints collected frequently have recently garnered interest in this domain [51]. In this review, we limit our focus to speech and language analysis for cognitive and thought disorders in the context of neurodegenerative disease and mental illness. In keeping with the Diagnostic and Statistical Manual of Mental Disorders (DSM-5) [52] classiﬁcation system, “cognitive disorders” refer to disturbances in memory and cognition, whereas “thought disorders” refer to the inability to produce and sustain coherent communication. Thus, Alzheimer’s disease (the most common form of dementia) is an example of a cognitive disorder, while schizophrenia is an example of a mental illness that presents as a thought disorder. With access to clinical speech and language databases along with recent developments in the ﬁelds of speech signal processing, computational linguistics, and machine learning, there is an increased potential for using computational methods to automate the analysis of speech and language datasets for clinical applications [2]. Objective analysis of this sort has the potential to overcome some of the limitations asso- ciated with the current state-of-the-art for improved diagno- sis, prediction, and characterization of cognitive and thought disorders. A high-level block diagram of existing methods in clinical-speech analytics is shown in Figure 1. Patients provide speech samples via a speech elicitation task. This could be passively collected speech, patient interviews, or recorded neuropsychological batteries. The resulting speech is transcribed, using either automatic speech recognition (ASR) or manual transcription, and a set of speech and language features are extracted that aim to measure different aspects of cognitive-linguistic change. These features become the input of a machine learning model that aims to predict a dependent variable of interest, e.g. detection of clinical conditions or assessment of social skills [53], [14], [13]. Perhaps the most important parts of the analysis frame- work in Figure 1 are the analytical methods used to extract clinically-relevant features from the samples. With a focus on cognitive and thought disorders, we provide a survey of the", "meta": {"paper_id": "1906.01157v2", "chunk_index": 2, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_3", "text": "of clinical conditions or assessment of social skills [53], [14], [13]. Perhaps the most important parts of the analysis frame- work in Figure 1 are the analytical methods used to extract clinically-relevant features from the samples. With a focus on cognitive and thought disorders, we provide a survey of the existing literature and common speech and language features used in this context. The review is not focused on a particular disease, but rather on the methods for extracting clinically- relevant measures from the resultant speech. A summary of the work reviewed in this paper can be seen in Table I and will be discussed in the subsequent sections. In the next section, we aim to place our review in context. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 3 We provide an overview of speech and language production, highlight the existing neuropsychological assessments used to evaluate speech and language, and provide an overview of the the speech and language dimensions that we focus on in this review. Following that section, the review is split into two parts: natural language processing (NLP) features and speech signal processing features. With NLP, we can measure the complexity and coherence of language and with speech signal processing, we can measure acoustic proxies related to cognitive process- ing. Finally, in Section V we discuss gaps in current research, propose future directions, and end with concluding remarks. II. BACKGROUND A. Spoken Language Production The production of spoken language in humans is a complex, multi-stage process that involves high levels of memory, cognition, and sensorimotor function. There are three distinct stages [55]: 1) Conceptualization: involves the formation of abstract ideas about the intended message to be communicated 2) Formulation: involves forming the exact linguistic con- struction of the utterance to be spoken 3) Articulation: involves actually producing sounds using the various components of the speech production system, i.e. lungs, glottis, larynx, vocal tract, etc. These stages are visually represented in the block diagram in Figure 2. In the conceptualization stage, pre-verbal ideas are formed to link a desired concept to be expressed to the spoken language that is eventually formed. The formulation stage con- sists of several distinct components: (a) lexical, syntactical, & grammatical formulations, (b) morpho-phonological encoding, and (c) phonetic encoding. This involves forming the linguistic structure of a spoken utterance, determining which syllables are needed to articulate the utterance, and the creation of an articulatory score containing instructions that are to be executed by the vocal apparatus in the articulation stage [55]. Cognitive and thought disorders have the ability to affect any of these stages, but broadly, they can be captured through analysis of “content” (what is said) and “form” (how it is said). Indeed, the tools used to characterize content and form of speech are agnostic to the underlying condition. It is the constellation of features shown to be affected that converge on the locus of deﬁcit for an individual. For example, speech that lacks coherence of ideas and jumps from topic to topic (impaired content), and is produced very rapidly and without", "meta": {"paper_id": "1906.01157v2", "chunk_index": 3, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_4", "text": "speech are agnostic to the underlying condition. It is the constellation of features shown to be affected that converge on the locus of deﬁcit for an individual. For example, speech that lacks coherence of ideas and jumps from topic to topic (impaired content), and is produced very rapidly and without pauses (impaired form), would point toward a thought or mood disorder, such as schizophrenia or mania. A person with dementia may present with reduced vocabulary size (impaired content), and with increased number and duration of pauses (impaired form). To reiterate, the speech and language measures are, themselves, agnostic to the underlying disorder. Rather, it is the constellation of deﬁcit patterns that associate with different etiologies. Our aim here is to provide an overview of the methods used to extract these constellations without focusing on a particular disease area. We refer to clinical applications in each section to highlight existing work that uses these features in clinical applications. B. Clinical Assessment of Speech & Language for Cognitive & Thought Disorders A variety of clinical protocols exist for the evaluation and diagnosis of disorders affecting cognitive function in psychiatry and neurology. The DSM-5 [52], published by the American Psychiatric Association (APA), provides the standard diagnostic criteria for psychiatric and neurocognitive disorders, and it is updated as knowledge in the ﬁeld evolves. The DSM-5 covers a large spectrum of psychiatric and cog- nitive disorders, such as depression, anxiety, schizophrenia, bipolar disorder, dementia, and several more. Based on these criteria, many evaluation methodologies have been developed in clinical practice for diagnosing and evaluating these cognitive disorders. The mental status exam- ination (MSE) is a commonly utilized and multi-faceted tool for screening an individual at a given point in time for signs of neurological and psychiatric disorders [56]. Components of the MSE evaluate affect, mood, appearance, judgment, speech, thought process, and overall cognitive ability through a variety of tasks and surveys. Related screenings include the mini-mental state examination (MMSE) [57], Addenbrooke’s Cognitive Examination (ACE) [58], and the Montreal Cogni- tive Assessment (MoCA) [59] for evaluating conditions like mild cognitive impairment (MCI), dementia, and Alzheimer’s disease (AD). Other forms of disorders, i.e. schizophrenia and bipolar disorder, can be evaluated with screenings such as the Clinical Assessment Interview for Negative Symptoms (CAINS) [60], Brief Negative Symptom Scale (BNSS) [61], the Social Skills Performance Assessment (SSPA) [53], and several others that measure the effects of thought and mood disorders. All of these neuropsychological batteries for evaluating cognitive health have a signiﬁcant speech and language as- sessment, as cognitive-linguistic function is a strong biomarker for neuropsychological health in many dimensions. However, ratings for narrative, recall, conversational, or other spoken language tasks are often subjective in nature and of variable reliability, making the underlying diagnosis more challenging [62]. While the diagnosis for many common psychiatric con- ditions has become more consistent over time as they are better understood, others (e.g. schizoaffective disorder) are often evaluated inconsistently by different clinical assessors due to the subjective nature of the test batteries applied [63], [64]. The speech and language samples collected during these screenings serve", "meta": {"paper_id": "1906.01157v2", "chunk_index": 4, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_5", "text": "many common psychiatric con- ditions has become more consistent over time as they are better understood, others (e.g. schizoaffective disorder) are often evaluated inconsistently by different clinical assessors due to the subjective nature of the test batteries applied [63], [64]. The speech and language samples collected during these screenings serve as potentially valuable databases for objective and automatically computable measures of cognitive-linguistic ability. Recent research suggests that analysis of this rich data allows us to explore several new objective dimensions for evaluation, which has a largely untapped potential to improve clinical assessment and outcomes. These new tools have the potential to provide a ﬁner-grained analysis of the resultant speech when compared against existing rating scales. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 4 Intent to speak Conceptualization Abstract idea formation Linguistic formulation Lexicon (word choice) Syntax (sentence structure, grammar, etc.) Semantics Pragmatics / Sentiment Formulation Morpho-phonological encoding: Mapping into language code Form phonetic sequences Form prosodic information Generate neuromuscular commands Phonetic encoding: Articulatory \"score\" Articulation Source Vibrate vocal folds Filter Respiration muscular action Vocalized speech Articulatory motor actions to shape vocal tract Acoustic feedback Proprioceptive feedback Fig. 2: Speech production block diagram model, adapted and modiﬁed from [54]. In this review, we focus primarily on the additional box we termed “Linguistic Formulation” within the formulation stage of speech production. Cognitive and thought disorders that affect this area have direct measurable outputs on the actual language content which can be studied by statistical text-based analysis. Additionally, they also have indirect downstream effects on the vocalized and articulated speech acoustics. Both of these areas are covered in our review. C. Speech & Language Dimensions of Interest Natural spoken language contains several measurable di- mensions that indicate various aspects of cognitive health. In this review, we are interested in the analysis of linguistic and acoustic speech features that are indicative of cognitive and thought disorders related to thought content and thought orga- nization. These include a variety of neurological impairments (e.g. MCI, dementia, AD, chronic traumatic encephalopathy) and psychiatric conditions (e.g. schizophrenia, schizoaffective disorder, bipolar disorder). Most of the work in this space exists in the context of textual language analysis, either by manual or automatic transcription of spoken utterances. Looking at Figure 2, we focus mainly on the “Linguistic formulation” area within the formulation stage. Neurological thought disorders all affect the ability of an individual to form complex thoughts and sentence structures, and may often have issues such as poverty of speech or disorganized speech. Therefore, we look at methods for examining thought content density, complexity of sentence syntax, semantic coherence, and sentiment analysis as they relate to these conditions. Analysis of acoustic speech samples leads to additional insight for characterizing neurological and psychiatric thought disorders, as impairments in language formation in turn affect the articulation of the spoken output. As seen in Figure 2, the articulation pathway that leads to speech output depends upon the cognitive ability required for the conceptualization and formulation stages of speech production. Cognitive and working memory disorders can lead to impairments in neuro- muscular", "meta": {"paper_id": "1906.01157v2", "chunk_index": 5, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_6", "text": "formation in turn affect the articulation of the spoken output. As seen in Figure 2, the articulation pathway that leads to speech output depends upon the cognitive ability required for the conceptualization and formulation stages of speech production. Cognitive and working memory disorders can lead to impairments in neuro- muscular motor coordination and proprioceptive feedback as well, affecting speech output [65]. Among the speech signal features considered are those related to temporal analysis and prosody (e.g. pause rate, phonation rate, periodicity of speech, etc.) and those related to frequency analysis (e.g. mean, variance, kurtosis of Mel frequency cepstral coefﬁcients). We note that the purpose of this review is to highlight recent research that identiﬁes and characterizes automatically computed speech and language features related to neurological and psychiatric disorders of thought content and formulation. In each part, we will provide an overview of commonly used techniques for extracting various speech and language features, present examples of their clinical application, and discuss the advantages and disadvantages of the methods reviewed. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 5 III. MEASURING COGNITIVE AND THOUGHT DISORDERS WITH NATURAL LANGUAGE PROCESSING In this section, we will provide a review of several families of natural language processing methods that range from simple lexical analysis to state-of-the-art language models that can be utilized for clinical assessment. The sections below present families of approaches in order of increasing complexity. In the ﬁrst section, we describe methods based on subjective evaluation of speech and lan- guage; then we discuss methods that rely on lexeme-level information, followed by methods that rely on sentence-level information, and end with methods that rely on semantics. For each section, we provide a description of representative approaches and a review of how these methods are used in clinical applications. We end each section with a discussion of the advantages and disadvantages of the approaches in that section. A. Early Work Simple analysis of written language samples has long been thought to provide valuable information regarding cognitive health. One of the best-known early examples of such work is the famous “nun study” by Snowdon et al. on linguistic ability as a predictor of Alzheimer’s disease (AD) [66]. In this work, manual evaluations of the linguistic abilities of 93 nuns were conducted by analysis of autobiographical essays they had written earlier in their lives. The researchers evaluated the linguistic structure of the essays by scoring the gram- matical complexity and idea density in the writing samples. In particular, the study found that low idea density in early life was a particularly strong predictor of reduced cognitive ability or the presence of AD in later life. Roughly 80% of the participants that were determined to lack linguistic complexity in their writings developed AD or had mental and cognitive disabilities in their older age. This work was groundbreaking in showing that linguistic structure and complexity can serve as a strong predictor for the onset of AD and potentially other forms of cognitive impairment. However, it required tedious manual analysis of writing samples and careful consideration that the", "meta": {"paper_id": "1906.01157v2", "chunk_index": 6, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_7", "text": "and cognitive disabilities in their older age. This work was groundbreaking in showing that linguistic structure and complexity can serve as a strong predictor for the onset of AD and potentially other forms of cognitive impairment. However, it required tedious manual analysis of writing samples and careful consideration that the scores given by different evaluators had a high correlation, due to the subjective nature of the scoring. These factors make in-clinic use prohibitive; as a result, these methods have received limited attention in follow-on work. The development of automated and quantitative metrics to analyze language complexity can potentially save several hours of research time to conduct similar linguistic studies to understand neurodegenerative disease and mental illness. Several techniques devised in the NLP literature have been utilized to address the challenge of conducting quantitative analysis to replace traditionally subjective and task-dependent methods of measuring linguistic complexity. B. First Order Lexeme-Level Analysis 1) Methods: Automated ﬁrst-order lexical analysis, i.e. at the lexeme-level or word-level, can generate objective language metrics to provide valuable insight into cognitive function. The most basic approaches treat a body of text as a bag of words, meaning the ordering of words within the text is not considered. This can be done by simply considering the frequency of usage of particular words and how they relate to a group of individuals. Specialized tools, such as Linguistic Inquiry and Word Count (LIWC) [6], are often used to analyze the content and categorize the vocabulary within a text. LIWC associates words in a text with categories associated with affective processes (i.e. positive/negative emotions, anxiety, sadness, etc.), cognitive processes (i.e. insight, certainty, cau- sation, etc.), social processes (i.e. friends, family, humans), the presence of dysﬂuencies (pauses, ﬁller words, etc.), and many others. The categorization of the lexicon allows for further tasks of interest, such as sentiment analysis based on the emotional categories. The frequency of usage and other statistics of words from particular categories can lend insight to overall language production. The concept of lexical diversity refers to a measure of unique vocabulary usage. The type-to-token ratio (TTR), given in Equation (1), is a well-known measure of lexical diversity, in which the number of unique words (types, V ) are compared against the total number of words (tokens, N). TTR = V N (1) However, TTR is negatively impacted for longer utterances, as the diversity of unique words typically plateaus as the number of total words increase. The moving average type-to-token ratio (MATTR) [67] is one method which aims to reduce the dependence on text length by considering TTR over a sliding window of the text. This approach does not have a length- based bias, but is considerably more variable as the parameters are estimated on smaller speech samples. Brunét’s Index (BI) [68], deﬁned in Equation (2), is another measure of lexical diversity that has a weaker dependence on text length, with a smaller value indicating a greater degree of lexical diversity, BI = N V −0.165. (2) An alternative is also provided by Honoré’s Statistic (HS) [69], deﬁned in Equation (3), which emphasizes", "meta": {"paper_id": "1906.01157v2", "chunk_index": 7, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_8", "text": "in Equation (2), is another measure of lexical diversity that has a weaker dependence on text length, with a smaller value indicating a greater degree of lexical diversity, BI = N V −0.165. (2) An alternative is also provided by Honoré’s Statistic (HS) [69], deﬁned in Equation (3), which emphasizes the use of words that are spoken only once (denoted by V1), HS = 100 log N 1 −V1/V . (3) The exponential and logarithm in the BI and the HS reduce the dependence on the text length, while still using all samples to estimate the diversity measure, unlike the MATTR. Measures of lexical density, which quantify the degree of information packaging within an utterance, may also be useful for cognitive assessment. Content words1 (i.e. nouns, verbs, adjectives, adverbs) tend to carry more information than function words2 (e.g. prepositions, conjunctions, interjections, 1Content words are also referred to as “open-class”, meaning new words are often added and removed to this category of words as language changes over time. 2Function words are also referred to as “closed-class” since words are rarely added to or removed from these categories. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 6 etc.). These can be used to compute notions of content density (CD) in written or spoken language, given in Equation (4), CD = # of verbs + nouns + adjectives + adverbs N . (4) Part-of-speech (POS) tagging of text samples is one way in which the word categories can be automatically determined; individual word tokens within a sentence are identiﬁed and labeled as the part-of-speech that they represent, typically from the Penn Treebank tagset [70]. Several automatic algorithms and available implementations exist for rule-based and statis- tical taggers, i.e. using a hidden Markov model (HMM) or maximum entropy Markov model (MEMM) implementation to determine POS tags with a statistical sequence model [71]. For example, the widely-used Stanford Tagger [72] uses a bidirectional MEMM model to assign POS tags to samples of text. Several notions of content density can be computed at the lexeme-level if POS tags can be automatically determined to reﬂect the role of each word in an utterance. Examples of these include: the propositional density (P-density), a measure of the number of expressed propositions (verbs, adjectives, adverbs, prepositions, and conjunctions) divided by the total number of words, and the content density, which is a measure of the ratio of content words to function words [14], [5]. 2) Clinical Applications: Several studies have utilized ﬁrst order lexical features to assess cognitive health by automated linguistic analysis. The simplest bag-of-words analysis for vocabulary usage can often provide valuable insight in this regard. For example, the work by Garrard et al. computed vocabulary statistics for participants with left- (n = 21) and right-predominant (n = 11) varieties of semantic dementia (SD) and, and compared them with language samples from healthy controls (n = 10) [4]. Classiﬁcation accuracy of over 90% was reached for categorizing the participants for two tasks: (1) participants with SD against the healthy control participants, and (2) classifying the left- and", "meta": {"paper_id": "1906.01157v2", "chunk_index": 8, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_9", "text": "= 11) varieties of semantic dementia (SD) and, and compared them with language samples from healthy controls (n = 10) [4]. Classiﬁcation accuracy of over 90% was reached for categorizing the participants for two tasks: (1) participants with SD against the healthy control participants, and (2) classifying the left- and right-predominant variants of SD. They used the concept of information gain to determine which word types were most useful in each classiﬁcation problem. Asgari et al. used the LIWC tool [6] to study the language of those with mild cognitive impairment (MCI), often a precursor to Alzheimer’s disease (AD) [7]. The transcripts of unstructured conversation with the study’s participants were analyzed with LIWC to generate a 68- dimensional vector of word counts that fall within each of the 68 subcategories in the LIWC lexicon. They were able to achieve over 80% classiﬁcation accuracy by selecting LIWC categories that best represented the difference in the MCI and healthy control datasets. Roark et al. considered a larger variety of speech and language features to detect MCI [14]. In this work, the authors compared the language of elderly healthy control participants and patients with MCI on the Wechsler Logical Memory I/II Test [73], in which participants are tested on their ability to retell a short narrative that has been told them at different time points3. Among the features considered included multiple measures of lexical density. POS tagging was performed on 3Asked to retell the story immediately (LM1) and after approximately 30 minutes (LM2) the transcripts of clinical interviews of patients with MCI and healthy control participants. Two measures of lexical density derived from the POS tags were the P-density and the content density. In particular, the content density was a strong indicator of group differences between healthy controls and patients with MCI. The automated language features were used in conjunction with speech features and clinical test scores to train a support vector machine (SVM) classiﬁer that achieved good leave-pair-out cross validation results in classifying the two groups (AUC = 0.732, 0.703, 0.861 when trained on language features, language features + speech features, and language + speech features + test scores, respectively)4. Bucks et al. [10] and Fraser et al. [5] both used several ﬁrst-order lexical features in their analysis of patients with AD. In [10], the authors successfully discriminated between a small sample of healthy older control participants (n = 16) and patients with AD (n = 8) using TTR (Equation (1)), BI (Equation (2)), and HS (Equation (3)) as measures of lexical diversity or vocabulary richness. They additionally considered the usage rates of other parts of speech (i.e. nouns, pronouns, adjectives, verbs). In particular, TTR, BI, verb- rate, and adjective-rate all indicated strong group differences between the participants with AD and healthy controls; the groups could be classiﬁed with a cross-validation accuracy of 87.5%. In [5], Fraser et al. performed similar work using the DementiaBank5 database to obtain patient transcripts. They additionally used other vocabulary-related features, such as frequency, familiarity, and imageability values for words in the transcripts. This work was in turn based", "meta": {"paper_id": "1906.01157v2", "chunk_index": 9, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_10", "text": "groups could be classiﬁed with a cross-validation accuracy of 87.5%. In [5], Fraser et al. performed similar work using the DementiaBank5 database to obtain patient transcripts. They additionally used other vocabulary-related features, such as frequency, familiarity, and imageability values for words in the transcripts. This work was in turn based on a previous study [12] in which similar features were extracted to study the language of participants with two different subtypes6 of primary progressive aphasia (PPA) and healthy control subjects. Berisha et al. performed a longitudinal analysis of non- scripted press conference transcripts from U.S. Presidents Ronald Reagan (who was diagnosed with AD late in life) and George H.W. Bush (no such diagnosis) [11]. Among the linguistic features that were tracked were the lexical diversity and lexical density for both presidents over several years worth of press conference transcripts. The study shows that the number of unique words used by Reagan over the period of his presidency steadily declined over time, while no such changes were seen for Bush. These declines predated his diagnosis of AD in 1994 by 6 years, suggesting that these computed lexical features may be useful in predicting the onset of AD pre-clinically. A related study examined the language in interview transcripts of professional American football players in the National Football League (NFL) [15], at high-risk for neurological damage in the form of chronic traumatic encephalopathy (CTE). The study longitudinally measured TTR (Equation 1) and CD7 (Equation 4) in interview tran- scripts of NFL players (n = 10) and NFL coaches/front 4Additional language and speech features will be discussed later 5https://dementia.talkbank.org/access/, Accessed August 20, 2019 6Progressive nonﬂuent aphasia (PNFA) and semantic dementia (SD) 7The authors in [15] refer to CD simply as “lexical density” (LD) IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 7 ofﬁce executives8 (n = 18). Previous work has shown that TTR and CD are expected to increase or remain constant as healthy individuals age [74], [75], [76]. However, this study demonstrated clear longitudinal declines in both variables for the NFL players while showing the expected increase in both variables for coaches and executives in similar contexts. 3) Advantages & Disadvantages: It is clear from the liter- ature that ﬁrst-order lexeme-level features, i.e. those related to lexical diversity and density, are useful biomarkers for detecting the presence or predicting the onset of a variety of conditions, such as MCI, AD, CTE, and potentially several others. POS tagging has several reliable and accurate imple- mentations, and these features are simple and easy to compute. Additionally, these linguistic measures are easily clinically interpretable for measuring cognitive-linguistic ability. However, lexeme-level features are limited in what informa- tion they provide alone, and many of the previously discussed works used these features in conjunction with several other speech and language features to build their models for classi- ﬁcation and prediction of disease onset. Since these measures are based on counting particular word types and tokens, they tell us little about how individual lexical units interact with each other in a full sentence or phrase. Additionally, measures of", "meta": {"paper_id": "1906.01157v2", "chunk_index": 10, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_11", "text": "speech and language features to build their models for classi- ﬁcation and prediction of disease onset. Since these measures are based on counting particular word types and tokens, they tell us little about how individual lexical units interact with each other in a full sentence or phrase. Additionally, measures of lexical diversity and lexical density provide little insight re- garding semantic similarity between words within a sentence. For example, the words “car”, “vehicle”, and “automobile” are all counted as unique words, despite there being a clear semantic similarity between them9 In the following sections, we will discuss more complex language measures that aim to address these issues. C. Sentence-Level Syntactical Analysis Generating free-ﬂowing speech requires that we not only de- termine which words best convey an idea, but also to determine the order in which to sequence the words in forming sentences. The complexity of the sentences we structure provides a great deal of insight into cognitive-linguistic health. In this section we provide an overview of various methods used to measure syntactic complexity as a proxy for cognitive health. 1) Methods: The ordering of words in sentences and sen- tences in paragraphs can also provide important insight into cognitive function. Many easy-to-compute and common struc- tural metrics of language include the mean length of a clause, mean length of sentence, ratio of number of clauses to number of sentences, and other related statistics [5]. Additionally, several more complicated methods for syntactical analysis of natural language can also be used to gain better insight for assessing linguistic complexity and cognitive health. A commonly used technique involves the parsing of natu- rally produced language based on language-dependent syntac- tical and grammatical rules. A constituency-based parse tree is generated to decompose a sentence or phrase into lexical units or tokens. In English, for example, sentences are read 8Coaches and executives were limited to those who were not former players experiencing similar head trauma to serve as a control in the language study. 9Note: lexical diversity is still a potentially useful measure in this case, as a diverse word choice may indicate higher cognitive function. she PRP was in a cook NN IN a school cafeteria DT NN DT NN AUX NP PP NP NP VP NP S Y: 0 F: 1 Y: 1 F: 1.5 Y: 1 F: 1 Y: 0 F: 0 Y: 0 F: 0 Y: 1 F: 1 Y: 0 F: 0 Y: 0 F: 0 Y: 1 F: 1 Y: 0 F: 0 Y: 0 F: 0 Y: 1 F: 0 Y: 2 F: 1 Y: 0 F: 1 1 2.5 1 1 2 2 1 0 1 1 2 1 1 0 0 0 Yngve: Frazier: (a) Constituency-based parsing of sample sentence (i.e. top-down and left to right). In the diagram, S = sentence, NP = noun phrase, VP = verb phrase, PP = prepositional phrase, PRP = personal pronoun, AUX = auxiliary verb, DT = determiner, NN = noun, and IN = preposition. The ﬁgure contains examples of both Yngve scoring (Y) [19], Frazier scoring (F) [20] for each", "meta": {"paper_id": "1906.01157v2", "chunk_index": 11, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_12", "text": "diagram, S = sentence, NP = noun phrase, VP = verb phrase, PP = prepositional phrase, PRP = personal pronoun, AUX = auxiliary verb, DT = determiner, NN = noun, and IN = preposition. The ﬁgure contains examples of both Yngve scoring (Y) [19], Frazier scoring (F) [20] for each branch of the tree. At the bottom is the total score of each type for each word token in the sentence summed up to the root of the tree. she was in a cook a school cafeteria (b) Dependency-based parsing of the same sample sentence. Lexical dependency distances can be computed.In this example, there are 7 total links, a total lexical dependency distance of 11, and an average distance of 11/7 = 1 4 7. Longer distances indicate greater linguistic complexity. Fig. 3: (a) A constituency-based and (b) dependency-based parsing of a simple sentence. Both adapted from [14]. left to right and are often parsed this way. An example of a common constituency-based left to right parse tree can be seen in Figure 3a for the sentence “She was a cook in a school cafeteria”, adapted from [14]. At the root node, the sentence is split into a noun phrase (“she”) and a verb phrase (“was a cook in a school cafeteria”). Then, the phrases are further parsed into individual tokens with a grammatical assignment (nouns, verbs, determiners, etc.). Simple sentences in the English language are often right-branching when using constituency- based parse trees. This means that the subject typically appears ﬁrst and is followed by the verb, object, and other modiﬁers. This is primarily the case for the sentence in Figure 3a. By contrast, left-branching sentences place verbs, objects, or modiﬁers before the main subject of a sentence [77]. Left- branching sentences are often cognitively more taxing as they involve more complex constructions that require a speaker to remember more information about the subject before the subject is explicitly mentioned. As a result, in English, the degree of left-branching within a particular parsing of a sentence can be used as a proxy for syntactic complexity. Once a parsing method has been implemented, various mea- sures of lexical and syntactical complexity can be computed IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 8 for each sentence or phrase. Yngve proposes one such method in [19]. Given the right-branching nature of simple English sentences, he proposes a measure of complexity based on the amount of left-branching in a given sentence. At each node in the parse tree, the rightmost branch is given a score of 0. Then, each branch to the left of it is given a score that is incremented by 1 when moving from right to left at a given node. The score for each token is the sum of scores up all branches to the root of the tree. An alternative scoring scheme for the same parse tree structure was proposed by Frazier [20]. He notes that embedded clauses within a sentence are an additional modiﬁer that can increase the complexity of the syntactical construction of that", "meta": {"paper_id": "1906.01157v2", "chunk_index": 12, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_13", "text": "sum of scores up all branches to the root of the tree. An alternative scoring scheme for the same parse tree structure was proposed by Frazier [20]. He notes that embedded clauses within a sentence are an additional modiﬁer that can increase the complexity of the syntactical construction of that sentence. Therefore, just as with left-branching language, the speaker or listener would need to retain more information in order to properly convey or interpret the full sentence, respectively. Frazier’s scoring method emphasizes the use of embedded clauses when evaluating the syntactic complexity. The scores are assigned to each lexeme as in Yngve’s scoring, but they are summed up to the root of the tree or the lowest node that is not the leftmost child of its parent node. Examples of both Yngve and Frazier scoring can be seen in Figure 3a. Another type of syntactical parsing of a sentence is known as dependency parsing, in which all nodes are treated as terminal nodes (no phrase categories such as verb phrase or noun phrase) [78]. A dependency-based parse tree aims to map the dependency of each word in a sentence or phrase to another within the same utterance. Methods proposed by Lin [79] and Gibson [80] provide some ways by which the lexical dependency distances can be determined. The general idea be- hind these methods is that longer lexical dependency distances within a sentence indicate a more complex linguistic structure, as the speaker and listener must remember more information about the dependencies between words in a sentence. An example of the same sentence is shown with a dependency- based parse tree in Figure 3b, also adapted from [14]. Mota et al. also propose a graph-theoretic approach for an- alyzing language structure as a marker of cognitive-linguistic ability with the construction of speech graphs [21], [22]. In this representation, the nodes are words that are connected to consecutive nodes in the sample text by edges representing lexical, grammatical, or semantic relationships between words in the text. As an examples, for a speech graph based on words in an utterance, the spoken language is ﬁrst transcribed and tokenized into individual lexemes, with each unique word by a graph node. Directed edges then connect consecutive words10. The researchers in this work suggest that structural graph features, i.e. loop density, distance (number of nodes) between words of interest, etc.) serve as clinically relevant objective language measures that give insight into cognitive function. An example speech-graph representation structure of an arbitrary utterance is seen in Figure 4a. The computed speech graph attributes (SGAs) are the features which are extracted from these graphs, and some common ones visualized in Figure 4b. The SGAs provide indirect measures of lexical diversity and 10Speech graphs in some studies, i.e. [81], may use POS tags or other node structures (a) Sample speech-graph representation of a spoken utterance. Each of the circular nodes represents a lexical unit (e.g. a single word) and the curved arrows represent edges which connect the relevant lexemes in the utterance. Attributes can be computed using the graph. N E", "meta": {"paper_id": "1906.01157v2", "chunk_index": 13, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_14", "text": "POS tags or other node structures (a) Sample speech-graph representation of a spoken utterance. Each of the circular nodes represents a lexical unit (e.g. a single word) and the curved arrows represent edges which connect the relevant lexemes in the utterance. Attributes can be computed using the graph. N E RE PE L3 L2 L1 LSC (b) Examples of speech graph attributes (SGAs). Examples include Nodes (N), Edges (E), Repeated Edges (RE) in same direction, Parallel Edges (PE), loops with 1, 2 and 3 nodes (L1, L2, L3), and the largest strongly connected component (LSC), i.e. the portion of the total graph that can be reached from all others when considering the directionality of edges. Fig. 4: (a) A sample speech-graph for a complete spoken utterance. (b) Example speech-graph attributes (SGAs). Both adapted from [22]. syntactic complexity. For example, N is the number of unique words, E is the total number of words, and repeated edges represent repeated words or phrases in text. 2) Clinical Applications: The structural aspects of spoken language have been shown to have clinical relevance for un- derstanding medical conditions that affect cognitive-linguistic ability. The previously mentioned work by Roark et al. also utilized several of the aforementioned methods to analyze the language of individuals with MCI and healthy control partic- ipants [14]. In addition to the lexeme-level features described in Section III-B, they also considered Yngve [19] and Frazier [20] scoring measures from constituency-based parsing of the transcripts of participant responses11. Mean, maximum, and average Yngve and Frazier scores were computed for each par- ticipant’s language samples. Roark et al. also used dependency parsing and computed lexical dependency distances, similar to the example in Figure 3b. Along with the lexical features and speech features, participants with MCI and healthy elderly control participants were classiﬁed successfully, as previously described in Section III-B. The speech-graph approach is used by Mota et al. to study the language of patients with schizophrenia and bipolar disor- 11Using the Charniak parser [82] IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 9 der (mania) [21], [22]. The researchers were able to identify structural features of the generated graphs (such as loop density, distance between words of interest, etc.) that serve as objective language measures containing clinically relevant information (e.g. ﬂight of thoughts, poverty of speech, etc.). Using these features, the researchers were able to visualize and quantify concepts such as the logorrhea (excessive wordiness and incoherence) associated with mania, evidenced by denser networks. Similarly, the alogia (poverty of speech) typical of schizophrenia was also visible in the generated speech-graph networks, as evidenced by a greater number of nodes per word and average total degree per node. Control participants, participants with schizophrenia, and participants with mania were classiﬁed with over 90% accuracy, signiﬁcantly improv- ing over traditional clinical measures, such as the Positive and Negative Syndrome Scale (PANSS) and Brief Psychiatric Rating Scale (BPRS) [21]. 3) Advantages & Disadvantages: Consideration of sentence-level syntactical complexity offers several advantages that address some of the drawbacks of lexeme-level analysis. As the work discussed here reveals,", "meta": {"paper_id": "1906.01157v2", "chunk_index": 14, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_15", "text": "signiﬁcantly improv- ing over traditional clinical measures, such as the Positive and Negative Syndrome Scale (PANSS) and Brief Psychiatric Rating Scale (BPRS) [21]. 3) Advantages & Disadvantages: Consideration of sentence-level syntactical complexity offers several advantages that address some of the drawbacks of lexeme-level analysis. As the work discussed here reveals, sentence structure metrics via syntactic parsing or speech-graph analysis offer powerful information in distinguishing healthy and clinical participants with schizophrenia, bipolar disorder/mania, mild cognitive impairment, and potentially several other conditions. Since sentence construction further taxes the cognitive-linguistic system beyond word ﬁnding, methods that capture sentence complexity provide more insight into the neurological health of the individual producing these utterances. This provides a multi-dimensional representation of cognitive-linguistics and allows for better characterization of different clinical conditions, as Mota et al. did with patients with schizophrenia and those with bipolar disorder/mania [21]. However, while offering the ability to analyze more complex sentence structures, sentence-level syntactical analysis is also prone to increased complexity due to large range of imple- mentation methodologies. For example, there are countless methods developed over the years for parsing language with different tools for measuring complexity relying on different algorithmic implementations of the language parsers, a widely studied topic in linguistic theory. A thorough empirical eval- uation of the various parsing methods is required to better characterize the performance of these methods in the context of clinical applications. D. Semantic Analysis Cognitive function is also characterized by one’s ability to convey organized and coherent thoughts through spoken or written language. Here, we will cover some of the fundamental methods in NLP for measuring semantic coherence that have been used in clinical applications. 1) Methods: Semantic similarity in natural language is typically measured computationally by embedding text into a high-dimensional vector space that represents its semantic content. Then, a notion of distance between vectors can be used to quantify semantic similarity or difference between the words or sentences represented by the vector embeddings. Fig. 5: A visual representation of latent semantic analysis (LSA) by singular value decomposition (SVD). Word embeddings are motivated by the distributional hy- pothesis in linguistics, a concept proposed by English linguist John R. Firth who famously stated “You shall know a word by the company it keeps” [83], i.e. that the inherent meaning of words is derived from their contextual usage in natural language. One of the earliest developed word embedding methods is latent semantic analysis (LSA) [24], in which word embeddings are determined by co-occurrence. In LSA, each unit of text (such as a sentence, paragraph, document, etc.) within a corpus is modeled as a bag of words. As per Firth’s hypothesis, the principal assumption of LSA is that words which occur together within a group of words will be semantically similar. As seen in Figure 5, a matrix (A) is generated in which each row is a unique word in the text (w1, . . . , wn) and each column represents a document or collection of text as described above (d1, . . . dd). The matrix entry values simply consist of the count of", "meta": {"paper_id": "1906.01157v2", "chunk_index": 15, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_16", "text": "a matrix (A) is generated in which each row is a unique word in the text (w1, . . . , wn) and each column represents a document or collection of text as described above (d1, . . . dd). The matrix entry values simply consist of the count of co-occurrence statistics, that is the number of times each word appears in each document. Then a singular value decomposition (SVD) is performed on A, such that A = UΣV T. Here, U and V are orthogonal matrices consisting of the left-singular and right- singular vectors (respectively) and Σ is a rectangular diagonal matrix of singular values. The diagonal elements of Σ can be thought to represent semantic categories, the matrix U represents a mapping from the words to the categories, and the matrix V represents a mapping of documents to the same categories. A subset of the r most signiﬁcant singular values is typically chosen, as shown by the matrix ˆΣ in Figure 5. This determines the dimension of the desired word embeddings (typically in the range of ~100-500). Similarly, the ﬁrst r columns of U form the matrix ˆU and the ﬁrst r rows of V T form the matrix ˆV T. The r-dimensional word embeddings for the n unique words in the corpus are given by the resulting rows of the product ˆU ˆΣ. Similarly, r-dimensional document embeddings can be generated by taking the d columns of the product ˆΣ ˆV T. In recent years, several new word embedding methods based on neural networks have gained popularity, such as word2vec [26] or GloVe [27], which have shown improved performance over LSA for semantic modeling when sufﬁcient training data is available [84]. As an example, we take a more detailed look at word2vec, proposed by Mikolov et al. , in which they present an efﬁcient method for predicting word IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 10 (a) Continuous bag-of-words (CBOW) Input Projection Output (b) Continuous skip-gram Input Projection Output Fig. 6: word2vec model architectures proposed in [26]. (a) In the CBOW model, the context words are inputs used to predict the center word. (b) In the skip-gram model, the center word is used to predict the context words. vectors based on very large corpora of text. They present two versions of the word2vec algorithm, a continuous bag- of-words (CBOW) model and continuous skip-gram model, seen in Figure 6. At the input in both implementations, every word in a corpus of text is uniquely one-hot encoded; i.e. in a corpus of V unique words, each word is uniquely encoded as a V -dimensional vector in which all elements are 0 except for a single 1. In the CBOW implementation (Figure 6a), the inputs are the context words in the particular neighborhood of a target center word, wt. In the skip-gram implementation (Figure 6b), the input is the center word and the objective is to predict the context words at the output. In both models, the latent hidden representation of dimension N gives an embedding for the word", "meta": {"paper_id": "1906.01157v2", "chunk_index": 16, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_17", "text": "in the particular neighborhood of a target center word, wt. In the skip-gram implementation (Figure 6b), the input is the center word and the objective is to predict the context words at the output. In both models, the latent hidden representation of dimension N gives an embedding for the word represented by the one-hot encoded input word. The training objective is to minimize the cross-entropy loss for the prediction outcomes. There are several other methods for word embeddings, each relying on the distributional hypothesis and each with various advantages and disadvantages. For example, LSA, word2vec and GloVe are simple to train and effective, but a major dis- advantage is that they do not handle out-of-vocabulary (OOV) words or consider words with multiple unrelated meanings. For example, the English word “bark” can refer to the bark of a dog or to the bark of a tree, but its vector representation would be an average representation, despite the drastically different usage in each context. Some methods based on deep neural networks (DNNs), such as recurrent neural network (RNN) / long-short term memory (LSTM) networks (e.g. ELMo [85]) or transformer architectures (e.g. BERT [86]) utilize contextual information to generate embeddings for OOV words. In addition to individual words, embeddings can also be learned at the sentence level. The simplest forms of sentence embeddings involve unweighted averaging of LSA, word2vec, GloVe, or other embeddings. Weighted averages can also be computed, such as by using term frequency-inverse docu- ment frequency (tf-idf) generated weights or Smooth Inverse Frequency (SIF) [29]. Others have found success learning sentence representations directly, such as in sent2vec [87]. Whole sentence encoders, such as InferSent [30] and the Universal Sentence Encoder (USE) [88] offer the advantage of learning a full sentence encoding that considers word order within a sentence; e.g. the sentences “The man bites the dog” and the “The dog bites the man” will each have different encodings though they contain the same words. Once an embedding has been deﬁned, a notion of semantic similarity or difference must also be deﬁned. Several notions of distance can be computed for vectors in high-dimensional space, such as Manhattan distance (ℓ1 norm), Euclidean dis- tance (ℓ2 norm), or many others. Empirically, the cosine similarity (cosine of the angle, θ, between vectors) has been found to work well in deﬁning semantic similarity between word and sentence vectors of many types. Cosine similarity can be computed using Equation (5) for vectors w1 and w2. CosSim(w1, w2) = cos θ = wT 1 w2 ∥w1∥2 ∥w2∥2 (5) In addition to word and sentence embedding semantic similarity measures, techniques such as topic modeling and semantic role labeling have also gained recently popularity in NLP and its applications to clinical language samples. Latent dirichlet analysis (LDA) is one such statistical topic modeling method which can be used to identify overarching themes in samples of text [31]. Other studies have utilized semantic role labeling, a probabilistic technique which automatically attempts to identify the semantic role a particularly entity plays in a sentence [34]. 2) Clinical Application: Many forms of mental illness can", "meta": {"paper_id": "1906.01157v2", "chunk_index": 17, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_18", "text": "modeling method which can be used to identify overarching themes in samples of text [31]. Other studies have utilized semantic role labeling, a probabilistic technique which automatically attempts to identify the semantic role a particularly entity plays in a sentence [34]. 2) Clinical Application: Many forms of mental illness can result in a condition known as formal thought disorder (FTD), which impairs an individual’s ability to produce semantically coherent language. FTD is most commonly associated with schizophrenia but is often present in other forms of mental illness such as mania, depression, and several others [89], [90]. Some common symptoms include poverty of speech (alogia), derailment of speech, and semantically incoherent speech (word salad) [90], [91]. Language metrics that track semantic coherence are potentially useful in clinical applications, such as measuring the coherence of language as it relates to FTD in schizophrenia. One of the ﬁrst studies to demonstrate this was conducted by Elvevåg et al. [25]. The language of patients with varying degrees of FTD (rated by standard clinical scales) was compared with a group of healthy control participants. The experimental tasks consisted of single word associations, IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 11 verbal ﬂuency (naming as many words as possible within a speciﬁc category), long interview responses (~1-2 minutes per response), and storytelling. LSA was utilized to embed the word tokens in the transcripts. The semantic coherence in each task was computed as follows: • Word Associations: Cosine similarity between cue word and response word, with an average coherence score for each participant • Verbal Fluency: Cosine similarity between ﬁrst and sec- ond word, second and third word, etc. were computed, with an average coherence score computed per participant • Interviews: Cosine similarity was computed between the question and participant responses. An average word vector was computed for the prompt question from the interviewer. Then a moving window (of size 2-6 words) for the participant response was used to average all the word vectors within the window and compute a cosine similarity between the question and response. The win- dow was moved over the entire participant response and a new cosine similarity was computed between the question and response window until reaching the end of the response. This method tracks how the cosine similarity behaves as the participant response goes farther from the question, with the expectation that the response would be more tangential over time with decreased coherence as the participant moves farther from the question. A regression line was ﬁt for each participant to measure the change in cosine similarity coherence over time, and the slope of the line was computed to measure the tangentiality of the response per participant. • Storytelling: Cosine similarity of the participant’s re- sponse was compared to the centroid participant response for all narrative utterances of the same story. This was used to predict the clinical rating for thought disordered language samples when asked to tell the same story. They demonstrated that the control participants had higher coherence scores compared to the FTD groups across all tasks. In", "meta": {"paper_id": "1906.01157v2", "chunk_index": 18, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_19", "text": "centroid participant response for all narrative utterances of the same story. This was used to predict the clinical rating for thought disordered language samples when asked to tell the same story. They demonstrated that the control participants had higher coherence scores compared to the FTD groups across all tasks. In a more recent study, predictive features of language for the onset of psychosis were studied by Bedi et al. [16]. Open- ended narrative-like interview transcripts of young individu- als who were determined to be at clinical high-risk (CHR) for psychosis were collected and analyzed to predict which individuals would eventually develop psychosis. Participants were tracked and interviewed over a period of two and a half years. In this study, LSA was again used to generate word embeddings. An average vector for each phrase was computed, and a cosine-similarity measure was computed to measure the semantic coherence between consecutive phrases (ﬁrst-order coherence) and every other phrase (second-order coherence). A distribution of the ﬁrst and second-order coherence scores (cosine similarities) was compiled for each participant, and several statistics were computed based on the distribution of coherence scores, e.g. maximum, minimum, standard de- viation, mean, median, 10th percentile, and 90th percentile. Each of these statistics was considered as a separating feature between the clinical and control samples. In addition to the semantic analysis, POS-tagging was performed to compute the frequency of use of each part-of-speech to obtain information about the structure of each participant’s naturally-produced language. The language features with the best predictive power in the classiﬁer were the minimum coherence between con- secutive phrases for each participant (maximum discontinuity) and the frequency of use of determiners (normalized by sen- tence length). This initial study only had 34 participants total (only 5 CHR+ participants) and was intended as a proof-of- principle exploration. In an expansion of this work, Corcoran et al. trained their classiﬁer using two larger datasets, in which one group of participants was questioned with a prompt- based protocol and another group of participants was given a narrative protocol in which they were required to provide longer answers (similar to the previous work) [17]. They note that the ﬁrst and second-order coherence metrics collected in the previous study were useful for determining semantic coher- ence with the narrative-style interview transcripts with longer responses. However, for the shorter prompt-based responses (often under 20 words), it is often difﬁcult to obtain these metrics. Therefore, coherence was-computed on the word-level rather than phrase-level by computing the cosine similarity between word embeddings within a response with an inter- word distance of k, with k ranging from 5 to 8. As before, typical statistics were computed on the coherence values obtained for each participant response (maximum, minimum, mean, median, 90th percentile, 10th percentile, etc.). They were able to successfully predict the onset of schizophrenia by discriminating the speech of healthy controls and those with early onset schizophrenia with ∼80% accuracy. Other studies make use of a variety of linguistic features to predict the presence of clinical conditions. For example, Kayi et al. identiﬁed predictive linguistic features of schizophrenia", "meta": {"paper_id": "1906.01157v2", "chunk_index": 19, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_20", "text": "successfully predict the onset of schizophrenia by discriminating the speech of healthy controls and those with early onset schizophrenia with ∼80% accuracy. Other studies make use of a variety of linguistic features to predict the presence of clinical conditions. For example, Kayi et al. identiﬁed predictive linguistic features of schizophrenia by analyzing laboratory writing samples of patients and con- trols for their semantic, syntactic, and pragmatic (sentimental) content [28]. A second dataset of social media messages from self-reporting individuals with schizophrenia over the Twitter API was also evaluated for the same types of content. The semantic content of the language was quantiﬁed by three methods: First, semantic role labeling was performed using the Semafor tool [34] to identify the role of individual words within a sentence or phrase Then, LDA was used to identify overarching themes that separated the clinical and control writing samples [31]. LDA identiﬁes topics in the text and also identiﬁes the top vocabulary used in each topic. Finally, clusters of word embeddings within the writing were generated using the k-means algorithm and GloVe word vector embeddings [27]. The frequency of each cluster was computed per document by checking the use of each word of the document in each cluster. The syntactic features used in this study again were obtained by computing the frequency of use of parts of speech (found by POS tagging) and by generating parse trees, using tools optimized for the corpus. Lastly, pragmatic features were found by performing sentiment analysis to classify the sentiment of the writing samples into distinct groups (very negative, negative, neutral, positive, very positive). They successfully showed a distinct set of predictive features that could accurately separate participants with schizophrenia from healthy controls in all of the language IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 12 analysis categories. However, when using a combination of features and various machine learning classiﬁers (random forest and support vector machine), they found that utilizing a combination of the semantic and pragmatic features led to the most promising accuracy (81.7%) in classiﬁcation of control participants and those with schizophrenia. The limited availability of language data in schizophrenia is always a difﬁcult challenge, so another study by Mitchell et al. ana- lyzed publicly available social media (Twitter) posts by self- identifying individuals with schizophrenia using LDA, LIWC generated-vectors, and various clustering techniques to show statistically signiﬁcant differences in their language patterns when compared to general users [8]. Another vector-space topic modeling approach was de- veloped by Yancheva and Rudzicz for analyzing transcripts of picture description tasks for participants with AD and healthy controls [32]. They propose a general method for generating information content units (ICUs), or topics of interest, from common images used in clinical description task evaluations, i.e. the famous Cookie Theft picture with reference speech samples [92]. The generated ICUs were compared with human-supplied ICUs from common usage in clinical practice, and most of the categories exhibited a close match. The study found that participants with AD and healthy controls were likely to discuss the same topics, but those with AD", "meta": {"paper_id": "1906.01157v2", "chunk_index": 20, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_21", "text": "picture with reference speech samples [92]. The generated ICUs were compared with human-supplied ICUs from common usage in clinical practice, and most of the categories exhibited a close match. The study found that participants with AD and healthy controls were likely to discuss the same topics, but those with AD had wider topic clusters with more irrelevant directions. Additionally, they were able to ﬁnd a small set of generated ICUs that had slightly better classiﬁcation performance than a much larger set of human selected ICUs for the same task, with ∼80% accuracy. Related work by Hernández-Domínguez et al. took a similar approach to generate a population-speciﬁc set of categories for participants with AD (n = 257), MCI (n = 43), and healthy controls (n = 217) [33]. The resulting features were signiﬁcantly correlated with severity as assessed by the MMSE, and classiﬁcation performance was characterized by the receiver operating characteristic (ROC) area under curve (AUC) performance of AUC ≈0.76 for all three groups. 3) Advantages and Disadvantages: While these studies have been successful in measuring the semantic coherence of language as it relates to thought disorders, there are several limitations. Recent work by Iter et al. identiﬁes and attempts to address some of these shortcomings when measuring semantic coherence for FTD in schizophrenia [18]. Interviews with a small sample of patients were collected and just the participant responses (of ~300 words each) were analyzed for their semantic content. They noted that when using the tangentiality model of semantic coherence (i.e. regression of the coherence over time with the sliding window) of Elvevåg et al. [25] and the incoherence model of semantic coherence of Bedi et al. [16], they were unable to convincingly separate their clinical and control participants based on language analysis. One reason for this was due to the presence of verbal ﬁllers, such as \"um\" or \"uh\" and many stop words without meaningful semantic content. Another reason is that longer sentences (or long moving windows) tend to be scored as more coherent due to a larger overlap of words. The third reason they identiﬁed (but did not address) is that repetitive sentences and phrases would be scored as highly coherent, even though repetition of ideas is common in FTD and should be scored negatively. The authors proposed a series of improvements to address some of these limitations. However, the sample sizes in this study were small (9 clinical participants and 5 control participants). Another issue with semantic coherence computation in clinical practice is difﬁculty with interpretability of computed metrics; for example, the cosine similarity between high di- mensional word vectors is a somewhat abstract concept which is difﬁcult for most to visualize. Recent work [13] attempted to address this issue by computing semantic coherence measures (using word2vec, InferSent, and SIF embeddings), lexical den- sity and diversity measures, and syntactic complexity measures as they relate to the language of patients with schizophrenia, patients with bipolar disorder, and healthy controls undergoing a validated clinical social skills assessment [53]. Linear regres- sion was used to determine a subset of language features across all", "meta": {"paper_id": "1906.01157v2", "chunk_index": 21, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_22", "text": "embeddings), lexical den- sity and diversity measures, and syntactic complexity measures as they relate to the language of patients with schizophrenia, patients with bipolar disorder, and healthy controls undergoing a validated clinical social skills assessment [53]. Linear regres- sion was used to determine a subset of language features across all categories that could effectively model the scores assigned by clinicians during the social skills performance assessment, in which participants were required to act out various role- playing conversational scenes with clinical assessors scored for cognitive performance. Then, these features were used to train simple binary classiﬁers (both naïve Bayes and logistic regression), for which leave-one-out cross-validation was used to determine their effectiveness at classifying groups of inter- est. For classifying clinical (patients with schizophrenia and bipolar I disorder) participants and healthy control participants, the selected feature subset achieved ROC curve AUC perfor- mance of AUC ≈0.90; for classifying within the clinical group (to separate participants with schizophrenia and bipolar disorder), the classiﬁer performance achieved AUC ≈0.80. IV. MEASURING COGNITIVE AND THOUGHT DISORDERS WITH SPEECH SIGNAL PROCESSING While cognitive-linguistic health is more directly observed through analysis of complex language production, additional information can be derived by speech signal analysis of individuals with cognitive impairments or thought disorders. This is because the acoustic stream is the physical mani- festation of the cognitive-linguistic processing that has gone into creating the message being conveyed, in near real-time. In this way, pauses during speech can be associated with difﬁculty in lexical retrieval (word-ﬁnding difﬁculties) or with extra processing time needed for message formulation. Pressed speech, that which is rapidly produced without insertions of natural pauses, can be associated with mania and “ﬂight of thoughts”. Conversely, reductions in the rhythmic and melodic variations in speech may be indicative of changes in mood. The information derived from the speech signal is used alone or in conjunction with many of the previously described methods to assess cognitive-linguistic health. This is either done directly by measuring different aspects of speech pro- duction including prosody, articulation, or vocal quality; or is done as a pre-processing step by using automatic speech recognition (ASR) for transcription of speech samples for follow-on linguistic analysis. In this section, we will review how various signal processing methods are used to extract clinically-relevant insight from an IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 13 individual’s speech samples for additional insight into detec- tion of disorders that affect cognition and thought. Referring back to Fig. 2, these include features extracted from vocal fold vibration (source), movement of the articulators (ﬁlter), and the overall rhythm of the speech signal (prosody). A. Methods 1) Prosodic features: Prosody refers to the rhythm and melody of speech. Examples of computable temporal prosodic features from recorded speech signals include the duration of voiced segments, duration of silent segments, loudness, measures of periodicity, fundamental frequency (F0), and many other similar features [14], [35]. These measures can indicate irregularities in the rhythm and timing of speech. Additionally, nonverbal speech cues, e.g. counting the num- ber of interruptions, interjections, natural turns, and response", "meta": {"paper_id": "1906.01157v2", "chunk_index": 22, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_23", "text": "duration of voiced segments, duration of silent segments, loudness, measures of periodicity, fundamental frequency (F0), and many other similar features [14], [35]. These measures can indicate irregularities in the rhythm and timing of speech. Additionally, nonverbal speech cues, e.g. counting the num- ber of interruptions, interjections, natural turns, and response times can also indicate identifying features of irregular speech patterns [39]. 2) Articulation features: Several spectral features that cap- ture movement of the articulators have been used in the clinical speech literature to measure the acoustic manifestation of the cognitive-linguistic deﬁcits discussed in Section 2. These include computing statistics related to the presence of additional formant harmonic frequencies, i.e. F1, F2, and F3, computing formant trajectories over time [93], or computing the vowel space area [94]. The spectral centroid can also be computed for each frame of speech signal that is analyzed [43]. The spectral centroid is essentially the center of mass for the frequency spectrum of a signal, and relates to the “brightness” or timbre of the perceived sound for audio. Time-frequency signal processing techniques are also com- monly used since acoustic speech signals are highly non- stationary. For example, computation of the mel-frequency cepstral coefﬁcients (MFCC) with the mel scale ﬁlterbank provides a compressed and whitened spectral representation of the speech [44]. These features are often used as inputs into an automatic speech recognition (ASR) system, but can also be monitored over time to identify irregularities in speech due to cognitive or thought disorders. As an example, common statistical features such as the mean, variance, skewness, and kurtosis of the MFCCs over time can be tracked for identiﬁcation of irregularities between healthy individuals and those with some cognitive or thought disorders [5]. 3) Vocal quality features: There is evidence that there are vocal quality changes associated with cognitive disorders [95]. These can be measured from the speech signal by isolating the source of speech production, involving the ﬂow of air through the lungs and glottis and affecting perceptible voice quality. Voice quality measures that have previously been used in the context of cognitive and thought disorders include: • jitter: small variations in glottal pulse timing during voiced speech • shimmer: small variations in glottal pulse amplitude dur- ing voiced speech • harmonic-to-noise ratio (HNR): the ratio of formant harmonics to inharmonic spectral peaks, i.e. those that are not whole number multiples of F0 These features alone are often difﬁcult to consistently compute and interpret, but can provide insight for the diagnosis and characterization of certain clinical conditions. 4) Automatic Speech Recognition: Recent improvements in ASR and in tools for easily implementing ASR systems have made possible the use of these systems in clinical speech analysis. This is most commonly done by using ASR in place of manual transcription for the extraction of linguistic features (i.e. features covered in Section III); however, this is often more error prone with regard to incorrect word substitutions, unintended insertions, or unintended deletions in the automatically generated transcript. The word error rate (WER) for an utterance of N words is given in Equation (6), WER", "meta": {"paper_id": "1906.01157v2", "chunk_index": 23, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_24", "text": "extraction of linguistic features (i.e. features covered in Section III); however, this is often more error prone with regard to incorrect word substitutions, unintended insertions, or unintended deletions in the automatically generated transcript. The word error rate (WER) for an utterance of N words is given in Equation (6), WER = # of insertions + deletions + substitutions N , (6) and is a typical statistic used to evaluate the performance of an ASR system. It is often more difﬁcult to maintain high accuracy (low WER) for ASR with pathological speech samples, as the relative dearth of this data makes it difﬁcult to train reliable ASR models optimized for this task. Other studies have also made use of ASR for paralinguistic feature extraction, such as the automated detection of ﬁlled pauses, natural turns, interjections, etc. Understanding the effects of ASR errors on downstream NLP tasks is an important area to address in which the current work is limited. Some recent at- tempts have been made to simulate ASR errors on text datasets and evaluate their effects on downstream tasks [96], [97], [98]. These potentially have future applications in language models that can analyze noisy datasets with ASR errors in clinical practice. B. Clinical Applications 1) Acoustic analysis: Disorders such as PPA, MCI, AD, and other forms of dementia are associated with a general slowing of thoughts in affected individuals. This has been shown to have detectable effects on speech production through acoustic analysis. In a study by König et al. , healthy controls and participants with MCI and AD were recorded as they were asked to perform various tasks, such as counting backwards, image description, sentence repeating, and verbal ﬂuency testing [35]. Temporal prosodic features such as the duration of voiced segments, silent segments, periodic segments, and ape- riodic segments were all computed. Then, the ratio of the mean durations of voiced segments to silent segments were also computed as features to express the continuity of speech in the study’s participants. As expected, it was shown that healthy control participants showed greater continuity in these metrics when compared to those with MCI or AD. These quantiﬁable alterations of speech in individuals with MCI and AD allowed the researchers to successfully separate patients with AD from healthy controls (approx. 87% accuracy), patients with MCI from controls (approx. 80% accuracy), and patients with MCI from patients with AD (approx. 80% accuracy). López-de- Ipiña et al. conducted another study in which acoustic fea- tures (related to prosody, spectral analysis, and features with emotional content) were extracted from spontaneous speech IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 14 samples to classify participants with AD at different stages (early, intermediate, and advanced) [38]. Among the computed prosodic features were the mean, median, and variance for durations of voiced and voiceless segments. Short-time energy computations were also computed for the collected samples in the time-domain. In the frequency-domain, the spectral centroid was determined for each speech sample. The authors also claim that features such as the contour of F0 and source features like shimmer,", "meta": {"paper_id": "1906.01157v2", "chunk_index": 24, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_25", "text": "variance for durations of voiced and voiceless segments. Short-time energy computations were also computed for the collected samples in the time-domain. In the frequency-domain, the spectral centroid was determined for each speech sample. The authors also claim that features such as the contour of F0 and source features like shimmer, jitter, and noise-harmonics ratio contain emotional content that can be useful in the automatic AD diagnosis. Lastly, they propose a new feature, which they term emotional temperature (ET), which is a normalized (in- dependent of participant) measure ranging from 0-100 based on several of prosodic and paralanguistic features that were previously mentioned12. The study revealed several interesting ﬁndings. First, the spontaneous speech analysis indicated that participants with AD exhibited higher proportions of voiceless speech and lower proportions of voiced speech, indicating a loss of ﬂuency and shortening of ﬂuent speech segments for those with AD. While classiﬁcation accuracy was good when using a set of prosodic speech features, they noted that accuracy improved when the emotional features (i.e. the proposed ET metric) were used13. Acoustic analysis of speech can make use of ASR to count dysﬂuencies in spoken language that are often associated with neurodegenerative decline. Pakhomov et al. made an early attempt to use ASR to extract many such prosodic features (pause-to-word ratio, normalized pause length, etc.) on picture- description task transcripts for participants with three variants of Frontotemporal Lobar Degeneration (FTLD) [40]. A more recent pair of studies by Tóth et al. explored using ASR for detection of MCI [36], [37]. However, in their work, only acoustic features were considered, and precise word transcripts were not required, mitigating the effect of the typically high WER for clinical speech samples. Instead, the authors trained a new ASR model with a focus of detecting individual phonemes. The features considered in this study were mostly prosodic (articulation rate, speech tempo, length of utterance, duration of silent and ﬁlled pauses, the number of silent and ﬁlled pauses). The focus of the study was to compare the effects of manually annotating transcripts with the faster ASR method. Since most ASR models cannot differentiate between ﬁlled pauses and meaningful voiced speech, their detection was a major focus of this work for automated MCI detection. The ASR model was trained with annotated ﬁlled pause samples to learn to detect them in spontaneous speech. The authors were able to show comparable results between the ASR and manual methods for MCI detection with the same feature set (82% accuracy for manual vs. 78% for ASR) [36]. While acoustic speech processing on its own has been less explored in detecting thought-disorder related mental illness, some researchers have found ways in which useful information can be derived solely from speech signals for this purpose. One example is seen in work by Tahir et al. [39]. 12The example in [38] shows that a typical ET value is approx. 95 for healthy control participants and approx. 50 for those with AD 13see Figure 9 in [38] In this study, patients with severe schizophrenia, receiving Cognitive Remediation Therapy (CRT), were differentiated from control participants", "meta": {"paper_id": "1906.01157v2", "chunk_index": 25, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_26", "text": "Tahir et al. [39]. 12The example in [38] shows that a typical ET value is approx. 95 for healthy control participants and approx. 50 for those with AD 13see Figure 9 in [38] In this study, patients with severe schizophrenia, receiving Cognitive Remediation Therapy (CRT), were differentiated from control participants with less severe schizophrenia (no CRT recommended) by non-verbal speech analysis. They note that nonverbal and conversational cues in speech often play a crucial role in communication, and that it is expected that individuals with schizophrenia would have a muted display of these features of speech. Cues used as inputs to a classiﬁer included interruptions, interjections, natural turns, response time, speaking rate, among others. Preliminary results from this study with participants with severe schizophrenia (n = 8) and less-severe forms of the disease (n = 7) indicate that these nonverbal cues show approximately 90% accuracy in classifying control participants from those with more severe forms of schizophrenia. They also attempted to validate the computed features by examining their correlation with tradi- tional subjective clinical assessments. Some of the computed objective nonverbal speech cue features had high correla- tion with subjective assessments; e.g. “poor rapport with interviewer” has a strong correlation with longer participant response times. The acoustics of bipolar disorder have also been studied, for example by Guidi et al. [41]. In this study, the authors propose an automated method for estimating the contour of F0 over time with a moving window approach as a proxy for mood changes. In particular, they study local rising and falling events of the F0 contour, including positive and negative slopes, amplitude, duration, and tilt to indicate different emotional states. The features were ﬁrst validated on a standard emotional speech database and then used to classify bipolar patients (n = 11) and healthy control subjects (n = 18). They noted that intra-subject analysis showed good speciﬁcity in classifying bipolar subjects and healthy controls across all contour features, but that directions of most were not consistent across different subjects. Due to limited data, they propose a study with a larger number of subjects including glottal, spectral, and energy features. 2) Combination of acoustic and textual features: Many dementia studies also use both acoustic and textual data with promising results. As an example, the previously mentioned work by Roark et al. (in Section III) also made use of acoustic speech samples to aid in the detection of MCI from naturally- produced spoken language. The researchers used manual and automated methods to estimate features related to the duration of speech during each utterance, including the quantity and du- ration of pause segments. Some of the features that were com- puted include fundamental frequency, total phonation time, total pause time, pauses per sample, total locution time (both phonation and pauses), verbal rate, and several others [14]. They conclude that automated speech analysis produces very similar results to manually computing these metrics from the speech samples, demonstrating the potential of automated speech signal processing for detecting MCI. Additionally, they found that a combination of linguistic complexity metrics and speech duration metrics", "meta": {"paper_id": "1906.01157v2", "chunk_index": 26, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_27", "text": "verbal rate, and several others [14]. They conclude that automated speech analysis produces very similar results to manually computing these metrics from the speech samples, demonstrating the potential of automated speech signal processing for detecting MCI. Additionally, they found that a combination of linguistic complexity metrics and speech duration metrics lead to improved classiﬁcation results. The previously described work on PPA subtypes in [12] was expanded by Fraser et al. in [42]. Acoustic features were also extracted and added to the previous set of linguistic features to improve the classiﬁcation results of PPA subtypes (PNFA IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 15 and semantic dementia) and healthy control participants. The added acoustic features included temporal prosodic features (i.e. speech duration, pause duration,pause to word ratio, etc.), mean and variance of F0 and ﬁrst three formants (F1, F2, F3), mean instantaneous power, mean and maximum ﬁrst autocorrelation function, instantaneous power, and vocal quality features, i.e. jitter and shimmer. The authors tested the relative signiﬁcance of all features using different feature reduction techniques and noted that more textual features were usually selected in each case. However, the addition of acoustic features had the greatest positive impact when attempting to differentiate between healthy control participants and those with one of the PPA subtypes, but proved less useful in distinguishing the subtypes. Their later study on AD [5] also used a similar hybrid approach with speech and language metrics to show good classiﬁcation separating AD participants from healthy controls [5]. The DementiaBank14 corpus was used to collect the data for this analysis. The study considered 370 distinctive features; linguistic features included grammatical features (from part-of-speech tagging), syntactic complexity (e.g. mean length of sentences, T-units, clauses, and maximum Yngve depth scoring for the parse tree, as described above), information content (speciﬁc and nonspeciﬁc word use), repetitiveness of meaningful words, and many more. Acoustic features associated with pathological speech were also identiﬁed by computation of MFCCs, their derivatives, and their second derivatives. To differentiate the clinical and control group, they considered mean, variance, skewness, and kurtosis of the MFCCs over time. After per- forming factor analysis on these features, they showed that most of the variance between controls and those with AD could be explained by semantic impairment, acoustic abnor- malities, syntactic impairment, and information impairment. 3) Impact of ASR on textual features: Several studies have also used ASR to generate transcripts of spoken language tasks for textual feature extraction for dementia detection. However, unlike the phone-level ASR model built in [36] and [37], this use case does require accurate word-level transcripts (i.e. a low WER). Previous work has shown that ASR accuracy is reduced for both elderly patients and those with dementia [99], [100], [101]. To address this, Zhou et al. performed a study in which the DementiaBank14 corpus was used to train an ASR model on domain data with elderly patients, both with and without AD [45]. They were able to show that an ASR model trained with a smaller in-domain dataset could improve WER-based accuracy than one trained with a", "meta": {"paper_id": "1906.01157v2", "chunk_index": 27, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_28", "text": "a study in which the DementiaBank14 corpus was used to train an ASR model on domain data with elderly patients, both with and without AD [45]. They were able to show that an ASR model trained with a smaller in-domain dataset could improve WER-based accuracy than one trained with a larger out-of- domain dataset. Additionally, they were able to conﬁrm that even with their model, diagnostic accuracy decreases with increasing WER, as expected, but the correlation between the two is relatively weak when selecting certain features that are more robust to ASR errors (such as word frequency and word length related features)15. Mirheidari et al. also used ASR with a combination of 14https://dementia.talkbank.org/access, accessed August 20, 2019 15The authors identify features that provide best diagnostic ability for gold- standard manual transcripts and transcripts with varying WER and ASR to identify these robust features, but they do not claim to understand why certain features seem more robust than others acoustic (temporal prosodic) and textual features (syntactical and semantic features) to diagnose and detect participants with neurodegenerative dementia (ND) and differentiate them from those with non-dementia related Functional Memory Disorder (FMD) with a conversational analysis dataset [46], [47]. With manual transcriptions, the classiﬁcation accuracy was over 90% in classifying the two groups, but it dropped to 79% when ASR was used. As expected, they found that the signiﬁcance of the syntactic and semantic textual features is diminished when transcriptions contain ASR errors. Sadeghian et al. attempted to improve the issue of transcription errors by training a custom ASR model using collected speech samples from participants with AD (n = 26) and healthy controls (n = 46) [9]. This was done by limiting the potential lexicon to the collected speech in their dataset as well as cleaning the audio ﬁles to reduce the WER. Their study used a combination of acoustic features (temporal prosodic features and F0 statistics) and textual features computed from both manual and ASR-generated transcripts (POS tags, syntactic complexity measures from [14], idea density, and LIWC features [6]). In their work, the best classiﬁcation results (over 90%) were seen when feature selection was performed using both the MMSE scores and computed acoustic and textual features, but using the computed features alone was nearly comparable and outperformed the MMSE scores on their own. Weiner et al. [48] instead compared the difference in the analysis of manual and ASR-derived transcripts for a large range of acoustic (prosody and timing related) and textual features (lexical diversity with Brunet’s index and Honoré’s statistic) for comparing participants with dementia and healthy controls. The off-the-shelf ASR model used in this work had a relatively high WER, but they were interestingly able to show that the WER itself was a reliable feature for classifying the different types of subjects. Additionally, many of the features they selected showed robustness to transcription quality, possibly even taking advantage of the poor ASR performance to identify participants with dementia. C. Advantages & Disadvantages It is intuitive that ﬁne-grained and discrete measures of “what is said” (language, in terms of lexical diversity, lexical density,", "meta": {"paper_id": "1906.01157v2", "chunk_index": 28, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_29", "text": "Additionally, many of the features they selected showed robustness to transcription quality, possibly even taking advantage of the poor ASR performance to identify participants with dementia. C. Advantages & Disadvantages It is intuitive that ﬁne-grained and discrete measures of “what is said” (language, in terms of lexical diversity, lexical density, semantic coherence, language complexity, etc.) may more directly capture early cognitive-linguistic changes in ill- ness and disease than measures of “how it is said” (analysis of speech acoustics). However, emerging data shows that acoustic analysis offers converging and complementary information to several of the textual features discussed in Section III. Most interestingly, changes in the outward ﬂow of speech may precede measurable language-based changes [39], [42]. A particular advantage of evaluating speech acoustics is that ASR or transcription is not necessarily a required step. Automated acoustic metrics can be extracted from non-labeled speech samples [35], [39], [38], [102]. Further, some of the metrics provide complementary and interpretable value that cannot be gleaned from transcripts (rate, pause metrics, speech prosody). These directly correspond with subjectively described clinical characteristics (e.g. pressed speech, halting IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 16 speech, ﬂat affect etc). A disadvantage is that not all acoustic metrics offer that level of transparency. This is a running theme in clinical speech analysis. Many of these features are not currently used in clinical diagnosis despite their powerful predictive power because they are difﬁcult to directly interpret (e.g. MFCCs); this means that clinicians can see the output of a complicated model but not understand why the model came to that decision or if it is considering clinically-relevant dimensions. For this reason, some effort has been undertaken to map the information contained in high-dimensional data to be easily visualized and interpreted by clinicians, but this remains a signiﬁcant challenge [103], [104]. V. CONCLUDING REMARKS AND FUTURE WORK An analysis of the existing literature reveals a set of future research directions to help advance the state of the art in this area. Here, we provide an overview of these directions and highlight some important open questions in this space. A. Characterizing Inter and Intra-Speaker Variability in Healthy Populations There is a great deal of variability to be expected in speech and language data. Extensive work on the language variables inﬂuencing inter- and intra-speaker variation suggest that any level of language (i.e. phonological, phonetics, semantics, syntax, morphology) is subject to both conscious/explicit and completely unconscious/subtle variation within a speaker. These conscious and unconscious sources of variability are conditioned by pragmatics, style-shifting, or register shift- ing [105], [106]. Similarly, speech acoustics are impacted by speaker identity, context, background noise, spoken language, etc. [107]. These individual sources of variability have yet to be fully characterized quantitatively for the features that we described in this paper. A more complete understanding of this variability in healthy populations helps to interpret changes observed in clinical populations. For example, this knowledge can help understand how typical or atypical is a particular semantic coherence score (e.g. in what percentile does the semantic coherence score fall?). Furthermore, this", "meta": {"paper_id": "1906.01157v2", "chunk_index": 29, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_30", "text": "in this paper. A more complete understanding of this variability in healthy populations helps to interpret changes observed in clinical populations. For example, this knowledge can help understand how typical or atypical is a particular semantic coherence score (e.g. in what percentile does the semantic coherence score fall?). Furthermore, this understanding can inform stratiﬁed sampling schemes that allow experimenters to match healthy and clinical cohorts on relevant aspects of speech/language production. This is critical for clinical trial design. B. Joint Optimization of Speech Elicitation and Speech & Language Analytics Algorithms published in the literature typically make use of previously-collected speech and language samples. These samples are often collected for other reasons and are only used by algorithm designers because they are available. A related challenge also arises in comparing the results of various analyses of speech and language analytics for cognitive assessment when the datasets used in each study are vastly different. This is evident in the large range of speech elicitation tasks used in many of the studies mentioned in this review. The lack of standardization in data collection for cognitive assessment therefore contributes to the problem of limited available data for any particular task. As a result, published results are potentially biased because these data sets are small and collected on a limited set of elicitation tasks. Deeper collaborations between speech neuroscientists, neuropsychologists, and speech technologists are required to push the state-of-the-art forward. There is an extensive literature on how to efﬁciently and reliably elicit speech to exert pressure on the underlying cognitive-linguistic processing [108]. The algorithms for extracting clinically- relevant information from speech and audio have been de- veloped independently from this work. We posit that joint ex- ploration of the elicitation-analytics space has the potential to result in improved sensitivity in detecting cognitive-linguistic change. This analysis will reveal which elicitation tasks and analytics are maximally sensitive for a given problem. The rate-limiting factor in being able to answer these questions is the lack of publicly-available data on a large scale. Unlike speech recognition research, data for clinical speech research is much more difﬁcult to collect and share because it’s scarcer, requires experts to label, and there are privacy issues. Until these issues are resolved, it will be difﬁcult to overcome some of the scientiﬁc challenges we highlight here. C. Robustness to Noisy Data The sensitivity of the features we describe herein and the follow-on models they drive are not well understood under noisy conditions. Our deﬁnition of noisy is rather loose here. For example, noise may arise from imperfect transcripts provided by an ASR engine, background noise that may corrupt the acoustics, or feature distribution mismatch between training and test data in supervised settings. Unimportant nuisance parameters for clinical applications (e.g. idiosyncratic features related to different speakers) are especially prob- lematic in acoustic analysis [107]. A better characterization of the sensitivity of these nuisance features can inform the development of new representations that are robust to various sources of noise. These models can improve the algorithms’ ability to generalize and can help understand the fundamental limits of speech", "meta": {"paper_id": "1906.01157v2", "chunk_index": 30, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_31", "text": "especially prob- lematic in acoustic analysis [107]. A better characterization of the sensitivity of these nuisance features can inform the development of new representations that are robust to various sources of noise. These models can improve the algorithms’ ability to generalize and can help understand the fundamental limits of speech as a diagnostic. D. Data-Driven and Interpretable Features Many of the features described herein are readily inter- pretable and, given the existing literature, it is reasonable to posit that they have clinical utility. However, if clinical speech data becomes available on a large scale, we expect that data- driven artiﬁcially intelligence (AI) systems will replace some of the domain-expert features described herein. For example, it is reasonable to expect that features that are optimized for a speciﬁc application (e.g. diagnosing schizophrenia) would outperform the general-purpose features described here. This improved performance likely comes at the expense of reduced feature interpretability. An area ripe for further exploration in clinical speech analytics, and clinical analytics in general, is the development of AI models that provide interpretable outputs when interrogated, such as in [104]. This area has received some attention recently and will continue to become more important as AI systems are deployed in healthcare. IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 17 REFERENCES [1] Center for Behavioral Health Statistics and Quality, “2016 national survey on drug use and health: Methodological summary and deﬁni- tions,” Substance Abuse and Mental Health Services Administration, Rockville, MD, 2017. [2] G. A. Cecchi, V. Gurev, S. J. Heisig, R. Norel, I. Rish, and S. R. Schrecke, “Computing the structure of language for neuropsychiatric evaluation,” IBM Journal of Research and Development, vol. 61, no. 2/3, pp. 1:1–1:10, Mar. 2017. [3] M. Prince, A. Wimo, M. Guerchet, G.-C. Ali, Y.-T. Wu, and M. Prina, “The Global Impact of Dementia: An analysis of prevalence, incidence, cost and trends - Executive Summary,” Alzheimer’s Disease Interna- tional, Tech. Rep., Sep. 2015. [4] P. Garrard, V. Rentoumi, B. Gesierich, B. Miller, and M. L. Gorno- Tempini, “Machine learning approaches to diagnosis and laterality effects in semantic dementia discourse,” Cortex, vol. 55, pp. 122–129, Jun. 2014. [5] K. C. Fraser, J. A. Meltzer, and F. Rudzicz, “Linguistic Features Iden- tify Alzheimer’s Disease in Narrative Speech,” Journal of Alzheimer’s Disease, vol. 49, no. 2, pp. 407–422, Oct. 2015. [6] Y. R. Tausczik and J. W. Pennebaker, “The psychological meaning of words: LIWC and computerized text analysis methods,” Journal of language and social psychology, vol. 29, no. 1, pp. 24–54, 2010. [7] M. Asgari, J. Kaye, and H. Dodge, “Predicting mild cognitive impair- ment from spontaneous spoken utterances,” Alzheimer’s & Dementia: Translational Research & Clinical Interventions, vol. 3, no. 2, pp. 219– 228, Jun. 2017. [8] M. Mitchell, K. Hollingshead, and G. Coppersmith, “Quantifying the language of schizophrenia in social media,” in Proc.of the 2nd Work- shop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, 2015, pp. 11–20. [9] R. Sadeghian, J. Schaffer, and S. Zahorian, “Speech Processing Ap- proach for Diagnosing Dementia in an Early Stage,” Interspeech 2017, pp.", "meta": {"paper_id": "1906.01157v2", "chunk_index": 31, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_32", "text": "the language of schizophrenia in social media,” in Proc.of the 2nd Work- shop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, 2015, pp. 11–20. [9] R. Sadeghian, J. Schaffer, and S. Zahorian, “Speech Processing Ap- proach for Diagnosing Dementia in an Early Stage,” Interspeech 2017, pp. 2705–2709, Aug. 2017. [10] R. S. Bucks, S. Singh, J. M. Cuerden, and G. K. Wilcock, “Analysis of spontaneous, conversational speech in dementia of Alzheimer type: Evaluation of an objective technique for analysing lexical perfor- mance,” Aphasiology, vol. 14, no. 1, pp. 71–91, Jan. 2000. [11] V. Berisha, S. Wang, A. LaCross, and J. Liss, “Tracking Discourse Complexity Preceding Alzheimer’s Disease Diagnosis: A Case Study Comparing the Press Conferences of Presidents Ronald Reagan and George Herbert Walker Bush,” Journal of Alzheimer’s Disease, vol. 45, no. 3, pp. 959–963, Mar. 2015. [12] K. C. Fraser, J. A. Meltzer, N. L. Graham, C. Leonard, G. Hirst, S. E. Black, and E. Rochon, “Automated classiﬁcation of primary progressive aphasia subtypes from narrative speech transcripts,” Cortex, vol. 55, pp. 43–60, Dec. 2012. [13] R. Voleti, S. Woolridge, J. M. Liss, M. Milanovic, C. R. Bowie, and V. Berisha, “Objective Assessment of Social Skills Using Automated Language Analysis for Identiﬁcation of Schizophrenia and Bipolar Disorder,” in Proc. Interspeech 2019, 2019, pp. 1433–1437. [14] B. Roark, M. Mitchell, J.-P. Hosom, K. Hollingshead, and J. Kaye, “Spoken Language Derived Measures for Detecting Mild Cognitive Impairment,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2081–2090, Sep. 2011. [15] V. Berisha, S. Wang, A. LaCross, J. Liss, and P. Garcia-Filion, “Longitudinal changes in linguistic complexity among professional football players,” Brain and Language, vol. 169, pp. 57–63, Jun. 2017. [16] G. Bedi, F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota, S. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran, “Automated analysis of free speech predicts psychosis onset in high-risk youths,” npj Schizophrenia, vol. 1, p. 15030, 2015. [17] C. M. Corcoran, F. Carrillo, D. Fernández-Slezak, G. Bedi, C. Klim, D. C. Javitt, C. E. Bearden, and G. A. Cecchi, “Prediction of psychosis across protocols and risk cohorts using automated language analysis,” World Psychiatry, vol. 17, no. 1, pp. 67–75, Feb. 2018. [18] D. Iter, J. Yoon, and D. Jurafsky, “Automatic Detection of Incoherent Speech for Diagnosing Schizophrenia,” in Proceedings of the Fifth Workshop on Computational Linguistics and Clinical Psychology: From Keyboard to Clinic, 2018, pp. 136–146. [19] V. H. Yngve, “A Model and an Hypothesis for Language Structure,” Proceedings of the American Philosophical Society, vol. 104, no. 5, 1960. [20] L. Frazier, “Syntactic Complexity,” in Natural Language Parsing. Cambridge, U.K.: Cambridge University Press, 1985. [21] N. B. Mota, N. A. P. Vasconcelos, N. Lemos, A. C. Pieretti, O. Ki- nouchi, G. A. Cecchi, M. Copelli, and S. Ribeiro, “Speech Graphs Provide a Quantitative Measure of Thought Disorder in Psychosis,” PLoS ONE, vol. 7, no. 4, p. e34928, Apr. 2012. [22] N. B. Mota, R. Furtado, P. P. C. Maia, M. Copelli, and S. Ribeiro, “Graph analysis of dream reports is", "meta": {"paper_id": "1906.01157v2", "chunk_index": 32, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_33", "text": "G. A. Cecchi, M. Copelli, and S. Ribeiro, “Speech Graphs Provide a Quantitative Measure of Thought Disorder in Psychosis,” PLoS ONE, vol. 7, no. 4, p. e34928, Apr. 2012. [22] N. B. Mota, R. Furtado, P. P. C. Maia, M. Copelli, and S. Ribeiro, “Graph analysis of dream reports is especially informative about psychosis,” Scientiﬁc Reports, vol. 4, no. 1, Jan. 2014. [23] L. Bertola, N. B. Mota, M. Copelli, T. Rivero, B. S. Diniz, M. A. Romano-Silva, S. Ribeiro, and L. F. Malloy-Diniz, “Graph analysis of verbal ﬂuency test discriminate between patients with Alzheimer’s dis- ease, mild cognitive impairment and normal elderly controls,” Frontiers in Aging Neuroscience, vol. 6, Jul. 2014. [24] T. K. Landauer and S. T. Dumais, “A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge.” Psychological Review, vol. 104, no. 2, pp. 211–240, 1997. [25] B. Elvevåg, P. W. Foltz, D. R. Weinberger, and T. E. Goldberg, “Quantifying incoherence in speech: An automated methodology and novel application to schizophrenia,” Schizophrenia Research, vol. 93, no. 1-3, pp. 304–316, Jul. 2007. [26] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of word representations in vector space,” arXiv preprint arXiv:1301.3781, 2013. [27] J. Pennington, R. Socher, and C. Manning, “Glove: Global Vectors for Word Representation.” Association for Computational Linguistics, 2014, pp. 1532–1543. [28] E. S. Kayi, M. Diab, L. Pauselli, M. Compton, and G. Coppersmith, “Predictive Linguistic Features of Schizophrenia,” in Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (* SEM 2017), 2017, pp. 241–250. [29] S. Arora, Y. Liang, and T. Ma, “A Simple but Tough-to-Beat Baseline for Sentence Embeddings,” in Proc. of 5th International Conference on Learning Representations, Toulon, France, 2017, p. 16. [30] A. Conneau, D. Kiela, H. Schwenk, L. Barrault, and A. Bordes, “Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,” arXiv:1705.02364 [cs], May 2017. [31] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of Machine Learning Research, vol. 3, no. Jan, pp. 993–1022, 2003. [32] M. Yancheva and F. Rudzicz, “Vector-space topic models for detecting Alzheimer’s disease,” in Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, 2016, pp. 2337–2346. [33] L. Hernández-Domínguez, S. Ratté, G. Sierra-Martínez, and A. Roche- Bergua, “Computer-based evaluation of Alzheimer’s disease and mild cognitive impairment patients during a picture description task,” Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Monitor- ing, vol. 10, pp. 260–268, 2018. [34] D. Das, N. Schneider, D. Chen, and N. A. Smith, “Probabilistic Frame- semantic Parsing,” in Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, ser. HLT ’10. Stroudsburg, PA, USA: Association for Computational Linguistics, 2010, pp. 948–956. [35] A. König, A. Satt, A. Sorin, R. Hoory, O. Toledo-Ronen, A. Der- reumaux, V. Manera, F. Verhey, P. Aalten, P. H. Robert, and R. David, “Automatic speech analysis for the assessment of patients with prede- mentia and Alzheimer’s disease,” Alzheimer’s & Dementia: Diagnosis, Assessment & Disease", "meta": {"paper_id": "1906.01157v2", "chunk_index": 33, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_34", "text": "2010, pp. 948–956. [35] A. König, A. Satt, A. Sorin, R. Hoory, O. Toledo-Ronen, A. Der- reumaux, V. Manera, F. Verhey, P. Aalten, P. H. Robert, and R. David, “Automatic speech analysis for the assessment of patients with prede- mentia and Alzheimer’s disease,” Alzheimer’s & Dementia: Diagnosis, Assessment & Disease Monitoring, vol. 1, no. 1, pp. 112–124, Mar. 2015. [36] L. Tóth, G. Gosztolya, V. Vincze, I. Hoffmann, G. Szatlóczki, Gréta, E. Biró, F. Zsura, M. Pákáski, and J. Kálmán, “Automatic Detection of Mild Cognitive Impairment from Spontaneous Speech Using ASR,” in Proc. Interspeech 2015. Dresden, Germany: ISCA, Sep. 2015, pp. 2694–2698. [37] L. Tóth, I. Hoffmann, G. Gosztolya, V. Vincze, G. Szatlóczki, Z. Bán- réti, M. Pákáski, and J. Kálmán, “A Speech Recognition-based Solution for the Automatic Detection of Mild Cognitive Impairment from Spontaneous Speech,” Current Alzheimer Research, vol. 15, no. 2, pp. 130–138, 2018. [38] K. López-de-Ipiña, J. B. Alonso, J. Solé-Casals, N. Barroso, P. Henriquez, M. Faundez-Zanuy, C. M. Travieso, M. Ecay-Torres, P. Martínez-Lage, and H. Eguiraun, “On Automatic Diagnosis of Alzheimer’s Disease Based on Spontaneous Speech Analysis and Emotional Temperature,” Cognitive Computation, vol. 7, no. 1, pp. 44– 55, Feb. 2015. [39] Y. Tahir, D. Chakraborty, J. Dauwels, N. Thalmann, D. Thalmann, and J. Lee, “Non-verbal speech analysis of interviews with schizophrenic IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 18 patients,” in Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference On. IEEE, 2016, pp. 5810–5814. [40] S. V. S. Pakhomov, G. E. Smith, D. Chacon, Y. Feliciano, N. Graff- Radford, R. Caselli, and D. S. Knopman, “Computerized analysis of speech and language to identify psycholinguistic correlates of fron- totemporal lobar degeneration,” Cognitive and Behavioral Neurology: Ofﬁcial Journal of the Society for Behavioral and Cognitive Neurology, vol. 23, no. 3, pp. 165–177, Sep. 2010. [41] A. Guidi, N. Vanello, G. Bertschy, C. Gentili, L. Landini, and E. P. Scilingo, “Automatic analysis of speech F0 contour for the charac- terization of mood changes in bipolar patients,” Biomedical Signal Processing and Control, vol. 17, pp. 29–37, Mar. 2015. [42] K. C. Fraser, F. Rudzicz, and E. Rochon, “Using Text and Acoustic Features to Diagnose Progressive Aphasia and its Subtypes,” in Proc. Interspeech 2013. Lyon, France: ISCA, Aug. 2013, pp. 2177–2181. [43] G. Peeters, “A large set of audio features for sound description (similarity and classiﬁcation) in the CUIDADO project,” CUIDADO IST Project Report, vol. 54, no. 0, pp. 1–25, 2004. [44] S. Davis and P. Mermelstein, “Comparison of parametric represen- tations for monosyllabic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics, Speech, and Signal Pro- cessing, vol. 28, no. 4, pp. 357–366, Aug. 1980. [45] L. Zhou, K. C. Fraser, and F. Rudzicz, “Speech Recognition in Alzheimer’s Disease and in its Assessment,” in Proc. Interspeech 2016. San Francisco, CA, USA: ISCA, Sep. 2016, pp. 1948–1952. [46] B. Mirheidari, D. Blackburn, M. Reuber, T. Walker, and H. Christensen, “Diagnosing People with Dementia Using Automatic Conversation Analysis,” in Proc. Interspeech 2016, Sep. 2016, pp. 1220–1224. [47] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A.", "meta": {"paper_id": "1906.01157v2", "chunk_index": 34, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_35", "text": "Proc. Interspeech 2016. San Francisco, CA, USA: ISCA, Sep. 2016, pp. 1948–1952. [46] B. Mirheidari, D. Blackburn, M. Reuber, T. Walker, and H. Christensen, “Diagnosing People with Dementia Using Automatic Conversation Analysis,” in Proc. Interspeech 2016, Sep. 2016, pp. 1220–1224. [47] B. Mirheidari, D. Blackburn, K. Harkness, T. Walker, A. Venneri, M. Reuber, and H. Christensen, “Toward the Automation of Diagnostic Conversation Analysis in Patients with Memory Complaints,” Journal of Alzheimer’s Disease, vol. 58, no. 2, pp. 373–387, May 2017. [48] J. Weiner, M. Engelbart, and T. Schultz, “Manual and Automatic Tran- scriptions in Dementia Detection from Speech,” in Proc. Interspeech 2017. Stockholm, Sweden: ISCA, 2017, pp. 3117–3121. [49] A. Jeromin and R. Bowser, “Biomarkers in Neurodegenerative Dis- eases,” in Neurodegenerative Diseases, P. Beart, M. Robinson, M. Rat- tray, and N. J. Maragakis, Eds. Cham: Springer International Pub- lishing, 2017, vol. 15, pp. 491–528. [50] M. Katsuno, K. Sahashi, Y. Iguchi, and A. Hashizume, “Preclinical progression of neurodegenerative diseases,” Nagoya Journal of Medical Science, vol. 80, no. 3, pp. 289–298, Aug. 2018. [51] H. H. Dodge, J. Zhu, N. C. Mattek, D. Austin, J. Kornfeld, and J. A. Kaye, “Use of High-Frequency In-Home Monitoring Data May Reduce Sample Sizes Needed in Clinical Trials,” PLOS ONE, vol. 10, no. 9, p. e0138095, Sep. 2015. [52] A. P. Association and A. P. Association, Eds., Diagnostic and Statistical Manual of Mental Disorders: DSM-5, 5th ed. Washington, D.C: American Psychiatric Association, 2013. [53] T. L. Patterson, S. Moscona, C. L. McKibbin, K. Davidson, and D. V. Jeste, “Social skills performance assessment among older patients with schizophrenia,” Schizophrenia Research, vol. 48, no. 2-3, pp. 351–360, Mar. 2001. [54] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T. F. Quatieri, “A review of depression and suicide risk assessment using speech analysis,” Speech Communication, vol. 71, pp. 10–49, Jul. 2015. [55] W. J. Levelt, “Producing spoken language: A blueprint of the speaker,” in The Neurocognition of Language, C. M. Brown and P. Hagoort, Eds. Oxford: Oxford University Press, USA, 1999, ch. 4, pp. 83–122. [56] P. T. Trzepacz and R. W. Baker, The Psychiatric Mental Status Examination. New York: Oxford University Press, 1993. [57] V. C. Pangman, J. Sloan, and L. Guse, “An examination of psy- chometric properties of the Mini-Mental State Examination and the Standardized Mini-Mental State Examination: Implications for clinical practice,” Applied Nursing Research, vol. 13, no. 4, pp. 209–213, Nov. 2000. [58] P. S. Mathuranath, P. J. Nestor, G. E. Berrios, W. Rakowicz, and J. R. Hodges, “A brief cognitive test battery to differentiate Alzheimer’s disease and frontotemporal dementia,” Neurology, vol. 55, no. 11, pp. 1613–1620, Dec. 2000. [59] Z. S. Nasreddine, N. A. Phillips, V. Bédirian, S. Charbonneau, V. Whitehead, I. Collin, J. L. Cummings, and H. Chertkow, “The Mon- treal Cognitive Assessment, MoCA: A Brief Screening Tool For Mild Cognitive Impairment,” Journal of the American Geriatrics Society, vol. 53, no. 4, pp. 695–699, Apr. 2005. [60] A. M. Kring, R. E. Gur, J. J. Blanchard, W. P. Horan, and S. P. Reise, “The Clinical Assessment Interview for Negative Symptoms (CAINS): Final", "meta": {"paper_id": "1906.01157v2", "chunk_index": 35, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_36", "text": "Assessment, MoCA: A Brief Screening Tool For Mild Cognitive Impairment,” Journal of the American Geriatrics Society, vol. 53, no. 4, pp. 695–699, Apr. 2005. [60] A. M. Kring, R. E. Gur, J. J. Blanchard, W. P. Horan, and S. P. Reise, “The Clinical Assessment Interview for Negative Symptoms (CAINS): Final Development and Validation,” American Journal of Psychiatry, vol. 170, no. 2, pp. 165–172, Feb. 2013. [61] B. Kirkpatrick, G. P. Strauss, L. Nguyen, B. A. Fischer, D. G. Daniel, A. Cienfuegos, and S. R. Marder, “The Brief Negative Symptom Scale: Psychometric Properties,” Schizophrenia Bulletin, vol. 37, no. 2, pp. 300–305, Mar. 2011. [62] D. Tzur Bitan, A. Grossman Giron, G. Alon, S. Mendlovic, Y. Bloch, and A. Segev, “Attitudes of mental health clinicians toward perceived inaccuracy of a schizophrenia diagnosis in routine clinical practice,” BMC Psychiatry, vol. 18, no. 1, p. 317, Dec. 2018. [63] P. D. Harvey and A. Pinkham, “Impaired self-assessment in schizophre- nia: Why patients misjudge their cognition and functioning,” Current Psychiatry, vol. 14, no. 4, pp. 53–59, 2015. [64] R. Pies, “How “Objective” Are Psychiatric Diagnoses?” Psychiatry (Edgmont), vol. 4, no. 10, pp. 18–22, Oct. 2007. [65] J. Krajewski, S. Schnieder, D. Sommer, A. Batliner, and B. Schuller, “Applying multiple classiﬁers and non-linear dynamics features for detecting sleepiness from speech,” Neurocomputing, vol. 84, pp. 65– 75, May 2012. [66] D. A. Snowdon, S. J. Kemper, J. A. Mortimer, L. H. Greiner, D. R. Wekstein, and W. R. Markesbery, “Linguistic Ability in Early Life and Cognitive Function and Alzheimer’s Disease in Late Life: Findings From the Nun Study,” JAMA, vol. 275, no. 7, pp. 528–532, Feb. 1996. [67] M. A. Covington and J. D. McFall, “Cutting the Gordian Knot: The Moving-Average Type–Token Ratio (MATTR),” Journal of Quantita- tive Linguistics, vol. 17, no. 2, pp. 94–100, May 2010. [68] E. Brunét, Le Vocabulaire de Jean Giraudoux. Structure et Évolution. Slatkine, 1978, no. 1. [69] A. Honoré, “Some Simple Measures of Richness of Vocabulary,” Association for Literary and Linguistic Computing Bulletin, vol. 7, no. 2, pp. 172–177, 1979. [70] M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz, “Building a Large Annotated Corpus of English: The Penn Treebank:,” Defense Technical Information Center, Fort Belvoir, VA, Tech. Rep., Apr. 1993. [71] D. Jurafsky and J. H. Martin, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguis- tics, and Speech Recognition (DRAFT), 3rd ed., Aug. 2017. [72] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer, “Feature- rich part-of-speech tagging with a cyclic dependency network,” in Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - NAACL ’03, vol. 1. Edmonton, Canada: Association for Computational Linguistics, 2003, pp. 173–180. [73] D. Wechsler, “Wechsler Memory Scale–Third Edition Manual,” San Antonio, TX: The Psychological Corp., 1997. [74] S. Kemper, “Adults’ diaries: Changes made to written narratives across the life span,” Discourse Processes, vol. 13, no. 2, pp. 207–223, Apr. 1990. [75] S. Kemper and A. Sumner, “The structure of verbal abilities in young and older adults.” Psychology and Aging, vol.", "meta": {"paper_id": "1906.01157v2", "chunk_index": 36, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_37", "text": "Antonio, TX: The Psychological Corp., 1997. [74] S. Kemper, “Adults’ diaries: Changes made to written narratives across the life span,” Discourse Processes, vol. 13, no. 2, pp. 207–223, Apr. 1990. [75] S. Kemper and A. Sumner, “The structure of verbal abilities in young and older adults.” Psychology and Aging, vol. 16, no. 2, pp. 312–322, 2001. [76] N. E. Carlozzi, N. L. Kirsch, P. A. Kisala, and D. S. Tulsky, “An Examination of the Wechsler Adult Intelligence Scales, Fourth Edition (WAIS-IV) in Individuals with Complicated Mild, Moderate and Severe Traumatic Brain Injury (TBI),” The Clinical Neuropsychologist, vol. 29, no. 1, pp. 21–37, Jan. 2015. [77] T. Berg, Structure in Language: A Dynamic Perspective, 1st ed., ser. Routledge Studies in Linguistics. New York, NY: Routledge, 2009, no. 10, oCLC: 605351697. [78] D. M. Magerman, “Statistical decision-tree models for parsing,” in Proceedings of the 33rd Annual Meeting on Association for Compu- tational Linguistics -. Cambridge, Massachusetts: Association for Computational Linguistics, 1995, pp. 276–283. [79] D. Lin, “On the structural complexity of natural language sentences,” in Proceedings of the 16th Conference on Computational Linguistics -, vol. 2. Copenhagen, Denmark: Association for Computational Linguistics, 1996, p. 729. [80] E. Gibson, “Linguistic complexity: Locality of syntactic dependencies,” Cognition, vol. 68, no. 1, pp. 1–76, Aug. 1998. [81] F. Carrillo, N. Mota, M. Copelli, S. Ribeiro, M. Sigman, G. Cecchi, and D. Fernandez Slezak, “Automated Speech Analysis for Psychosis Evaluation,” in Machine Learning and Interpretation in Neuroimaging, I. Rish, G. Langs, L. Wehbe, G. Cecchi, K.-m. K. Chang, and B. Murphy, Eds. Cham: Springer International Publishing, 2016, vol. 9444, pp. 31–39. [82] E. Charniak, “A Maximum-entropy-inspired Parser,” in Proceedings of the 1st North American Chapter of the Association for Computational IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING (JSTSP), SPECIAL ISSUE 19 Linguistics Conference, ser. NAACL 2000. Association for Compu- tational Linguistics, 2000, pp. 132–139. [83] E. Haugen and J. R. Firth, “Papers in linguistics 1934-1951,” Language, vol. 34, no. 4, p. 498, Oct. 1958. [84] E. Altszyler, M. Sigman, S. Ribeiro, and D. F. Slezak, “Comparative study of LSA vs Word2vec embeddings in small corpora: A case study in dreams database,” arXiv preprint arXiv:1610.01520, 2016. [85] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word representations,” arXiv preprint arXiv:1802.05365, 2018. [86] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre- training of Deep Bidirectional Transformers for Language Understand- ing,” arXiv:1810.04805 [cs], Oct. 2018. [87] M. Pagliardini, P. Gupta, and M. Jaggi, “Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features,” arXiv:1703.02507 [cs], Mar. 2017. [88] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-Cespedes, S. Yuan, C. Tar et al., “Universal Sentence Encoder,” arXiv preprint arXiv:1803.11175, 2018. [89] A. M. Colman, A Dictionary of Psychology. Oxford University Press, 2015. [90] S. C. Yudofsky, R. E. Hales, and A. P. Publishing, Eds., The Ameri- can Psychiatric Publishing Textbook of Neuropsychiatry and Clinical Neurosciences, 4th ed. Washington, DC: American Psychiatric Pub, 2002. [91] S. L. Videbeck, Psychiatric-Mental Health Nursing.", "meta": {"paper_id": "1906.01157v2", "chunk_index": 37, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_38", "text": "[89] A. M. Colman, A Dictionary of Psychology. Oxford University Press, 2015. [90] S. C. Yudofsky, R. E. Hales, and A. P. Publishing, Eds., The Ameri- can Psychiatric Publishing Textbook of Neuropsychiatry and Clinical Neurosciences, 4th ed. Washington, DC: American Psychiatric Pub, 2002. [91] S. L. Videbeck, Psychiatric-Mental Health Nursing. Lippincott Williams & Wilkins, 2010. [92] H. Goodglass and E. Kaplan, “The assessment of aphasia and related disorders,” 1983. [93] R. L. Horwitz-Martin, T. F. Quatieri, A. C. Lammert, J. R. Williamson, Y. Yunusova, E. Godoy, D. D. Mehta, and J. R. Green, “Relation of Automatically Extracted Formant Trajectories with Intelligibility Loss and Speaking Rate Decline in Amyotrophic Lateral Sclerosis,” in Proc. Interspeech 2016. San Francisco, CA, USA: ISCA, 2016, pp. 1205– 1209. [94] S. Sandoval, V. Berisha, R. L. Utianski, J. M. Liss, and A. Spanias, “Automatic assessment of vowel space area,” The Journal of the Acoustical Society of America, vol. 134, no. 5, pp. EL477–EL483, Nov. 2013. [95] J. C. Hailstone, G. R. Ridgway, J. W. Bartlett, J. C. Goll, A. H. Buckley, S. J. Crutch, and J. D. Warren, “Voice processing in dementia: A neuropsychological and neuroanatomical analysis,” Brain, vol. 134, no. 9, pp. 2535–2547, Sep. 2011. [96] M. N. Stuttle, J. D. Williams, and S. Young, “A framework for dialogue data collection with a simulated ASR channel,” in Eighth International Conference on Spoken Language Processing, 2004. [97] E. Simonnet, S. Ghannay, N. Camelin, and Y. Estève, “Simulating ASR errors for training SLU systems,” in LREC 2018, Eleventh International Conference on Language Resources and Evaluation. Miyazaki, Japan: European Language Resources Association, May 2018, p. 7. [98] R. Voleti, J. M. Liss, and V. Berisha, “Investigating the Effects of Word Substitution Errors on Sentence Embeddings,” in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2019, pp. 7315–7319. [99] R. Vipperla, S. Renals, and J. Frankel, “Longitudinal study of ASR performance on ageing voices,” in Proc. Interspeech 2008. Brisbane, Australia: ISCA, Sep. 2008, pp. 2550–2553. [100] V. Young and A. Mihailidis, “Difﬁculties in Automatic Speech Recog- nition of Dysarthric Speakers and Implications for Speech-Based Applications Used by the Elderly: A Literature Review,” Assistive Technology, vol. 22, no. 2, pp. 99–112, 2010. [101] D. Hakkani-Tür, D. Vergyri, and G. Tur, “Speech-based automated cognitive status assessment,” in Proc. Interspeech 2010. Makuhari, Chiba, Japan: ISCA, Sep. 2010, pp. 258–261. [102] A. König, N. Linz, J. Tröger, M. Wolters, J. Alexandersson, and P. Robert, “Fully Automatic Speech-Based Analysis of the Semantic Verbal Fluency Task,” Dementia and Geriatric Cognitive Disorders, vol. 45, no. 3-4, pp. 198–209, 2018. [103] Y. Jiao, V. Berisha, and J. Liss, “Interpretable phonological features for clinical applications,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Mar. 2017, pp. 5045–5049. [104] M. Tu, V. Berisha, and J. Liss, “Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks,” in Proc. Interspeech 2017. Stockholm, Sweden: ISCA, Aug. 2017, pp. 1849– 1853. [105] N. Coupland, Style: Language Variation and Identity, ser. Key Topics in Sociolinguistics. Cambridge University Press, 2007. [106] N. Schilling, “Investigating", "meta": {"paper_id": "1906.01157v2", "chunk_index": 38, "filename": "1906.01157v2.pdf", "chunk_length": 512}}
{"id": "1906.01157v2_chunk_39", "text": "Tu, V. Berisha, and J. Liss, “Interpretable Objective Assessment of Dysarthric Speech Based on Deep Neural Networks,” in Proc. Interspeech 2017. Stockholm, Sweden: ISCA, Aug. 2017, pp. 1849– 1853. [105] N. Coupland, Style: Language Variation and Identity, ser. Key Topics in Sociolinguistics. Cambridge University Press, 2007. [106] N. Schilling, “Investigating stylistic variation,” The handbook of lan- guage variation and change, pp. 325–349, 2013. [107] M. Benzeghiba, R. De Mori, O. Deroo, S. Dupont, T. Erbes, D. Jouvet, L. Fissore, P. Laface, A. Mertins, C. Ris, R. Rose, V. Tyagi, and C. Wellekens, “Automatic speech recognition and speech variability: A review,” Speech Communication, vol. 49, no. 10-11, pp. 763–786, Oct. 2007. [108] K. D. Mueller, R. L. Koscik, L. R. Clark, B. P. Hermann, S. C. Johnson, and L. S. Turkstra, “The Latent Structure and Test–Retest Stability of Connected Language Measures in the Wisconsin Registry for Alzheimer’s Prevention (WRAP),” Archives of Clinical Neuropsy- chology, vol. 33, no. 8, pp. 993–1005, Dec. 2018. Rohit Voleti is a Ph.D. student in the School of Electrical, Computer, & Energy Engineering at Ari- zona State University (ASU) in Tempe, AZ, USA. Prior to his time at ASU, he obtained a B.S. and M.S. in Electrical Engineering from the University of California, Los Angeles (UCLA), and worked as a systems engineer in the medical device industry in Southern California. Julie M. Liss is a Professor of Speech & Hear- ing Science and Associate Dean of the College of Health Solutions at Arizona State University (ASU) in Tempe, AZ, USA. Her research explores the ways in which speech and language change in the context of neurological damage or disease. Visar Berisha is an Associate Professor in the Col- lege of Health Solutions and Fulton Entrepreneurial Professor in the School of Electrical Computer & Energy Engineering at Arizona State University (ASU) in Tempe, AZ, USA. His research interests include computational models of speech production and perception, clinical speech analytics, and statis- tical signal processing.", "meta": {"paper_id": "1906.01157v2", "chunk_index": 39, "filename": "1906.01157v2.pdf", "chunk_length": 326}}
{"id": "1906.01161v2_chunk_0", "text": "Resolving Gendered Ambiguous Pronouns with BERT Matei Ionita University of Pennsylvania matei@sas.upenn.edu Yury Kashnitsky Moscow Institute of Physics and Technology yury.kashnitsky@phystech.edu Ken Krige Kitsong kenkrige@gmail.com Vladimir Larin PJSC Sberbank vlarine@gmail.com Denis Logvinenko Jet Infosystems dennis_l@rambler.ru Atanas Atanasov Soﬁa University a.atanasov1@ocado.com Abstract Pronoun resolution is part of coreference res- olution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding and a nec- essary component of machine translation sys- tems, chat bots and assistants. Neural machine learning systems perform far from ideally in this task, reaching as low as 73% F1 scores on modern benchmark datasets. Moreover, they tend to perform better for masculine pronouns than for feminine ones. Thus, the problem is both challenging and important for NLP re- searchers and practitioners. In this project, we describe our BERT-based approach to solving the problem of gender-balanced pronoun reso- lution. We are able to reach 92% F1 score and a much lower gender bias on the benchmark dataset shared by Google AI Language team. 1 Introduction In this work, we are dealing with gender bias in pronoun resolution. A more general task of coref- erence resolution is reviewed in Sec. 2. In Sec. 3, we give an overview of a related Kaggle com- petition. Then, Sec. 4 describes the GAP dataset and Google AI’s heuristics to resolve pronomial coreference in a gender-agnostic way, so that pro- noun resolution is done equally well in cases of masculine and feminine pronouns. In Sec. 5, we provide the details of our BERT-based solution while in Sec. 6 we analyze pleasantly low gender bias speciﬁc for our system (our code is shared on GitHub1). Lastly, in Sec. 7, we draw conclusions and express some ideas for further research. 2 Related work Among popular approaches to coreference res- olution are:2 rule-based, mention pair, mention ranking, and clustering. As for rule-based ap- proaches, they describe naïve Hobbs algorithm (Hobbs, 1986) which, in spite of being naïve, has shown state-of-the-art performance on the OntoNotes dataset3 up to 2010. Recent state-of-the-art approaches (Lee et al., 2018, 2017; Peters et al., 2018a) are pretty complex examples of mention ranking systems. The 2017 version is the ﬁrst end-to-end corefer- ence resolution model that didn’t utilize syntactic parsers or hand-engineered mention detectors. In- stead, it used LSTMs and attention mechanism to improve over previous NN-based solutions. Some more state-of-the-art coreference resolu- tion systems are reviewed in (Webster et al., 2018) as well as popular datasets with ambiguous pro- nouns: Winograd schemas (Levesque et al., 2012), WikiCoref (Ghaddar and Langlais, 2016), and 1https://github.com/Yorko/ gender-unbiased_BERT-based_pronoun_ resolution 2https://bit.ly/2JbKxv1 3https://catalog.ldc.upenn.edu/ LDC2013T19 arXiv:1906.01161v2 [cs.CL] 13 Jun 2019 The Deﬁnite Pronoun Resolution Dataset (Prad- han et al., 2007). We also refer to the GAP paper for a brief review of gender bias in machine learn- ing. We further outline that e2e-coref model (Lee et al., 2018), in spite of being state-of-the-art in coreference resolution, didn’t show good results in the pronoun resolution task that we tackled, so we only used e2e-coref predictions as an additional feature. 3 Kaggle competition “Gendered Pronoun", "meta": {"paper_id": "1906.01161v2", "chunk_index": 0, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_1", "text": "bias in machine learn- ing. We further outline that e2e-coref model (Lee et al., 2018), in spite of being state-of-the-art in coreference resolution, didn’t show good results in the pronoun resolution task that we tackled, so we only used e2e-coref predictions as an additional feature. 3 Kaggle competition “Gendered Pronoun Resolution” Following Kaggle competition “Gendered Pro- noun Resolution”,4 for each abstract from Wikipedia pages we are given a pronoun, and we try to predict the right coreference for it, i.e. to which named entity (A or B) it refers. Let’s take a look at this simple example: “John entered the room and saw [A] Julia. [Pronoun] She was talking to [B] Mary Hendriks and looked so extremely gorgeous that John was stunned and couldn’t say a word.” Here “Julia” is marked as entity A, “Mary Hen- driks” – as entity B, and pronoun “She” is marked as Pronoun. In this particular case the task is to correctly identify to which entity the given pro- noun refers. If we feed this sentence into a coreference res- olution system (see Fig. 1 and online demo5), we see that it correctly identiﬁes that “she” refers to Julia, it also correctly clusters together two men- tions of “John” and detects that Mary Hendriks is a two-word span. For instance, if you take an abstract like this it’s pretty hard to resolve coreference. “Roxanne, a poet who now lives in France. Is- abel believes that she is there to help Roxanne dur- ing her pregnancy with her toddler infant, but later realizes that her father and step-mother sent her there so that Roxanne would help the shiftless Is- abel gain some direction in life. Shortly after she (pronoun) arrives, Roxanne conﬁdes in Isabel that her French husband, Claude-Henri has left her.” Google AI and Kaggle (organizers of this com- petition) provided the GAP dataset (Webster et al., 2018) with 4454 snippets from Wikipedia articles, in each of them named entities A and B are labeled 4https://www.kaggle.com/c/ gendered-pronoun-resolution 5https://bit.ly/2I4tECI along with a pronoun. The dataset is labeled, i.e. for each sentence a correct coreference is speci- ﬁed, one of three mutually-exclusive classes: ei- ther A or B or “Neither”. Thus, the prediction task is actually that of multiclass classiﬁcation type. Moreover, the dataset is balanced w.r.t. mascu- line and feminine pronouns. Thus, the competition was supposed to address the problem of building a coreference resolution system which is not sus- ceptible to gender bias, i.e. works equally well for masculine and feminine pronouns. These are the columns provided in the dataset (Webster et al., 2018): • ID - Unique identiﬁer for an example (matches to Id in output ﬁle format) • Text - Text containing the ambiguous pro- noun and two candidate names (about a para- graph in length) • Pronoun - target pronoun (text) • Pronoun-offset - character offset of Pronoun in Text • A - ﬁrst name candidate (text) • A-offset - character offset of name A in Text • B - second name candidate • B-offset - character offset of name B in Text •", "meta": {"paper_id": "1906.01161v2", "chunk_index": 1, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_2", "text": "length) • Pronoun - target pronoun (text) • Pronoun-offset - character offset of Pronoun in Text • A - ﬁrst name candidate (text) • A-offset - character offset of name A in Text • B - second name candidate • B-offset - character offset of name B in Text • URL - URL of the source Wikipedia page for the example Evaluation metric chosen for the competition6 is multiclass logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or “Neither”. For each pronoun, a set of predicted probabilities (one for each class) is submitted. The formula is then logloss = −1 N N X i=1 M X j=1 yij log pij, where N is the number of samples in the test set, M is 3, log is the natural logarithm, yij is 1 if ob- servation i belongs to class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j. 6https://www.kaggle.com/c/ gendered-pronoun-resolution/overview/ evaluation Figure 1: Coreference resolution visualized with HuggingFace demo https://huggingface.co/coref/. Unfortunately, the chosen evaluation metric does not reﬂect the mentioned above goal of build- ing a gender-unbiased coreference resolution al- gorithm, i.e. the metric does not account for gen- der imbalance - logarithmic loss may not reﬂect the fact that e.g. predicted pronoun coreference is much worse for masculine pronouns than for feminine ones. Therefore, we explore gender bias separately in Sec. 6 and compare our results with those published by the Google AI Language team (reviewed in Sec. 4). 4 Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns Google AI Language team addresses the prob- lem of gender bias in pronoun resolution (when systems favor masculine entities) and a gender- balanced labeled corpus of 8,908 ambiguous pronoun-name pairs sampled to provide diverse coverage of challenges posed by real-world text (Webster et al., 2018) (further referred to as the GAP dataset). They run 4 state-of-the-art coref- erence resolution models (Lee et al., 2013; Clark and Manning, 2015; Wiseman et al., 2016; Lee et al., 2017) on the OntoNotes and GAP datasets reporting F1 scores separately for masculine and feminine pronoun-named entity pairs (metrics M and F in the paper). Also they measure “gender bias” deﬁned as B = F / M. In general, they con- clude, these models perform better for masculine pronoun-named entity pairs, but still pronoun res- olution is challenging - all achieved F1 scores are less than 0.7 for both datasets. Further, they propose simple heuristics (called surface, structural and Wikipedia cues). The best reported cues are “Parallelism” (if the pronoun is a subject or direct object, select the closest can- didate with the same grammatical argument) and “URL” (select the syntactically closest candidate which has a token overlap with the page title). They compare the performance of “Parallelism + URL” cue with e2e-coref (Lee et al., 2017) on the GAP dataset and, surprisingly enough, con- clude that heuristics work better achieving bet- ter F1 scores (0.742 for M and 0.716 for F) at the same time being less gender-biased (some of heuristics", "meta": {"paper_id": "1906.01161v2", "chunk_index": 2, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_3", "text": "They compare the performance of “Parallelism + URL” cue with e2e-coref (Lee et al., 2017) on the GAP dataset and, surprisingly enough, con- clude that heuristics work better achieving bet- ter F1 scores (0.742 for M and 0.716 for F) at the same time being less gender-biased (some of heuristics are totally gender-unbiased, for “Paral- lelism + URL” B = F / M = 0.96). Finally, they explored Transformer architecture (Vaswani et al., 2017) for this task and observed that the coreference signal is localized on speciﬁc heads and that these heads are in the deep layers of the network. In Sec. 5 we conﬁrm this obser- vation. Actually, they select the candidate which attends most to the pronoun (“Transformer heuris- tic” in the paper). Even though they conclude that Transformer models implicitly learn language understanding relevant to coreference resolution, as for F1 scores, they didn’t make it work bet- ter than e2e-coref or Parallelism cues (F1 scores lower that 0.63). More to that, proposed Trans- formers heuristics are a bit biased towards mascu- line pronouns with B from 0.95 to 0.98. Further we report a much stronger gender- unbiased BERT-based (Devlin et al., 2018) pro- noun resolution system. 5 System BERT (Devlin et al., 2018) is a transformer archi- tecture, pre-trained on a large corpus (Wikipedia + BookCorpus), with 12 to 24 transformer layers. Each layer learns a 1024-dimensional representa- tion of the input token, with layer 1 being similar to a standard word embedding, layer 24 special- ized for the task of predicting missing words from context. At the same time BERT embeddings are learned for a second auxiliary task of resolving whether two consequent sentences are connected to each other or not. In general, motivated by (Tenney et al., 2019), we found that BERT provides very good token em- beddings for the task in hand. Our proposed pipeline is built upon solutions by teams “Ken Krige” and “[ods.ai] ﬁve zeros” (placed 5 and 22 in the ﬁnal leaderboard7 corre- spondingly). The way these two teams approached the competition task are described in two Kaggle posts.89 The combined pipeline includes several subroutines: • Extracting BERT-embeddings for named en- tities A, B, and pronouns • Fine-tuning BERT classiﬁer • Hand-crafted features • Neural network architectures • Correcting mislabeled instances 5.1 Extracting BERT-embeddings for named entities A, B, and pronouns We concatenated embeddings for entities A, B, and Pronoun taken from Cased and Uncased large BERT “frozen” (not ﬁne-tuned) models.10 We no- ticed that extracting embeddings from intermedi- ate layers (from -4 to -6) worked best for the task. Also we added pointwise products of embeddings for Pronoun and entity A, Pronoun and entity B as well as AB - PP. First of these embedding vectors expresses similarity between pronoun and A, the second one expresses similarity between pronoun and B, the third vector is supposed to represent the extent to which entities A and B are similar to each other but differ from the Pronoun. 7https://www.kaggle.com/c/ gendered-pronoun-resolution/leaderboard 8https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 90668 9https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 90431 10https://github.com/google-research/ bert 5.2 Fine-tuning BERT classiﬁer Apart from", "meta": {"paper_id": "1906.01161v2", "chunk_index": 3, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_4", "text": "the second one expresses similarity between pronoun and B, the third vector is supposed to represent the extent to which entities A and B are similar to each other but differ from the Pronoun. 7https://www.kaggle.com/c/ gendered-pronoun-resolution/leaderboard 8https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 90668 9https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 90431 10https://github.com/google-research/ bert 5.2 Fine-tuning BERT classiﬁer Apart from extracting embeddings from original BERT models, we also ﬁne-tuned BERT classi- ﬁer for the task in hand. We made appropri- ate changes to the “run_classiﬁer.py” script from Google’s repository.11 Preprocessing input data for the BERT input layer included stripping text to 64 symbols, then into 4 segments, running BERT Wordpiece for each segment, adding start and end tokens (with truncation if needed) and concatenat- ing segments back together. The whole prepro- cessing is reproduced in a Kaggle Kernel12 as well as in our ﬁnal code on GitHub.13 5.3 Hand-crafted features Apart from BERT embeddings, we also added 69 features which can be grouped into several cate- gories: • Neuralcoref,14 Stanford CoreNLP (Manning et al., 2014) and e2e-coref (Lee et al., 2017) model predictions. It turned out that these models performed not really well in the task in hand, but their predictions worked well as additional features. • Predictions of a Multi-Layered Perceptron trained with ELMo (Peters et al., 2018b) em- beddings • Syntactic roles of entities A, B, and Pro- noun (subject, direct object, attribute etc.) ex- tracted with SpaCy 15. • Positional and frequency-based (distances between A, B, Pronoun and derivations, whether they all are in the same sentence or Pronoun is in the following one etc.). Many of these features we motivated by the Hobbs algorithm (Hobbs, 1986) for coreference res- olution. • Named entities predicted for A and B with SpaCy 11https://github.com/google-research/ bert 12https://www.kaggle.com/kenkrige/ bert-example-prep 13https://github.com/Yorko/ gender-unbiased_BERT-based_pronoun_ resolution 14https://github.com/huggingface/ neuralcoref 15https://spacy.io/ • GAP heuristics outlined in the corresponding paper (Webster et al., 2018) and brieﬂy dis- cussed in Sec. 4 We need to mention that adding all these fea- tures had only minor effect on the quality of pro- noun resolution (resulted in a 0.01 decrease in log- arithmic loss when measured on the Kaggle test dataset) as compared to e.g. ﬁne-tuning BERT classiﬁer. 5.4 Neural network architectures Final setup includes: • 6 independently trained ﬁne-tuned BERT classiﬁers with preprocessing described in Subsec. 5.2. In Tables 1, 2, and 3, we re- fer to their averaged prediction as to that of a “ﬁne-tuned” model ( ) • 5 multi-layered perceptrons trained with dif- ferent combinations of BERT embeddings for A, B, Pronoun (see Subsec. 5.1) and hand- crafted features (see Subsec. 5.3), all together referred to as “frozen” in Tables 1, 2, and 3 ( ). Using MLPs with pre-trained BERT embeddings is motivated by (Tenney et al., 2019). Two MLPs- separate for Cased and Uncased BERT models - both taking 9216- d input and outputting 112-d vectors. Two Siamese networks were trained on top of dis- tances between Pronoun and A-embeddings, Pronoun and B-embeddings as inputs. One more MLP took only 69-dimensional fea- ture vectors as an input. Finally, a single dense layer mapped outputs from the men-", "meta": {"paper_id": "1906.01161v2", "chunk_index": 4, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_5", "text": "both taking 9216- d input and outputting 112-d vectors. Two Siamese networks were trained on top of dis- tances between Pronoun and A-embeddings, Pronoun and B-embeddings as inputs. One more MLP took only 69-dimensional fea- ture vectors as an input. Finally, a single dense layer mapped outputs from the men- tioned 5 models into 3 classes corresponding to named entities A, B or “Neither”. • Blending ( ) involves taking predicted probabilities for A, B and “Neither” with weight 0.65 for the “ﬁne-tuned” model and summing the result with 0.35 times corre- sponding probabilities output by the “frozen” model. In the next Section, we perform the analysis identical to the one done in (Webster et al., 2018) to measure the quality of pronoun resolution and the severity of gender bias in the task in hand. GAP test Kaggle test ﬁne-tuned 0.29 0.192 frozen 0.299 0.226 blend 0.257 0.185 Table 1: Logarithmic loss reported for the GAP test set, and Kaggle test (Stage 2) data for the model with ﬁne- tuned BERT classiﬁer ( ), MLPs with pre-trained BERT embeddings and hand-crafted features ( ) and a blend of the previous two ( ). There are 66 correc- tions done for GAP test labels as described in Subsec. 5.5. 5.5 Correcting mislabeled instances During the competition, 158 label corrections were proposed for the GAP dataset16 - when Pro- noun is said to mention A but actually mentions B and vice versa. For the GAP test set, this re- sulted in 66 pronoun coreferences being corrected. It’s important to mention that the observed misla- beling is a bit biased against female pronouns (39 mislabeled feminine pronouns versus 27 misla- beled masculine ones), and it turned out that most of the gender bias for F1 score and accuracy comes from these mislabeled examples. 6 Results In Table 1, we report logarithmic loss that we got on GAP test (“gap-test.tsv”), and Kaggle test (Stage 2) datasets. Kaggle competition results can also be seen on the ﬁnal competition leader- board.17 We report GAP test results as well to further compare with the results reported in the GAP paper: measured are logarithmic loss, F1 score and accuracy for masculine and feminine pronouns (Table 2). Logarithmic loss and accu- racy are computed for a 3-class classiﬁcation prob- lem (A, B, or Neither) while F1 is computed for a 2-class problem (A or B) to compare with re- sults reported by the Google AI Language team in (Webster et al., 2018). We also incorporated 66 label corrections as de- scribed in 5.5 and, interestingly enough, this lead to a conclusion that with corrected labels, models are less susceptible to gender bias. Table 3 reports the same metric in case of corrected labeling, and we see that in this case the proposed models are 16https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 81331 17https://www.kaggle.com/c/ gendered-pronoun-resolution/leaderboard Logarithmic loss Accuracy F1 score M F O B M F O B M F O B ﬁne-tuned 0.294 0.398 0.346 0.738 0.908 0.884 0.896 0.974 0.927 0.9 0.914 0.971 frozen 0.308 0.368 0.338 0.837 0.883 0.866 0.874 0.981 0.904 0.882 0.893", "meta": {"paper_id": "1906.01161v2", "chunk_index": 5, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_6", "text": "proposed models are 16https://www.kaggle.com/c/ gendered-pronoun-resolution/discussion/ 81331 17https://www.kaggle.com/c/ gendered-pronoun-resolution/leaderboard Logarithmic loss Accuracy F1 score M F O B M F O B M F O B ﬁne-tuned 0.294 0.398 0.346 0.738 0.908 0.884 0.896 0.974 0.927 0.9 0.914 0.971 frozen 0.308 0.368 0.338 0.837 0.883 0.866 0.874 0.981 0.904 0.882 0.893 0.976 blend 0.259 0.338 0.299 0.766 0.907 0.883 0.895 0.974 0.923 0.898 0.911 0.973 Table 2: Performance of the proposed two models and their blending on the GAP test set, split by Masculine, Feminine (Bias shows F/M in case of F1 and accuracy, and M/F in case of logarithmic loss), and Overall. almost gender-unbiased. These results imply that: • Overall, in terms of F1 score, the proposed solution compares very favorably with the re- sults reported in the GAP paper, achieving as high as 0.911 overall F1 score, compared to 0.729 for “Parallelism + URL” heuristic from (Webster et al., 2018); • Blending model predictions improves loga- rithmic loss pretty well but does not impact F1 score and accuracy that much. It can be explained: logarithmic loss is high for con- ﬁdent and at the same time incorrect predic- tions. Blending averages predicted probabil- ities so that they end up less extreme (not so close to 0 or 1); • With original labeling, all models are some- what susceptible to gender bias, especially in terms of logarithmic loss. However, in terms of F1 score, gender bias is still less than for e2e-coref and “Parallelism + URL” heuristic reported in (Webster et al., 2018); • Fixing some incorrect labels almost elimi- nates gender bias, when we talk about F1 score and accuracy of pronoun resolution. 7 Conclusions and further work We conclude that we managed to propose a BERT- based approach to pronoun resolution which re- sults in considerably better quality (as measured in terms of F1 score and accuracy) than in case of pronoun resolution done with heuristics described in the GAP paper. Moreover, the proposed solu- tion is almost gender-unbiased - pronoun resolu- tion is done almost equally well for masculine and feminine pronouns. Further we plan to investigate which semantic and syntactic information is carried by different BERT layers and how it refers to coreference reso- lution. We are also going to benchmark our system on OntoNotes, Winograd, and DPR datasets. Acknowledgments Authors would like to thank Open Data Science18 community for all insightful discussions related to Natural Language Processing and, more gener- ally, to Deep Learning. Authors are also grateful to Kaggle and Google AI Language teams for or- ganizing the Gendered Pronoun Resolution chal- lenge. References Kevin Clark and Christopher D. Manning. 2015. Entity-centric coreference resolution with model stacking. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pages 1405–1415, Beijing, China. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. Cite arxiv:1810.04805Comment: 13 pages. Abbas Ghaddar and Phillippe Langlais. 2016. Wiki- coref:", "meta": {"paper_id": "1906.01161v2", "chunk_index": 6, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_7", "text": "Conference on Natu- ral Language Processing (Volume 1: Long Papers), pages 1405–1415, Beijing, China. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. Cite arxiv:1810.04805Comment: 13 pages. Abbas Ghaddar and Phillippe Langlais. 2016. Wiki- coref: An english coreference-annotated corpus of wikipedia articles. In Proceedings of the Tenth In- ternational Conference on Language Resources and Evaluation (LREC 2016), Paris, France. European Language Resources Association (ELRA). J Hobbs. 1986. Readings in natural language process- ing. chapter Resolving Pronoun References, pages 339–352. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA. Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu, and Dan Jurafsky. 2013. Deterministic coreference resolu- tion based on entity-centric, precision-ranked rules. Comput. Linguist., 39(4):885–916. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference reso- lution. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Process- ing, pages 188–197, Copenhagen, Denmark. Asso- ciation for Computational Linguistics. 18https://ods.ai Logarithmic loss Accuracy F1 score M F O B M F O B M F O B ﬁne-tuned 0.268 0.311 0.29 0.863 0.914 0.905 0.91 0.99 0.932 0.919 0.926 0.987 frozen 0.292 0.306 0.299 0.954 0.886 0.89 0.888 1.005 0.908 0.906 0.907 0.997 blend 0.241 0.273 0.257 0.882 0.913 0.908 0.91 0.995 0.928 0.921 0.924 0.992 Table 3: Performance of the proposed two models and their blending on the GAP test set with 66 corrected labels, split by Masculine, Feminine (Bias shows F/M in case of F1 and accuracy, and M/F in case of logarithmic loss), and Overall. Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- ﬁne inference. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 2 (Short Papers), pages 687–692, New Orleans, Louisiana. Association for Computational Linguistics. Hector J. Levesque, Ernest Davis, and Leora Morgen- stern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Con- ference on Principles of Knowledge Representa- tion and Reasoning, KR’12, pages 552–561. AAAI Press. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP natural lan- guage processing toolkit. In Association for Compu- tational Linguistics (ACL) System Demonstrations, pages 55–60. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018a. Deep contextualized word representations. Cite arxiv:1802.05365Comment: NAACL 2018. Originally posted to openreview 27 Oct 2017. v2 updated for NAACL camera ready. Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018b. Deep contextualized word rep- resentations. CoRR, abs/1802.05365. Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jessica Macbride, and Linnea Micciulla. 2007. Unrestricted coreference: Identifying entities and events in ontonotes. International Conference on Semantic Computing, 0:446–453. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipan- jan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for", "meta": {"paper_id": "1906.01161v2", "chunk_index": 7, "filename": "1906.01161v2.pdf", "chunk_length": 512}}
{"id": "1906.01161v2_chunk_8", "text": "Unrestricted coreference: Identifying entities and events in ontonotes. International Conference on Semantic Computing, 0:446–453. Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R. Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R. Bowman, Dipan- jan Das, and Ellie Pavlick. 2019. What do you learn from context? probing for sentence structure in con- textualized word representations. In International Conference on Learning Representations. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems 30, pages 5998–6008. Curran As- sociates, Inc. Kellie Webster, Marta Recasens, Vera Axelrod, and Ja- son Baldridge. 2018. Mind the gap: A balanced cor- pus of gendered ambiguous pronouns. In Transac- tions of the ACL. Sam Wiseman, Alexander M. Rush, and Stuart M. Shieber. 2016. Learning global features for coref- erence resolution. In Proceedings of the 2016 Con- ference of the North American Chapter of the Asso- ciation for Computational Linguistics: Human Lan- guage Technologies, pages 994–1004, San Diego.", "meta": {"paper_id": "1906.01161v2", "chunk_index": 8, "filename": "1906.01161v2.pdf", "chunk_length": 193}}
{"id": "1906.01185v1_chunk_0", "text": "Joint Effects of Context and User History for Predicting Online Conversation Re-entries Xingshan Zeng1,2, Jing Li3∗, Lu Wang4, Kam-Fai Wong1,2 1The Chinese University of Hong Kong, Hong Kong 2MoE Key Laboratory of High Conﬁdence Software Technologies, China 3Tencent AI Lab, Shenzhen, China 4Northeastern University, Boston, MA, United States 1,2{xszeng,kfwong}@se.cuhk.edu.hk 3ameliajli@tencent.com, 4luwang@ccs.neu.edu Abstract As the online world continues its exponen- tial growth, interpersonal communication has come to play an increasingly central role in opinion formation and change. In order to help users better engage with each other online, we study a challenging problem of re-entry pre- diction foreseeing whether a user will come back to a conversation they once participated in. We hypothesize that both the context of the ongoing conversations and the users’ previous chatting history will affect their continued in- terests in future engagement. Speciﬁcally, we propose a neural framework with three main layers, each modeling context, user history, and interactions between them, to explore how the conversation context and user chatting his- tory jointly result in their re-entry behavior. We experiment with two large-scale datasets collected from Twitter and Reddit. Results show that our proposed framework with bi- attention achieves an F1 score of 61.1 on Twit- ter conversations, outperforming the state-of- the-art methods from previous work. 1 Introduction Interpersonal communication plays an important role in information exchange and idea sharing in our daily life. We are involved in a wide variety of dialogues every day, ranging from kitchen ta- ble conversations to online discussions, all help us make decisions, better understand important social issues, and form personal ideology. However, in- dividuals have limited attentions to engage in the massive amounts of online conversations. There thus exists a pressing need to develop automatic conversation management tools to keep track of the discussions one would like to keep engaging in. To meet such demand, we study the prob- lem of predicting online conversation re-entries, ∗Jing Li is the corresponding author. …… H1: Is there literally no one on twitter who wants to talk about LET ME IN with me? :( H2: I think the change in overall tone was enough to let LMI stand on it's own. Love Giacchino's score too. H3: I think if i had seen LMI again before making my top ten it would have made the cut. Oh well. H4: it's not as bad as I remembered on the blu-ray. Looks like shit next to Avatar, but so does everything lol …… User History of U1 Conversation 1 Conversation 2 T1[U2]: Instead of focusing on when Oscars got it wrong... Let's talk about when the Oscars got it right… T2[U1]: The Hurt Locker, The Departed, NCFOM, LOTR, Schindler's List, Braveheart, Gladiator, The Godfather Part 1 & 2. ...... …… T1[U3]: Almost fell asleep in the first hour of INCEPTION. In the theatre. T2[U4]: lol do you not like it? T3[U5]: Meh. MEMENTO = far better film. T4[U1]: apples and oranges, plain and simple. …… T5[U1]: Inception and Memento. Same filmmaker, but completely different scope, themes, ideas, genres, etc. Figure 1: Sample tweets in the chatting history of", "meta": {"paper_id": "1906.01185v1", "chunk_index": 0, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_1", "text": "of INCEPTION. In the theatre. T2[U4]: lol do you not like it? T3[U5]: Meh. MEMENTO = far better film. T4[U1]: apples and oranges, plain and simple. …… T5[U1]: Inception and Memento. Same filmmaker, but completely different scope, themes, ideas, genres, etc. Figure 1: Sample tweets in the chatting history of user U1 and two Twitter conversation snippets U1 engaged in. Hi: the i-th tweet in U1’s history. Ti[Uj]: the i-th turn posted by Uj. First entries by U1 are highlighted in blue in both conversations. U1 only returns to the second one. where we aim to forecast whether the users will return to a discussion they once entered. What will draw a user back? To date, prior ef- forts for re-entry prediction mainly focus on mod- eling users engagement patterns in the ongoing conversations (Backstrom et al., 2013) or rely on the social network structure (Budak and Agrawal, 2013), largely ignoring the rich information in users’ previous chatting history. Here we argue that effective prediction of one’s re-entry behavior requires the understanding of both the conversation context—what has been discussed in the dialogue under consideration, and user chatting history (henceforth user history)— what conversation topics the users are actively in- volved in. In Figure 1, we illustrate how the two factors together affect a user’s re-entry behavior. Along with two conversations that user U1 partici- pated in, also shown is their chatting history in pre- vious discussions. U1 comes back to the second conversation since it involves topics on movies (e.g. mentioning Memento and Inception) and thus arXiv:1906.01185v1 [cs.CL] 4 Jun 2019 suits their interests according to the chatting his- tory, which also talked about movies. In this work, we would like to focus on the joint effects of conversation context and user history, ig- noring other information. It would be a more chal- lenging yet general task, since information like so- cial networks may be not available in some certain scenarios. To study how conversation context and user history jointly affect user re-entries, we pro- pose a novel neural framework that incorporates and aligns the indicative representations from the two information source. To exploit the joint ef- fects, four mechanisms are employed here: sim- ple concatenation of the two types of representa- tion, attention mechanism over turns in context, memory networks (Sukhbaatar et al., 2015) — able to learn context attentions in aware of user his- tory, and bi-attention (Seo et al., 2016) — further capturing interactions from two directions (con- text to history and history to context). More im- portantly, our framework enables the re-entry pre- diction and corresponding representations to be learned in an end-to-end manner. On the contrary, previous methods for the same task rely on hand- crafted features (Backstrom et al., 2013; Budak and Agrawal, 2013), which often require labor- intensive and time-consuming feature engineering processes. To the best of our knowledge, we are the ﬁrst to explore the joint effect of conversation context and user history on predicting re-entry be- havior in a neural network framework. We experiment with two large-scale datasets, one from Twitter (Zeng", "meta": {"paper_id": "1906.01185v1", "chunk_index": 1, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_2", "text": "often require labor- intensive and time-consuming feature engineering processes. To the best of our knowledge, we are the ﬁrst to explore the joint effect of conversation context and user history on predicting re-entry be- havior in a neural network framework. We experiment with two large-scale datasets, one from Twitter (Zeng et al., 2018), the other from Reddit which is newly collected1. Our framework with bi-attention signiﬁcantly outper- forms all the comparing methods including the previous state of the art (Backstrom et al., 2013). For instance, our model achieves an F1 score of 61.1 on Twitter conversations, compared to an F1 score of 57.0 produced by Backstrom et al. (2013), which is based on a rich set of handcrafted features. Further experiments also show that the model with bi-attention can consistently outper- form comparisons given varying lengths of con- versation context. It shows that bi-attention mech- anism can well align users’ personal interests and conversation context in varying scenarios. After probing into the proposed neural frame- work with bi-attention, we ﬁnd that meaningful 1The datasets and codes are released at: https:// github.com/zxshamson/re-entry-prediction representations are learned via exploring the joint effect of conversation context and user history, which explains the effectiveness of our framework in predicting re-entry behavior. Finally, we carry out a human study, where we ask two humans to perform on the same task of ﬁrst re-entry pre- diction. The model with bi-attention outperforms both humans, suggesting the difﬁculty of the task as well as the effectiveness of our proposed frame- work. 2 Related Work Response Prediction. Previous work on re- sponse prediction mainly focuses on predicting whether users will respond to a given social media post or thread. Efforts have been made to measure the popularity of a social media post via modeling the response patterns in replies or retweets (Artzi et al., 2012; Zhang et al., 2015). Some stud- ies investigate post recommendation by predict- ing whether a response will be made by a given user (Chen et al., 2012; Yan et al., 2012; Hong et al., 2013; Alawad et al., 2016). In addition to post-level prediction, other studies focus on response prediction at the conversation-level. Zeng et al. (2018) investigate microblog conversation recommendation by ex- ploiting latent factors of topics and discourse with a Bayesian model, which often requires domain expertise for customized learning algorithms. Our neural framework can automatically acquire the interactions among important components that contribute to the re-entry prediction problem, and can be easily adapted to new domains. For the prediction of re-entry behavior in online conver- sations, previous methods rely on the extraction of manually-crafted features from both the conversa- tion context and the user’s social network (Back- strom et al., 2013; Budak and Agrawal, 2013). Here we tackle a more challenging task, where the re-entries are predicted without using any infor- mation from social network structure, which en- sures the generalizability of our framework to sce- narios where such information is unavailable. Online Conversation Behavior Understanding. Our work is also in line with conversational be- havior understanding, including how users inter- act in online", "meta": {"paper_id": "1906.01185v1", "chunk_index": 2, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_3", "text": "re-entries are predicted without using any infor- mation from social network structure, which en- sures the generalizability of our framework to sce- narios where such information is unavailable. Online Conversation Behavior Understanding. Our work is also in line with conversational be- havior understanding, including how users inter- act in online discourse (Ritter et al., 2010) and how such behavior signals the future trajectory, including their continued engagement (Backstrom et al., 2013; Jiao et al., 2018) and the appear- t1 a1 Turn Encoder LSTM . . . . . . . . . m1 Turn Encoder m|u| Turn Encoder . . . . . . . . . . . . . . . . . . Interaction Modeling Layer for Combining Context and User History rO Linear + Sigmoid Context Modeling Layer User History Modeling Layer Embedding H\" # LSTM LSTM t|c| a|c| H|%| # H\" & H|'| & ((*, ,) Turn Encoder Embedding Embedding Embedding Figure 2: The generic framework for re-entry predic- tion. We implement it with three encoders (Average Embedding, CNN, and BiLSTM) for turn modeling and four mechanisms (Simple Concatenation, Atten- tion, Memory Networks, and Bi-attention) for model- ing interactions between context and user history. ance of impolite behavior (Zhang et al., 2018). To better understand the structure of conversations, Recurrent Neural Network (RNN)-based methods have been exploited to capture temporal dynam- ics (Cheng et al., 2017; Zayats and Ostendorf, 2018; Jiao et al., 2018). Different from the above work, our model not only utilizes the conversa- tions themselves, but also leverages users’ prior posts in other discussions. 3 Neural Re-entry Prediction Combining Context and User History This section describes our neural network-based conversation re-entry prediction framework ex- ploring the joint effects of context and user his- tory. Figure 2 shows the overall architecture of our framework, consisting of three main layers: con- text modeling layer, user history modeling layer, and interaction modeling layer to learn how in- formation captured by the previous two layers in- teract with each other and make decisions condi- tioned on their joint effects. Here we adopt four mechanisms for interaction modeling: simple con- catenation, attention, memory networks, and bi- attention, which will be described later. 3.1 Input and Output We start with formulating model input and output. At input layer, our model is fed with two types of information, the chatting history of the target user u and the observed context of the target con- versation c. The goal of our model is to output a Bernoulli distribution p(u, c) indicating the es- timated likelihood of whether u will re-engage in the conversation c. Below gives more details. Formally, we formulate the context of c as a sequence of chronologically ordered turns ⟨t1, t2, · · · , t|c|⟩, where the last turn t|c| is posted by u (we then predict u’s re-entries afterwards). Each turn t is represented by a sequence of words wt, and an auxiliary triple, at = ⟨it, rt, ut⟩, where it, rt, and ut are three indexes indicating the posi- tion of turn t, which turn t", "meta": {"paper_id": "1906.01185v1", "chunk_index": 3, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_4", "text": "turn t|c| is posted by u (we then predict u’s re-entries afterwards). Each turn t is represented by a sequence of words wt, and an auxiliary triple, at = ⟨it, rt, ut⟩, where it, rt, and ut are three indexes indicating the posi- tion of turn t, which turn t replies to, and the au- thor of t, respectively. Here at is used to record the replying structures as well as the user’s involve- ment pattern. For the user history, we formulate it as a collection of u’s chatting messages {m1, m2, · · · , m|u|}, all posted before the time t|c| occurs. Each message m is denoted as its word sequence, wm. In the following, we explain how the aforemen- tioned representations are processed by our model to make predictions. The three main layers in Fig- ure 2 are described in Sections 3.2, 3.3, and 3.4, respectively. The learning objective is presented in Section 3.5. 3.2 Context Modeling Layer The context modeling layer captures representa- tions from the observed context for the target con- versation c. To this end, we jointly model the content in each turn (henceforth turn modeling) and the turn interactions in conversation structure (henceforth structure modeling). Turn Modeling. The turn representations are modeled via turn-level word sequence with a turn encoder. We exploit three encoders here: Aver- age Embedding (Averaging each word’s embed- ding representation), CNN (Convolutional Neu- ral Networks), and BiLSTM (Bidirectional Long Short-Term Memory). BiLSTM’s empirical per- formance turns out to be slightly better (will be reported in Table 2). Concretely, given the conversation turn t, each word wi of t is represented as a vector mapped by an embedding layer I(·), which is initialized by pre-trained embeddings and updated during train- ing. The embedded vector I(wi) is then fed into the turn encoder, yielding the turn representation for t, denoted by HT t .2 2For all the BiLSTM encoders in this work, without other- wise speciﬁed, we take the concatenation of all hidden states from both the directions as its learned representations. Structure Modeling. To learn the conversa- tional structure representations for c, our model applies BiLSTM, namely structure encoder, to capture the interactions between adjacent turns in its context. Each state of this structure encoder se- quentially takes t’s turn representation, HT t , con- catenated with the auxiliary triple, at, as input to produce the structure representation HC. Our in- tuition is that HC should capture both the content of the conversation and interaction patterns among its participants. Then HC, considered as the con- text representation for c, is sent to interaction mod- eling layer as part of its input. 3.3 User History Modeling Layer To encode the user history for target user u, in this layer, we ﬁrst apply the same encoder in turn modeling to encode each chatting message m by u, as they both explore the post-level representa- tions. The turn encoder is sequentially fed with the embedded word in m, and produce the message- level representation HM m . All messages in u’s user history are further", "meta": {"paper_id": "1906.01185v1", "chunk_index": 4, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_5", "text": "same encoder in turn modeling to encode each chatting message m by u, as they both explore the post-level representa- tions. The turn encoder is sequentially fed with the embedded word in m, and produce the message- level representation HM m . All messages in u’s user history are further concatenated into a matrix HU, serving as u’s user history representation and the input of the next layer. 3.4 Interaction Modeling Layer To capture whether the discussion points in c match the interests of u, HC (from context mod- eling) and HU (from user history modeling) are merged through an interaction modeling mecha- nism over the two sources of information. We hy- pothesize that users will be likely to come back to a conversation if its topic ﬁts their own inter- ests. Here, we explore four different mechanisms for interaction modeling. Their learned interac- tion representation, denoted as rO, is fed into a sigmoid-activated neural perceptron (Glorot et al., 2011), for predicting ﬁnal output p(u, c). It indi- cates how likely the target user u will re-engage in the target conversation c. We then describe the four mechanisms to learn rO in turn below. Simple Concatenation. Here we simply put context representation (last state) and user rep- resentations (with average pooling) side by side, yielding rO = [HC |c|; P|u| j HU j /|u|] as the interac- tion representation for re-entry prediction. Attention. To capture the context information useful for re-entry prediction, we exploit an at- tention mechanism (Luong et al., 2015) over HC. Attentions are employed to “soft-address” impor- tant context turns according to their similarity with user representation (with average pooling). Here we adopt dot attention weights and deﬁne the at- tended interaction representation as: rO = |c| X i αi ·HC i , αi = softmax(HC i · |u| X j HU j /|u|) (1) Memory Networks. To further recognize in- dicative chatting messages in user history, we also apply end-to-end memory networks (MemN2N) (Sukhbaatar et al., 2015) for interac- tion modeling. It can be seen as a recurrent atten- tion mechanism over chatting messages (stored in memory). Hence fed with context representation, memory networks will yield a memory-aware vec- tor as interaction representation: rO = |u| X j αj ·fturn(HU j ), αj = softmax(HC |c| ·HU j ) (2) where fturn(·) denotes the unit function used for turn modeling. Here we adopt multi-hop memory mechanism to allow deep user interests to be learned from chatting history. For more details, we refer the readers to Sukhbaatar et al. (2015). Bi-attention. Inspired by Seo et al. (2016), we also apply bi-attention mechanism to explore the joint effects of context and user history. Intu- itively, the bi-attention mechanism looks for evi- dence, if any, indicating the topics of the current conversation that align with the user’s interests from two directions (i.e. context to history and his- tory to context), such as the names of two movies Inception and Let Me In shown in Figure 1. Con- cretely, bi-attention mechanism captures context- aware attention over user history messages: αU ij =", "meta": {"paper_id": "1906.01185v1", "chunk_index": 5, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_6", "text": "conversation that align with the user’s interests from two directions (i.e. context to history and his- tory to context), such as the names of two movies Inception and Let Me In shown in Figure 1. Con- cretely, bi-attention mechanism captures context- aware attention over user history messages: αU ij = exp(fscore(HC i , HU j )) P|u| j′=1 exp(fscore(HC i , HU j′)) (3) where the alignment score function takes a form of fscore(HC i , HU j ) = Wbi−att[HC i ; HU j ; HC i ◦HU j ]. It captures the similarity of the i-th context turn and the j-th user history message. The weight vec- tor Wbi−att is learnable in training. Likewise, we compute user-aware attention over context turns. Afterwards, the bi-directional attended representations are concatenated and passed into a ReLU-activated multilayer percep- tron (MLP), yielding representation r. r, as turn- level representation, is then sequentially fed into a two-layer BiLSTM, to produce the interaction rep- resentation rO. 3.5 Learning Objective For parameter learning in our model, we design the objective function based on cross-entropy loss as following: L = − X i \u0002 λyi log(ˆyi) + µ(1 −yi) log(1 −ˆyi) \u0003 (4) where the two terms reﬂect the prediction on pos- itive and negative instances, respectively. More- over, to take the potential data imbalance into ac- count, we adopt two trade-off weights λ and µ. The parameter values are set based on the pro- portion of positive and negative instances in the training set (see Section 4). ˆyi denotes the re-entry probability estimated from p(u, c) for the i-th in- stance, and yi is the corresponding binary ground- truth label (1 for re-entry and 0 for the opposite). 4 Experimental Setup Data Collection and Statistic Analysis. To study re-entry behavior in online conversations, we collected two datasets: one is released by Zeng et al. (2018) containing Twitter conversa- tions formed by tweets from the TREC 2011 mi- croblog track data3 (henceforth Twitter), and the other is newly collected from Reddit (henceforth Reddit), a popular online forum. In our datasets, the conversations from Twitter concern diverse topics, while those from Reddit focus on the po- litical issues. Both datasets are in English. To build the Reddit dataset, we ﬁrst downloaded a large corpus publicly available on Reddit plat- form.4 Then, we selected posts and comments in subreddit “politics” posted from Jan to Dec 2008. Next, we formed Reddit posts and comments into conversations with replying relations revealed by the “parent id” of each comment. Last, we re- moved conversations with only one turn. In our main experiment, we focus on ﬁrst re- entry prediction, i.e. we predict whether a user u will come back to a conversation c, given current turns until u’s ﬁrst entry in c as context and u’s past chatting messages (posted before u engaging in c). For model training and evaluation, we ran- domly select 80%, 10%, and 10% conversations to form training, development, and test sets. The statistics of the two datasets are shown in Table 1. As can be seen, users participate", "meta": {"paper_id": "1906.01185v1", "chunk_index": 6, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_7", "text": "context and u’s past chatting messages (posted before u engaging in c). For model training and evaluation, we ran- domly select 80%, 10%, and 10% conversations to form training, development, and test sets. The statistics of the two datasets are shown in Table 1. As can be seen, users participate twice on 3https://trec.nist.gov/data/tweets/ 4https://www.reddit.com/r/datasets/ comments/3bxlg7/i_have_every_publicly_ available_reddit_comment/ Twitter Reddit # of users 10,122 13,134 # of conversations 7,500 29,477 # of re-entry instances 5,875 12,780 # of non re-entry instances 8,677 39,988 Avg. # of convs per user 1.7 5.9 Avg. # of msgs in user history 3.9 8.4 Avg. # of entries per user per conv 2.0 1.3 Avg. # of turns per conv 5.2 3.7 Avg. # of users per conv 2.3 2.6 Table 1: Statistics of two datasets. 2^0 2^4 2^8 2^12 2^16 0 10 20 30 40 50 # of users # of msgs in user history Twitter Reddit > (a) User history 2^0 2^4 2^8 2^12 2^16 2 9 16 23 >30 # of convs # of turns in conv. context Twitter Reddit (b) Conversation context Figure 3: Distributions of message number in user his- tory and turn number in conversation context on the two datasets. average in Twitter conversations, and the number is only 1.3 on Reddit. This results in the severe im- balance over instances of re-entry and non re-entry (negative samples where users do not come back) on both datasets. Therefore, strategies should be adopted for alleviating the data imbalance issue, as done in Eq. (4). It indicates the sparse user ac- tivity in conversations, where most users engage in a conversation only once or twice. Thus pre- dicting user re-entries only with context will not perform well, and the complementary information underlying user history should be leveraged. We further study the distributions of message number in user history and turn number in con- versation context on both datasets. As shown in Figure 3, there exists severe sparsity in either user history or conversation context. Thus combining them both might help alleviate the sparsity in one information source. We also notice that Twitter and Reddit users exhibit different conversation be- haviors. Reddit users tend to engage in more con- versations, resulting in more messages in user his- tory (as shown in Figure 3(a)). Twitter users are more likely to stay within each conversation, lead- ing to lengthy discussions and larger re-entry fre- quencies on average, as shown in Figure 3(b) and Table 1. Data Preprocessing and Model Setting. For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (Pennington et al., 2014).5 For the Reddit dataset, we ﬁrst ap- plied the open source natural language toolkit (NLTK) (Loper and Bird, 2002) for word to- kenization. Then, we replaced links with the generic tag “URL” and removed all the non- alphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embed- ding layer with 200-dimensional Glove embed- ding (Pennington et al., 2014), where Twitter ver- sion", "meta": {"paper_id": "1906.01185v1", "chunk_index": 7, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_8", "text": "removed all the non- alphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embed- ding layer with 200-dimensional Glove embed- ding (Pennington et al., 2014), where Twitter ver- sion is used for our Twitter dataset and the Com- mon Crawl version applied on Reddit dataset.6 All the hyper-parameters are tuned on the develop- ment set by grid search. The batch size is set to 32. Adam optimizer (Kingma and Ba, 2014) is adopted for parameter learning with initial learn- ing rate selected among {10−3, 10−4, 10−5}. For the BiLSTM encoders, we set the size of their hid- den states to 200 (100 for each direction). For the CNN encoders, we use ﬁlter windows of 2, 3, and 4, each with 50 feature maps. In MemN2N inter- action mechanism, we set hop numbers to 3. In the learning loss, we set µ = 1 and λ = 2, the weights to tackle data imbalance. For re-entry prediction, a user is considered to come back if the estimated probability for re-entry is larger than 0.5. Baselines and Comparisons. For comparisons, we consider three baselines. RANDOM baseline: randomly pick up a “yes-or-no” answer. HISTORY baseline: predict based on users’ history re-entry rate before current conversation, which will an- swer “yes” if the rate exceeds a pre-deﬁned thresh- old (set on development data), and “no” other- wise. (For users who lack such information be- fore current conversation, it predicts “yes or no” randomly.) ALL-YES baseline: always answers “yes” in re-entry prediction. Its assumption is that users tend to be drawn back to the conversations they once participated by the platform’s auto mes- sages inviting them to return. For supervised models, we compare with CCCT, the state-of-the-art method proposed by 5https://nlp.stanford.edu/projects/ glove/preprocess-twitter.rb 6https://nlp.stanford.edu/projects/ glove/ Backstrom et al. (2013), where the bagged deci- sion tree with manually-crafted features (including arrival patterns, timing effects, most related terms, etc.) are employed for re-entry prediction. We do not compare with Budak and Agrawal (2013), since most of its features are related to social net- works or Twitter group information, which is un- available in our data. In our proposed neural framework, we further compare varying encoders for turn modeling and mechanisms to model the interactions between user history and conversation context. We ﬁrst compare three turn encoders — AVG-EMBED (av- erage embedding), CNN, and BILSTM, to exam- ine their performance in turn representation learn- ing. Their results are compared on our variant only with context modeling layer and the best en- coder (turned out to be BILSTM) is applied on the full model. For the interaction modeling layer, we also study the effectiveness of four mechanisms to combine user history and conversation context — simple concatenation (CON), attention (ATT), memory networks (MEM), and bi-attention (BIA). 5 Results and Analysis This section ﬁrst discusses prediction results of ﬁrst re-entry in Section 5.1. We then present the results of the second and third re-entry predic- tion in Section 5.2, as well as", "meta": {"paper_id": "1906.01185v1", "chunk_index": 8, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_9", "text": "and conversation context — simple concatenation (CON), attention (ATT), memory networks (MEM), and bi-attention (BIA). 5 Results and Analysis This section ﬁrst discusses prediction results of ﬁrst re-entry in Section 5.1. We then present the results of the second and third re-entry predic- tion in Section 5.2, as well as an analysis on user history effects. Section 5.3 then provides expla- nations on what we learn from the joint effects from context and user history, indicative of user re-entries. Finally, we conduct a human study to compare human performance on the same task with our best model (Section 5.4). 5.1 First Re-entry Prediction Results In main experiment, we adopt the automatic eval- uation metrics — AUC, F1 score, precision, and recall, and focus on the prediction of the major re-entry type — ﬁrst re-entry, where conversation context up to user’s ﬁrst participation is given. As shown in Table 1, most users, if re-entry, only re- turn once to a conversation. Also, in conversation management, the prediction of ﬁrst re-entry is a challenging yet practical problem. We will dis- cuss second and third re-entry prediction later in Section 5.2. The comparison results are reported in Table 2. On both datasets, we observe: • First re-entry prediction is challenging. All models produce AUC and F1 scores below 70. Models Twitter Reddit AUC F1 Score Precision Recall AUC F1 Score Precision Recall Baselines RANDOM 51.0 45.0 40.3 50.9 49.4 32.6 24.5 48.7 HISTORY 50.1 46.4 42.2 51.4 50.7 35.2 26.9 50.9 ALL-YES 50.0 54.9 37.9 100.0 50.0 38.5 23.8 100.0 S.O.T.A CCCT 57.7 57.0 45.5 76.4 59.9 39.8 44.7 36.0 W/O History AVG-EMBED 60.4 59.0 43.5 91.8 63.7 42.4 31.0 67.2 CNN 58.8 59.1 43.2 93.5 64.0 42.8 31.1 68.5 BILSTM 60.4 59.4 45.8 85.0 64.1 43.1 31.4 69.5 With History BILSTM+CON 51.0 58.0 40.9 100.0 50.1 38.6 24.0 98.3 BILSTM+ATT 58.4 59.0 44.6 87.3 60.3 41.3 27.8 82.4 BILSTM+MEM 61.3 59.9 45.7 87.5 65.5 43.7 31.8 69.9 BILSTM+BIA 62.7 61.1 47.0 87.7 67.1 45.4 33.9 68.9 Table 2: Results on ﬁrst re-entry prediction. The best results in each column are in bold. Model BILSTM+BIA yields signiﬁcantly better AUC and F1 scores than all other comparisons (p < 0.05, paired t-test). In particular, models built on rules and features with shallow content and network features per- form poorly, suggesting the need of better under- standing of conversations or more information like user’s chatting history. We also observe that HIS- TORY yields only slightly better results than RAN- DOM. It suggests that users’ re-entries depend on not only their past re-entry patterns, but also the conversation context. • Well-encoded user chatting history is effec- tive. Among neural models, our BILSTM+MEM and BILSTM+BIA models outperform other comparisons by successfully modeling users’ pre- vious messages and their alignment with the top- ics of ongoing conversations. However, the oppo- site observation is drawn for BILSTM+CON and BILSTM+ATT. It is because the interactions be- tween context and user history are effective yet complex, requiring well-designed merging mech- anisms to exploit their joint effects. • Bi-attention mechanism better aligns the users’", "meta": {"paper_id": "1906.01185v1", "chunk_index": 9, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_10", "text": "with the top- ics of ongoing conversations. However, the oppo- site observation is drawn for BILSTM+CON and BILSTM+ATT. It is because the interactions be- tween context and user history are effective yet complex, requiring well-designed merging mech- anisms to exploit their joint effects. • Bi-attention mechanism better aligns the users’ interests and the conversation topics. BIL- STM+BIA achieves the best AUC and F1 scores, signiﬁcantly outperforming all other comparison models on both datasets. In particular, it beats BILSTM+MEM, which also able to learn the in- teraction between user history and conversation content, indicating the effectiveness of bi-attention over memory networks in this task. Interestingly, comparing the results on the two datasets, we notice all models yield better recall and F1 on Twitter than Reddit. This is due to the 50 60 70 80 90 100 1 2 3 All-Yes CCCT BiLSTM BiLSTM+MEM BiLSTM+BiA (a) Twitter Dataset 30 40 50 60 70 80 1 2 3 All-Yes CCCT BiLSTM BiLSTM+MEM BiLSTM+BiA (b) Reddit Dataset Figure 4: F1 scores for prediction on the ﬁrst, second, and third re-entries (given the conversation context un- til the last entry). X-axis: # of turns in the given con- versation context. Both ﬁgures, from left to right, show the F1 scores by ALL-YES, CCCT, BILSTM, BIL- STM+MEM, and BILSTM+BIA. fact that Reddit users are more likely to abandon conversations, reﬂected as the fewer number of en- tries in Table 1. Twitter users, on the other hand, tend to stay longer in the conversations, which en- courages all models to predict the return of users. 5.2 Predicting Re-entries with Varying Context and User History Here we study the effects of varying conversation context and user history over re-entry prediction. Results with Varying Context. We ﬁrst dis- cuss model performance given different amounts of conversation context by varying the number of user entries. Figure 4 shows the F1 scores for pre- dicting the ﬁrst, second, and third re-entries. For predicting second or third re-entries, turns of cur- rent context until given user’s second or third entry 30 40 50 60 70 80 90 100 # of msgs in user history Twitter Reddit Figure 5: F1 scores of model BILSTM+BIA on ﬁrst re-entry prediction, with varying numbers of chatting messages given in user history. will be given. As can be seen, all models’ perfor- mance monotonically increases when more con- text is observed. Our BILSTM+BIA uniformly outperforms other methods in all setups. Interest- ingly, baseline ALL-YES achieves the most per- formance gain when additional context is given. This implies that the more a user contributes to a conversation, the more likely they will come back. Results with Varying User History. We further analyze how model performance differs when dif- ferent amounts of messages are given in the user history. From Figure 5, we can see that it generally yields better F1 scores when more messages are available for the user history, suggesting the use- fulness of chatting history to signal user re-entries. The performance on Reddit does not increase as fast as observed on Twitter, which may mainly be- cause the", "meta": {"paper_id": "1906.01185v1", "chunk_index": 10, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_11", "text": "5, we can see that it generally yields better F1 scores when more messages are available for the user history, suggesting the use- fulness of chatting history to signal user re-entries. The performance on Reddit does not increase as fast as observed on Twitter, which may mainly be- cause the context from Reddit conversations is of- ten limited. 5.3 Further Discussion We further discuss our models with an ablation study and a case study to understand and interpret their prediction results. Ablation Study. To examine the contribution of each component in our framework, we present an ablation study on ﬁrst re-entry prediction task. Ta- ble 3 shows the results of our best full model (BILSTM+BIA) together with its variant without using turn-level auxiliary meta at (deﬁned in Sec- tion 3.1 to record user activity and replying rela- tions in context), and that without structure mod- eling layer (to capture conversation discourse in context described in Section 3.2); also compared are variants without using user chatting history (described in Section 3.3). Our full model yields the best F1 scores, show- Models Twitter Reddit F1 Pre Rec F1 Pre Rec W/O History W/O SML 58.8 42.6 95.1 39.6 25.2 92.9 With SML 59.4 45.9 85.0 43.1 31.4 69.5 With History W/O SML 57.5 43.2 86.7 43.8 31.3 74.4 W/O Meta 60.4 46.6 86.1 44.3 31.3 75.8 Full model 61.1 47.0 87.7 45.4 33.9 68.9 Table 3: Results of our variants. SML: structure mod- eling layer. Meta: auxiliary triples at. Our full model BILSTM+BIA obtains the best F1. Models Conv. 1 (C1) Conv. 2 (C2) CCCT 1.0 1.0 BILSTM 0.386 0.480 BILSTM+MEM 0.583 0.712 BILSTM+BIA 0.460 0.581 Table 4: Predicted probabilities by different models for user U1’s re-entry to conversations C1 and C2 in Fig- ure 1. CCCT can only yield binary outputs. For other neural models, predicting threshold is 0.5. ing the joint effects of context and user history can usefully indicate user re-entries. We also see that auxiliary triples, though conveying simple meta data for context turns, are helpful in our task. In addition, interestingly, conversation structure looks more effective in models leveraging user his- tory, because they can learn deeper semantic rela- tions between context turns and user chatting mes- sages. Case Study. We further utilize a case study based on the sample conversations shown in Fig- ure 1 to demonstrate what our model learns. Ta- ble 4 displays the outputs from different models on estimating how likely U1 will re-engage in con- versation 1 (C1) and conversation 2 (C2), where U1 returns to the latter. All neural models suc- cessfully forecast that U1 is more likely to re- engage in C2, while only BILSTM+BIA yields correct results (given threshold 0.5). We further visualize the attention weights out- put by BILSTM+BIA’s bi-attention mechanism with a heatmap in Figure 6. As can be seen, it as- signs higher attention values to turns T2 and T3 in conversation C2, due to their topical similar- ity with user U1’s interests, i.e. movies, as in- ferred from their previous messages about Let Me In. The attention", "meta": {"paper_id": "1906.01185v1", "chunk_index": 11, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_12", "text": "mechanism with a heatmap in Figure 6. As can be seen, it as- signs higher attention values to turns T2 and T3 in conversation C2, due to their topical similar- ity with user U1’s interests, i.e. movies, as in- ferred from their previous messages about Let Me In. The attention weights then guide the ﬁnal pre- diction for higher chance of re-entry to C2 rather than C1. Conv. 1 Conv. 2 T1[U2] T2[U1] T1[U3] T2[U4] T3[U5] T4[U1] H1 H2 H3 H4 Figure 6: Attention output of model BILSTM+BIA for the two sample conversations in Figure 1. Predictor Twitter Reddit Human 1 26 (29) 30 (30) Human 2 25 (28) 28 (29) BILSTM+BIA 35 33 Table 5: Numbers of correct predictions made by hu- mans, reading conversation context only and further seeing users’ chatting history (boldfaced numbers), compared to the results of our best model in same set- ting. A random guess gives 25 (out of 50 pairs). 5.4 Comparing with Humans We are also interested in how human performs for the ﬁrst re-entry prediction task, in order to ﬁnd out how challenging such a task is. To achieve this, we design a human evaluation. Concretely, from each dataset, we randomly sample 50 users who have been involved in at least 4 conversations, with both re-entry and non re-entry behaviors ex- hibited. Then for each user u, we construct paired samples based on randomly selected conversations c1 and c2, where u re-engage in one but not the other. The rest of the conversations that u partic- ipated in are collected as their user history. Then, we invite two humans who are ﬂuent speakers of English, to predict which conversation user u will re-engage, after reading the context up to user’s ﬁrst participation in the paired conversations c1 and c2. They are requested to make a second pre- diction after reading user’s chatting history. Humans’ prediction performance is shown in Table 5 along with BILSTM+BIA model’s out- put on the same data. As can be seen, humans can only give marginally better predictions than a ran- dom guess, i.e., 25 out of 50 pairs. Their perfor- mance improves after reading the user’s previous posts, however, still falls behind our model’s pre- dictions. This indicates the ability of our model to learn from large-scaled data and align users’ interests with conversation content. In addition, we notice that humans yield better performance on Reddit conversations than Twitter. It might be due to the fact that Reddit conversations are more fo- cused, and it is easier for humans to identify the discussion points. While for Twitter discussions, the informal language usage further hinders hu- mans’ judgment. 6 Conclusion We study the joint effects of conversation context and user chatting history for re-entry prediction. A novel neural framework is proposed for learn- ing the interactions between two source of infor- mation. Experimental results on two large-scale datasets from Twitter and Reddit show that our model with bi-attention yields better performance than the previous state of the art. Further discus- sions show that the model learns meaningful rep- resentations", "meta": {"paper_id": "1906.01185v1", "chunk_index": 12, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_13", "text": "is proposed for learn- ing the interactions between two source of infor- mation. Experimental results on two large-scale datasets from Twitter and Reddit show that our model with bi-attention yields better performance than the previous state of the art. Further discus- sions show that the model learns meaningful rep- resentations from conversation context and user history and hence exhibits consistent better per- formance given varying lengths of context or his- tory. We also conduct a human study on the ﬁrst re-entry prediction task. Our proposed model is observed to outperform humans, beneﬁting from its effective learning from large-scaled data. Acknowledgements This work is partly supported by HK RGC GRF (14232816, 14209416, 14204118), NSFC (61877020). Lu Wang is supported in part by National Science Foundation through Grants IIS- 1566382 and IIS-1813341. We thank the three anonymous reviewers for the insightful sugges- tions on various aspects of this work. References Noor Aldeen Alawad, Aris Anagnostopoulos, Stefano Leonardi, Ida Mele, and Fabrizio Silvestri. 2016. Network-aware recommendations of novel tweets. In Proceedings of the 39th International ACM SI- GIR Conference on Research and Development in Information Retrieval, pages 913–916. ACM. Yoav Artzi, Patrick Pantel, and Michael Gamon. 2012. Predicting responses to microblog posts. In Pro- ceedings of the 2012 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 602–606. Association for Computational Linguis- tics. Lars Backstrom, Jon M. Kleinberg, Lillian Lee, and Cristian Danescu-Niculescu-Mizil. 2013. Charac- terizing and curating conversation threads: expan- sion, focus, volume, re-entry. In Sixth ACM Inter- national Conference on Web Search and Data Min- ing, WSDM 2013, Rome, Italy, February 4-8, 2013, pages 13–22. Ceren Budak and Rakesh Agrawal. 2013. On participa- tion in group chats on Twitter. In Proceedings of the 22nd International Conference on World Wide Web, pages 165–176. ACM. Kailong Chen, Tianqi Chen, Guoqing Zheng, Ou Jin, Enpeng Yao, and Yong Yu. 2012. Collaborative per- sonalized tweet recommendation. In Proceedings of the 35th international ACM SIGIR Conference on Research and development in information retrieval, pages 661–670. ACM. Hao Cheng, Hao Fang, and Mari Ostendorf. 2017. A factored neural network model for characterizing online discussions in vector space. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2296–2306. Xavier Glorot, Antoine Bordes, and Yoshua Bengio. 2011. Deep sparse rectiﬁer neural networks. In Pro- ceedings of the fourteenth international conference on artiﬁcial intelligence and statistics, pages 315– 323. Liangjie Hong, Aziz S Doumith, and Brian D Davison. 2013. Co-factorization machines: modeling user in- terests and predicting individual decisions in Twit- ter. In Proceedings of the sixth ACM International Conference on Web Search and Data Mining, pages 557–566. ACM. Yunhao Jiao, Cheng Li, Fei Wu, and Qiaozhu Mei. 2018. Find the conversation killers: A predictive study of thread-ending posts. In Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 1145–1154. International World Wide Web Conferences Steering Committee. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Edward Loper and Steven Bird. 2002. Nltk: The natu- ral language toolkit.", "meta": {"paper_id": "1906.01185v1", "chunk_index": 13, "filename": "1906.01185v1.pdf", "chunk_length": 512}}
{"id": "1906.01185v1_chunk_14", "text": "In Proceedings of the 2018 World Wide Web Conference on World Wide Web, pages 1145–1154. International World Wide Web Conferences Steering Committee. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Edward Loper and Steven Bird. 2002. Nltk: The natu- ral language toolkit. In Proceedings of the ACL-02 Workshop on Effective tools and methodologies for teaching natural language processing and computa- tional linguistics-Volume 1, pages 63–70. Associa- tion for Computational Linguistics. Minh-Thang Luong, Hieu Pham, and Christopher D Manning. 2015. Effective approaches to attention- based neural machine translation. arXiv preprint arXiv:1508.04025. Jeffrey Pennington, Richard Socher, and Christo- pher D. Manning. 2014. Glove: Global vectors for word representation. In Empirical Methods in Nat- ural Language Processing (EMNLP), pages 1532– 1543. Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un- supervised modeling of Twitter conversations. In Human Language Technologies: Conference of the North American Chapter of the Association of Com- putational Linguistics, pages 172–180. Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2016. Bidirectional attention ﬂow for machine comprehension. arXiv preprint arXiv:1611.01603. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. 2015. End-to-end memory networks. In Advances in neural information processing systems, pages 2440–2448. Rui Yan, Mirella Lapata, and Xiaoming Li. 2012. Tweet recommendation with graph co-ranking. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1, pages 516–525. Association for Computational Linguistics. Victoria Zayats and Mari Ostendorf. 2018. Conver- sation modeling on reddit using a graph-structured LSTM. TACL, 6:121–132. Xingshan Zeng, Jing Li, Lu Wang, Nicholas Beauchamp, Sarah Shugars, and Kam-Fai Wong. 2018. Microblog conversation recommendation via joint modeling of topics and discourse. In Proceed- ings of the Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, Volume 1 (Long Papers), pages 375–385. Justine Zhang, Jonathan P Chang, Cristian Danescu- Niculescu-Mizil, Lucas Dixon, Yiqing Hua, Nithum Thain, and Dario Taraborelli. 2018. Conversations gone awry: Detecting early signs of conversational failure. arXiv preprint arXiv:1805.05345. Qi Zhang, Yeyun Gong, Ya Guo, and Xuanjing Huang. 2015. Retweet behavior prediction using hierarchi- cal Dirichlet process. In AAAI, pages 403–409.", "meta": {"paper_id": "1906.01185v1", "chunk_index": 14, "filename": "1906.01185v1.pdf", "chunk_length": 356}}
{"id": "1906.01195v1_chunk_0", "text": "Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs Deepak Nathani∗ Jatin Chauhan∗ Charu Sharma∗ Manohar Kaul Department of Computer Science and Engineering, IIT Hyderbad {deepakn1019,chauhanjatin100,charusharma1991}@gmail.com, mkaul@iith.ac.in Abstract The recent proliferation of knowledge graphs (KGs) coupled with incomplete or partial in- formation, in the form of missing relations (links) between entities, has fueled a lot of research on knowledge base completion (also known as relation prediction). Several re- cent works suggest that convolutional neural network (CNN) based models generate richer and more expressive feature embeddings and hence also perform well on relation prediction. However, we observe that these KG embed- dings treat triples independently and thus fail to cover the complex and hidden information that is inherently implicit in the local neighbor- hood surrounding a triple. To this effect, our paper proposes a novel attention-based feature embedding that captures both entity and rela- tion features in any given entity’s neighbor- hood. Additionally, we also encapsulate re- lation clusters and multi-hop relations in our model. Our empirical study offers insights into the efﬁcacy of our attention-based model and we show marked performance gains in comparison to state-of-the-art methods on all datasets. 1 Introduction Knowledge graphs (KGs) represent knowledge bases (KBs) as a directed graph whose nodes and edges represent entities and relations be- tween entities, respectively. For example, in Fig- ure 1, a triple (London, capital of, United King- dom) is represented as two entities: London and United Kingdom along with a relation (capital of) linking them. KGs ﬁnd uses in a wide vari- ety of applications such as semantic search (Be- rant et al., 2013; Berant and Liang, 2014), dia- logue generation (He et al., 2017; Keizer et al., 2017), and question answering (Zhang et al., 2016; ∗Equal Contribution Diefenbach et al., 2018), to name a few. How- ever, KGs typically suffer from missing rela- tions (Socher et al., 2013a; West et al., 2014). This problem gives rise to the task of knowledge base completion (also referred to as relation pre- diction), which entails predicting whether a given triple is valid or not. State-of-the-art relation prediction methods are known to be primarily knowledge embedding based models. They are broadly classiﬁed as translational models (Bordes et al., 2013; Yang et al., 2015; Trouillon et al., 2016) and con- volutional neural network (CNN) (Nguyen et al., 2018; Dettmers et al., 2018) based models. While translational models learn embeddings using sim- ple operations and limited parameters, they pro- duce low quality embeddings. In contrast, CNN based models learn more expressive embeddings due to their parameter efﬁciency and consideration of complex relations. However, both translational and CNN based models process each triple inde- pendently and hence fail to encapsulate the seman- tically rich and latent relations that are inherently present in the vicinity of a given entity in a KG. Motivated by the aforementioned observa- tions, we propose a generalized attention-based graph embedding for relation prediction. For node classiﬁcation, graph attention networks (GATs) (Veliˇckovi´c et al., 2018) have been shown to focus on the most relevant portions of the graph, namely the node features in a", "meta": {"paper_id": "1906.01195v1", "chunk_index": 0, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_1", "text": "entity in a KG. Motivated by the aforementioned observa- tions, we propose a generalized attention-based graph embedding for relation prediction. For node classiﬁcation, graph attention networks (GATs) (Veliˇckovi´c et al., 2018) have been shown to focus on the most relevant portions of the graph, namely the node features in a 1-hop neighborhood. Given a KG and the task of relation prediction, our model generalizes and extends the attention mech- anism by guiding attention to both entity (node) and relation (edge) features in a multi-hop neigh- borhood of a given entity / node. Our idea is: 1) to capture multi-hop rela- tions (Lin et al., 2015) surrounding a given node, 2) to encapsulate the diversity of roles played by an entity in various relations, and 3) arXiv:1906.01195v1 [cs.LG] 4 Jun 2019 to consolidate the existing knowledge present in semantically similar relation clusters (Valverde- Rebaza and de Andrade Lopes, 2012). Our model achieves these objectives by assigning different weight mass (attention) to nodes in a neighbor- hood and by propagating attention via layers in an iterative fashion. However, as the model depth increases, the contribution of distant enti- ties decreases exponentially. To resolve this is- sue, we use relation composition as proposed by (Lin et al., 2015) to introduce an auxiliary edge between n-hop neighbors, which then readily al- lows the ﬂow of knowledge between entities. Our architecture is an encoder-decoder model where our generalized graph attention model and Con- vKB (Nguyen et al., 2018) play the roles of an encoder and decoder, respectively. Moreover, this method can be extended for learning effec- tive embeddings for Textual Entailment Graphs (KOTLERMAN et al., 2015), where global learn- ing has proven effective in the past as shown by (Berant et al., 2015) and (Berant et al., 2010). Our contributions are as follows. To the best of our knowledge, we are the ﬁrst to learn new graph attention based embeddings that speciﬁcally target relation prediction on KGs. Secondly, we generalize and extend graph attention mechanisms to capture both entity and relation features in a multi-hop neighborhood of a given entity. Fi- nally, we evaluate our model on challenging re- lation prediction tasks for a wide variety of real- world datasets. Our experimental results indicate a clear and substantial improvement over state- of-the-art relation prediction methods. For in- stance, our attention-based embedding achieves an improvement of 104% over the state-of-the-art method for the Hits@1 metric on the popular Free- base (FB15K-237) dataset. The rest of the paper is structured as follows. We ﬁrst provide a review of related work in Sec- tion 2 and then our detailed approach in Section 3. Experimental results and dataset descriptions are reported in Section 4 followed by our conclusion and future research directions in Section 5. 2 Related Work Recently, several variants of KG embeddings have been proposed for relation prediction. These methods can be broadly classiﬁed as: (i) composi- tional, (ii) translational, (iii) CNN based, and (iv) graph based models. Christopher Nolan Revenant Tom Hardy Leonardo DiCaprio United Kingdom London Jonathan Nolan colleague born_in nationality? colleague? acted_in acted_in born_in? capital_of directed", "meta": {"paper_id": "1906.01195v1", "chunk_index": 1, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_2", "text": "of KG embeddings have been proposed for relation prediction. These methods can be broadly classiﬁed as: (i) composi- tional, (ii) translational, (iii) CNN based, and (iv) graph based models. Christopher Nolan Revenant Tom Hardy Leonardo DiCaprio United Kingdom London Jonathan Nolan colleague born_in nationality? colleague? acted_in acted_in born_in? capital_of directed colleague brother_of Figure 1: Subgraph of a knowledge graph contains ac- tual relations between entities (solid lines) and inferred relations that are initially hidden (dashed lines). RESCAL (Nickel et al., 2011), NTN (Socher et al., 2013b), and the Holo- graphic embedding model (HOLE) (Nickel et al., 2016) are examples of compositional based mod- els. Both RESCAL and NTN use tensor products which capture rich interactions, but require a large number of parameters to model relations and are thus cumbersome to compute. To combat these drawbacks, HOLE creates more efﬁcient and scalable compositional representations using the circular correlation of entity embeddings. In comparison, translational models like TransE (Bordes et al., 2013), DIST- MULT (Yang et al., 2015) and ComplEx (Trouil- lon et al., 2016) propose arguably simpler models. TransE considers the translation operation between head and tail entities for relations. DIST- MULT (Yang et al., 2015) learns embeddings using a bilinear diagonal model which is a special case of the bilinear objective used in NTN and TransE. DISTMULT uses weighted element- wise dot products to model entity relations. ComplEx (Trouillon et al., 2016) generalizes DISTMULT (Yang et al., 2015) by using complex embeddings and Hermitian dot products instead. These translational models are faster, require fewer parameters and are relatively easier to train, but result in less expressive KG embeddings. Recently, two CNN based models have been proposed for relation prediction, namely ConvE (Dettmers et al., 2018) and Con- vKB (Nguyen et al., 2018). ConvE uses 2-D convolution over embeddings to predict links. It comprises of a convolutional layer, a fully connected projection layer and an inner product layer for the ﬁnal predictions. Different feature maps are generated using multiple ﬁlters to ex- tract global relationships. Concatenation of these feature maps represents an input triple. These models are parameter efﬁcient but consider each triple independently without taking into account the relationships between the triples. A graph based neural network model called R-GCN (Schlichtkrull et al., 2018) is an exten- sion of applying graph convolutional networks (GCNs) (Kipf and Welling, 2017) to relational data. It applies a convolution operation to the neighborhood of each entity and assigns them equal weights. This graph based model does not outperform the CNN based models. Existing methods either learn KG embeddings by solely focusing on entity features or by taking into account the features of entities and relations in a disjoint manner. Instead, our proposed graph at- tention model holistically captures multi-hop and semantically similar relations in the n-hop neigh- borhood of any given entity in the KG. 3 Our Approach We begin this section by introducing the notations and deﬁnitions used in the rest of the paper, fol- lowed by a brief background on graph attention networks (GATs) (Veliˇckovi´c et al., 2018). Fi- nally, we describe our proposed", "meta": {"paper_id": "1906.01195v1", "chunk_index": 2, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_3", "text": "neigh- borhood of any given entity in the KG. 3 Our Approach We begin this section by introducing the notations and deﬁnitions used in the rest of the paper, fol- lowed by a brief background on graph attention networks (GATs) (Veliˇckovi´c et al., 2018). Fi- nally, we describe our proposed attention archi- tecture for knowledge graphs followed by our de- coder network. 3.1 Background A knowledge graph is denoted by G = (E, R), where E and R represent the set of entities (nodes) and relations (edges), respectively. A triple (es, r, eo) is represented as an edge r be- tween nodes es and er in G1. Embedding models try to learn an effective representation of entities, relations, and a scoring function f, such that for a given input triple t = (es, r, eo), f(t) gives the likelihood of t being a valid triple. For example, Figure 1 shows the subgraph from a KG which infers missing links represented by dashed lines using existing triples such as (London, captial of, United Kingdom). 3.2 Graph Attention Networks (GATs) Graph convolutional networks (GCNs) (Kipf and Welling, 2017) gather information from the entity’s neighborhood and all neighbors contribute equally in the information pass- ing. To address the shortcomings of GCNs, (Veliˇckovi´c et al., 2018) introduced graph at- tention networks (GATs). GATs learn to assign 1 From here onwards, the pairs “node / entity” and “edge / relation” will be used interchangeably varying levels of importance to nodes in every node’s neighborhood, rather than treating all neighboring nodes with equal importance, as is done in GCN. The input feature set of nodes to a layer is x = {⃗x1, ⃗x2, ..., ⃗xN}. A layer produces a transformed set of node feature vectors x′ = {⃗x′ 1, ⃗x′ 2, ..., ⃗x′ N}, where ⃗xi and ⃗x′ i are input and output embed- dings of the entity ei, and N is number of entities (nodes). A single GAT layer can be described as eij = a(W⃗xi, W ⃗xj) (1) where eij is the attention value of the edge (ei, ej) in G, W is a parametrized linear transformation matrix mapping the input features to a higher di- mensional output feature space, and a is any atten- tion function of our choosing. Attention values for each edge are the impor- tance of the edge (ei, ej) ′s features for a source node ei. Here, the relative attention αij is com- puted using a softmax function over all the val- ues in the neighborhood. Equation 2 shows the output of a layer. GAT employs multi-head atten- tion to stabilize the learning process as credited to (Vaswani et al., 2017). ⃗x′ i = σ X j∈Ni αijW ⃗xj ! (2) The multihead attention process of concatenating K attention heads is shown as follows in Equation 3. ⃗x′ i = K k=1 σ X j∈Ni αk ijWk ⃗xj ! (3) where ∥represents concatenation, σ represents any non-linear function, αk ij are normalized atten- tion coefﬁcients of edge (ei, ej) calculated by the k-th attention mechanism, and Wk represents the corresponding linear", "meta": {"paper_id": "1906.01195v1", "chunk_index": 3, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_4", "text": "shown as follows in Equation 3. ⃗x′ i = K k=1 σ X j∈Ni αk ijWk ⃗xj ! (3) where ∥represents concatenation, σ represents any non-linear function, αk ij are normalized atten- tion coefﬁcients of edge (ei, ej) calculated by the k-th attention mechanism, and Wk represents the corresponding linear transformation matrix of the k-th attention mechanism. The output embedding in the ﬁnal layer is calculated using averaging, in- stead of the concatenation operation, to achieve multi-head attention, as is shown in the following Equation 4. ⃗x′ i = σ 1 K K X k=1 X j∈Ni αk ijWk ⃗xj ! (4) 3.3 Relations are Important Despite the success of GATs, they are unsuitable for KGs as they ignore relation (edge) features, U.S. Chevrolet Ethan Horvath Barack Obama Washington D.C Samuel L. Jackson Michelle Obama wife_of president_of plays_for brand_of capital_of born_in native_of ﬁrst_lady αi1 αi2 αi3 αi4 αi5 αi6 ei Figure 2: This ﬁgure shows the aggregation process of our graph attentional layer. αij represents relative attention values of the edge. The dashed lines represent an auxiliary edge from a n-hop neighbors, in this case n = 2. which are an integral part of KGs. In KGs, enti- ties play different roles depending on the relation they are associated with. For example, in Figure 1, entity Christopher Nolan appears in two differ- ent triples assuming the roles of a brother and a director. To this end, we propose a novel embed- ding approach to incorporate relation and neigh- boring node features in the attention mechanism. We deﬁne a single attentional layer, which is the building block of our model. Similar to GAT, our framework is agnostic to the particular choice of attention mechanism. Each layer in our model takes two embedding matrices as input. Entity embeddings are repre- sented by a matrix H ∈RNe×T , where the i-th row is the embedding of entity ei, Ne is the total number of entities, and T is the feature dimension of each entity embedding. With a similar construc- tion, the relation embeddings are represented by a matrix G ∈RNr×P . The layer then outputs the corresponding embedding matrices, H′ ∈RNe×T ′ and G′ ∈RNr×P ′. In order to obtain the new embedding for an entity ei, a representation of each triple associ- ated with ei is learned. We learn these embed- dings by performing a linear transformation over the concatenation of entity and relation feature vectors corresponding to a particular triple tk ij = (ei, rk, ej), as is shown in Equation 5. This oper- ation is also illustrated in the initial block of Fig- ure 4. ⃗ cijk = W1[⃗hi∥⃗hj∥⃗gk] (5) where ⃗ cijk is the vector representation of a triple tk ij. Vectors ⃗hi,⃗hj, and ⃗gk denote embeddings of entities ei, ej and relation rk, respectively. Addi- tionally, W1 denotes the linear transformation ma- trix. Similar to (Veliˇckovi´c et al., 2018), we learn softmax αij hi hj gk Figure 3: Attention Mechanism the importance of each triple tk ij denoted by bijk. We perform a linear transformation parameterized by a weight matrix", "meta": {"paper_id": "1906.01195v1", "chunk_index": 4, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_5", "text": "and relation rk, respectively. Addi- tionally, W1 denotes the linear transformation ma- trix. Similar to (Veliˇckovi´c et al., 2018), we learn softmax αij hi hj gk Figure 3: Attention Mechanism the importance of each triple tk ij denoted by bijk. We perform a linear transformation parameterized by a weight matrix W2 followed by application of the LeakyRelu non-linearity to get the absolute at- tention value of the triple (Equation 6). bijk = LeakyReLU \u0010 W2cijk \u0011 (6) To get the relative attention values softmax is ap- plied over bijk as shown in Equation 7. Figure 3 shows the computation of relative attention values αijk for a single triple. αijk = softmaxjk(bijk) (7) = exp(bijk) P n∈Ni P r∈Rin exp(binr) where Ni denotes the neighborhood of entity ei and Rij denotes the set of relations connecting en- tities ei and ej. The new embedding of the entity ei is the sum of each triple representation weighted by their attention values as shown in Equation 8. ⃗h′ i = σ X j∈Ni X k∈Rij αijk ⃗ cijk ! (8) As suggested by (Veliˇckovi´c et al., 2018), multi- head attention which was ﬁrst introduced by (Vaswani et al., 2017), is used to stabilize the learning process and encapsulate more informa- tion about the neighborhood. Essentially, M in- dependent attention mechanisms calculate the em- beddings, which are then concatenated, resulting in the following representation: ⃗h′ i = M m=1 σ X j∈Ni αm ijkcm ijk ! (9) Graph Attention Layer 1 Triple N Christian Bale acted_in The Machinist Triple 1 Barack Obama president_of United States Attention Head 1 Attention Head 2 Graph Attention Layer 2 Training Loss Figure 4: This ﬁgure shows end-to-end architecture of our model. Dashed arrows in the ﬁgure represent concate- nation operation. Green circles represents initial entity embedding vectors and yellow circles represents initial relation embedding vectors. This is the graph attention layer shown in Fig- ure 4. We perform a linear transformation on in- put relation embedding matrix G, parameterized by a weight matrix WR ∈RT×T ′, where T ′ is the dimensionality of output relation embeddings (Equation 10). G′ = G.WR (10) In the ﬁnal layer of our model, instead of concate- nating the embeddings from multiple heads we employ averaging to get ﬁnal embedding vectors for entities as shown in Equation 11. ⃗h′ i = σ 1 M M X m=1 X j∈Ni X k∈Rij αm ijkcm ijk ! (11) However, while learning new embeddings, entities lose their initial embedding information. To re- solve this issue, we linearly transform Hi to obtain Ht using a weight matrix WE ∈RT i×T f , where Hi represents the input entity embeddings to our model, Ht represents the transformed entity em- beddings, T i denotes the dimension of an initial entity embedding, and T f denotes the dimension of the ﬁnal entity embedding. We add this ini- tial entity embedding information to the entity em- beddings obtained from the ﬁnal attentional layer, Hf ∈RNe×T f as shown in Equation 12. H′′ = WEHt + Hf (12) In our architecture, we extend the", "meta": {"paper_id": "1906.01195v1", "chunk_index": 5, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_6", "text": "and T f denotes the dimension of the ﬁnal entity embedding. We add this ini- tial entity embedding information to the entity em- beddings obtained from the ﬁnal attentional layer, Hf ∈RNe×T f as shown in Equation 12. H′′ = WEHt + Hf (12) In our architecture, we extend the notion of an edge to a directed path by introducing an aux- iliary relation for n-hop neighbors between two entities. The embedding of this auxiliary rela- tion is the summation of embeddings of all the relations in the path. Our model iteratively ac- cumulates knowledge from distant neighbors of an entity. As illustrated in ﬁgure 2, in the ﬁrst layer of our model, all entities capture informa- tion from their direct in-ﬂowing neighbors. In the second layer, U.S gathers information from en- tities Barack Obama, Ethan Horvath, Chevrolet, and Washington D.C, which already possess infor- mation about their neighbors Michelle Obama and Samuel L. Jackson, from a previous layer. In gen- eral, for a n layer model the incoming informa- tion is accumulated over a n-hop neighborhood. The aggregation process to learn new entity em- beddings and the introduction of an auxiliary edge between n-hop neighbors is also shown in Figure 2. We normalize the entity embeddings after every generalized GAT layer and prior to the ﬁrst layer, for every main iteration. 3.4 Training Objective Our model borrows the idea of a translational scoring function from (Bordes et al., 2013), which learns embeddings such that for a given valid triple tk ij = (ei, rk, ej), the condition⃗hi+⃗gk ≈⃗hj holds, i.e., ej is the nearest neighbor of ei connected via relation rk. Speciﬁcally, we try to learn entity and relation embeddings to minimize the L1-norm dis- similarity measure given by dtij = ∥⃗hi+ ⃗gk−⃗hj∥1. We train our model using hinge-loss which is given by the following expression L(Ω) = X tij∈S X t′ ij∈S′ max{dt′ ij −dtij +γ, 0} (13) where γ > 0 is a margin hyper-parameter, S is the set of valid triples, and S′ denotes the set of invalid triples, given formally as S′ = {tk i′j | e′ i ∈E \\ ei} | {z } replace head entity ∪{tk ij′ | e′ j ∈E \\ ej} | {z } replace tail entity 3.5 Decoder Our model uses ConvKB (Nguyen et al., 2018) as a decoder. The aim of the convolutional layer is to analyze the global embedding properties of a triple tk ij across each dimension and to generalize the transitional characteristics in our model. The score function with multiple feature maps can be written formally as: f(tk ij) = Ω m=1 ReLU([⃗hi,⃗gk,⃗hj] ∗ωm) ! .W where ωm represents the mth convolutional ﬁlter, Ωis a hyper-parameter denoting number of ﬁlters used, ∗is a convolution operator, and W ∈RΩk×1 represents a linear transformation matrix used to compute the ﬁnal score of the triple. The model is trained using soft-margin loss as L = X tk ij∈{S∪S′} log(1+exp(ltk ij.f(tk ij)))+ λ 2 ∥W∥2 2 where ltk ij = ( 1 for tk ij ∈S −1 for tk ij ∈S′ 4", "meta": {"paper_id": "1906.01195v1", "chunk_index": 6, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_7", "text": "represents a linear transformation matrix used to compute the ﬁnal score of the triple. The model is trained using soft-margin loss as L = X tk ij∈{S∪S′} log(1+exp(ltk ij.f(tk ij)))+ λ 2 ∥W∥2 2 where ltk ij = ( 1 for tk ij ∈S −1 for tk ij ∈S′ 4 Experiments and Results 4.1 Datasets To evaluate our proposed method, we use ﬁve benchmark datasets: WN18RR (Dettmers et al., 2018), FB15k-237 (Toutanova et al., 2015), NELL-995 (Xiong et al., 2017), Uniﬁed Medi- cal Language Systems (UMLS) (Kok and Domin- gos, 2007) and Alyawarra Kinship (Lin et al., 2018). Previous works (Toutanova et al., 2015; Dettmers et al., 2018) suggest that the task of relation prediction in WN18 and FB15K suffers from the problem of inverse relations, whereby one can achieve state-of-the-art results using a simple reversal rule based model, as shown by (Dettmers et al., 2018). Therefore, correspond- ing subset datasets WN18RR and FB15k-237 were created to resolve the reversible relation problem in WN18 and FB15K. We used the data splits pro- vided by (Nguyen et al., 2018). Table 1 provides statistics of all datasets used. 4.2 Training Protocol We create two sets of invalid triples, each time re- placing either the head or tail entity in a triple by an invalid entity. We randomly sample equal num- ber of invalid triples from both the sets to ensure robust performance on detecting both head and tail entity. Entity and relation embeddings produced by TransE (Bordes et al., 2013; Nguyen et al., 2018) are used to initialize our embeddings. We follow a two-step training procedure, i.e., we ﬁrst train our generalized GAT to encode in- formation about the graph entities and relations and then train a decoder model like ConvKB (Nguyen et al., 2018) to perform the relation pre- diction task. The original GAT update Equation 3 only aggregates information passed from 1-hop neighborhood, while our generalized GAT uses in- formation from the n-hop neighborhood. We use auxiliary relations to aggregate more information about the neighborhood in sparse graphs. We use Adam to optimize all the parameters with initial learning rate set at 0.001. Both the entity and re- lation embeddings of the ﬁnal layer are set to 200. The optimal hyper-parameters set for each dataset are mentioned in our supplementary section. 4.3 Evaluation Protocol In the relation prediction task, the aim is to pre- dict a triple (ei, rk, ej) with ei or ej missing, i.e., predict ei given (rk, ej) or predict ej given (ei, rk). We generate a set of (N −1) corrupt triples for each entity ei by replacing it with ev- ery other entity ei′ ∈E \\ ei, then we assign a score to each such triple. Subsequently, we sort these scores in ascending order and get the rank of a correct triple (ei, rk, ej). Similar to previous work ((Bordes et al., 2013), (Nguyen et al., 2018), (Dettmers et al., 2018)), we evaluate all the mod- els in a ﬁltered setting, i.e, during ranking we re- move corrupt triples which are already present in one of the", "meta": {"paper_id": "1906.01195v1", "chunk_index": 7, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_8", "text": "rank of a correct triple (ei, rk, ej). Similar to previous work ((Bordes et al., 2013), (Nguyen et al., 2018), (Dettmers et al., 2018)), we evaluate all the mod- els in a ﬁltered setting, i.e, during ranking we re- move corrupt triples which are already present in one of the training, validation, or test sets. This whole process is repeated by replacing the tail en- tity ej, and averaged metrics are reported. We report mean reciprocal rank (MRR), mean rank (MR) and the proportion of correct entities in the top N ranks (Hits@N) for N = 1, 3, and 10. 4.4 Results and Analysis Tables 2 and 3 present the prediction results on the test sets of all the datasets. The results clearly demonstrate that our proposed method2 2 Our work # Edges Dataset # Entities # Relations Training Validation Test Total Mean in-degree Median in-degree WN18RR 40,943 11 86,835 3034 3134 93,003 2.12 1 FB15k-237 14,541 237 272,115 17,535 20,466 310,116 18.71 8 NELL-995 75,492 200 149,678 543 3992 154,213 1.98 0 Kinship 104 25 8544 1068 1074 10,686 82.15 82.5 UMLS 135 46 5216 652 661 6529 38.63 20 Table 1: Dataset statistics WN18RR FB15K-237 Hits@N Hits@N MR MRR @1 @3 @10 MR MRR @1 @3 @10 DistMult (Yang et al., 2015) 7000 0.444 41.2 47 50.4 512 0.281 19.9 30.1 44.6 ComplEx (Trouillon et al., 2016) 7882 0.449 40.9 46.9 53 546 0.278 19.4 29.7 45 ConvE (Dettmers et al., 2018) 4464 0.456 41.9 47 53.1 245 0.312 22.5 34.1 49.7 TransE (Bordes et al., 2013) 2300 0.243 4.27 44.1 53.2 323 0.279 19.8 37.6 44.1 ConvKB (Nguyen et al., 2018) 1295 0.265 5.82 44.5 55.8 216 0.289 19.8 32.4 47.1 R-GCN (Schlichtkrull et al., 2018) 6700 0.123 20.7 13.7 8 600 0.164 10 18.1 30 Our work 1940 0.440 36.1 48.3 58.1 210 0.518 46 54 62.6 Table 2: Experimental results on WN18RR and FB15K-237 test sets. Hits@N values are in percentage. The best score is in bold and second best score is underlined. NELL-995 Kinship Hits@N Hits@N MR MRR @1 @3 @10 MR MRR @1 @3 @10 DistMult (Yang et al., 2015) 4213 0.485 40.1 52.4 61 5.26 0.516 36.7 58.1 86.7 ComplEx (Trouillon et al., 2016) 4600 0.482 39.9 52.8 60.6 2.48 0.823 73.3 89.9 97.11 ConvE (Dettmers et al., 2018) 3560 0.491 40.3 53.1 61.3 2.03 0.833 73.8 91.7 98.14 TransE (Bordes et al., 2013) 2100 0.401 34.4 47.2 50.1 6.8 0.309 0.9 64.3 84.1 ConvKB (Nguyen et al., 2018) 600 0.43 37.0 47 54.5 3.3 0.614 43.62 75.5 95.3 R-GCN (Schlichtkrull et al., 2018) 7600 0.12 8.2 12.6 18.8 25.92 0.109 3 8.8 23.9 Our work 965 0.530 44.7 56.4 69.5 1.94 0.904 85.9 94.1 98 Table 3: Experimental results on NELL-995 and Kinship test sets. Hits@N values are in percentage. The best score is in bold and second best score is underlined. signiﬁcantly outperforms state-of-the-art results on ﬁve metrics for FB15k-237, and on two metrics for WN18RR. We downloaded publicly available source codes to reproduce results of the state-of- the-art methods345678 on all", "meta": {"paper_id": "1906.01195v1", "chunk_index": 8, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_9", "text": "test sets. Hits@N values are in percentage. The best score is in bold and second best score is underlined. signiﬁcantly outperforms state-of-the-art results on ﬁve metrics for FB15k-237, and on two metrics for WN18RR. We downloaded publicly available source codes to reproduce results of the state-of- the-art methods345678 on all the datasets. Attention Values vs Epochs: We study the dis- tribution of attention with increasing epochs for a particular node. Figure 5 shows this distribution on FB15k-237. In the initial stages of the learn- ing process, the attention is distributed randomly. As the training progresses and our model gathers more information from the neighborhood, it as- signs more attention to direct neighbors and takes minor information from the more distant neigh- bors. Once the model converges, it learns to gather multi-hop and clustered relation information from the n-hop neighborhood of the node. PageRank Analysis: We hypothesize that com- 3 TransE 4 DistMult 5 ComplEx 6 R-GCN 7 ConvE 8 ConvKB plex and hidden multi-hop relations among enti- ties are captured more succinctly in dense graphs as opposed to sparse graphs. To test this hypoth- esis, we perform an analysis similar to ConvE, where they study the correlation between mean PageRank and increase in MRR relative to Dist- Mult. We notice a strong correlation coefﬁcient of r = 0.808. Table 4 indicates that when there is an increase in PageRank values, there is also a corresponding increase in MRR values. We ob- serve an anomaly to our observed correlation in case of NELL-995 versus WN18RR and attribute this to the highly sparse and hierarchical struc- ture of WN18RR which poses as a challenge to our method that does not capture information in a top- down recursive fashion. 4.5 Ablation Study We carry out an ablation study on our model, where we analyze the behavior of mean rank on a 1hop 2hop 3hop n-hop value 10.8-11.5 10.1-10.8 9.48-10.1 8.81-9.48 8.14-8.81 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (a) Epoch 0 1hop 2hop 3hop n-hop value 17.4-20.6 14.3-17.4 11.1-14.3 8.02-11.1 4.88-8.02 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (b) Epoch 1200 1hop 2hop 3hop n-hop value 17.3-20.3 14.3-17.3 11.3-14.3 8.39-11.3 5.41-8.39 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (c) Epoch 2400 Figure 5: Learning process of our model on FB15K-237 dataset. Y-axis represents attention values ×1e−5. 1hop 2hop 3hop n-hop value 255-269 241-255 228-241 214-228 200-214 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (a) Epoch 0 1hop 2hop 3hop n-hop value 691-827 555-691 419-555 282-419 146-282 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (b) Epoch 1800 1hop 2hop 3hop n-hop value 383-448 318-383 253-318 188-253 122-188 attention values ( X 1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (c) Epoch 3600 Figure 6: Learning process of our model on WN18RR dataset. Y-axis represents attention values ×1e−5 Dataset PageRank Relative Increase NELL-995 1.32 0.025 WN18RR 2.44 -0.01 FB15k-237 6.87 0.237 UMLS 740 0.247 Kinship 961 0.388 Table 4: Mean PageRank ×10−5", "meta": {"paper_id": "1906.01195v1", "chunk_index": 9, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_10", "text": "1e-5 ) 0.0 0.2 0.4 0.6 0.8 1.0 (c) Epoch 3600 Figure 6: Learning process of our model on WN18RR dataset. Y-axis represents attention values ×1e−5 Dataset PageRank Relative Increase NELL-995 1.32 0.025 WN18RR 2.44 -0.01 FB15k-237 6.87 0.237 UMLS 740 0.247 Kinship 961 0.388 Table 4: Mean PageRank ×10−5 vs relative increase in MRR wrt. DistMult. test set when we omit path generalization (−PG), i.e., removing n-hop information, and omit re- lation Information (−Relations) from our model. Figure 7 shows that our model performs better than the two ablated models and we see a signiﬁ- cant drop in the results when using ablated models on NELL-995. Removing the relations from the proposed model has a huge impact on the results which suggests that the relation embeddings play a pivotal role in relation prediction. 5 Conclusion and Future Work In this paper, we propose a novel approach for relation prediction. Our approach improves over the state-of-the-art models by signiﬁcant margins. Our proposed model learns new graph attention- 1000 1500 2000 2500 3000 Epochs 1000 2000 3000 4000 5000 Mean Rank Our model -PG -Relations Figure 7: Epochs vs Mean Rank for our model and two ablated models on NELL-995. −PG (green) rep- resents the model after removing n-hop auxiliary rela- tions or path generalization, −Relations (blue) repre- sents model without taking relations into account and Our model (red) represents the entire model. based embeddings that speciﬁcally cater to rela- tion prediction on KGs. Additionally, we gener- alize and extend graph attention mechanisms to capture both entity and relation features in a multi- hop neighborhood of a given entity. Our detailed and exhaustive empirical analysis gives more in- sight into our method’s superiority for relation pre- diction on KGs. The proposed model can be ex- tended to learn embeddings for various tasks us- ing KGs such as dialogue generation (He et al., 2017; Keizer et al., 2017), and question answer- ing (Zhang et al., 2016; Diefenbach et al., 2018). In the future, we intend to extend our method to better perform on hierarchical graphs and capture higher-order relations between entities (like mo- tifs) in our graph attention model. Acknowledgments We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. References Jonathan Berant, Noga Alon, Ido Dagan, and Ja- cob Goldberger. 2015. Efﬁcient global learning of entailment graphs. Computational Linguistics, 41(2):221–263. Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), 2013, pages 1533– 1544. Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2010. Global learning of focused entailment graphs. In Proceedings of the 48th Annual Meet- ing of the Association for Computational Linguis- tics, pages 1220–1229, Uppsala, Sweden. Associa- tion for Computational Linguistics. Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Com- putational Linguistics (ACL), 2014. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013.", "meta": {"paper_id": "1906.01195v1", "chunk_index": 10, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_11", "text": "Linguis- tics, pages 1220–1229, Uppsala, Sweden. Associa- tion for Computational Linguistics. Jonathan Berant and Percy Liang. 2014. Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Com- putational Linguistics (ACL), 2014. Antoine Bordes, Nicolas Usunier, Alberto Garcia- Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi- relational data. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems (NIPS), 2013, pages 2787–2795. Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. 2018. Convolutional 2d knowledge graph embeddings. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence (AAAI), 2018. Dennis Diefenbach, Kamal Singh, and Pierre Maret. 2018. Wdaqua-core1: a question answering ser- vice for rdf knowledge bases. In Companion of the The Web Conference 2018 on The Web Confer- ence (WWW), 2018, pages 1087–1091. International World Wide Web Conferences Steering Committee. He He, Anusha Balakrishnan, Mihail Eric, and Percy Liang. 2017. Learning symmetric collaborative di- alogue agents with dynamic knowledge graph em- beddings. In Proceedings of the 55th Annual Meet- ing of the Association for Computational Linguistics (ACL), 2017. Simon Keizer, Markus Guhe, Heriberto Cuayahuitl, Ioannis Efstathiou, Klaus-Peter Engelbrecht, Mi- hai Dobre, Alex Lascarides, and Oliver Lemon. 2017. Evaluating persuasion strategies and deep re- inforcement learning methods for negotiation dia- logue agents. In Proceedings of the 15th Confer- ence of the European Chapter of the Association for Computational Linguistics (ACL), 2017. Thomas N. Kipf and Max Welling. 2017. Semi- supervised classiﬁcation with graph convolutional networks. In International Conference on Learning Representations (ICLR), 2017. Stanley Kok and Pedro Domingos. 2007. Statisti- cal predicate invention. In Proceedings of the 24th International Conference on Machine Learn- ing, (ICML), 2007. LILI KOTLERMAN, IDO DAGAN, BERNARDO MAGNINI, and LUISA BENTIVOGLI. 2015. Tex- tual entailment graphs. Natural Language Engi- neering, 21(5):699724. Xi Victoria Lin, Richard Socher, and Caiming Xiong. 2018. Multi-hop knowledge graph reasoning with reward shaping. In Proceedings of the 2018 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), 2018. Yankai Lin, Zhiyuan Liu, Huanbo Luan, Maosong Sun, Siwei Rao, and Song Liu. 2015. Modeling rela- tion paths for representation learning of knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015. Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Phung. 2018. A novel embed- ding model for knowledge base completion based on convolutional neural network. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies (NAACL), 2018, vol- ume 2, pages 327–333. Maximilian Nickel, Lorenzo Rosasco, Tomaso A Pog- gio, et al. 2016. Holographic embeddings of knowl- edge graphs. In (AAAI), 2016, volume 2, pages 3–2. Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. 2011. A three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on International Conference on Machine Learning, (ICML), 2011. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolu- tional networks. In European", "meta": {"paper_id": "1906.01195v1", "chunk_index": 11, "filename": "1906.01195v1.pdf", "chunk_length": 512}}
{"id": "1906.01195v1_chunk_12", "text": "three-way model for collective learning on multi-relational data. In Proceedings of the 28th International Conference on International Conference on Machine Learning, (ICML), 2011. Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018. Modeling relational data with graph convolu- tional networks. In European Semantic Web Confer- ence (ESWC), 2018, pages 593–607. Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013a. Reasoning with neural ten- sor networks for knowledge base completion. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahra- mani, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 26 (NIPS), 2013, pages 926–934. Richard Socher, Danqi Chen, Christopher D Manning, and Andrew Ng. 2013b. Reasoning with neural ten- sor networks for knowledge base completion. In Advances in neural information processing systems (NIPS), 2013, pages 926–934. Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoi- fung Poon, Pallavi Choudhury, and Michael Ga- mon. 2015. Representing text for joint embedding of text and knowledge bases. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2015, pages 1499– 1509. Th´eo Trouillon, Johannes Welbl, Sebastian Riedel, ´Eric Gaussier, and Guillaume Bouchard. 2016. Complex embeddings for simple link prediction. In Inter- national Conference on Machine Learning (ICML), 2016, pages 2071–2080. Jorge Carlos Valverde-Rebaza and Alneu de An- drade Lopes. 2012. Link prediction in complex net- works based on cluster information. In Proceedings of the 21st Brazilian Conference on Advances in Ar- tiﬁcial Intelligence, (SBIA), 2012, pages 92–101. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- nett, editors, Advances in Neural Information Pro- cessing Systems (NIPS), 2017, pages 5998–6008. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. 2018. Graph Attention Networks. International Conference on Learning Representations (ICLR), 2018. Robert West, Evgeniy Gabrilovich, Kevin Murphy, Shaohua Sun, Rahul Gupta, and Dekang Lin. 2014. Knowledge base completion via search-based ques- tion answering. In Proceedings of the 23rd Inter- national Conference on World Wide Web, (WWW), 2014, pages 515–526. Wenhan Xiong, Thien Hoang, and William Yang Wang. 2017. Deeppath: A reinforcement learning method for knowledge graph reasoning. In Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2017. Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding Entities and Relations for Learning and Inference in Knowledge Bases. International Conference on Learning Rep- resentations (ICLR), 2015. Yuanzhe Zhang, Kang Liu, Shizhu He, Guoliang Ji, Zhanyi Liu, Hua Wu, and Jun Zhao. 2016. Ques- tion answering over knowledge base with neural at- tention combining global knowledge information. arXiv preprint arXiv:1606.00979, 2016.", "meta": {"paper_id": "1906.01195v1", "chunk_index": 12, "filename": "1906.01195v1.pdf", "chunk_length": 465}}
{"id": "1906.01195v1_chunk_13", "text": "preprint arXiv:1606.00979, 2016.", "meta": {"paper_id": "1906.01195v1", "chunk_index": 13, "filename": "1906.01195v1.pdf", "chunk_length": 3}}
{"id": "1906.01243v1_chunk_0", "text": "Learning to Explain: Answering Why-Questions via Rephrasing Allen Nie1 Erin D. Bennett2 Noah D. Goodman1,2 1Department of Computer Science 2Department of Psychology Stanford University anie@cs.stanford.edu {erindb,ngoodman}@stanford.edu Abstract Providing plausible responses to why ques- tions is a challenging but critical goal for lan- guage based human-machine interaction. Ex- planations are challenging in that they require many different forms of abstract knowledge and reasoning. Previous work has either relied on human-curated structured knowledge bases or detailed domain representation to generate satisfactory explanations. They are also of- ten limited to ranking pre-existing explana- tion choices. In our work, we contribute to the under-explored area of generating natu- ral language explanations for general phenom- ena. We automatically collect large datasets of explanation-phenomenon pairs which allow us to train sequence-to-sequence models to gen- erate natural language explanations. We com- pare different training strategies and evaluate their performance using both automatic scores and human ratings. We demonstrate that our strategy is sufﬁcient to generate highly plausi- ble explanations for general open-domain phe- nomena compared to other models trained on different datasets. 1 Introduction Allowing machines to provide human accept- able explanations has long been a difﬁcult task for natural language interaction (Carenini and Moore, 1993). In order to provide explanations, systems need to acquire sophisticated domain- knowledge (Winograd, 1971), conduct causal rea- soning over complex set of events (Hesslow, 1988) and over narrative chains (Chambers and Jurafsky, 2008), and apply commonsense knowl- edge (Levesque et al., 2011). Past work has demonstrated that by leveraging human-curated structured knowledge bases such as WordNet (Miller, 1995) or ConceptNet (Liu and Singh, 2004), a system can learn to rank or choose between multiple plausible explanations Phenomenon The city councilmen refused the demonstrators a permit because ? Original The city councilmen feared violence. L2E-Seq2Seq (greedy): They were not allowed to march in the city. L2E-Seq2Seq (beam): They did not have a permit. LM-1B: They were not allowed to use the Cape Town airport. L2W: It was the only thing in the city that could be done. Open-Subtitle: I don’t know. Figure 1: We show the original Winograd schema sen- tence, the original offered explanation, and generated responses from our models. and reach high accuracy (Luo et al., 2016; Sasaki et al., 2017). Recent successes have also shown that structured knowledge is not needed if one can train a language model on a large quantity of text. Such model can rank explanations based on the probability that each explanation might appear in natural text (Trinh and Le, 2018). While ranking explanations is an important task, the nature of explanation is more general than this. For one phenomenon, there might be many acceptable, natural, and useful explanations. In our work, instead of simply ranking or choosing explanations generated by humans, we propose to advance this important domain by directly gener- ating the explanation. We measure success based on whether the generated sequence is grammat- ically correct and is a ﬂuent, natural, and plau- sible explanation. This task has two advantages. First, it allows us to explore whether such a task is computationally feasible given the", "meta": {"paper_id": "1906.01243v1", "chunk_index": 0, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_1", "text": "important domain by directly gener- ating the explanation. We measure success based on whether the generated sequence is grammat- ically correct and is a ﬂuent, natural, and plau- sible explanation. This task has two advantages. First, it allows us to explore whether such a task is computationally feasible given the current learn- ing framework. Second, answering open-domain why-questions with plausible answers can make arXiv:1906.01243v1 [cs.CL] 4 Jun 2019 chitchat dialogue system more engaging, espe- cially in response to “why” questions (which pre- vious systems typically answer with degenerate responses such as “I don’t know”). We show that simply training a language model on previously existing datasets is not enough. However, by leveraging dependency parsing pat- terns, we are able to construct two new datasets that will allow modern neural networks to learn to generate general-domain explanations plausi- ble to humans. These new datasets of naturally occurring self-explanations (statements with “be- cause”, unprompted by a question) provide excel- lent training signal for generating novel explana- tions for a given phenomenon. We conduct hu- man experiments on the important features that contribute to plausible explanations, and we de- scribe a simple procedure that can rephrase Why- questions into a statement so our model can also function as a single-round chitchat chatbot that can answer Why-questions. 2 Learning to Explain We use the discourse extractor developed by Nie et al. (2017). This extractor ﬁrst ﬁlters sentences that contain a particular discourse marker (in our case, the marker “because”). It then uses prede- ﬁned, pattern-based rules on the dependency parse obtained from the Stanford CoreNLP dependency parser (Manning et al., 2014) to split the sentence into two semantically complete sentence clauses, which can be referred as S1 and S2. Dependency parsing allows us to isolate explanations and phe- nomena from exogenous modifying phrases. Us- ing these patterns to parse sentences with “be- cause” also allows us to deal with the free order of the explanation and phenomenon in English. We formulate the L2E task as: given the phenomenon S1, the model needs to learn to generate a plausi- ble explanation S2. In addition to retrieving the phenomenon- explanation pair, we additionally retrieve ﬁve sen- tences that immediately precede the phenomenon to provide context. We concatenate the context with S1 using a special separation token, resulting in the sequence C1, C2, ..., C5 <SEP> S1. We hypothesize that context will allow the model to generate more thematically relevant ex- planations. We refer to this setting as the L2EC task. At last, we describe a procedure in Algo- Algorithm 1 Q-to-S1 Input: question q, dependency parsed. Remove “Why”. Start at the ROOT of q: subj = NSUBJ or NSUBJPASS aux = ﬁrst dependent in [AUX, COP, AUXPASS] vp(lemma) = all remaining dependents if aux in [“do”, “does”, “did”] then vp = apply tense/person of aux to vp(lemma) else vp = aux vp(lemma) end if s = subj vp rithm 1 that uses dependency parsing to turn Why- questions into the statement format of S1. This al- lows us to generate explanations as responses to Why-questions. 3 Model 3.1", "meta": {"paper_id": "1906.01243v1", "chunk_index": 1, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_2", "text": "then vp = apply tense/person of aux to vp(lemma) else vp = aux vp(lemma) end if s = subj vp rithm 1 that uses dependency parsing to turn Why- questions into the statement format of S1. This al- lows us to generate explanations as responses to Why-questions. 3 Model 3.1 Language Modeling Language modeling focuses on modeling the joint probability of a sequence p(X = x1, ..., xn). Us- ing chain rule, this can be decomposed as p(X) = Qn t=1 p(xt|x<t), the product of conditional proba- bilities. The model parameterized by θ optimizes to maximize the log of the likelihood function L(X; θ) = Pn t=1 pθ(xt|x<t). In a neural lan- guage model, proposed by Bengio et al. (2003), a recurrent neural network is trained by truncated backpropogation through time to learn to model (theoretically) an inﬁnitely long sequence. 3.2 Sequence to Sequence Modeling First introduced by Sutskever et al. (2014), sequence-to-sequence (Seq2Seq) modeling esti- mates a conditional probability distribution of se- quence Y given sequence X. p(Y |X), where X = {x1, ..., xn}, and Y = {y1, ..., yk}. The overall objective function is similar to a language model: to maximize the log-likelihood of the probabil- ity of the Y sequence given the X sequence: L(Y, X; θ, ψ) = Pk t=1 pθ,ψ(yt|y<t, X), with pa- rameters θ for the encoder and ψ for the decoder. In our work, we experiment with different archi- tectures for the encoder and decoder. 4 Data We provide data accessibility statements in Ap- pendix A.1 for each dataset we use to train and evaluate our models. Our constructed dataset and web demo code are publicly available1. Source Dataset Task Data Length NewsCrawl L2E 2.07M 29.4 NewsCrawl L2EC 2.57M 149.4 Winograd L2E 61 18.0 COPA L2E 250 14.2 News Commentary L2E/L2EC 6301 28.6 Table 1: Top are training datasets and bottom are eval- uation datasets for each task. We report the average length of sentences for each dataset (S1 and S2 com- bined). News Commentary with context has 156.3 words on average. 4.1 Training Data NewsCrawl Dataset We build up our training dataset from two large news datasets: Gigaword Fifth Edition (Parker et al., 2011) and NewsCrawl (Bojar et al., 2018). These two datasets con- tain news stories from 2001-2017, and are non- overlapping. We built our dataset of News expla- nation pairs using the pipeline described in Sec- tion 2 and then split into training, validation, and test. More details are reported in Appendix A.2. BookCorpus BookCorpus is a set of unpub- lished novels (Romance, Fantasy, Science ﬁction, and Teen genres) collected by Zhu et al. (2015). We use a publicly available pre-trained BookCor- pus language model from Holtzman et al. (2018). We refer to this model as L2W. Language Modeling One Billion This dataset (LM-1B) is currently the largest standard training dataset for language modeling, roughly the same size as BookCorpus. This dataset is a subset of the NewsCrawl dataset, from 2007-2011. We use a pre-trained language model on this corpus from Jozefowicz et al. (2016). We refer to this model as", "meta": {"paper_id": "1906.01243v1", "chunk_index": 2, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_3", "text": "This dataset (LM-1B) is currently the largest standard training dataset for language modeling, roughly the same size as BookCorpus. This dataset is a subset of the NewsCrawl dataset, from 2007-2011. We use a pre-trained language model on this corpus from Jozefowicz et al. (2016). We refer to this model as LM-1B. 4.2 Evaluation Data News Commentary (NC) Dataset We collect pairs from a public dataset that contains predom- inantly commentary written about current news2. We use this dataset as the main evaluation of the news-based explanation because 1). It is a sepa- rate dataset without any overlap with NewsCrawl; 1https://github.com/windweller/L2EWeb 2https://www.project-syndicate.org/about 2). This dataset still belongs to the same news do- main, so it provides an in-domain evaluation for L2E, L2EC and LM-1B models. Winograd Schema Challenge Subset (WSC-G) We use 61 example sentences in the Winograd Schema Challenge that contain the words “be- cause” or “so”. Similar to Trinh and Le (2018), we substitute the ambiguous pronouns with the cor- rect referent and ask the model to generate the cor- rect explanation “the trophy is too big” to the phe- nomenon “The trophy doesn’t ﬁt in the suitcase”. Choice of Plausible Alternatives (COPA) Roemmele et al. (2011) proposed a task that contains questions such as “The women met for coffee. What was the CAUSE of this?”, and the model is asked to choose between two pre-deﬁned causes. In our setting, we directly ask the model to generate a cause. For language models, we append “because” to the end of each COPA sentence and ask the model to generate the rest. 5 Experiments 5.1 Language Model Training We use the same language model described in Holtzman et al. (2018). We train 10 epochs for both L2E and L2EC. We use a one layer LSTM (Hochreiter and Schmidhuber, 1997) with 2048 hidden state dimensions and 256 word di- mensions. We chose these hyperparameters by tuning on the validation set of each task. Our lan- guage model achieved 51.64 perplexity on the L2E test set, and 37.61 perlexity on the L2EC test set. 5.2 Seq2Seq Model Training We experiment with two architectures: LSTM encoder-decoder and Transformer (Vaswani et al., 2017). We ﬁnd that with the L2E task, the Trans- former architecture performed better, and for the L2EC task, the LSTM encoder-decoder performed better. We suspect that Transformer is worse when the source sequence is too long. We tune each ar- chitecture’s hyperparameters extensively and we pick the best architecture for each task to evaluate on the evaluation datasets. 5.3 Automatic Evaluation We use automatic metrics to evaluate the 8 models’ performance on the News Commentary dataset. Even though this is a non-overlapping held-out dataset to our news training data, it is still Data S1 Generated S2 Rank Reference S2 Rank NewsCrawl That banned his most threatening challenger, Rally leader Alassane Ouattara, from running for president because ? He was born in Burkina Faso. — He is only half-Ivorian. — NewsCrawl The victim was only saved because ? He was wearing a seatbelt. — The dog turned on the former lifeguard. — NewsCrawl I voted", "meta": {"paper_id": "1906.01243v1", "chunk_index": 3, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_4", "text": "most threatening challenger, Rally leader Alassane Ouattara, from running for president because ? He was born in Burkina Faso. — He is only half-Ivorian. — NewsCrawl The victim was only saved because ? He was wearing a seatbelt. — The dog turned on the former lifeguard. — NewsCrawl I voted for George W. Bush because ? I thought he was the best person for the job. — That’s the name you heard a lot of talk about. — WSC-G An hour later John left because ? He didn’t feel safe. 0.0 John promised Bill to leave. 0.67 COPA The woman gave the man her phone number because ? She was too busy to be bothered by the man. 0.17 She was attracted to him. 0.5 NC Moreover, ordinary Russians are becoming allergic to liberal democracy because ? They see it as a threat to their own interests. 0.16 Liberal technocrats have consistently served as window dressing for an illiberal Kremlin regime. 0.19 Table 2: Example pairs from our highest performing models with the original sentence as a reference. Human ranking score lower is better. We provide examples of especially poor-rated generations in the Appendix. Model L2E L2EC Acc Perp Acc Perp LSTM 36.2 41.4 36.0 41.3 Transformer 38.2 33.1 27.8 96.7 Table 3: We report the best per-token accuracy and perplexity evaluated for each tuned architecture on the L2E/L2EC validation dataset. within the same domain. We ﬁnd that L2E/L2EC based models obtained higher scores across all automatic metrics in Table 4. Our results also demonstrate that context matters for explanation. The L2EC task models, trained on context, can generate higher quality explanations than context- free L2E task models. 5.4 Human Evaluation Ranking Explanations We evaluate the mod- els’ relative performance on generating explana- tions through a survey with human evaluators. 75 participants were recruited using Amazon’s Mechanical Turk (AMT). Each evaluator saw 10 prompts from a single dataset, and ranked 7 to 9 explanations: the original explanation extracted from the dataset and the explanations generated by different models. 30 participants saw prompts from our Winograd dataset, 30 participants saw prompts from News Commentary, and 15 partic- ipants saw prompts from COPA. We report the re- sults of this evaluation in the Human Ranking sub- section of Table 4. Rating Explanations In a followup survey, 60 human evaluators on AMT rated explanations gen- erated by the L2E-Seq2Seq model with beam search and the original (between participants). Ratings were from 0 (extremely bad) to 1 (ex- tremely good) along various dimensions of expla- nation quality. Results of this study are shown in Table 5. Generated explanations overall were rated worse than human explanations, but tended to be more good than bad (≥0.5) on all measures. 6 Discussion The nature of phenomenon-explanation mapping has always been one-to-many. People can offer drastically different explanations to the same phe- nomenon. We argue that requiring the machine to generate plausible explanations is more useful and therefore a better goal for models to achieve. Models trained on traditional chatbot corpora are unable to answer why questions because of data", "meta": {"paper_id": "1906.01243v1", "chunk_index": 4, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_5", "text": "been one-to-many. People can offer drastically different explanations to the same phe- nomenon. We argue that requiring the machine to generate plausible explanations is more useful and therefore a better goal for models to achieve. Models trained on traditional chatbot corpora are unable to answer why questions because of data sparsity. We note that the generated results are not similar to the original explanations but are often acceptable by human assessment. Features of Explanations In the human rating experiment, our model was overall rated higher than the original explanations only on the gram- maticality measure. However, this measure seems least representative of the overall explanation Model BLEU ROUGE METEOR Human Ranking Greedy Beam Greedy Beam Greedy Beam COPA WSC-G NC L2E-Seq2Seq 0.55 0.37 18.8 18.3 7.4 7.6 0.412 0.409 0.454 L2EC-Seq2Seq 0.40 0.47 19.9 19.7 8.6 8.8 — — 0.433 L2E-LM 0.25 0.20 15.9 16.8 6.1 6.7 0.515 0.572 0.479 L2EC-LM 0.36 0.38 17.0 17.7 6.7 7.3 — — 0.432 LM-1B† 0.18 — 16.9 — 7.1 — 0.526 0.484 0.454 L2W† 0.00 0.00 14.0 13.9 6.7 6.8 0.511 0.523 0.625 L2WC 0.13 0.14 12.8 12.7 5.7 5.7 — — 0.546 OpenSubtitle† 0.04 0.0 13.0 13.4 1.9 3.7 0.827 0.823 0.811 Reference 100 100 100 100 100 100 0.266 0.238 0.267 Table 4: BLUE, ROGUE, METEOR are evaluated on News Commentary test data. Any model with C in the name is evaluated with full context. Models with † are pre-trained models from other work. Only L2E-Seq2Seq uses the Transformer architecture, the rest LSTM. In human ranking, we report the average rank across participants. Top ranking is 0 and lowest ranking is 1. Original L2E-Seq2Seq Goodness 0.699 [0.67, 0.72] 0.500 [0.45, 0.55] Relatedness 0.723 [0.70, 0.74] 0.590 [0.55, 0.63] Grammaticality 0.684 [0.66, 0.71] 0.738 [0.70, 0.77] Helpfulness 0.696 [0.67, 0.72] 0.512 [0.47, 0.56] Plausibility 0.710 [0.69, 0.73] 0.543 [0.50, 0.59] Table 5: Results of rating study with human evaluators, average rating and bootstrapped 95% CI. quality: ratings for most features were highly cor- related with each other (0.771-0.865), but not with grammaticality (0.196-0.323). This shows that, while we can achieve plausible explanations with our models, more research is required in order to reach human-level quality. Explaining as Generating Even though formu- lating the task of providing explanation as a se- quence generation task allows us to leverage the rapid advancements in the natural language gen- eration community, we sidestep a vast amount of literature that aims to provide informatively cor- rect explanations as well as grounding explana- tions theoretically to the causal understanding of the situation (Halpern and Pearl, 2005). We also suffer from the same drawbacks noticed in natu- ral language generation papers such as brevity and generic responses, failure to leverage long context, and being data hungry (Holtzman et al., 2018). Exploring Linguistic Structures The curated dataset of explanation-phenomenon pairs provides an opportunity to explore descriptive structures and features of explanations. In principle, one can use this dataset to formulate frequent and com- mon syntactic and semantic patterns for natural- sounding explanations. This would aid our under- standing of how why-questions can", "meta": {"paper_id": "1906.01243v1", "chunk_index": 5, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_6", "text": "Linguistic Structures The curated dataset of explanation-phenomenon pairs provides an opportunity to explore descriptive structures and features of explanations. In principle, one can use this dataset to formulate frequent and com- mon syntactic and semantic patterns for natural- sounding explanations. This would aid our under- standing of how why-questions can be addressed satisfactorily. 7 Conclusion We present the task of generating plausible expla- nations as an important goal for neural sequence- to-sequence models. We curate a large dataset of phenomenon-explanation pairs so that these mod- els can learn to provide plausible explanations as judged by humans, and formulate responses to general Why-questions. Acknowledgement We thank Barry Haddow and Michael Hahn for their advice and assistance. The research is based upon work supported by the Defense Ad- vanced Research Projects Agency (DARPA), via the Air Force Research Laboratory (AFRL, Grant No. FA8650-18-C-7826). The views and con- clusions contained herein are those of the authors and should not be interpreted as necessarily rep- resenting the ofﬁcial policies or endorsements, ei- ther expressed or implied, of DARPA, the AFRL or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. References Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic lan- guage model. Journal of machine learning research, 3(Feb):1137–1155. Ondˇrej Bojar, Rajen Chatterjee, Christian Feder- mann, Mark Fishel, Yvette Graham, Barry Had- dow, Matthias Huck, Antonio Jimeno Yepes, Philipp Koehn, Christof Monz, et al. 2018. Proceedings of the third conference on machine translation. In Pro- ceedings of the Third Conference on Machine Trans- lation: Research Papers. Giuseppe Carenini and Johanna D Moore. 1993. Gen- erating explanations in context. In IUI, volume 93, pages 175–182. Nathanael Chambers and Dan Jurafsky. 2008. Unsu- pervised learning of narrative event chains. In Pro- ceedings of ACL-08: HLT, pages 789–797, Colum- bus, Ohio. Association for Computational Linguis- tics. Joseph Y Halpern and Judea Pearl. 2005. Causes and explanations: A structural-model approach. part ii: Explanations. The British journal for the philosophy of science, 56(4):889–911. Germund Hesslow. 1988. The problem of causal selec- tion. Contemporary science and natural explana- tion: Commonsense conceptions of causality, pages 11–32. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735–1780. Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 1638–1649, Melbourne, Aus- tralia. Association for Computational Linguistics. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. OpenNMT: Open-source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstra- tions, pages 67–72, Vancouver, Canada. Association for Computational Linguistics. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. Hugo Liu and Push Singh. 2004. Conceptneta practi- cal", "meta": {"paper_id": "1906.01243v1", "chunk_index": 6, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_7", "text": "of ACL 2017, System Demonstra- tions, pages 67–72, Vancouver, Canada. Association for Computational Linguistics. Hector J Levesque, Ernest Davis, and Leora Morgen- stern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense reasoning, volume 46, page 47. Hugo Liu and Push Singh. 2004. Conceptneta practi- cal commonsense reasoning tool-kit. BT technology journal, 22(4):211–226. Zhiyi Luo, Yuchen Sha, Kenny Q Zhu, Seung-won Hwang, and Zhongyuan Wang. 2016. Common- sense causal reasoning between short texts. In KR, pages 421–431. Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David Mc- Closky. 2014. The Stanford CoreNLP Natural Lan- guage Processing Toolkit. In Association for Com- putational Linguistics (ACL) System Demonstra- tions, pages 55–60. George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39– 41. Allen Nie, Erin D Bennett, and Noah D Goodman. 2017. Dissent: Sentence representation learning from explicit discourse relations. arXiv preprint arXiv:1710.04334. Robert Parker, David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda. 2011. English gigaword ﬁfth edi- tion LDC2011T07. Philadelphia: Linguistic Data Consortium. Web download ﬁle. Melissa Roemmele, Cosmin Adrian Bejan, and An- drew S Gordon. 2011. Choice of plausible alterna- tives: An evaluation of commonsense causal reason- ing. In AAAI Spring Symposium: Logical Formal- izations of Commonsense Reasoning, pages 90–95. Shota Sasaki, Sho Takase, Naoya Inoue, Naoaki Okazaki, and Kentaro Inui. 2017. Handling multi- word expressions in causality estimation. In IWCS 201712th International Conference on Computa- tional SemanticsShort papers. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural net- works. In Advances in neural information process- ing systems, pages 3104–3112. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 5998–6008. Terry Winograd. 1971. Procedures as a represen- tation for data in a computer program for under- standing natural language. Technical report, MAS- SACHUSETTS INST OF TECH CAMBRIDGE PROJECT MAC. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut- dinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19–27. Data S1 Generated S2 Rank Reference S2 Rank NewsCrawl Perhaps the student was so good because ? He was so good at it. — Thursday wasn’t the ﬁrst time he acted as a moderator. — NewsCrawl Cheaper prices drove computers out of the walls of these big companies because ? They were cheaper than the ones that were being used. — You suddenly didn’t need all that money anymore. — WSC-G Sam and Amy are passionately in love, but Amy’s parents are unhappy about it because ? They don’t want to be the ﬁrst female president. 0.87 Sam and Amy are ﬁfteen. 0.37 COPA The hamburger meat browned because ? That’s what it is. 0.83 The cook grilled", "meta": {"paper_id": "1906.01243v1", "chunk_index": 7, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_8", "text": "anymore. — WSC-G Sam and Amy are passionately in love, but Amy’s parents are unhappy about it because ? They don’t want to be the ﬁrst female president. 0.87 Sam and Amy are ﬁfteen. 0.37 COPA The hamburger meat browned because ? That’s what it is. 0.83 The cook grilled it. 0.0 NC The desperately poor may accept handouts because ? They are the only ones who can afford it. 0.83 They feel they have to. 0.12 Table S1: Bad example pairs from our lowest performing models with the original sentence as a reference. Human ranking score lower is better. Full list of WSC-G and COPA generations can be found in https://github. com/windweller/L2EWeb/blob/master/WinogradS2Generation.ipynb. A Supplementary Materials A.1 Data Accessibility Statement The majority of the data we use are publicly avail- able. We provide speciﬁc instructions on how to obtain these data below: Gigaword 5th Edition This dataset is provided through Linguistic Data Consortium (LDC): https://catalog.ldc.upenn.edu/ LDC2011T07. Even though this dataset is only available through subscription, most university libraries should have existing subscriptions, and only 20% of our training data comes from this dataset. News Crawl Dataset The shufﬂed version of this dataset is publicly available3. We requested the original un-shufﬂed dataset from Barry Haddow4 so that we can extract context for L2EC task. We believe this dataset can be easily accessed by the public upon an email request. BookCorpus This dataset is no longer pub- licly available. However, there are many neu- ral language models pre-trained on this dataset that are publicly available. We used one that can be accessed from https://github.com/ ari-holtzman/l2w. News Commentary Dataset This is also publicly available through the WMT workshop5 similar to 3http://www.statmt.org/wmt18/ translation-task.html 4http://homepages.inf.ed.ac.uk/ bhaddow/ 5http://data.statmt.org/wmt18/ the NewsCrawl dataset. This dataset is not shuf- ﬂed. Winograd Schema Challenge The original version of this dataset is publicly available https://cs.nyu.edu/davise/papers/ WinogradSchemas/WS.html. We use a pro- cessed version from Trinh and Le (2018), which can be accessed through Google Cloud Stor- age: gs://commonsense-reasoning/ reproduce/commonsense_test/ wsc273.json. Choice of Plausible Alternatives This dataset is available at http://people.ict.usc. edu/˜gordon/copa.html. A.2 Training Data Curation In order to automatically curate a sizable amount of training data, we choose large corpora that are made of news articles, due to the well-formedness of sentences and there are many phenomenon- explanation pairs in news stories. We use Giga- word ﬁfth edition (Parker et al., 2011) which con- tains news stories from seven news agencies over the span of 2001-2010. We extracted paragraphs and tokenized the sentences. We discard non- English characters. Another large dataset of new articles comes from WMT-18, the NewsCrawl dataset (Bojar et al., 2018). This dataset spans from 2007-2017 collected from the RSS (Rich Site Summary) feed of 18 news agencies. The translation-task/news-commentary-v13. en.gz only overlapping agency between Gigaword and NewsCrawl is Los Angeles Times. In addition to the randomly shufﬂed dataset we obtained from the WMT-18 website, we additionally contacted the organization for the unshufﬂed version of data. We refer to this dataset as the NewsCrawl-ordered. This dataset is slightly larger than the current re- leased version of NewsCrawl and contains a cou-", "meta": {"paper_id": "1906.01243v1", "chunk_index": 8, "filename": "1906.01243v1.pdf", "chunk_length": 512}}
{"id": "1906.01243v1_chunk_9", "text": "Times. In addition to the randomly shufﬂed dataset we obtained from the WMT-18 website, we additionally contacted the organization for the unshufﬂed version of data. We refer to this dataset as the NewsCrawl-ordered. This dataset is slightly larger than the current re- leased version of NewsCrawl and contains a cou- ple of months of early 2018 data. We shufﬂe and then split both datasets into train/valid/test in stan- dard 0.9/0.05/0.05. We use the validation and test set on this task to pick the best performing model. A.3 Language Model Details We use adaptive gradient descent (AdaGrad) with learning rate 0.1 and weight decay of 1e-6. A.4 Seq2Seq Model Details We built and trained our Seq2Seq model using OpenNMT (Klein et al., 2017). For the L2E task, we used a 6-layer Transformer model, with hid- den dimension 512, feedforward layer dimension 2048, and 8 attention heads. We train with dropout rate of 0.1 with Noam optimizer. For the L2EC task, we used a 2-layer LSTM model with 650 hidden dimension size for both encoder and de- coder, as well as for word embedding. We train with dropout rate of 0.2 and Noam optimizer. −1 −0.8 −0.6 −0.4 −0.2 0 0.2 0.4 0.6 0.8 1 grammaticality relatedness plausibility good_explanation helpfulness grammaticality relatedness plausibility good_explanation helpfulness Figure S1: Correlations of human ratings on Winograd Schema Challenge explanations G G G G G G G G G G G G G G G G G G G G G G G Winograd & COPA News Commentary L2E−S2S(g) L2E−S2S(b) L2E−LM LM−1B L2W OpenSubtitle Human L2E−S2S(b) L2EC−S2S L2E−LM L2EC−LM LM−1B L2W L2WC OpenSubtitle Human best worst G G G Winograd COPA News Figure S2: The average ranking of each model’s gener- ated response (lower is better). Figure S3: Screenshot of raking study. Figure S4: Screenshot of ratings study.", "meta": {"paper_id": "1906.01243v1", "chunk_index": 9, "filename": "1906.01243v1.pdf", "chunk_length": 302}}
{"id": "1906.01268v1_chunk_0", "text": "Exploiting Sentential Context for Neural Machine Translation Xing Wang Tencent AI Lab brightxwang@tencent.com Zhaopeng Tu Tencent AI Lab zptu@tencent.com Longyue Wang Tencent AI Lab vinnylywang@tencent.com Shuming Shi Tencent AI Lab shumingshi@tencent.com Abstract In this work, we present novel approaches to exploit sentential context for neural machine translation (NMT). Speciﬁcally, we ﬁrst show that a shallow sentential context extracted from the top encoder layer only, can improve translation performance via contextualizing the encoding representations of individual words. Next, we introduce a deep sentential context, which aggregates the sentential context representations from all the internal layers of the encoder to form a more compre- hensive context representation. Experimental results on the WMT14 English⇒German and English⇒French benchmarks show that our model consistently improves per- formance over the strong TRANSFORMER model (Vaswani et al., 2017), demonstrating the necessity and effectiveness of exploiting sentential context for NMT. 1 Introduction Sentential context, which involves deep syntactic and semantic structure of the source and target lan- guages (Nida, 1969), is crucial for machine trans- lation. In statistical machine translation (SMT), the sentential context has proven beneﬁcial for predicting local translations (Meng et al., 2015; Zhang et al., 2015). The exploitation of sentential context in neural machine translation (NMT, Bah- danau et al., 2015), however, is not well studied. Recently, Lin et al. (2018) showed that the trans- lation at each time step should be conditioned on the whole target-side context. They introduced a deconvolution-based decoder to provide the global information from the target-side context for guid- ance of decoding. In this work, we propose simple yet effec- tive approaches to exploiting source-side global sentence-level context for NMT models. We use encoder representations to represent the source- side context, which are summarized into a senten- tial context vector. The source-side context vector is fed to the decoder, so that translation at each step is conditioned on the whole source-side con- text. Speciﬁcally, we propose two types of senten- tial context: 1) the shallow one that only exploits the top encoder layer, and 2) the deep one that ag- gregates the sentence representations of all the en- coder layers. The deep sentential context can be viewed as a more comprehensive global sentence representation, since different types of syntax and semantic information are encoded in different en- coder layers (Shi et al., 2016; Peters et al., 2018; Raganato and Tiedemann, 2018). We validate our approaches on top of the state- of-the-art TRANSFORMER model (Vaswani et al., 2017). Experimental results on the benchmarks WMT14 English⇒German and English⇒French translation tasks show that exploiting sentential context consistently improves translation perfor- mance across language pairs. Among the model variations, the deep strategies consistently outper- form their shallow counterparts, which conﬁrms our claim. Linguistic analyses (Conneau et al., 2018) on the learned representations reveal that the proposed approach indeed provides richer linguis- tic information. The contributions of this paper are: • Our study demonstrates the necessity and ef- fectiveness of exploiting source-side senten- tial context for NMT, which beneﬁts from fusing useful contextual information across encoder layers. • We propose several strategies to better cap- ture useful", "meta": {"paper_id": "1906.01268v1", "chunk_index": 0, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_1", "text": "approach indeed provides richer linguis- tic information. The contributions of this paper are: • Our study demonstrates the necessity and ef- fectiveness of exploiting source-side senten- tial context for NMT, which beneﬁts from fusing useful contextual information across encoder layers. • We propose several strategies to better cap- ture useful sentential context for neural ma- chine translation. Experimental results em- pirically show that the proposed approaches achieve improvement over the strong base- line model TRANSFORMER. arXiv:1906.01268v1 [cs.CL] 4 Jun 2019 layer 2 layer 1 layer 3 (a) Vanilla layer 2 layer 1 layer 3 (b) Shallow Sentential Context layer 2 layer 1 layer 3 (c) Deep Sentential Context Figure 1: Illustration of the proposed approache. As on a 3-layer encoder: (a) vanilla model without sentential context, (b) shallow sentential context representation (i.e. blue square) by exploiting the top encoder layer only; and (c) deep sentential context representation (i.e. brown square) by exploiting all encoder layers. The circles denote hidden states of individual tokens in the input sentence, and the squares denote the sentential context representations. The red up arrows denote that the representations are fed to the subsequent decoder. This ﬁgure is best viewed in color. 2 Approach Like a human translator, the encoding process is analogous to reading a sentence in the source lan- guage and summarizing its meaning (i.e. senten- tial context) for generating the equivalents in the target language. When humans translate a source sentence, they generally scan the sentence to cre- ate a whole understanding, with which in mind they incrementally generate the target sentence by selecting parts of the source sentence to trans- late at each decoding step. In current NMT mod- els, the attention model plays the role of selecting parts of the source sentence, but lacking a mech- anism to guarantee that the decoder is aware of the whole meaning of the sentence. In response to this problem, we propose to augment NMT mod- els with sentential context, which represents the whole meaning of the source sentence. 2.1 Framework Figure 1 illustrates the framework of the proposed approach. Let g = g(X) be the sentential context vector, and g(·) denotes the function to summa- rize the source sentence X, which we will discuss in the next sections. There are many possible ways to integrate the sentential context into the decoder. The target of this paper is not to explore this whole space but simply to show that one fairly straight- forward implementation works well and that sen- tential context helps. In this work, we incorporate the sentential context into decoder as dl i = f(LAYERdec( bDl−1), cl i), (1) bDl−1 = FFNl(Dl−1, g), (2) where dl i is the l-th layer decoder state at decod- ing step i, cl i is a dynamic vector that selects cer- tain parts of the encoder output, FFNl(·) is a dis- tinct feed-forward network associated with the l-th layer of the decoder, which reads the l −1-th layer output Dl−1 and the sentential context g. In this way, at each decoding step i, the decoder is aware of the", "meta": {"paper_id": "1906.01268v1", "chunk_index": 1, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_2", "text": "selects cer- tain parts of the encoder output, FFNl(·) is a dis- tinct feed-forward network associated with the l-th layer of the decoder, which reads the l −1-th layer output Dl−1 and the sentential context g. In this way, at each decoding step i, the decoder is aware of the sentential context g embedded in bDl−1. In the following sections, we discuss the choice of g(·), namely shallow sentential context (Fig- ure 1b) and deep sentential context (Figure 1c), which differ at the encoder layers to be exploited. It should be pointed out that the new parameters introduced in the proposed approach are jointly updated with NMT model parameters in an end- to-end manner. 2.2 Shallow Sentential Context Shallow sentential context is a function of the top encoder layer output HL: g = g(HL) = GLOBAL(HL), (3) where GLOBAL(·) is the composition function. Choices of GLOBAL(·) Two intuitive choices are mean pooling (Iyyer et al., 2015) and max pooling (Kalchbrenner et al., 2014): GLOBALMEAN = MEAN(HL), (4) GLOBALMAX = MAX(HL). (5) Recently, Lin et al. (2017) proposed a self- attention mechanism to form sentence representa- tion, which is appealing for its ﬂexibility on ex- tracting implicit global features. Inspired by this, g3 g2 g1 r2 r1 r3 r0 (a) RNN g3 g2 g1 gi βi,1 βi,2 βi,3 di-1 (b) TAM Figure 2: Illustration of the deep functions. “TAM” model dynamically aggregates sentence representa- tions at each decoding step with state di−1. we propose an attentive mechanism to learn sen- tence representation: GLOBALATT = ATT(g0, HL), (6) g0 = MAX(H0), (7) where H0 is the word embedding layer, and its max pooling vector g0 serves as the query to ex- tract features to form the ﬁnal sentential context representation. 2.3 Deep Sentential Context Deep sentential context is a function of all encoder layers outputs {H1, . . . , HL}: g = g(H1, . . . , HL) = DEEP(g1, . . . , gL), (8) where gl is the sentence representation of the l-th layer Hl, which is calculated by Equation 3. The motivation for this mechanism is that recent stud- ies reveal that different encoder layers capture lin- guistic properties of the input sentence at different levels (Peters et al., 2018), and aggregating layers to better fuse semantic information has proven to be of profound value (Shen et al., 2018; Dou et al., 2018; Wang et al., 2018; Dou et al., 2019). In this work, we propose to fuse the global information across layers. Choices of DEEP(·) In this work, we investigate two representative functions to aggregate informa- tion across layers, which differ at whether the de- coding information is taken into account. RNN Intuitively, we can treat G = {g1, . . . , gL} as a sequence of representations, and recurring all the representations with an RNN: DEEPRNN = RNN(G). (9) We use the last RNN state as the sentence rep- resentation: g = rL. As seen, the RNN-based aggregation repeatedly revises the sentence rep- resentations of the sequence with each recurrent step. As a side effect coming together", "meta": {"paper_id": "1906.01268v1", "chunk_index": 2, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_3", "text": "and recurring all the representations with an RNN: DEEPRNN = RNN(G). (9) We use the last RNN state as the sentence rep- resentation: g = rL. As seen, the RNN-based aggregation repeatedly revises the sentence rep- resentations of the sequence with each recurrent step. As a side effect coming together with the proposed approach, the added recurrent induc- tive bias of RNNs has proven beneﬁcial for many sequence-to-sequence learning tasks such as ma- chine translation (Dehghani et al., 2018). TAM Recently, Bapna et al. (2018) proposed a novel transparent attention model (TAM) to train very deep NMT models. In this work, we apply TAM to aggregate sentence representations: DEEPTAM = L X l=1 βi,lgl, (10) βi = ATTg(dl i−1, G), (11) where ATTg(·) is an attention model with its own parameters, that speciﬁcs which context repre- sentations is relevant for each decoding output. Again, dl i−1 is the decoder state in the l-th layer. Comparing with its RNN counterpart, the TAM mechanism has three appealing strengths. First, TAM dynamically generates the weights βi based on the decoding information at every decoding step dl i−1, while RNN is unaware of the decoder states and the associated parameters are ﬁxed after training. Second, TAM allows the model to adjust the gradient ﬂow to different layers in the encoder depending on its training phase. 3 Experiment We conducted experiments on WMT14 En⇒De and En⇒Fr benchmarks, which contain 4.5M and 35.5M sentence pairs respectively. We re- ported experimental results with case-sensitive 4- gram BLEU score. We used byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge op- erations to alleviate the out-of-vocabulary prob- lem. We implemented the proposed approaches on top of TRANSFORMER model (Vaswani et al., 2017). We followed Vaswani et al. (2017) to set the model conﬁgurations, and reproduced their re- ported results. We tested both Base and Big mod- els, which differ at the layer size (512 vs. 1024) and the number of attention heads (8 vs. 16). 3.1 Ablation Study We ﬁrst investigated the effect of components in the proposed approaches, as listed in Table 1. Shallow Sentential Context (Rows 3-5) All the shallow strategies achieve improvement over the # Model GLOBAL(·) DEEP(·) # Para. Train Decode BLEU 1 BASE n/a n/a 88.0M 1.39 3.85 27.31 2 MEDIUM n/a n/a +25.2M 1.08 3.09 27.81 3 SHALLOW Mean Pooling n/a +18.9M 1.35 3.45 27.58 4 Max Pooling +18.9M 1.34 3.43 27.81↑ 5 Attention +19.9M 1.22 3.23 28.04⇑ 6 DEEP Attention RNN +26.8M 1.03 3.14 28.38⇑ 7 TAM +26.4M 1.07 3.03 28.33⇑ Table 1: Impact of components on WMT14 En⇒De translation task. BLEU scores in the table are case sensitive. “Train” denotes the training speed (steps/second), and “Decode” denotes the decoding speed (sentences/second) on a Tesla P40. “TAM” denotes the transparent attention model to implement the function DEEP(·). “↑/ ⇑”: signiﬁcant over TRANSFORMER counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). baseline Base model, validating the importance of sentential context in NMT. Among them, atten- tive mechanism (Row 5) obtains the best perfor- mance in terms of BLEU score, while maintains", "meta": {"paper_id": "1906.01268v1", "chunk_index": 3, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_4", "text": "to implement the function DEEP(·). “↑/ ⇑”: signiﬁcant over TRANSFORMER counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). baseline Base model, validating the importance of sentential context in NMT. Among them, atten- tive mechanism (Row 5) obtains the best perfor- mance in terms of BLEU score, while maintains the training and decoding speeds. Therefore, we used the attentive mechanism to implement the function GLOBAL(·) as the default setting in the following experiments. Deep Sentential Context (Rows 6-7) As seen, both RNN and TAM consistently outperform their shallow counterparts, proving the effectiveness of deep sentential context. Introducing deep context signiﬁcantly improves translation performance by over 1.0 BLEU point, while only marginally de- creases the training and decoding speeds. Compared to Strong Base Model (Row 2) As our model has more parameters than the Base model, we build a new baseline model (MEDIUM in Table 1) which has a similar model size as the proposed deep sentential context model. We change the ﬁlter size from 1024 to 3072 in the de- coder’s feed-forward network (Eq.2). As seen, the proposed deep sentential context models also out- perform the MEDIUM model over 0.5 BLEU point. 3.2 Main Result Experimental results on both WMT14 En⇒De and En⇒Fr translation tasks are listed in Table 2. As seen, exploiting deep sentential context rep- resentation consistently improves translation per- formance across language pairs and model archi- tectures, demonstrating the necessity and effec- tiveness of modeling sentential context for NMT. Among them, TRANSFORMER-BASE with deep sentential context achieves comparable perfor- mance with the vanilla TRANSFORMER-BIG, with only less than half of the parameters (114.4M Model En⇒De En⇒Fr TRANSFORMER-BASE 27.31 39.32 + DEEP (RNN) 28.38⇑ 40.15⇑ + DEEP (TAM) 28.33⇑ 40.27⇑ TRANSFORMER-BIG 28.58 41.41 + DEEP (RNN) 29.04↑ 41.87 + DEEP (TAM) 29.19⇑ 42.04⇑ Table 2: Case-sensitive BLEU scores on WMT14 En⇒De and En⇒Fr test sets. “↑/ ⇑”: signiﬁcant over TRANSFORMER counterpart (p < 0.05/0.01), tested by bootstrap resampling. vs. 264.1M, not shown in the table). Fur- thermore, DEEP (TAM) consistently outperforms DEEP (RNN) in the TRANSFORMER-BIG conﬁg- uration. One possible reason is that the big models beneﬁt more from the improved gradient ﬂow with the transparent attention (Bapna et al., 2018). 3.3 Linguistic Analysis To gain linguistic insights into the global and deep sentence representation, we conducted prob- ing tasks1 (Conneau et al., 2018) to evaluate lin- guistics knowledge embedded in the encoder out- put and the sentence representation in the varia- tions of the Base model that are trained on En⇒De translation task. The probing tasks are classiﬁca- tion problems that focus on simple linguistic prop- erties of sentences. The 10 probing tasks are cate- gories into three groups: (1) Surface information. (2) Syntactic information. (3) Semantic informa- tion. For each task, we trained the classiﬁer on the train set, and validated the classiﬁer on the vali- dation set. We followed Hao et al. (2019) and Li 1https://github.com/facebookresearch/ SentEval/tree/master/data/probing Model Surface Syntactic Semantic SeLen WC Avg TrDep ToCo BShif Avg Tense SubN ObjN SoMo CoIn Avg L4 IN BASE 94.18 66.24 80.21 43.91 77.36 69.25 63.51 88.03 83.77", "meta": {"paper_id": "1906.01268v1", "chunk_index": 4, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_5", "text": "train set, and validated the classiﬁer on the vali- dation set. We followed Hao et al. (2019) and Li 1https://github.com/facebookresearch/ SentEval/tree/master/data/probing Model Surface Syntactic Semantic SeLen WC Avg TrDep ToCo BShif Avg Tense SubN ObjN SoMo CoIn Avg L4 IN BASE 94.18 66.24 80.21 43.91 77.36 69.25 63.51 88.03 83.77 83.68 52.22 60.57 73.65 L5 IN BASE 93.40 63.95 78.68 44.36 78.26 71.36 64.66 88.84 84.05 84.56 52.58 61.56 74.32 L6 IN BASE 92.20 63.00 77.60 44.74 79.02 71.24 65.00 89.24 84.69 84.53 52.13 62.47 74.61 + SSR 92.09 62.54 77.32 44.94 78.39 71.31 64.88 89.17 85.79 85.21 53.14 63.32 75.33 + DSR 91.86 65.61 78.74 45.52 78.77 71.62 65.30 89.08 85.89 84.91 53.40 63.33 75.32 Table 3: Performance on the linguistic probing tasks of evaluating linguistics embedded in the encoder outputs. “BASE” denotes the representations from TRANFORMER-BASED encoder. “SSR” denotes shallow sentence repre- sentation. “DSR” denotes deep sentence representation. “AVG” denotes the average accuracy of each category. et al. (2019) to set the model conﬁgurations. We also listed the results of lower layer representa- tions (L = 4, 5) in TRANSFORMER-BASE to con- duct better comparison. The accuracy results on the different test sets are shown in Table 3. From the tale, we can see that • For different encoder layers in the baseline model (see “L4 in BASE”, “L5 in BASE” and “L6 in BASE”), lower layers embed more about surface information while higher lay- ers encode more semantics, which are consis- tent with previous ﬁndings in (Raganato and Tiedemann, 2018). • Integrating the shallow sentence representa- tion (“+ SSR”) obtains improvement over the baseline on semantic tasks (75.33 vs. 74.61), while fails to improve on the surface (77.32 vs. 77.60) and syntactic tasks (64.88 vs. 65.00). This may indicate that the shallow representations that exploits only the top en- coder layer (“L6 in BASE”) encodes more se- mantic information. • Introducing deep sentence representation (“+ DSR”) brings more improvements. The rea- son is that our deep sentence representation is induced from the sentence representations of all the encoder layers, and lower layers that contain abound surface and syntactic infor- mation are exploited. Along with the above translation experiments, we believe that the sentential context is necessary for NMT by enriching the source sentence repre- sentation. The deep sentential context which is in- duced from all encoder layers can improve trans- lation performance by offering different types of syntax and semantic information. 4 Related Work Sentential context has been successfully applied in SMT (Meng et al., 2015; Zhang et al., 2015). In these works, sentential context representation which is generated by the CNNs is exploited to guided the target sentence generation. In broad terms, sentential context can be viewed as a sen- tence abstraction from a speciﬁc aspect. From this point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In", "meta": {"paper_id": "1906.01268v1", "chunk_index": 5, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_6", "text": "point of view, domain information (Foster and Kuhn, 2007; Hasler et al., 2014; Wang et al., 2017b) and topic information (Xiao et al., 2012; Xiong et al., 2015; Zhang et al., 2016) can also be treated as the sentential context, the exploitation of which we leave for future work. In the context of NMT, several researchers leverage document-level context for NMT (Wang et al., 2017a; Choi et al., 2017; Tu et al., 2018), while we opt for sentential context. In addition, contextual information are used to improve the en- coder representations (Yang et al., 2018, 2019; Lin et al., 2018). Our approach is complementary to theirs by better exploiting the encoder represen- tations for the subsequent decoder. Concerning guiding the NMT generation with source-side con- text, Zheng et al. (2018) split the source content into translated and untranslated parts, while we fo- cus on exploiting global sentence-level context. 5 Conclusion In this work, we propose to exploit sentential context for neural machine translation. Speciﬁ- cally, the shallow and the deep strategies exploit the top encoder layer and all the encoder lay- ers, respectively. Experimental results on WMT14 benchmarks show that exploiting sentential con- text improves performances over the state-of-the- art TRANSFORMER model. Linguistic analyses re- veal that the proposed approach indeed captures more linguistic information as expected. References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In ICLR. Ankur Bapna, Mia Chen, Orhan Firat, Yuan Cao, and Yonghui Wu. 2018. Training deeper neural machine translation models with transparent attention. In EMNLP. Heeyoul Choi, Kyunghyun Cho, and Yoshua Bengio. 2017. Context-dependent word representation for neural machine translation. Computer Speech & Language, 45:149–160. Alexis Conneau, Germ´an Kruszewski, Guillaume Lample, Lo¨ıc Barrault, and Marco Baroni. 2018. What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic proper- ties. In ACL. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. 2018. Univer- sal transformers. arXiv preprint arXiv:1807.03819. Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Shuming Shi, and Tong Zhang. 2018. Exploiting deep representa- tions for neural machine translation. In EMNLP. Zi-Yi Dou, Zhaopeng Tu, Xing Wang, Longyue Wang, Shuming Shi, and Tong Zhang. 2019. Dynamic layer aggregation for neural machine translation with routing-by-agreement. In AAAI. George Foster and Roland Kuhn. 2007. Mixture-model adaptation for smt. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 128–135. Association for Computational Linguis- tics. Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and Zhaopeng Tu. 2019. Modeling recurrence for transformer. In NAACL. Eva Hasler, Barry Haddow, and Philipp Koehn. 2014. Combining domain and topic adaptation for smt. In Proceedings of AMTA, volume 1, pages 139–151. Mohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber, and Hal Daum´e III. 2015. Deep unordered compo- sition rivals syntactic methods for text classiﬁcation. In ACL. Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. In ACL. Philipp Koehn. 2004. Statistical Signiﬁcance Tests for Machine Translation Evaluation. In EMNLP. Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, and", "meta": {"paper_id": "1906.01268v1", "chunk_index": 6, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_7", "text": "rivals syntactic methods for text classiﬁcation. In ACL. Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. In ACL. Philipp Koehn. 2004. Statistical Signiﬁcance Tests for Machine Translation Evaluation. In EMNLP. Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, and Zhaopeng Tu. 2019. Infor- mation aggregation for multi-head attention with routing-by-agreement. In NAACL. Junyang Lin, Xu Sun, Xuancheng Ren, Shuming Ma, Jinsong Su, and Qi Su. 2018. Deconvolution-based global decoding for neural machine translation. In COLING. Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured self-attentive sentence embedding. In ICLR. Fandong Meng, Zhengdong Lu, Mingxuan Wang, Hang Li, Wenbin Jiang, and Qun Liu. 2015. Encod- ing source language with convolutional neural net- work for machine translation. In ACL. Eugene A Nida. 1969. Science of translation. Lan- guage, pages 483–498. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep- resentations. In NAACL. Alessandro Raganato and J¨org Tiedemann. 2018. An analysis of encoder representations in transformer- based machine translation. In EMNLP 2018 work- shop BlackboxNLP. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In ACL. Yanyao Shen, Xu Tan, Di He, Tao Qin, and Tie-Yan Liu. 2018. Dense information ﬂow for neural ma- chine translation. In NAACL. Xing Shi, Inkit Padhi, and Kevin Knight. 2016. Does string-based neural mt learn source syntax? In EMNLP. Zhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. 2018. Learning to remember translation his- tory with a continuous cache. TACL. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In NIPS. Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017a. Exploiting cross-sentence context for neural machine translation. In EMNLP. Qiang Wang, Fuxue Li, Tong Xiao, Yanyang Li, Yin- qiao Li, and Jingbo Zhu. 2018. Multi-layer repre- sentation fusion for neural machine translation. In COLING. Rui Wang, Andrew Finch, Masao Utiyama, and Ei- ichiro Sumita. 2017b. Sentence embedding for neu- ral machine translation domain adaptation. In ACL. Xinyan Xiao, Deyi Xiong, Min Zhang, Qun Liu, and Shouxun Lin. 2012. A topic similarity model for hierarchical phrase-based translation. Deyi Xiong, Min Zhang, and Xing Wang. 2015. Topic-based coherence modeling for statistical ma- chine translation. IEEE/ACM Transactions on Au- dio, Speech and Language Processing (TASLP), 23(3):483–493. Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao, Xing Wang, and Zhaopeng Tu. 2019. Context-aware self-attention networks. In AAAI. Baosong Yang, Zhaopeng Tu, Derek F. Wong, Fan- dong Meng, Lidia S. Chao, and Tong Zhang. 2018. Modeling localness for self-attention networks. In EMNLP. Jiajun Zhang, Dakun Zhang, and Jie Hao. 2015. Local translation prediction with global sentence represen- tation. In IJCAI. Jian Zhang, Liangyou Li, Andy Way, and Qun Liu. 2016. Topic-informed neural machine translation. In COLING. Zaixiang Zheng, Hao Zhou, Shujian Huang, Lili Mou, Xinyu Dai, Jiajun Chen, and Zhaopeng Tu. 2018. Modeling past", "meta": {"paper_id": "1906.01268v1", "chunk_index": 7, "filename": "1906.01268v1.pdf", "chunk_length": 512}}
{"id": "1906.01268v1_chunk_8", "text": "Dakun Zhang, and Jie Hao. 2015. Local translation prediction with global sentence represen- tation. In IJCAI. Jian Zhang, Liangyou Li, Andy Way, and Qun Liu. 2016. Topic-informed neural machine translation. In COLING. Zaixiang Zheng, Hao Zhou, Shujian Huang, Lili Mou, Xinyu Dai, Jiajun Chen, and Zhaopeng Tu. 2018. Modeling past and future for neural machine trans- lation. TACL.", "meta": {"paper_id": "1906.01268v1", "chunk_index": 8, "filename": "1906.01268v1.pdf", "chunk_length": 58}}
{"id": "1906.01282v1_chunk_0", "text": "Lattice-Based Transformer Encoder for Neural Machine Translation Fengshun Xiao1,2, Jiangtong Li2,3, Hai Zhao1,2,∗, Rui Wang4, Kehai Chen4 1Department of Computer Science and Engineering, Shanghai Jiao Tong University 2Key Laboratory of Shanghai Education Commission for Intelligent Interaction and Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China 3College of Zhiyuan, Shanghai Jiao Tong University, China 4National Institute of Information and Communications Technology (NICT) {felixxiao, keep moving-lee}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, {wangrui, khchen}@nict.go.jp Abstract Neural machine translation (NMT) takes deterministic sequences for source representations. However, either word- level or subword-level segmentations have multiple choices to split a source sequence with different word segmentors or different subword vocabulary sizes. We hypothesize that the diversity in segmentations may affect the NMT performance. To integrate different segmentations with the state-of-the-art NMT model, Transformer, we propose lattice-based encoders to explore effective word or subword representation in an automatic way during training. We propose two methods: 1) lattice positional encoding and 2) lattice-aware self-attention. These two methods can be used together and show complementary to each other to further improve translation performance. Experiment results show superiorities of lattice-based encoders in word-level and subword-level representations over conventional Transformer encoder. 1 Introduction Neural machine translation (NMT) has achieved great progress with the evolvement of model structures under an encoder-decoder framework (Sutskever et al., 2014; Bahdanau et al., 2014). Recently, the self-attention based Transformer model has achieved state-of-the- art performance on multiple language pairs (Vaswani et al., 2017; Marie et al., 2018). Both representations of source and target sentences in ∗Corresponding author. This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100) and key projects of National Natural Science Foundation of China (No. U1836222 and No. 61733011). Rui Wang was partially supported by JSPS grant-in-aid for early-career scientists (19K20354): “Unsupervised Neural Machine Translation in Universal Scenarios” and NICT tenure-track researcher startup fund “Toward Intelligent Machine Translation”. v0 v1 v2 v3 mao-yi-fa-zhan ju fu-zong-cai v0 v1 v2 v3 mao-yi fa-zhan-ju fu-zong-cai v0 v1 v2 v3 v4 v5 mao-yi fa-zhan ju fu zong-cai (1)Segmentaion 1 (2)Segmentaion 2 (3)Segmentation 3 (4)Lattice v8 mao yi fa zhan ju fu zong cai v7 v6 v5 v4 v3 v2 v1 v0 e0:2:mao-yi e2:4:fa-zhan e4:5:ju e5:6:fu e6:8:zong-cai e2:5:fa-zhan-ju e0:4:mao-yi-fa-zhan e5:8:fu-zong-cai c1 c2 c3 c4 c5 c6 c7 c8 Figure 1: Incorporating three different segmentation for a lattice graph. The original sentence is “mao-yi- fa-zhan-ju-fu-zong-cai”. In Chinese it is “贸易发展局 副总裁”. In English it means “The vice president of Trade Development Council” NMT can be factorized in character (Costa-Jussa and Fonollosa, 2016), word (Sutskever et al., 2014), or subword (Sennrich et al., 2015) level. However, only using 1-best segmentation as inputs limits NMT encoders to express source sequences sufﬁciently and reliably. Many East Asian languages, including Chinese are written without explicit word boundary, so that their sentences need to be segmented into words ﬁrstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word", "meta": {"paper_id": "1906.01282v1", "chunk_index": 0, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_1", "text": "to be segmented into words ﬁrstly (Zhao et al., 2019; Cai et al., 2017; Cai and Zhao, 2016; Zhao et al., 2013; Zhao and Kit, 2011). By different segmentors, each sentence can be segmented into multiple forms as shown in Figure 1. Even for those alphabetical languages with clear word boundary like English, there is still an issue about selecting a proper subword vocabulary size, which determines the segmentation granularities for word representation. In order to handle this problem, Morishita et al. (2018) used hierarchical subword features to represent sequence with different subword granularities. Su et al. (2017) proposed the ﬁrst word-lattice based recurrent neural network arXiv:1906.01282v1 [cs.CL] 4 Jun 2019 (RNN) encoders which extended Gated Recurrent Units (GRUs) (Cho et al., 2014) to take in multiple sequence segmentation representations. Sperber et al. (2017) incorporated posterior scores to Tree-LSTM for building a lattice encoder in speech translation. All these existing methods serve for RNN-based NMT model, where lattices can be formulized as directed graphs and the inherent directed structure of RNN facilitates the construction of lattice. Meanwhile, the self- attention mechanism is good at learning the dependency between characters in parallel, which can partially compare and learn information from multiple segmentations (Cherry et al., 2018). Therefore, it is challenging to directly apply the lattice structure to Transformer. In this work, we explore an efﬁcient way of integrating lattice into Transformer. Our method can not only process multiple sequences segmented in different ways to improve translation quality, but also maintain the characteristics of parallel computation in the Transformer. 2 Background 2.1 Transformer Transformer stacks self-attention and point-wise, fully connected layers for both encoders and decoders. Decoder layers also have another sub- layer which performs attention over the output of the encoder. Residual connections around each layer are employed followed by layer normalization (Ba et al., 2016). To make use of the order of the sequence, Vaswani et al. (2017) proposed Positional Encodings to indicate the absolute or relative position of tokens in input sequence which are calculated as: p(j,2i) = sin(j/100002i/d) p(j,2i+1) = cos(j/100002i/d), where j is the position, i is the dimension and d is the model dimension. Then positional encodings p1:M = {p1, ..., pM} are added to the embedding of each token t1:M = {t1, ..., tM} and are propagated to higher layers via residual connections. 2.2 Self-Attention Transformer employs H attention heads to perform self-attention over a sequence individually and ﬁnally applies concatenation and linear transformation to the results from Conditions Explanation lad i < j = p < q ei:j is left adjacent to ep:q. rad p < q = i < j ei:j is right adjacent to ep:q. inc i ≤p < q ≤j ei:j includes ep:q. ind p ≤i < j ≤q ei:jis included in ep:q. its i < p < j < q or ei:j is intersected with ep:q. p < i < q < j pre i < j < p < q ei:j is preceding edge to ep:q. suc p < q < i < j ei:j is succeeding edge to ep:q. Table", "meta": {"paper_id": "1906.01282v1", "chunk_index": 1, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_2", "text": "its i < p < j < q or ei:j is intersected with ep:q. p < i < q < j pre i < j < p < q ei:j is preceding edge to ep:q. suc p < q < i < j ei:j is succeeding edge to ep:q. Table 1: Relations possibly satisﬁed by any two different edges ei:j and ep:q in the lattice. Note that two equal signs cannot stand at the same time in condition inequality for inc and ind. . each head, which is called multi-head attention (Vaswani et al., 2017). Every single head attention in multi-head attention is calculated in a scaled dot product form: uij = (tiW Q)(tjW K)T √ d , (1) where d is the model dimension, t1:M is the input sequence and uij are normalized by a softmax function: αij = exp(uij) PM k=1 exp(uik) , (2) and αij are used to calculate the ﬁnal output hidden representations: oi = M X j=1 αij(tjW V ), (3) where o1:M is outputs and W Q,W K, and W V are learnable projections matrices for query, key, and value in a single head, respectively. 3 Models 3.1 Lattices Lattices can represent multiple segmentation sequences in a directed graph, as they merge the same subsequence of all candidate subsequences using a compact way. As shown in Figure 1, we follow Su et al. (2017) to apply different segmentator to segment an element1 sequence c1:N = {c1, c2, ..., cN} into different word or subword sequences to construct a lattice G = ⟨V, E⟩, a directed, connected, and acyclic graph, where V is node set and E is edge 1Character for word lattice and minimum subword unit in our predeﬁned subword segmentations for subword lattice. Input Embedding Lattice sequence Inputs Lattice Positional Encoding Lattice-aware self-attention Add & Norm Add & Norm Feed Forward Hidden representations N x t1 t2 t3 t4 t5 t1 t2 t3 t4 t5 Figure 2: The architecture of lattice-based Transformer encoder. Lattice positional encoding is added to the embeddings of lattice sequence inputs. Different colors in lattice-aware self-attention indicate different relation embeddings. set, node vi ∈V denotes the gap between ci and ci+1, edge ei:j ∈E departing from vi and arrives at vj (i < j) indicates a possible word or subword unit covering subsequence ci+1:j. All the edges in the lattice G are the actual input tokens for NMT. For two different edges ei:j and ep:q, all possible relations can be enumerated as in Table 1. 3.2 Lattice-Based Encoders We place all edges E in the lattice graph into an input sequence t1:M = {t1, t2, ..., tM} for Transformer; then we modify the positional encoding to indicate the positional information of input tokens, namely all edges in the lattice graph. In addition, we propose a lattice-aware self- attention to directly represent position relationship among tokens. The overall architecture is shown in Figure 2. Lattice Positional Encoding (LPE) Original positional encoding indicates the order of the sequence in an ascending form {p1, p2, ..., pM}. We hypothesize that increasing positional encodings can", "meta": {"paper_id": "1906.01282v1", "chunk_index": 2, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_3", "text": "addition, we propose a lattice-aware self- attention to directly represent position relationship among tokens. The overall architecture is shown in Figure 2. Lattice Positional Encoding (LPE) Original positional encoding indicates the order of the sequence in an ascending form {p1, p2, ..., pM}. We hypothesize that increasing positional encodings can indicate the order of sequential sentence. As shown in Figure 3, we scan a source sequence by element c1:N = {c1, c2, ..., cN} (for example, ci is character in Figure 3) and record their position p1:N = {p1, p2, ..., pN}. Then we use the positional encoding of the ﬁrst element in lattice edge to represent current token’s position, which can ensure that every edge in each path departing from v0 and arriving at vN in lattice will v8 mao yi fa zhan ju fu zong cai v7 v6 v5 v4 v3 v2 v1 v0 mao-yi:1 fa-zhan:3 ju:5 fu:6 zong-cai:7 fa-zhan-ju:3 mao-yi-fa-zhan:1 fu-zong-cai:6 v0 v1 v2 v3 v4 v5 mao-yi:1 fa-zhan:2 ju:3 fu:4 zong-cai:5 (1)position encodings (2)LPE and LSA 1 2 3 4 5 6 7 8 rad inc inc lad pre its self lad self suc suc suc rad rad lad ind Figure 3: Lattice positional encoding pi+1 (in green) for edge ei:j in the lattice graph and the relation embeddings r in lattice-aware self-attention based on the timestep of token fa-zhan-ju (in red) and fu (in purple). have an increasing positional encoding order. The property mentioned above is easy to prove, since start and end points vi, vj of each edge ei:j strictly satisfy i < j and next edge ej:k will start from vj and thus get a larger positional encoding. Formally, for any input token tk, namely edge ei:j covering elements ci+1:j, positional encoding pi+1 will be used to represent its position and be added to its embedding. Lattice-aware Self-Attention (LSA) We also directly modify self-attention to a lattice-aware way which makes self-attention aware of the relations between any two different edges. We modiﬁed Equations (1) and (3) in the same way of Shaw et al. (2018) to indicate edge relation: eij = (tiW Q)(tjW K + rK ij )T √ d , (4) oi = M X j=1 αij(tjW V + rV ij), (5) where rK ij and rV ij are relation embeddings which are added to the keys and values to indicate relation between input tokens ti and tj, namely edges ep:q and ek:l in lattice graph, respectively. To facilitate parallel computation, we add an additional embedding (self) for a token when it is conducted dot-product attention with itself, so we train eight (seven in Table 1) different relation embeddings aV 1:8 and aK 1:8 as look-up table for keys and values, respectively. rK ij and rV ij can look up for aV 1:8 and aK 1:8 based on the relation between ti and tj. Figure 3 shows an example of embeddings in lattice-aware self-attentions based on the timestep of token fa-zhan-ju and fu. System Input MT05 MT02 MT03 MT04 MT06 MT08 ALL RNN PKU 31.42 34.68 33.08 35.32 31.61 23.58 31.76 CTB 31.38 34.95", "meta": {"paper_id": "1906.01282v1", "chunk_index": 3, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_4", "text": "aK 1:8 based on the relation between ti and tj. Figure 3 shows an example of embeddings in lattice-aware self-attentions based on the timestep of token fa-zhan-ju and fu. System Input MT05 MT02 MT03 MT04 MT06 MT08 ALL RNN PKU 31.42 34.68 33.08 35.32 31.61 23.58 31.76 CTB 31.38 34.95 32.85 35.44 31.75 23.33 31.78 MSR 29.92 34.49 32.06 35.10 31.23 23.12 31.35 Lattice-RNN Lattice 32.40 35.75 34.32 36.50 32.77 24.84 32.95 Transformer PKU 41.67 43.61 41.62 43.66 40.25 31.62 40.24 CTB 41.87 43.72 42.11 43.58 40.41 31.76 40.35 MSR 41.17 43.11 41.38 43.60 39.67 31.02 39.87 Transformer + LPE Lattice 42.37 43.71 42.67 44.43 41.14 32.09 40.93↑ Transformer + LSA 42.28 43.56 42.73 43.81 41.01 32.39 40.77↑ Transformer + LPE + LSA 42.65 44.14 42.24 44.81 41.37 32.98 41.26↑ Table 2: Evaluation of translation performance on NIST Zh-En dataset. RNN and Lattice-RNN results are from (Su et al., 2017). We highlight the highest BLEU score in bold for each set. ↑indicates statistically signiﬁcant difference (p <0.01) from best baseline. Since self-attention is computed parallelly, we generate a matrix with all lattice embeddings in it for each sentence which can be easily incorporated into standard self-attention by matrix multiplication. We use different relation embeddings for different Transformer layers but share the same one between different heads in a single layer. 4 Experiments 4.1 Setup We conducted experiments on the NIST Chinese- English (Zh-En) and IWSLT 2016 English- German (En-De) datasets. The Zh-En corpus consists of 1.25M sentence pairs and the En-De corpus consists of 191K sentence pairs. For Zh- En task, we chose the NIST 2005 dataset as the validation set and the NIST 2002, 2003, 2004, 2006, and 2008 datasets as test sets. For En- De task, tst2012 was used as validation set and tst2013 and tst2014 were used as test sets. For both tasks, sentence pairs with either side longer than 50 were dropped. We used the case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) as the evaluation metric and sign-test (Collins et al., 2005) for statistical signiﬁcance test. For Zh-En task, we followed Su et al. (2017) to use the toolkit2 to train segmenters on PKU, MSR (Emerson, 2005), and CTB corpora (Xue et al., 2005), then we generated word lattices with different segmented training data. Both source and target vocabularies are limited to 30K. For En-De task, we adopted 8K, 16K and 32K 2https://nlp.stanford.edu/software/segmenter.html#Download BPE merge operations (Sennrich et al., 2015) to get different segmented sentences for building subword lattices. 16K BPE merge operations are employed on the target side. We set batch size to 1024 tokens and accumulated gradient 16 times before a back- propagation. During training, we set all dropout to 0.3 and chose the Adam optimizer (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98 and ϵ = 10−9 for parameters tuning. During decoding, we used beam search algorithm and set the beam size to 20. All other conﬁgurations were the same with Vaswani et al. (2017). We implemented our model based on the OpenNMT (Klein et al., 2017) and trained and", "meta": {"paper_id": "1906.01282v1", "chunk_index": 4, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_5", "text": "β2 = 0.98 and ϵ = 10−9 for parameters tuning. During decoding, we used beam search algorithm and set the beam size to 20. All other conﬁgurations were the same with Vaswani et al. (2017). We implemented our model based on the OpenNMT (Klein et al., 2017) and trained and evaluated all models on a single NVIDIA GeForce GTX 1080 Ti GPU. 4.2 Overall Performance From Table 2, we see that our LPE and LSA models both outperform the Transformer baseline model of 0.58 and 0.42 BLEU respectively. When we combine LPE and LSA together, we get a gain of 0.91 BLEU points. Table 3 shows that our method also works well on the subword level. The base Transformer system has about 90M parameters and our LPE and LSA models introduce 0 and 6k parameters over it, respectively, which shows that our lattice approach improves Transformer with little parameter accumulation. During training, base Transformer performs about 0.714 steps per second while LPE + LSA model can process around 0.328. As lattice-based method usually seriously slows down the training, our lattice design and implementation over the Transformer only shows moderate efﬁciency System Input tst2012 tst2013 tst2014 RNN 16k 26.24 28.22 24.17 Transformer 8k 27.31 29.56 25.57 16k 27.35 29.02 25.12 32k 27.15 28.61 24.88 + LPE Lattice 27.34 29.48 25.88↑ + LSA 27.44 29.73↑ 25.65 + LPE + LSA 27.76 30.28↑ 26.22↑ Table 3: Evaluation of translation performance on IWSLT2016 En-De dataset. RNN results are reported from Morishita et al. (2018). ↑indicates statistically signiﬁcant difference (p <0.01) from best baseline. . Systems PE PE + LSA ALL 40.54 40.90 Table 4: Translation performance (BELU score) with normal positional encodings and normal positional encodings with LSA model on NIST Zh-En dataset. reduction. 4.3 Analysis3 Effect of Lattice-Based Encoders To show the effectiveness of our method, we placed all edges in the lattice of a single sequence in a relative right order based on their ﬁrst character, then we applied normal positional encodings (PE) to the lattice inputs on our base Transformer model. As shown in Table 4, our LPE and LSA method outperforms normal positional encodings by 0.39 and 0.23 BLEU respectively which shows that our methods are effective. Complementary of LPE and LSA Our LPE method allows edges in all paths in an increasing positional encoding order which seems to focus on long-range order but ignore local disorder. While our LSA method treats all preceding and succeeding edges equally which seems to address local disorder better but ignore long-range order. To show the complementary of these two methods, we also placed all edges of lattice in a single sequence in a relative right order based on their ﬁrst character and use normal positional encodings and our LSA method; we obtained a BLEU of 40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneﬁcial to our LSA model. 3All analysis experiments conducted on NIST dataset. 5 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et", "meta": {"paper_id": "1906.01282v1", "chunk_index": 5, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_6", "text": "40.90 which is 0.13 higher than single LSA model. From this, we can see that long-range position information is indeed beneﬁcial to our LSA model. 3All analysis experiments conducted on NIST dataset. 5 Related Work Neural network based methods have been applied to several natural language processing tasks (Li et al., 2018; Zhang et al., 2019; Chen et al., 2018, 2017; Li et al., 2019; He et al., 2018; Zhou and Zhao, 2019), especially to NMT (Bahdanau et al., 2015; Wang et al., 2017a,b, 2018; Wang et al., 2018; Zhang et al., 2018; Zhang and Zhao, 2019). Our work is related to the source side representations for NMT. Generally, the NMT model uses the word as a basic unit for source sentences modeling. In order to obtain better source side representations and avoid OOV problems, recent research has modeled source sentences at character level (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Yang et al., 2016; Lee et al., 2016), subword level (Sennrich et al., 2015; Kudo, 2018; Wu and Zhao, 2018) and mixed character-word level (Luong and Manning, 2016). All these methods show better translation performance than the word level model. As models mentioned above only use 1-best segmentation as inputs, lattice which can pack many different segmentations in a compact form has been widely used in statistical machine translation (SMT) (Xu et al., 2005; Dyer et al., 2008) and RNN-based NMT (Su et al., 2017; Sperber et al., 2017). To enhance the representaions of the input, lattice has also been applied in many other NLP tasks such as named entity recognition (Zhang and Yang, 2018), Chinese word segmentation (Yang et al., 2019) and part-of-speech tagging (Jiang et al., 2008; Wang et al., 2013). 6 Conclusions In this paper, we have proposed two methods to incorporate lattice representations into Transformer. Experimental results in two datasets on word-level and subword-level respectively validate the effectiveness of the proposed approaches. Different from Veliˇckovi´c et al. (2017), our work also provides an attempt to encode a simple labeled graph into Transformer and can be used in any tasks which need Transformer encoder to learn sequence representation. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the 3rd International Conference on Learning Representations (ICLR 2015). Deng Cai and Hai Zhao. 2016. Neural word segmentation learning for Chinese. arXiv preprint arXiv:1606.04300. Deng Cai, Hai Zhao, Zhisong Zhang, Yuan Xin, Yongjian Wu, and Feiyue Huang. 2017. Fast and accurate neural word segmentation for Chinese. arXiv preprint arXiv:1704.07047. Kehai Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro Sumita, and Tiejun Zhao. 2017. Neural machine translation with source dependency representation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), pages 2846– 2852. Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun", "meta": {"paper_id": "1906.01282v1", "chunk_index": 6, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_7", "text": "Chen, Rui Wang, Masao Utiyama, Lemao Liu, Akihiro Tamura, Eiichiro Sumita, and Tiejun Zhao. 2017. Neural machine translation with source dependency representation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), pages 2846– 2852. Kehai Chen, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Tiejun Zhao. 2018. Syntax-directed attention for neural machine translation. In Proceedings of the 32nd AAAI Conference on Artiﬁcial Intelligence (AAAI 2018), pages 4792– 4799. Colin Cherry, George Foster, Ankur Bapna, Orhan Firat, and Wolfgang Macherey. 2018. Revisiting character-based neural machine translation with capacity and compression. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018), pages 4295– 4305. Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078. Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 531–540. Marta R Costa-Jussa and Jos´e AR Fonollosa. 2016. Character-based neural machine translation. arXiv preprint arXiv:1603.00810. Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing word lattice translation. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL 2008), pages 1012–1020. Thomas Emerson. 2005. The second international Chinese word segmentation bakeoff. In Proceedings of the fourth SIGHAN workshop on Chinese Language Processing, pages 123–133. Shexia He, Zuchao Li, Hai Zhao, and Hongxiao Bai. 2018. Syntax for semantic role labeling, to be, or not to be. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 2061–2071. Wenbin Jiang, Haitao Mi, and Qun Liu. 2008. Word lattice reranking for Chinese word segmentation and part-of-speech tagging. In Proceedings of the 22nd International Conference on Computational Linguistics (COLING 2008), pages 385–392. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander M Rush. 2017. Opennmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810. Taku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 66– 75. Jason Lee, Kyunghyun Cho, and Thomas Hofmann. 2016. Fully character-level neural machine translation without explicit segmentation. arXiv preprint arXiv:1610.03017. Zuchao Li, Jiaxun Cai, Shexia He, and Hai Zhao. 2018. Seq2seq dependency parsing. In Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018), pages 3203–3214. Zuchao Li, Shexia He, Hai Zhao, Yiqing Zhang, Zhuosheng Zhang, Xi Zhou, and Xiang Zhou. 2019. Dependency or span, end-to-end uniform semantic role labeling. In Proceedings of the 33rd AAAI Conference on Artiﬁcial Intelligence (AAAI 2019). Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W. Black. 2015. Character-based neural machine translation. arXiv preprint arXiv:1511.04586. Minh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL", "meta": {"paper_id": "1906.01282v1", "chunk_index": 7, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_8", "text": "2019). Wang Ling, Isabel Trancoso, Chris Dyer, and Alan W. Black. 2015. Character-based neural machine translation. arXiv preprint arXiv:1511.04586. Minh-Thang Luong and Christopher D. Manning. 2016. Achieving open vocabulary neural machine translation with hybrid word-character models. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), pages 1054–1063. Benjamin Marie, Rui Wang, Atsushi Fujita, Masao Utiyama, and Eiichiro Sumita. 2018. Nict’s neural and statistical machine translation systems for the wmt18 news translation task. In Proceedings of the Third Conference on Machine Translation, Volume 2: Shared Task Papers, pages 453–459. Makoto Morishita, Jun Suzuki, and Masaaki Nagata. 2018. Improving neural machine translation by incorporating hierarchical subword features. In Proceedings of the 27th International Conference on Computational Linguistics (COLING 2018), pages 618–629. Kishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL 2002), pages 311– 318. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Matthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2017. Neural lattice-to-sequence models for uncertain inputs. arXiv preprint arXiv:1704.00559. Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, and Yang Liu. 2017. Lattice- based recurrent neural network encoders for neural machine translation. In Proceedings of the 31st AAAI Conference on Artiﬁcial Intelligence (AAAI 2017), pages 3302–3308. Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. In Proceedings of the 28th Conference on Neural Information Processing Systems (NIPS 2014), pages 3104–3112. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the 31stst Conference on Neural Information Processing Systems (NIPS 2017), pages 5998–6008. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint arXiv:1710.10903. Rui Wang, Andrew Finch, Masao Utiyama, and Eiichiro Sumita. 2017a. Sentence embedding for neural machine translation domain adaptation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL 2017), pages 560–566. Rui Wang, Masao Utiyama, Andrew Finch, Lemao Liu, Kehai Chen, and Eiichiro Sumita. 2018. Sentence selection and weighting for neural machine translation domain adaptation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26:1727–1741. Rui Wang, Masao Utiyama, Lemao Liu, Kehai Chen, and Eiichiro Sumita. 2017b. Instance weighting for neural machine translation domain adaptation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP 2017), pages 1482–1488. Rui Wang, Masao Utiyama, and Eiichiro Sumita. 2018. Dynamic sentence sampling for efﬁcient training of neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 298– 304. Zhiguo Wang, Chengqing Zong, and Nianwen Xue. 2013. A lattice-based framework for joint Chinese word segmentation, POS tagging and parsing. In Proceedings of the 51st", "meta": {"paper_id": "1906.01282v1", "chunk_index": 8, "filename": "1906.01282v1.pdf", "chunk_length": 512}}
{"id": "1906.01282v1_chunk_9", "text": "for efﬁcient training of neural machine translation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 298– 304. Zhiguo Wang, Chengqing Zong, and Nianwen Xue. 2013. A lattice-based framework for joint Chinese word segmentation, POS tagging and parsing. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 623–627. Yingting Wu and Hai Zhao. 2018. Finding better subword segmentation for neural machine translation. In The Seventeenth China National Conference on Computational Linguistics (CCL 2018), pages 53–64. Jia Xu, Evgeny Matusov, Richard Zens, and Hermann Ney. 2005. Integrated Chinese word segmentation in statistical machine translation. In International Workshop on Spoken Language Translation (IWSLT 2005). Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. 2005. The Penn Chinese TreeBank: Phrase structure annotation of a large corpus. Natural language engineering, 11(2):207–238. Jie Yang, Yue Zhang, and Shuailong Liang. 2019. Subword encoding in lattice LSTM for Chinese word segmentation. In Proceedings of the 17th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL 2019). Zhen Yang, Wei Chen, Feng Wang, and Bo Xu. 2016. A character-aware encoder for neural machine translation. In Proceedings of the 26th International Conference on Computational Linguistics (COLING 2016), pages 3063–3070. Huan Zhang and Hai Zhao. 2019. Minimum divergence vs. maximum margin: An empirical comparison on seq2seq models. In Proceedings of the Seventh International Conference on Learning Representations (ICLR 2019). Yue Zhang and Jie Yang. 2018. Chinese NER using lattice LSTM. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL 2018), pages 1554–1564. Zhisong Zhang, Rui Wang, Masao Utiyama, Eiichiro Sumita, and Hai Zhao. 2018. Exploring recombination for efﬁcient decoding of neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP 2018), pages 4785–4790. Zhuosheng Zhang, Yafang Huang, and Hai Zhao. 2019. Neural-based pinyin-to-character conversion with adaptive vocabulary. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019). Hai Zhao, Deng Cai, Changning Huang, and Chunyu Kit. 2019. Chinese word segmentation: Another decade review (2007-2017). arXiv preprint arXiv:1901.06079. Hai Zhao and Chunyu Kit. 2011. Integrating unsupervised and supervised word segmentation: The role of goodness measures. Information Sciences, 181(1):163–183. Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-Liang Lu. 2013. An empirical study on word segmentation for Chinese machine translation. In International Conference on Intelligent Text Processing and Computational Linguistics (CICLing 2013), pages 248–263. Junru Zhou and Hai Zhao. 2019. Head-driven phrase structure grammar parsing on Penn Treebank. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL 2019).", "meta": {"paper_id": "1906.01282v1", "chunk_index": 9, "filename": "1906.01282v1.pdf", "chunk_length": 439}}
{"id": "1906.01359v1_chunk_0", "text": "NNE: A Dataset for Nested Named Entity Recognition in English Newswire Nicky Ringland1 † Xiang Dai1,2 ‡ Ben Hachey1,3 Sarvnaz Karimi2 Cecile Paris2 James R. Curran1 1University of Sydney, Sydney, Australia 2CSIRO Data61, Sydney, Australia 3Digital Health CRC, Sydney, Australia †nicky.ringland@sydney.edu.au ‡dai.dai@csiro.au Abstract Named entity recognition (NER) is widely used in natural language processing applica- tions and downstream tasks. However, most NER tools target ﬂat annotation from popu- lar datasets, eschewing the semantic informa- tion available in nested entity mentions. We describe NNE—a ﬁne-grained, nested named entity dataset over the full Wall Street Journal portion of the Penn Treebank (PTB). Our an- notation comprises 279,795 mentions of 114 entity types with up to 6 layers of nesting. We hope the public release of this large dataset for English newswire will encourage development of new techniques for nested NER. 1 Introduction Named entity recognition—the task of identifying and classifying entity mentions in text—plays a crucial role in understanding natural language. It is used for many downstream language process- ing tasks, e.g., coreference resolution, question answering, summarization, entity linking, relation extraction and knowledge base population. How- ever, most NER tools are designed to capture ﬂat mention structure over coarse entity type schemas, reﬂecting the available annotated datasets. Focusing on ﬂat mention structures ignores im- portant information that can be useful for down- stream tasks. Figure 1 includes examples of nested named entities illustrating several phenomena: • Entity-entity relationships can be embedded in nested mentions. For instance, the location of the ‘Ontario Supreme Court’ is indicated by the embedded STATE mention ‘Ontario’; • Entity attribute values can be embedded in nested mentions. For instance, the title is the embedded ROLE ‘Former U.N. Ambassador’, which also encodes the employment relation ... the Ontario Supreme Court said it will postpone ... state government Former U.N. Ambassador Jeane Kirkpatrick ... org:other role first name role per role per ... this wealthy Southern California beach community ... state region Figure 1: Example nested mentions in NNE. between the PERSON ‘Jane Kirkpatrick‘ and ORG ‘U.N.’; • Part-whole relationships can be encoded in nested mention structure. For instance, the REGION ‘Southern California’ is part of the STATE ‘California’. Recent work has demonstrated increasing in- terest in nested entity structure, including lo- cal approaches (Xu et al., 2017; Sohrab and Miwa, 2018), hypergraph-based approaches (Lu and Roth, 2015; Muis and Lu, 2017; Katiyar and Cardie, 2018; Wang and Lu, 2018), cascaded ap- proaches (Alex et al., 2007; Ju et al., 2018), and parsing approaches (Finkel and Manning, 2009; Wang et al., 2018). See Dai (2018) for a survey. Yet these techniques have seen little translation from the research literature to toolsets or down- stream applications. To facilitate ongoing research on nested NER, we introduce NNE—a large, manually- annotated, nested named entity dataset over En- glish newswire. This new annotation layer over the Wall Street Journal portion of the PTB includes 279,795 mentions. All mentions are annotated, including nested structures with depth as high as arXiv:1906.01359v1 [cs.CL] 4 Jun 2019 six layers. A ﬁne-grained entity type schema is used, extending the ﬂat BBN", "meta": {"paper_id": "1906.01359v1", "chunk_index": 0, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_1", "text": "over En- glish newswire. This new annotation layer over the Wall Street Journal portion of the PTB includes 279,795 mentions. All mentions are annotated, including nested structures with depth as high as arXiv:1906.01359v1 [cs.CL] 4 Jun 2019 six layers. A ﬁne-grained entity type schema is used, extending the ﬂat BBN (Weischedel and Brunstein, 2005) annotation from 64 to 114 entity types. We are publicly releasing the standoff anno- tations along with detailed annotation guidelines and scripts for knitting annotations onto the under- lying PTB corpus.1 Benchmark results using re- cent state-of-the-art approaches demonstrate that good accuracy is possible, but complexity and run time are open challenges. As a new layer over the already rich collection of PTB annotations, NNE provides an opportunity to explore joint modelling of nested NER and other tasks at an unprecedented scale and detail. 2 The NNE dataset Annotation Scheme: BBN (Weischedel and Brunstein, 2005) is a pronoun coreference and en- tity type corpus, annotated with 64 types of enti- ties, numerical and time expressions. We use its ﬂat entity schema as a starting point to design our schema. We analyzed existing BBN annotations to develop and automatically apply structured pre- annotation for predictable entity types. Additional ﬁne-grained categories and further structural ele- ments of entities, inspired by Sekine et al. (2002) and Nothman et al. (2013), are used to augment the BBN schema. We adhere to the following general principles when annotating nested named entities in the corpus: • Annotate all named entities, all time and date (TIMEX) and numerical (NUMEX) entities, in- cluding all non-sentence initial words in title case, and instances of proper noun mentions that are not capitalized. • Annotate all structural elements of entities. These elements could be other entities, such as ‘Ontario’ (STATE) in ‘Ontario Supreme Court’ (GOVERNMENT), or structural com- ponents such as ‘40’ (CARDINAL) and ‘miles’ (UNIT) in ‘40 miles’ (QUANTITY:1D), as well as the internal structure induced by syntactic elements, such as coordination. • Add consistent substructure to avoid spurious ambiguity. For example, the token ‘Toronto’, which is a CITY, would be labeled as part 1https://github.com/nickyringland/nested named entities of an ORG:EDU organization span ‘Univer- sity of Toronto’. We add layers of annota- tions to allow each token to be annotated as consistently as possible, e.g., [University of [Toronto]CITY]ORG:EDU. • Add additional categories to avoid category confusion. Some entities are easy to identify, but difﬁcult to categorize consistently. For in- stance, a hotel (or any business at a ﬁxed lo- cation) has both organizational and locative qualities, or is at least treated metonymously as a location. Rather than requiring annota- tors to make an ambiguous decision, we elect to add category HOTEL to simplify the indi- vidual annotation decision. We also apply this principle when adding MEDIA, FUND, and BUILDING categories. • Pragmatic annotation. Many annotation de- cisions are ambiguous and difﬁcult, thus may require substantial research. For instance, knowing that ‘The Boeing Company’ was named after founder ‘William E. Boeing’ would allow us to annotate ‘Boeing’ with an embedded PERSON entity. However, this does not apply for other companies,", "meta": {"paper_id": "1906.01359v1", "chunk_index": 1, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_2", "text": "categories. • Pragmatic annotation. Many annotation de- cisions are ambiguous and difﬁcult, thus may require substantial research. For instance, knowing that ‘The Boeing Company’ was named after founder ‘William E. Boeing’ would allow us to annotate ‘Boeing’ with an embedded PERSON entity. However, this does not apply for other companies, such as ‘Sony Corporation’. To let annotation deci- sions be made without reference to external knowledge, we label all tokens that seem to be the names of people as NAME, regardless of whether they are actually a person’s name. Entity types and mention frequencies can be found in Appendix A. See Ringland (2016) for an- notation guidelines and extended discussion of an- notation decisions. Annotation Process: Although some existing annotation tools allow nested structures (e.g., Brat (Stenetorp et al., 2012)), we built a custom tool that allowed us to create a simple and fast way to add layers of entities, and suggest reusing exist- ing structured annotations for the same span. Using the annotations from BBN as underly- ing annotations, the annotator is shown a screen with the target sentence, as well as the previous and next sentences, if any. A view of the whole article is also possible to help the annotator with contextual cues. When annotators select a span, they are prompted with suggestions based on their own previous annotations, and common entities. Some entities are repeated frequently in an article, Depth Number % Three most frequent categories 1 118,525 45.5 CORP (22,752), DATE (15,927), PER (13,460) 2 106,144 40.8 CARDINAL (19,834), NAME (18,640), UNIT (14,871) 3 31,573 12.1 CARDINAL (11,697), MULT (5,859), NAME (3,450) 4 3,813 1.5 CARDINAL (1,650), MULT (1,041), UNIT (400) 5 327 0.1 CARDINAL (154), MULT (96), UNIT (51) 6 4 0.0 UNIT (1), CITY-STATE (1), MULT (1) Table 1: Number of spans at each layer of nesting with their most frequent categories. or over many articles in the corpus. The annotation tool allows a user to add a speciﬁed annotation to all strings matching those tokens in the same arti- cle, or in all articles. Four annotators, each with a background in lin- guistics and/or computational linguistics were se- lected and briefed on the annotation task and pur- pose. The WSJ portion of the PTB consists of 25 sections (00–24). Each annotator started with a subset of section 00 as annotation training, and was given feedback before moving on to other sec- tions. Weekly meetings were held with all annota- tors to discuss ambiguities in the guidelines, gaps in the annotation categories, edge cases and am- biguous entities and to resolve discrepancies. Total annotation time for the corpus was 270 hours, split between the four annotators. Sections 00 and 23 were doubly annotated, and section 02 was annotated by all four annotators. An addi- tional 17 hours was used for adjudicating these sections annotated by multiple annotators. Dataset Analysis: The resulting NNE dataset includes a large number of entity mentions of sub- stantial depth, with more than half of mentions oc- curring inside another mentions. Of the 118,525 top-level entity mentions, 47,020 (39.6%) do not", "meta": {"paper_id": "1906.01359v1", "chunk_index": 2, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_3", "text": "17 hours was used for adjudicating these sections annotated by multiple annotators. Dataset Analysis: The resulting NNE dataset includes a large number of entity mentions of sub- stantial depth, with more than half of mentions oc- curring inside another mentions. Of the 118,525 top-level entity mentions, 47,020 (39.6%) do not have any nested structure embedded. The re- maining 71,505 mentions contain 161,270 men- tions, averaging 2.25 structural mentions per each of these top-layer entity mentions. Note that one span can be assigned multiple entity types. For example, the span ‘1993’ can be annotated as both DATE and YEAR. In NNE, 19,144 out of 260,386 total spans are assigned multiple types. Table 1 lists the number of spans occurring at each depth. To measure how clearly the annotation guidelines delineate each category, and how reliable our an- notations are, inter-annotator agreement was cal- culated using annotations on Section 02, which was annotated by all four annotators. An adju- dicated version was created by deciding a correct existing candidate label from within the four pos- sibilities, or by adjusting one of them on a token level. For the purposes of inter-annotator agree- ment, a tag stack is calculated for each word, es- sentially ﬂattening each token’s nested annotation structure into one label. For example, the tag of to- ken ‘California’ in the third sentence of Figure 1 is STATE REGION, while ‘beach’ is O O. Agreement using Fleiss’ kappa over all tokens is 0.907. Con- sidering only tokens that are part of at least one mention according to at least one annotator, Fleiss’ kappa is 0.832. Both results are above the 0.8 threshold for good reliability (Carletta, 1996). Av- erage precision, recall and F1 score across four an- notators with respect to the adjudicated gold stan- dard are 94.3, 91.8 and 93.0. 3 Benchmark results We evaluate three existing NER models on our dataset: (1) the standard BiLSTM-CRF model which can handle only ﬂat entities (Lample et al., 2016); (2) hypergraph-based (Wang and Lu, 2018); and, (3) transition-based (Wang et al., 2018) models. The latter two models were pro- posed to recognize nested mentions. We follow CoNLL evaluation schema in requiring an exact match of mention start, end and entity type (Sang and Meulder, 2003). We use sections 02 as devel- opment set, sections 23 and 24 as test set, and the remaining sections as training set. The model that performs best on the development set is evaluated on the test set for the ﬁnal result. Since the stan- dard BiLSTM-CRF model cannot handle nested entities, we use either the outermost (BiLSTM- CRF-TOP in Table 2) or the innermost mentions (BiLSTM-CRF-BOTTOM) for training. We also combine the outputs from these two ﬂat NER models, and denote the result as BiLSTM-CRF- BOTH. From Table 2, we can see that single ﬂat NER models can achieve high precision but suffer from low recall. For example, the model pretrained on outermost (top) mentions has 38.0 recall, as P R F1 BiLSTM-CRF-TOP 89.9 38.0 53.5 BiLSTM-CRF-BOTTOM 93.8 62.0 74.7 BiLSTM-CRF-BOTH 92.2 85.8 88.9 Hypergraph 91.8 91.0 91.4 Transition", "meta": {"paper_id": "1906.01359v1", "chunk_index": 3, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_4", "text": "we can see that single ﬂat NER models can achieve high precision but suffer from low recall. For example, the model pretrained on outermost (top) mentions has 38.0 recall, as P R F1 BiLSTM-CRF-TOP 89.9 38.0 53.5 BiLSTM-CRF-BOTTOM 93.8 62.0 74.7 BiLSTM-CRF-BOTH 92.2 85.8 88.9 Hypergraph 91.8 91.0 91.4 Transition 77.4 70.1 73.6 Table 2: NER results on NNE using different methods. around 60% of mentions are nested within oth- ers. The hypergraph-based model performs best on our dataset, presumably because it can capture mentions from different levels and does not suffer from issues of structural ambiguity during infer- ence (Muis and Lu, 2017; Wang and Lu, 2018). However, its decoding speed of 9 words per sec- ond is slow due to the large number of entity categories of our dataset.2 The transition-based method has a higher decode speed of 57 words per second, but has much lower precision than ﬂat NER models. 4 Related Work Other corpora with nested entities: We brieﬂy compare existing annotated English corpora in- volving nested entities. A comparison of statistics between our dataset and two widely used bench- mark datasets is shown in Table 3. The ACE cor- pora (Mitchell et al., 2004; Walker et al., 2005) consist of data of various types annotated for enti- ties, relations and events. The entity component of ACE is framed in terms of nominal modiﬁcation, and nested mentions are only annotated in nominal mentions, not inside other named entity mentions. For example, in ACE2005, ‘Secretary of Home- land Security Tom Ridge’ is annotated as a PER- SON, containing two other PERSON annotations: ‘Secretary’ and ‘Secretary of Homeland Security’. In contrast, our annotations capture more interac- tions between different semantic spans: PERSON consisting of ROLE and NAME, and ROLE contain- ing GOVERNMENT. The GENIA corpus (Kim et al., 2003) is a richly-annotated corpus for bio-text mining that has 36 entity types among 2,000 MEDLINE ab- stracts. Due to the biomedical domain’s special- ized terminology and complex naming conven- tions, entities of interest, such as genes, proteins or 2The decoding time complexity of the method proposed by Wang and Lu (2018) is O(cmn), where m is the number of entity types, n is the sentence length, and c is the maximal mention length. Item NNE GENIA ACE2005 Documents 2,312 2,000 464 Sentences 49,208 18,546 12,548 Sentences 32,387 9,533 4,266 w. nesting Tokens 1.1M 0.5M 0.3M Mentions 279,795 92,681 30,966 Entity types 114 36 7 Mentions 5.69 4.99 2.46 per sentence Top-level mentions 118,525 76,582 23,464 Maximum depth 6 4 6 Table 3: A comparison between NNE and two com- monly used corpora with nested entities. disease names, often nest. For example, the RNA ‘CIITA mRNA’ contains a DNA mention ‘CIITA’. In addition to these two commonly used nested entity corpora, Byrne (2007) and Alex et al. (2007) introduced datasets with nested entities in histori- cal archive and biomedical domains, respectively. However, their datasets are not publicly available. Four percent of entity mentions annotated in the English entity discovery and linking task in TAC- KBP track include nesting (Ji et al., 2014).", "meta": {"paper_id": "1906.01359v1", "chunk_index": 4, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_5", "text": "(2007) and Alex et al. (2007) introduced datasets with nested entities in histori- cal archive and biomedical domains, respectively. However, their datasets are not publicly available. Four percent of entity mentions annotated in the English entity discovery and linking task in TAC- KBP track include nesting (Ji et al., 2014). Resources built on the PTB: A lots of ef- fort has been made on adding syntactic and se- mantic information to the PTB (Marcus et al., 1993). PropBank (Kingsbury et al., 2002) ex- tended the PTB with the predicate argument re- lationships between verbs and their arguments. NomBank (Meyers et al., 2004) extended the ar- gument structure for instances of common nouns. Vadas and Curran (2007), and Ficler and Goldberg (2016) extended the PTB with noun phrase and co- ordination annotations, respectively. Our dataset is built on top of the PTB and en- riches the full ecosystem of resources and systems that stem from it. 5 Summary We present NNE, a large-scale, nested, ﬁne- grained named entity dataset. We are optimistic that NNE will encourage the development of new NER models that recognize structural informa- tion within entities, and therefore understand ﬁne- grained semantic information captured. Addition- ally, our annotations are built on top of the PTB, so that the NNE dataset will allow joint learning models to take advantage of semantic and syntac- tic annotations, and ultimately to understand and exploit the true structure of named entities. Acknowledgments We would like to thank annotators for their excel- lent work: Kellie Webster, Vivian Li, Joanne Yang and Kristy Hughes. We also thank three anony- mous reviewers for their insightful comments. References Beatrice Alex, Barry Haddow, and Claire Grover. 2007. Recognising nested named entities in biomedical text. In BioNLP, pages 65–72. Kate Byrne. 2007. Nested named entity recognition in historical archive text. In ICSC, pages 589–596. Jean Carletta. 1996. Assessing agreement on classiﬁ- cation tasks: The Kappa statistic. Comput. Linguist., 22(2):249–254. Xiang Dai. 2018. Recognizing complex entity men- tions: A review and future directions. In ACL-SRW, pages 37–44. Jessica Ficler and Yoav Goldberg. 2016. Coordination annotation extension in the Penn tree bank. In ACL, pages 834–842. Jenny Rose Finkel and Christopher Manning. 2009. Nested named entity recognition. In EMNLP, pages 141–150. Heng Ji, Joel Nothman, and Ben Hachey. 2014. Overview of TAC-KBP2014 entity discovery and linking tasks. In TAC, pages 1333–1339. Meizhi Ju, Makoto Miwa, and Sophia Ananiadou. 2018. A neural layered model for nested named en- tity recognition. In NAACL, pages 1446–1459. Arzoo Katiyar and Claire Cardie. 2018. Nested named entity recognition revisited. In NAACL, pages 861– 871. J-D Kim, Tomoko Ohta, Yuka Tateisi, and Jun ichi Tsu- jii. 2003. GENIA corpus a semantically annotated corpus for bio-textmining. Bioinformatics, 19:i180– i182. Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding predicate argument structure to the Penn Treebank. In HLT, pages 252–256. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL, pages 260–270. Wei Lu and Dan Roth. 2015. Joint mention extrac- tion and classiﬁcation with mention hypergraphs. In EMNLP, pages", "meta": {"paper_id": "1906.01359v1", "chunk_index": 5, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_6", "text": "structure to the Penn Treebank. In HLT, pages 252–256. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In NAACL, pages 260–270. Wei Lu and Dan Roth. 2015. Joint mention extrac- tion and classiﬁcation with mention hypergraphs. In EMNLP, pages 857–867. Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Comput. Linguist., 19. Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The nombank project: An interim report. Alexis Mitchell, Stephanie Strassel, Shudong Huang, and Ramez Zakhary. 2004. ACE 2004 multilingual training corpus. LDC. Aldrian Obaja Muis and Wei Lu. 2017. Labeling gaps between words: Recognizing overlapping mentions with mention separators. In EMNLP, pages 2608– 2618. Joel Nothman, Nicky Ringland, Will Radford, Tara Murphy, and James R Curran. 2013. Learning mul- tilingual named entity recognition from Wikipedia. Artif. Intell., 194:151–175. Nicky Ringland. 2016. Structured Named Entities. Ph.D. thesis, University of Sydney. Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition. In CoNLL. Satoshi Sekine, Kiyoshi Sudo, and Chikashi Nobata. 2002. Extended named entity hierarchy. In LREC. Mohammad Golam Sohrab and Makoto Miwa. 2018. Deep exhaustive model for nested named entity recognition. In EMNLP, pages 2843–2849. Pontus Stenetorp, Sampo Pyysalo, Goran Topi´c, Tomoko Ohta, Sophia Ananiadou, and Jun’ichi Tsu- jii. 2012. brat: a web-based tool for NLP-assisted text annotation. In EACL, pages 102–107. David Vadas and James Curran. 2007. Adding noun phrase structure to the Penn Treebank. In ACL, pages 240–247. Christopher Walker, Stephanie Strassel, Julie Medero, and Kazuaki Maeda. 2005. ACE 2005 multilingual training corpus. LDC. Bailin Wang and Wei Lu. 2018. Neural segmental hy- pergraphs for overlapping mention recognition. In EMNLP, pages 204–214. Bailin Wang, Wei Lu, Yu Wang, and Hongxia Jin. 2018. A neural transition-based model for nested mention recognition. In EMNLP, pages 1011–1017. Ralph Weischedel and Ada Brunstein. 2005. BBN pro- noun coreference and entity type corpus. LDC. Mingbin Xu, Hui Jiang, and Sedtawut Watcharawit- tayakul. 2017. A local detection approach for named entity recognition and mention detection. In ACL, pages 1237–1247. A Full annotation scheme Category Frequency Category Frequency Category Frequency CARDINAL 43873 STREET 475 QUANTITY2D 81 NAME 28537 GRPORG 437 PRODUCTFOOD 80 ORGCORP 23339 ORGPOLITICAL 436 SUBURB 78 UNIT 19289 VEHICLE 432 GRPLOC 63 DATE 17381 LAW 419 HOTEL 55 PER 14960 ORGEDU 411 QUANTITYOTHER 55 DURATION 13655 CONTINENT 354 FUND 54 MONEY 12640 BUILDING 346 SONG 54 MULT 7851 SEASON 337 SPACE 53 FIRST 6797 GPE 333 RIVER 52 CITY 6723 FOLD 313 WAR 51 PERCENT 6542 MIDDLE 313 CHEMICAL 45 REL 6170 TIME 296 BRIDGE 44 CORPJARGON 5560 WEIGHT 293 PLAY 42 HON 5524 OCEAN 291 STADIUM 37 NATIONALITY 5193 LOCATIONOTHER 261 AWARD 36 GOVERNMENT 4674 EVENT 260 ORGRELIGIOUS 35 COUNTRY 4047 DISEASE 246 AIRPORT 32 QUAL 3903 QUANTITY1D 220 ANIMATE 29 YEAR 3421 CITYSTATE 220 GOD 29 MONTH 3385 WOA 207 HOSPITAL 25 STATE 3245 TVSHOW 172 ATTRACTION 24 ORDINAL 2590 ELECTRONICS 167 WEAPON 23", "meta": {"paper_id": "1906.01359v1", "chunk_index": 6, "filename": "1906.01359v1.pdf", "chunk_length": 512}}
{"id": "1906.01359v1_chunk_7", "text": "STADIUM 37 NATIONALITY 5193 LOCATIONOTHER 261 AWARD 36 GOVERNMENT 4674 EVENT 260 ORGRELIGIOUS 35 COUNTRY 4047 DISEASE 246 AIRPORT 32 QUAL 3903 QUANTITY1D 220 ANIMATE 29 YEAR 3421 CITYSTATE 220 GOD 29 MONTH 3385 WOA 207 HOSPITAL 25 STATE 3245 TVSHOW 172 ATTRACTION 24 ORDINAL 2590 ELECTRONICS 167 WEAPON 23 IPOINTS 2395 SPORTSTEAM 166 MUSEUM 17 ROLE 2368 DATEOTHER 164 ENERGY 17 RATE 2141 QUANTITY3D 156 SPEED 14 MEDIA 1712 NAMEMOD 155 PAINTING 13 DAY 1631 GRPPER 154 BAND 10 NUMDAY 1495 BOOK 149 SPORTSSEASON 8 INI 1445 ARMY 139 SCINAME 7 NORPOTHER 1247 FACILITY 129 ADDRESSNON 3 ORGOTHER 1099 PRODUCTDRUG 116 ALBUM 3 PERIODIC 1066 HURRICANE 107 TEMPERATURE 2 REGION 864 SPORTSEVENT 100 NATURALDISASTER 2 NORPPOLITICAL 731 RELIGION 99 CONCERT 2 AGE 661 NICKNAME 96 STATION 1 INDEX 657 LANGUAGE 92 BORDER 1 PRODUCTOTHER 656 FILM 89 CHANNEL 1", "meta": {"paper_id": "1906.01359v1", "chunk_index": 7, "filename": "1906.01359v1.pdf", "chunk_length": 140}}
{"id": "1906.01472v2_chunk_0", "text": "Evaluating Discourse in Structured Text Representations Elisa Ferracane1, Greg Durrett2, Junyi Jessy Li1 and Katrin Erk1 1Department of Linguistics 2Department of Computer Science The University of Texas at Austin elisa@ferracane.com, gdurrett@cs.utexas.edu jessy@austin.utexas.edu, katrin.erk@mail.utexas.edu Abstract Discourse structure is integral to understand- ing a text and is helpful in many NLP tasks. Learning latent representations of discourse is an attractive alternative to acquiring expen- sive labeled discourse data. Liu and Lapata (2018) propose a structured attention mecha- nism for text classiﬁcation that derives a tree over a text, akin to an RST discourse tree. We examine this model in detail, and evalu- ate on additional discourse-relevant tasks and datasets, in order to assess whether the struc- tured attention improves performance on the end task and whether it captures a text’s dis- course structure. We ﬁnd the learned latent trees have little to no structure and instead fo- cus on lexical cues; even after obtaining more structured trees with proposed model modi- ﬁcations, the trees are still far from captur- ing discourse structure when compared to dis- course dependency trees from an existing dis- course parser. Finally, ablation studies show the structured attention provides little beneﬁt, sometimes even hurting performance.1 1 Introduction Discourse describes how a document is organized, and how discourse units are rhetorically connected to each other. Taking into account this structure has shown to help many NLP end tasks, including summarization (Hirao et al., 2013; Durrett et al., 2016), machine translation (Joty et al., 2017), and sentiment analysis (Ji and Smith, 2017). However, annotating discourse requires considerable effort by trained experts and may not always yield a structure appropriate for the end task. As a result, having a model induce the discourse structure of a text is an attractive option. Our goal in this paper is to evaluate such an induced structure. 1Code and data available at https://github.com/ elisaF/structured Inducing structure has been a recent popular approach in syntax (Yogatama et al., 2017; Choi et al., 2018; Bisk and Tran, 2018). Evaluations of these latent trees have shown they are inconsistent, shallower than their explicitly parsed counterparts (Penn Treebank parses) and do not resemble any linguistic syntax theory (Williams et al., 2018). For discourse, Liu and Lapata (2018) (L&L) in- duce a document-level structure while performing text classiﬁcation with a structured attention that is constrained to resolve to a non-projective depen- dency tree. We evaluate the document-level struc- ture induced by this model. In order to compare the induced structure to existing linguistically- motivated structures, we choose Rhetorical Struc- ture Theory (RST) (Mann and Thompson, 1988), a widely-used framework for discourse structure, because it also produces tree-shaped structures.2 We evaluate on some of the same tasks as L&L, but add two more tasks we theorize to be more discourse-sensitive: text classiﬁcation of writing quality, and sentence order discrimination (as pro- posed by Barzilay and Lapata (2008)). Our research uncovers multiple negative results. We ﬁnd that, contrary to L&L, the structured at- tention does not help performance in most cases; further, the model is not learning discourse. In- stead, the model learns trees", "meta": {"paper_id": "1906.01472v2", "chunk_index": 0, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_1", "text": "writing quality, and sentence order discrimination (as pro- posed by Barzilay and Lapata (2008)). Our research uncovers multiple negative results. We ﬁnd that, contrary to L&L, the structured at- tention does not help performance in most cases; further, the model is not learning discourse. In- stead, the model learns trees with little to no struc- ture heavily inﬂuenced by lexical cues to the task. In an effort to induce better trees, we propose sev- eral principled modiﬁcations to the model, some of which yield more structured trees. However, even the more structured trees bear little resem- blance to ground truth RST trees. We conclude the model holds promise, but re- 2The Penn Discourse Treebank (PDTB; Prasad et al., 2008) captures lexically-grounded discourse for individual connectives and adjacent sentences, and does not span an en- tire document; Segmented Discourse Representation Theory (Lascarides and Asher, 2008) is a graph. arXiv:1906.01472v2 [cs.CL] 10 Jun 2019 inter-sentence biLSTM structured attention s1 s2 st … d1 semantic discourse e1 d2 e2 dt et y max-pooling document-level words sentences class structured attention +compose structured attention +compose document sentence Compose eʹt eʹ2 eʹ1 … … Figure 1: Model of Liu and Lapata (2018) with the document-level portion (right) that composes sentences into a document representation. quires moving beyond text classiﬁcation, and in- jecting supervision (as in Strubell et al. (2018)). Our contributions are (1) comprehensive perfor- mance results on existing and additional tasks and datasets showing document-level structured atten- tion is largely unhelpful, (2) in-depth analyses of induced trees showing they do not represent dis- course, and (3) several principled model changes to produce better structures but that still do not re- semble the structure of discourse. 2 Rhetorical Structure Theory (RST) In RST, coherent texts consist of minimal units, which are linked to each other, recursively, through rhetorical relations (Mann and Thompson, 1988). Thus, the goal of RST is to describe the rhetorical organization of a text by using a hier- archical tree structure that captures the commu- nicative intent of the writer. An RST discourse tree can further be represented as a discourse de- pendency tree. We follow the algorithm of Hirao et al. (2013) to create an unlabelled dependency tree based on the nuclearity of the tree. 3 Models We present two models: one for text classiﬁcation, and one for sentence ordering. Both are based on the L&L model, with a design change to cause stronger percolation of information up the tree (we also experiment without this change). Text classiﬁcation The left-hand side of Figure 1 presents an overview of the model: the model op- erates ﬁrst at the sentence-level to create sentence representations, and then at the document-level to create a document representation from the previ- ously created sentence representations. In more detail, the model composes GloVe embeddings (Pennington et al., 2014) into a sentence represen- tation using structured attention (from which a tree can be derived), then sentence representations into a single document representation for class predic- tion. At both sentence and document level, each object (word or sentence, respectively) attends to other objects", "meta": {"paper_id": "1906.01472v2", "chunk_index": 1, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_2", "text": "composes GloVe embeddings (Pennington et al., 2014) into a sentence represen- tation using structured attention (from which a tree can be derived), then sentence representations into a single document representation for class predic- tion. At both sentence and document level, each object (word or sentence, respectively) attends to other objects that could be its parent in the tree. Since the sentence and document-level parts of the model are identical, we focus on the document level (Figure 1, right), which is of interest to us for evaluating discourse effects. Sentence representations s1, . . . , st are fed to a bidirectional LSTM, and the hidden representa- tions [h1, . . . , ht] consist of a semantic part (et) and a structure part (dt): [et, dt] = ht. Unnor- malized scores fij representing potentials between parent i and child j are calculated using a bilinear function over the structure vector: tp = tanh(Wpdi); tc = tanh(Wcdj) (1) fij = tT p Watc (2) The matrix-tree theorem allows us to compute marginal probabilities aij of dependency arcs un- der the distribution over non-projective depen- dency trees induced by fij (details in Koo et al. (2007)). This computation is fully differentiable, allowing it to be treated as another neural network layer in the model. We importantly note the model only uses the marginals. We can post-hoc use the Chu-Liu-Edmonds algorithm to retrieve the highest-scoring tree under f, which we call fbest (Chu and Liu, 1965; Edmonds, 1967). The semantic vectors of sentences e are then up- dated using this attention. Here we diverge from the L&L model: in their implementation,3 each node is updated based on a weighted sum over its parents in the tree (their paper states both parents and children).4 We instead inform each node by a sum over its children, more in line with past work where information more intuitively perco- lates from children to parents and not the other way (Ji and Smith, 2017) (we also run experiments without this design change). We calculate the con- text for all possible children of that sentence as: ci = n X k=1 aikek (3) where aik is the probability that k is the child of i, and ek is the semantic vector of the child. The children vectors are then passed through a non-linear function, resulting in the updated se- mantic vector e′ i for parent node i. e′ i = tanh(Wr[ei, ci]) (4) 3https://github.com/nlpyang/structured 4We found similar results for using both parents and chil- dren as well as using parents only. Yelp Debates WQ WQTC WSJSO L&L(orig) 68.51 | 68.27 (0.19) 81.82 | 79.48 (2.90) 84.14 | 82.69 (1.36) 80.73 | 79.63 (1.03) 96.17 | 95.29 (0.84) L&L(ours) 68.51 | 68.23 (0.23) 78.88 | 77.81 (1.80) 84.14 | 82.70 (1.36) 82.49 | 81.11 (0.95) 95.57 | 94.76 (1.11) −doc attn 68.34 | 68.13 (0.17) 82.89 | 81.42 (1.08) 83.75 | 82.80 (0.94) 80.60 | 79.25 (0.94) 95.57 | 95.11 (0.42) −both attn 68.19 | 68.05 (0.13) 79.95 | 77.34 (1.79) 84.27 | 83.16 (1.25) 77.58 | 76.16 (1.25) 95.23 | 94.68", "meta": {"paper_id": "1906.01472v2", "chunk_index": 2, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_3", "text": "| 81.11 (0.95) 95.57 | 94.76 (1.11) −doc attn 68.34 | 68.13 (0.17) 82.89 | 81.42 (1.08) 83.75 | 82.80 (0.94) 80.60 | 79.25 (0.94) 95.57 | 95.11 (0.42) −both attn 68.19 | 68.05 (0.13) 79.95 | 77.34 (1.79) 84.27 | 83.16 (1.25) 77.58 | 76.16 (1.25) 95.23 | 94.68 (0.37) L&L(reported) 68.6 76.5 - - - Table 1: Max | mean (standard deviation) accuracy on the test set averaged across four training runs with different initialization weights. Bolded numbers are within 1 standard deviation of the best performing model. L&L(orig) uses the original L&L code; L&L(ours) includes the design change and bug ﬁx. L&L(reported) lists results re- ported by L&L on a single training run. Finally, a max pooling layer over e′ i followed by a linear layer produces the predicted document class y. The model is trained with cross entropy loss. Additionally, the released L&L implementation has a bug where attention scores and marginals are not masked correctly in the matrix-tree computa- tion, which we correct. Sentence order discrimination This model is identical, except for task-speciﬁc changes. The goal of this synthetic task, proposed by Barzilay and Lapata (2008), is to capture discourse coher- ence. A negative class is created by generating random permutations of a text’s original sentence ordering (the positive class). A coherence score is produced for each positive and negative example, with the intuition that the originally ordered text will be more coherent than the jumbled version. Because we compare two examples at a time (orig- inal and permuted order), we modify the model to handle paired inputs and replace cross-entropy loss with a max-margin ranking loss. 4 Experiments We evaluate the model on four text classiﬁcation tasks and one sentence order discrimination task. 4.1 Datasets Details and statistics are included in Appendix A.5 Yelp (in L&L, 5-way classiﬁcation) comprises customer reviews from the Yelp Dataset Challenge (collected by Tang et al. (2015)). Each review is labeled with a 1 to 5 rating (least to most positive). Debates (in L&L, binary classiﬁcation) are tran- scribed debates on Congressional bills from the U.S. House of Representatives (compiled by Thomas et al. (2006), preprocessed by Yogatama 5Of the document-level datasets used in L&L (SNLI was sentence-level), we omit IMDB and Czech Movies be- cause on IMDB their model did not outperform prior work, and Czech (a language with freer word order than English) highlighted the non-projectivity of their sentence-level trees, which is not the focus of our work. and Smith (2014)). Each speech is labeled with 1 or 0 indicating whether the speaker voted in favor of or against the bill. Writing quality (WQ) (not in L&L, binary clas- siﬁcation) contains science articles from the New York Times (extracted from Louis and Nenkova (2013)). Each article is labeled as either ‘very good’ or ‘typical’ to describe its writing quality. While both classes contain well-written text, Louis and Nenkova (2013) ﬁnd features associated with discourse including sentiment, readability, along with PDTB-style discourse relations are helpful in distinguishing between the two classes. Writing quality with topic control (WQTC) (not in L&L,", "meta": {"paper_id": "1906.01472v2", "chunk_index": 3, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_4", "text": "either ‘very good’ or ‘typical’ to describe its writing quality. While both classes contain well-written text, Louis and Nenkova (2013) ﬁnd features associated with discourse including sentiment, readability, along with PDTB-style discourse relations are helpful in distinguishing between the two classes. Writing quality with topic control (WQTC) (not in L&L, binary classiﬁcation) is similar to WQ, but controlled for topic using a topic simi- larity list included with the WQ source corpus.6 Wall Street Journal Sentence Order (WSJSO) (not in L&L, sentence order discrimination) is the WSJ portion of PTB (Marcus et al., 1993). 4.2 Settings For each experiment, we train the model four times varying only the random seed for weight initializations. The model is trained for a ﬁxed amount of time, and the model from the timestep with highest development performance is chosen. We report accuracies on the test set, and tree anal- yses on the development set. Our implementation is built on the L&L released implementation, with changes as noted in Section 3. Preprocessing and training details are in Appendix A. 4.3 Results We report accuracy (as in prior work) in Table 1, and perform two ablations: removing the struc- tured attention at the document level, and remov- ing it at both document and sentence levels. Addi- tionally, we run experiments on the original code 6An analysis in section 4.3 shows the WQ-trained model focuses on lexical items strongly related to the article topic. Yelp Debates WQ WQTC WSJSO tree height 2.049 2.751 2.909 4.035 2.288 prop. of leaf nodes 0.825 0.849 0.958 0.931 0.892 norm. arc length 0.433 0.397 0.420 0.396 0.426 % vacuous trees 73% 38% 42% 14% 100% Table 2: Statistics for learned trees averaged across four runs (similar results without the design change or bug ﬁx are in the Appendix Table 6). See Table 4 for gold statistics on WQTC. without the design change or bug ﬁx to conﬁrm our ﬁndings are similar (see L&L(orig) in Table 1). Document-level structured attention does not help. Structured attention at the sentence level helps performance for all except WQ, where no form of attention helps. However, structured atten- tion at the document level yields mostly negative results, in contrast to the improvements reported in L&L. In Yelp, WSJSO, and WQ, there is no differ- ence. In Debates, the attention hurts performance. Only in WQTC does the structured attention pro- vide a beneﬁt. While a single training run could produce the improvements seen in L&L, the re- sults across four runs depict a more accurate pic- ture. When inducing structures, it is particularly important to repeat experiments as the structures can be highly inconsistent due to the noise caused by random initialization (Williams et al., 2018). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ROOT (1)madam speaker, i rise in opposition to h.r. 3283 on both process and policy grounds....(17)look beyond the majority’s smoke and mirrors, and vote against this ill-timed and ill-conceived legislation. Figure 2: Learned dependency tree from Debates. Trees do not learn discourse. Although docu- ment level", "meta": {"paper_id": "1906.01472v2", "chunk_index": 4, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_5", "text": "14 15 16 17 ROOT (1)madam speaker, i rise in opposition to h.r. 3283 on both process and policy grounds....(17)look beyond the majority’s smoke and mirrors, and vote against this ill-timed and ill-conceived legislation. Figure 2: Learned dependency tree from Debates. Trees do not learn discourse. Although docu- ment level structured attention provides little ben- eﬁt in performance, we probe whether the model could still be learning some discourse. We visu- ally inspect the learned fbest trees and in Table 2 we report statistics on them (see Appendix Table 6 for similar results with the original code). The visual inspection (Figure 2) reveals shallow trees (also reported in L&L), but furthermore the trees have little to no structure.7 We observe an interesting pattern where the model picks one of the ﬁrst two or last two sentences as the root, and 7While shallow trees are expected in PDTB-style dis- course, even these trees would exhibit meaningful structure between adjacent sentences, which is entirely absent here. Yelp uuu, sterne, star, rating, deduct, 0, edit, un- derwhelmed, update, allgemein Debates oppose, republican, majority, thank, gentle- man, leadership, california, measure, presi- dent, vote WQ valley, mp3, ﬁrm, capital, universal, ven- ture, silicon, analyst, capitalist, street Table 3: Top 10 words most associated with the root sentence (measured with PPMI). all other sentences are children of that node. We label these trees as ‘vacuous’ and the strength of this pattern is reﬂected in the tree statistics (Table 2). The height of trees is small, showing the trees are shallow. The proportion of leaf nodes is high, that is, most nodes have no children. Finally, the normalized arc length is high, where nodes that are halfway into the document still connect to the root. We further probe the root sentence, as the model places so much attention on it. We hypothesize the root sentence has strong lexical cues for the task, suggesting the model is instead attending to particular words. In Yelp, reviewers often start or end with a sentiment-laden sentence summarizing their rating. In Debates, speakers begin or end their speech by stating their stance on the bill. In WQ and WQTC, the interpretation of the root is less clear. In WSJSO, we ﬁnd the root is always the ﬁrst sentence of the correctly ordered docu- ment, which is reasonable and commonly attested in a discourse tree, but the remainder of the vacu- ous tree is entirely implausible. To conﬁrm our suspicion that the root sentence is lexically marked, we measure the association between words appearing in the root sentence and those elsewhere by calculating their positive point- wise mutual information scores (Table 3). In Yelp, we ﬁnd root words often express sen- timent and explicitly mention the number of stars given (‘sterne’ in German, or ‘uuu’ as coined by a particularly proliﬁc Yelper), which are clear indi- cators of the rating label. For Debates, words ex- press speaker opinion, politeness and stance which are strong markers for the binary voting label. The list for WQ revolves around tech, suggesting the model is learning topics instead of", "meta": {"paper_id": "1906.01472v2", "chunk_index": 5, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_6", "text": "as coined by a particularly proliﬁc Yelper), which are clear indi- cators of the rating label. For Debates, words ex- press speaker opinion, politeness and stance which are strong markers for the binary voting label. The list for WQ revolves around tech, suggesting the model is learning topics instead of writing quality. Thus, in WQTC we control for topics. 5 Learning better structure We next probe whether the structure in L&L can be improved to be more linguistically appropriate, while still performing well on the end task. Given that structured attention helps only on WQTC and Acc height leaf arc vacuous Full 81.11 4.035 0.931 0.396 14% -biLSTM 77.80 11.51 0.769 0.353 4% -biLSTM, +w 75.57 7.364 0.856 0.359 3% -biLSTM, +p 77.11 10.430 0.790 0.349 3% -biLSTM, +4p 81.71 9.588 0.811 0.353 3% parsed RST - 25.084 0.567 0.063 0% Table 4: Mean test accuracy and tree statistics on the WQTC dev set (averaged across four runs). -biLSTM removes the document-level biLSTM, +w uses the weighted sum, +p performs 1 extra percolation, and +4p does 4 levels of percolation. The last row are (‘gold’) parsed RST discourse dependency trees. learns vacuous trees less frequently, we focus on this task. We experiment with three modiﬁca- tions. First, we remove the document-level biL- STM since it performs a level of composition that might prevent the attention from learning the true structure. Second, we note equation 3 captures possible children only at one level of the tree, but not possible subtrees. We thus perform an addi- tional level of percolation over the marginals to in- corporate the children’s children of the tree. That is, after equation 4, we calculate: c′ i = n X k=1 aike′ i; e′′ i = tanh(Wr[e′ i, c′ i]) (5) Third, the max-pooling layer gives the model a way of aggregating over sentences while ignoring the learned structure. Instead, we propose a sum that is weighted by the probability of a given sen- tence being the root, i.e., using the learned root attention score ar i : yi = Pn i=1 ar i e′′ i . We include ablations of these modiﬁcations and additionally derive RST discourse dependency trees,8 collapsing intrasentential nodes, as an ap- proximation to the ground truth. The results (Table 4) show that simply remov- ing the biLSTM produces trees with more struc- ture (deeper trees, fewer leaf nodes, shorter arc lengths, and less vacuous trees), conﬁrming our in- tuition that it was doing the work for the structured attention. However, it also results in lower per- formance. Changing the pooling layer from max to weighted sum both hurts performance and re- sults in shallower trees (though still deeper than Full), which we attribute to this layer still being a pooling function. Introducing an extra level of tree percolation yields better trees but also a drop in performance. Finally, using 4 levels of percola- 8We use the RST parser in Feng and Hirst (2014) and fol- low Hirao et al. (2013) to derive discourse dependency trees. tion both reaches the accuracy of Full and retains the", "meta": {"paper_id": "1906.01472v2", "chunk_index": 6, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_7", "text": "of tree percolation yields better trees but also a drop in performance. Finally, using 4 levels of percola- 8We use the RST parser in Feng and Hirst (2014) and fol- low Hirao et al. (2013) to derive discourse dependency trees. tion both reaches the accuracy of Full and retains the more structured trees.9 We hypothesize accu- racy doesn’t surpass Full because this change also introduces extra parameters for the model to learn. While our results are a step in the right direc- tion, the structures are decidedly not discourse when compared to the parsed RST dependency trees, which are far deeper with far fewer leaf nodes, shorter arcs and no vacuous trees. Impor- tantly, the tree statistics show the structures do not follow the typical right-branching structure in news: the trees are shallow, nodes often connect to the root instead of a more immediate parent, and the vast majority of nodes have no children. In work concurrent to ours, Liu et al. (2019) pro- poses a new iterative algorithm for the structured attention (in the same spirit as our extra percola- tions) and applies it to a transformer-based sum- marization model. However, even these induced trees are not comparable to RST discourse trees. The induced trees are multi-rooted by design (each root is a summary sentence) which is unusual for RST;10 their reported tree height and edge agree- ment with RST trees are low. 6 Conclusion In this paper, we evaluate structured attention in document representations as a proxy for discourse structure. We ﬁrst ﬁnd structured attention at the document level is largely unhelpful, and second it instead captures lexical cues resulting in vacu- ous trees with little structure. We propose several principled changes to induce better structures with comparable performance. Nevertheless, calculat- ing statistics on these trees and comparing them to parsed RST trees shows they still contain no meaningful discourse structure. We theorize some amount of supervision, such as using ground-truth discourse trees, is needed for guiding and con- straining the tree induction. Acknowledgments We thank the reviewers for insightful feedback. We acknowledge the Texas Advanced Computing Center for grid resources. The ﬁrst author was supported by the NSF Graduate Research Fellow- ship Program under Grant No. 2017247409. 9More than 4 levels caused training to become unstable. 10Less than 25% of trees in the RST Discourse Treebank (Carlson et al., 2001) have more than 1 root; less than 8% have more than 2 roots. References Regina Barzilay and Mirella Lapata. 2008. Model- ing Local Coherence - An Entity-Based Approach. Computational Linguistics, 34(1):1–34. Yonatan Bisk and Ke Tran. 2018. Inducing grammars with and for neural machine translation. In Proceed- ings of the 2nd Workshop on Neural Machine Trans- lation and Generation, pages 25–35. Association for Computational Linguistics. Lynn Carlson, Daniel Marcu, and Mary Ellen Okurovsky. 2001. Building a discourse-tagged cor- pus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, pages 1–10. Association for Computational Linguistics. Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciﬁc tree structures. In", "meta": {"paper_id": "1906.01472v2", "chunk_index": 7, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_8", "text": "and Mary Ellen Okurovsky. 2001. Building a discourse-tagged cor- pus in the framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and Dialogue, pages 1–10. Association for Computational Linguistics. Jihun Choi, Kang Min Yoo, and Sang-goo Lee. 2018. Learning to compose task-speciﬁc tree structures. In Proceedings of the 2018 Association for the Ad- vancement of Artiﬁcial Intelligence (AAAI). Y.J. Chu and T.H. Liu. 1965. On the shortest arbores- cence of a directed graph. Science Sinica, 14:1396– 1400. Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein. 2016. Learning-based single-document summariza- tion with compression and anaphoricity constraints. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1998–2008. Association for Computational Linguistics. Jack Edmonds. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards, 71:233–240. Vanessa Wei Feng and Graeme Hirst. 2014. A linear- time bottom-up discourse parser with constraints and post-editing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Lin- guistics (Volume 1: Long Papers), pages 511–521. Association for Computational Linguistics. Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-document summarization as a tree knapsack problem. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Process- ing, pages 1515–1520. Yangfeng Ji and Noah A. Smith. 2017. Neural Dis- course Structure for Text Categorization. In Pro- ceedings of the 55th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 996–1005. Association for Computa- tional Linguistics. Shaﬁq Joty, Francisco Guzm´an, Llu´ıs M`arquez, and Preslav Nakov. 2017. Discourse structure in ma- chine translation evaluation. Computational Lin- guistics, 43(4):683–722. Terry Koo, Amir Globerson, Xavier Carreras, and Michael Collins. 2007. Structured prediction mod- els via the matrix-tree theorem. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL). Alex Lascarides and Nicholas Asher. 2008. Segmented discourse representation theory: Dynamic semantics with discourse structure. In Computing meaning, pages 87–124. Springer. Yang Liu and Mirella Lapata. 2018. ”Learning Struc- tured Text Representations”. Transactions of the As- sociation for Computational Linguistics, 6:63–75. Yang Liu, Ivan Titov, and Mirella Lapata. 2019. Single document summarization as tree induction. In Pro- ceedings of the 2019 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, Vol- ume 1 (Long and Short Papers), pages 1745–1755, Minneapolis, Minnesota. Association for Computa- tional Linguistics. Annie Louis and Ani Nenkova. 2013. What Makes Writing Great? First Experiments on Article Qual- ity Prediction in the Science Journalism Domain. TACL. William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional the- ory of text organization. Text-Interdisciplinary Jour- nal for the Study of Discourse, 8(3):243–281. Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computa- tional linguistics, 19(2):313–330. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. Associa-", "meta": {"paper_id": "1906.01472v2", "chunk_index": 8, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_9", "text": "and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computa- tional linguistics, 19(2):313–330. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532–1543. Associa- tion for Computational Linguistics. Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt- sakaki, Livio Robaldo, Aravind K Joshi, and Bon- nie L Webber. 2008. The Penn Discourse TreeBank 2.0. In LREC. Evan Sandhaus. 2008. The New York Times annotated corpus. Linguistic Data Consortium, Philadelphia, 6(12):e26752. Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. 2018. Linguistically-informed self-attention for semantic role labeling. In Proceedings of the 2018 Confer- ence on Empirical Methods in Natural Language Processing, pages 5027–5038. Association for Computational Linguistics. Duyu Tang, Bing Qin, and Ting Liu. 2015. ”Learning Semantic Representations of Users and Products for Document Level Sentiment Classiﬁcation”. In Pro- ceedings of the 53rd Annual Meeting of the Associ- ation for Computational Linguistics and the 7th In- ternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1014– 1023. Association for Computational Linguistics. Matt Thomas, Bo Pang, and Lillian Lee. 2006. ”Get out the vote: Determining support or opposition from Congressional ﬂoor-debate transcripts”. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 327–335. Association for Computational Linguis- tics. Adina Williams, Andrew Drozdov, and Samuel R. Bowman. 2018. Do latent tree learning models iden- tify meaningful structure in sentences? Transac- tions of the Association for Computational Linguis- tics, 6:253–267. Dani Yogatama, Phil Blunsom, Chris Dyer, Edward Grefenstette, and Wang Ling. 2017. Learning to compose words into sentences with reinforcement learning. ICLR. Dani Yogatama and Noah A. Smith. 2014. ”Linguis- tic Structured Sparsity in Text Categorization”. In Proceedings of the 52nd Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 786–796. Association for Com- putational Linguistics. A Appendices Datasets Statistics for the datasets are listed in Table 5. For WQ, the very good class was created by Louis and Nenkova (2013) using as a seed the 63 articles in the New York Times corpus (Sandhaus, 2008) deemed to be high-quality writing by a team of expert journalists. The class was then expanded by adding all other science articles in the NYT cor- pus that were written by the seed authors (4,253 articles). For the typical class, science articles by all other authors were included (19,520). Because the data is very imbalanced, we undersample the typical class to be the same size as the very good. We split this data into 80/10/10 for training, devel- opment and test, with both classes equally repre- sented in each partition. For WQTC, the original dataset authors provide a list of the 10 most topically similar articles for each article.11 We make use of this list to explicitly sample topically similar documents. 11http://www.cis.upenn.edu/˜nlp/ corpora/scinewscorpus.html Preprocessing For Debates and Yelp, we follow the same preprocessing steps as in L&L, but do not set a minimum frequency threshold when creating the word embeddings. For", "meta": {"paper_id": "1906.01472v2", "chunk_index": 9, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_10", "text": "10 most topically similar articles for each article.11 We make use of this list to explicitly sample topically similar documents. 11http://www.cis.upenn.edu/˜nlp/ corpora/scinewscorpus.html Preprocessing For Debates and Yelp, we follow the same preprocessing steps as in L&L, but do not set a minimum frequency threshold when creating the word embeddings. For our three datasets, sen- tences are split and tokenized using Stanford Core NLP. Training For all models, we use the Adagrad optimizer with a learning rate of 0.05. For WQ, WQTC, and WSJSO, gradient clipping is per- formed using the global norm with a ratio of 1.0. The batch size is 32 for all models except WSJSO uses 16. All models are trained for a maximum of 8 hours on a GeForce GTX 1080 Ti card. Results Because our results hinge on multiple runs of experiments each initialized with different random weights, we include here more detailed versions of our results to more accurately illustrate their variability. Table 6 supplements Table 2 with tree statistics from L&L(orig), the model without the design change or bug ﬁx, to illustrate the de- rived trees on this model are similar. Finally, Table 7 is a more detailed version of Table 4, which addi- tionally includes maximum accuracy, standard de- viation for accuracy, as well as the average parent entropy calculated over the latent trees. Number of documents Dataset Classes Total Train Dev Test Vocab. Yelp 5 333K 266,522 33,333 33,317 53K Debates 2 1.5K 1,050 102 374 21K WQ 2 7.7K 6,195 775 763 150K WQTC 2 7.8K 6,241 777 794 131K WSJSO - 2.4K 1,950 (35,165) 247 (4,392) 241 (4,383) 49K Table 5: Statistics for the datasets used in the text classiﬁcation and discrimination tasks (calculated after prepro- cessing). For WSJSO, the number of generated pairs are in parentheses. Yelp Debates WQ WQTC WSJSO tree height 2.049 (2.248) 2.751 (2.444) 2.909 (2.300) 4.035 (2.468) 2.288 (2.368) prop. of leaf nodes 0.825 (0.801) 0.849 (0.869) 0.958 (0.971) 0.931 (0.966) 0.892 (0.888) norm. arc length 0.433 (0.468) 0.397 (0.377) 0.420 (0.377) 0.396 (0.391) 0.426 (0.374) % vacuous trees 73% (68%) 38% (40%) 42% (28%) 14% (21%) 100% (56%) Table 6: Statistics for the learned trees averaged across four runs on the L&L(ours) model with comparisons (in parentheses) to results using the original L&L code without the design change or bug ﬁx. Accuracy tree height prop. of leaf parent entr. norm. arc length % vacuous trees Full 82.49 | 81.11 (0.95) 4.035 0.931 0.774 0.396 14% -biLSTM 80.35 | 77.80 (1.72) 11.51 0.769 1.876 0.353 4% -biLSTM, +p 78.72 | 77.11 (2.18) 10.430 0.790 0.349 0.349 3% -biLSTM, +4p 82.75 | 81.71 (0.70) 9.588 0.811 1.60 0.353 3% -biLSTM, +w 78.46 | 75.57 (2.52) 7.364 0.856 1.307 0.359 3% -biLSTM, +w, +p 77.08 | 74.78 (2.58) 8.747 0.826 1.519 0.349 4% parsed RST - 25.084 0.567 2.711 0.063 0% Table 7: Max | mean (standard deviation) test accuracy and tree statistics of the WQTC dev set (averaged across four training runs with different initialization weights). Bolded numbers are within 1 standard deviation of the best performing model.", "meta": {"paper_id": "1906.01472v2", "chunk_index": 10, "filename": "1906.01472v2.pdf", "chunk_length": 512}}
{"id": "1906.01472v2_chunk_11", "text": "0.826 1.519 0.349 4% parsed RST - 25.084 0.567 2.711 0.063 0% Table 7: Max | mean (standard deviation) test accuracy and tree statistics of the WQTC dev set (averaged across four training runs with different initialization weights). Bolded numbers are within 1 standard deviation of the best performing model. +w uses the weighted sum, +p adds 1 extra level of percolation, +4p adds 4 levels of percolation. The last row are the (‘gold’) parsed RST discourse dependency trees.", "meta": {"paper_id": "1906.01472v2", "chunk_index": 11, "filename": "1906.01472v2.pdf", "chunk_length": 79}}
{"id": "1906.01573v1_chunk_0", "text": "A Study of Feature Extraction techniques for Sentiment Analysis Avinash M and Sivasankar E* Abstract Sentiment Analysis refers to the study of systematically extracting the meaning of subjective text . When analysing sentiments from the subjective text us- ing Machine Learning techniques,feature extraction becomes a signiﬁcant part. We perform a study on the performance of feature extraction techniques TF-IDF(Term Frequency-Inverse Document Frequency) and Doc2vec (Document to Vector) us- ing Cornell movie review datasets, UCI sentiment labeled datasets, stanford movie review datasets,effectively classifying the text into positive and negative polarities by using various pre-processing methods like eliminating Stop Words and Tokeniza- tion which increases the performance of sentiment analysis in terms of accuracy and time taken by the classiﬁer.The features obtained after applying feature extraction techniques on the text sentences are trained and tested using the classiﬁers Logis- tic Regression,Support Vector Machines,K-Nearest Neighbours , Decision Tree and Bernoulli Nave Bayes. 1 Introduction In Machine Learning,a feature refers to the information which can be extracted from any data sample.A feature uniquely describes the properties possessed by the data.The data used in machine learning consists of features projected onto a high dimensional feature space.These high dimensional features must be mapped onto a small number of low dimensional variables that preserves information about the data as much as possible.Feature extraction is one of the dimensionality reduction tech- Avinash M National Institute of Technology,Tiruchirapalli , Tanjore Main Road, National Highway 67, Near BHEL Trichy, Tiruchirappalli, Tamil Nadu, 620015. e-mail: avinash.sai001@gmail.com Sivasankar E National Institute of Technology,Tiruchirapalli , Tanjore Main Road, National Highway 67, Near BHEL Trichy, Tiruchirappalli, Tamil Nadu, 620015. e-mail: sivasankar@nitt.edu 1 arXiv:1906.01573v1 [cs.CL] 4 Jun 2019 2 Avinash M and Sivasankar E* niques used in machine learning to map higher dimensional data onto a set of low dimensional potential features.Extracting informative and essential features greatly enhances the performance of machine learning models and reduces the computa- tional complexity. The growth of modern web applications like facebook,twitter persuaded users to express their opinions on products,persons and places. Millions of consumers re- view the products on online shopping websites like amazon,ﬂipkart.These reviews act as valuable sources of information that can increase the quality of services pro- vided.With the growth of enormous amount of user generated data,lot of efforts are made to analyze the sentiment from the consumer reviews.But analyzing unstruc- tured form of data and extracting sentiment out of it requires a lot of natural lan- guage processing(nlp) and text mining methodologies.Sentiment analysis attempts to derive the polarities from the text data using nlp and text mining techniques.The classiﬁcation algorithms in machine learning require the most appropriate set of features to classify the text as positive polarity and negative polarity.Hence, feature extraction plays a prominent role in sentiment analysis. 2 Literature Review Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan [1] have conducted a study on sentiment analysis using machine learning techniques.They compared the per- formance of machine learning techniques with human generated baselines and pro- posed that machine learning techniques are quite good in comparison to human generated baselines.A. P. Jain and V. D. Katkar [2] have", "meta": {"paper_id": "1906.01573v1", "chunk_index": 0, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_1", "text": "Shivakumar Vaithyanathan [1] have conducted a study on sentiment analysis using machine learning techniques.They compared the per- formance of machine learning techniques with human generated baselines and pro- posed that machine learning techniques are quite good in comparison to human generated baselines.A. P. Jain and V. D. Katkar [2] have performed sentiment anal- ysis on twitter data using data mining techniques.They analyzed the performance of various data mining techniques and proposed that data mining classiﬁers can be a good choice for sentiment prediction.Tim O’Keefe and Irena Koprinska [3] have done research on feature selection and weighting methods in sentiment Analysis. In their research they have combined various feature selection techniques with feature weighing methods to estimate the performance of classiﬁcation algorithms. Shereen Albitar, Sebastien Fournier, Bernard Espinasse [4] have a proposed an effective TF-IDF based text-to-text semantic similarity measure for text classiﬁcation. Quoc Le and Tomas Mikolov [5] introduced distributed representation of sentences and documents for text classiﬁcation. Parinya Sanguansat [6] performed paragraph2vec based sentiment analysis on social media for business in Thailand.Izzet Fatih enturk and Metin Bilgin [7] performed sentiment analysis on twitter data using doc2vec. Analysing all the above techniques gave us deep insight about various methodolo- gies used in sentiment analysis.Therefore, we propose a comparison based study on the performance of feature techniques used in sentiment analysis and the results are described in the following sections. The proposed study involves two different features extraction techniques TF-IDF and Doc2Vec. These techniques are used to extract features from the text data. These features are then used in identifying the polarity of text data using classiﬁcation al- A Study of Feature Extraction techniques for Sentiment Analysis 3 gorithms.The performance evaluation of TF-IDF and Doc2vec are experimented on 5 benchmark datasets of varying sizes. These datasets contains user reviews from various domains. Experimental results shows that the performance of feature extrac- tion techniques trained and tested using these benchmark datasets.A lot of factors affects the sentiment analysis of text data.When the text data is similar it becomes difﬁcult to extract suitable features.If the features extracted are not informative,it signiﬁcantly affects the performance of classiﬁcation algorithms.When the patterns lie closest to each other,drawing separating surfaces can be tricky for the classi- ﬁers.Classiﬁers like SVM aim to maximize the margin between two classes of in- terest. 3 Methodology The performance evaluation of feature extraction techniques TF-IDF and Doc2Vec requires datasets of various sizes. The tests are conducted on the publicly available datasets related to movie reviews and reviews related to other commercial products. The performance is evaluated by varying the sizes of datasets and the accuracy of each of the two feature extraction techniques is measured using the classiﬁers.The ﬁrst data set is a part of movie review dataset [8] consisting of 1500 reviews.The sec- ond dataset used is Sentiment Labelled dataset [9] consisting of 2000 reviews taken from UCI repository of datasets.The third dataset used is a part of polarity dataset v2.0 [10] taken from CS cornell.edu.The fourth dataset sentence polarity dataset v1.0 [11] taken from CS cornell.edu.The ﬁfth dataset used is Large movie", "meta": {"paper_id": "1906.01573v1", "chunk_index": 1, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_2", "text": "ond dataset used is Sentiment Labelled dataset [9] consisting of 2000 reviews taken from UCI repository of datasets.The third dataset used is a part of polarity dataset v2.0 [10] taken from CS cornell.edu.The fourth dataset sentence polarity dataset v1.0 [11] taken from CS cornell.edu.The ﬁfth dataset used is Large movie review dataset [8] taken from ai.stanford.edu. The reviews are categorized into positive and negative. We perform analysis on reviews in 3 stages Preprocessing,Feature Extrac- tion and classiﬁcation.Figure 1 shows the experimental setup for sentiment analysis. 3.1 Pre-processing We perform Pre-processing in two stages- Tokenization and by eliminating stop words. Tokenization refers to the process of breaking the text into meaningful data that retains the information about the text.Text data contain some unwanted words that does not give any meaning to the data called as stop words.These words are removed from the text during feature extraction procedures. 3.2 Feature Extraction Two techniques are used for feature extraction -TF-IDF and Doc2vec.Figure 2 shows the steps involved in extracting features using both the techniques. 4 Avinash M and Sivasankar E* Fig. 1 Experimental Setup 3.2.1 TF-IDF TF-IDF is short form for term frequency-inverse document frequency.TF-IDF is one of the largely used methods in information retrieval and text mining.TF-IDF is a weight metric which determines the importance of word for that document. 3.2.1.1 TF Term Frequency measures number of times a particular term t occured in a document d. Frequency increases when the term has occured multiple times.TF is calculated by taking ratio of frequency of term t in document d to number of terms in that particular document d. TF(t,d) = Number of times term t appears in a document d Total number terms in a document d (1) A Study of Feature Extraction techniques for Sentiment Analysis 5 Fig. 2 Feature Extraction Procedure 3.2.1.2 IDF TF measures only the frequency of a term t.Some terms like stop words occur multi- ple times but may not be useful.Hence Inverse Document Frequency(IDF) is used to measure term’s importance.IDF gives more importance to the rarely occuring terms in the document d. IDF is calculated as: IDF(t) = loge Total number of documents Total number of documents with term t in it (2) The ﬁnal weight for a term t in a document d is calculated as: TF −IDF(t,d) = TF(t,d)×IDF(t) (3) 6 Avinash M and Sivasankar E* 3.2.2 Doc2Vec An extended version to word2vec, doc2vec model was put forward by Le and Miklov(2014) [5] to improve the learning of embeddings from word to word se- quences. doc2vec can be applied for word n-gram, sentence, paragraph or document. Doc2vec is a set of approaches to represent documents as ﬁxed length low dimen- sional vectors. Doc2Vec is a three layer neural network with an input ,one hidden layer and an ouput layer.Doc2vec was proposed in two forms: dbow and dmpv.In Word2vec two algorithms continuous bag of words (CBOW) and skip-gram(SG) algorithms are implemented using deep Learning, in Doc2vec these algorithms cor- respond to distributed memory(DM) and distributed bag of words(DBoW). 3.3 Classiﬁcation Classiﬁers are trained on training data sets", "meta": {"paper_id": "1906.01573v1", "chunk_index": 2, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_3", "text": "an ouput layer.Doc2vec was proposed in two forms: dbow and dmpv.In Word2vec two algorithms continuous bag of words (CBOW) and skip-gram(SG) algorithms are implemented using deep Learning, in Doc2vec these algorithms cor- respond to distributed memory(DM) and distributed bag of words(DBoW). 3.3 Classiﬁcation Classiﬁers are trained on training data sets with the features obtained from the fea- ture extraction techniques TF-IDF and Doc2vec and with the corresponding output labels from the dataset. The test data is tested with the trained classiﬁers to predict the sentiments and the accuracy is measured for test data.Classiﬁers Logistic Re- gression , KNearest Neighbors, Decision Tree, Bernoulli Nave Bayes and support vector machines with linear and rbf kernels are used for sentiment analysis of the datasets. 3.3.1 Logistic Regression(LR) In linear regression we try to predict the value y (i) for the i th training sample using a linear function Y = hθ(x) = θ Tx. This is clearly not a great solution to predict binary-valued labels.(y(i) ∈{0,1}).Logistic regression use a different hypothesis that predicts the probability that a given example belongs to class “1” and the prob- ability that the example belongs to class “0” . Logistic Regression is formulated as: P(y = 1|x) = 1 1+e−θT x (4) P(y = 0|x) = 1−hθ(x) (5) 3.3.2 K-Nearest Neighbors(KNN) In KNN,the input is represented in the feature space by k-closest training samples. KNN uses the procedure of class membership. Each object is classiﬁed by taking its neighbors votes and the object is assigned to class which has got maximum number of votes.The training examples are represented as vectors in a multi-dimensional A Study of Feature Extraction techniques for Sentiment Analysis 7 feature space with a class label. The training of the KNN algorithm involves stor- ing training samples as feature vectors with their corresponding labels. k is decided by the user during classiﬁcation and the unlabeled data is assigned to class which is more frequent among k of the training samples nearest to that unlabeled sam- ple.Euclidean distance is used to calculate the distance. 3.3.3 Naive Bayes(NB) Bayes theorem forms the basis for Naive Bayes classiﬁer.It assumes that features are strongly independent and calculates the probability.Multi variate Bernoulli event model is one of the naive bayes classiﬁers used in sentimental analysis. If Xi is a boolean expressing the occurrence or absence of the ith term from the vocabulary , then the likelihood of a document given a class Ck is given by: P(x|Ck) = n ∏ i=1 pxi ki(1−pki)1−xi (6) 3.3.4 Support Vector Machines(SVM) The training samples are represented as points in the feature space.SVM performs classiﬁcation by separating the points with a set of margin planes.The boundary hyperplane is chosen which maximizes the distance to the training samples.Support vectors are the points that determine the margin planes. Data which can be separated linearly is classiﬁed using Linear kernel and the data which is not linearly separable is classiﬁed using RBF kernel. 3.3.5 Decision Tree(DT) Decision Trees performs classiﬁcation by using yes or no conditions.A decision tree consists of edges and nodes.The root node doesn’t contain incoming", "meta": {"paper_id": "1906.01573v1", "chunk_index": 3, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_4", "text": "planes. Data which can be separated linearly is classiﬁed using Linear kernel and the data which is not linearly separable is classiﬁed using RBF kernel. 3.3.5 Decision Tree(DT) Decision Trees performs classiﬁcation by using yes or no conditions.A decision tree consists of edges and nodes.The root node doesn’t contain incoming edges,nodes which contain out going edges are called test nodes,nodes with no outgoing nodes are called decision nodes.Edges represent the conditions.Each decision node holds a class label.When the unlabeled data samples are to be classﬁed,they pass through series of test nodes ﬁnally leading to the decision node with a class label and the class label is assigned to that unlabeled data sample. 4 Experimental Results The performance of TF-IDF and Doc2Vec is veriﬁed by experimenting the feature extraction techniques on 5 different datasets of varying sizes. First the experiment 8 Avinash M and Sivasankar E* is done on a small data set(700 positive and 800 negative reviews) of movie reviews taken from Large polarity dataset 1.0 [8]. Secondly, the experiment is done on Sen- timent Labelled dataset [9](2000 reviews) of UCI data repository. The third exper- iment is done on a corpus (100 negative and 100 positive documents) made from the polarity dataset v2.0 [10](1000 negative and 1000 positive documents) taken from the CS.cornell.edu. The fourth experiment is done on sentence polarity dataset v1.0(5331 positive and 5331 negative reviews) taken from CS.Cornell.edu [11] . Fifth experiment is done on Large movie review dataset [8](25000 positive reviews and 25000 negative reviews) taken from the ai.stanford.edu. Using regular expres- sions and string processing methods sentences and labels are separated from the datasets. The sentences obtained are fed into feature extraction techniques TF-IDF and Doc2Vec to generate vector(real numbers) features for each sentence.The Split of training and testing samples is done by either hold out method where 50% data is used for training and 50% data is used for testing or by 10-fold cross validation(CV) where 9 folds are used for training and 1 fold is used for testing.Table 1 shows the methods used for splitting training and test datasets. Table 1 Methods used for training and testing Datasets Dataset-1 Dataset-2 Dataset-3 Dataset-4 Dataset-5 Method used Hold out 10 fold CV 10 fold CV 10 fold CV Hold out Training samples 800 9 folds 9 folds 9 folds 25000 Testing samples 700 1 fold 1 fold 1 fold 25000 Table 2 Hyper parameters tuned for TF-IDF Vectorizer Parameters and Datasets Dataset-1 Dataset-2 Dataset-3 Dataset-4 Dataset-5 min df 5 5 5 5 5 max df 0.8 * n d 0.8 * n d 0.8 * n d 0.8 * n d 0.8 * n d encoding utf-8 utf-8 utf-8 utf-8 utf-8 sublinear df True True True True True use idf True True True True True stop words English English English English English Table 3 Hyper parameters tuned for Doc2Vec Vectorizer Parameters and Datasets Dataset-1 Dataset-2 Dataset-3 Dataset-4 Dataset-5 min count 1 1 1 1 1 window size 10 10 10 10 10 vector size 100 100 100 100 100 sample 1e-4 1e-5 1e-5 1e-5 1e-4 negative", "meta": {"paper_id": "1906.01573v1", "chunk_index": 4, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_5", "text": "words English English English English English Table 3 Hyper parameters tuned for Doc2Vec Vectorizer Parameters and Datasets Dataset-1 Dataset-2 Dataset-3 Dataset-4 Dataset-5 min count 1 1 1 1 1 window size 10 10 10 10 10 vector size 100 100 100 100 100 sample 1e-4 1e-5 1e-5 1e-5 1e-4 negative 5 0 5 5 5 workers 7 1 1 1 7 dm 1 0 0 0 1 Each training and testing data set obtained by any of the above methods are fed into feature extraction techniques TF-IDF and Doc2Vec to generate vectors.The A Study of Feature Extraction techniques for Sentiment Analysis 9 TF-IDF vectorizer used is tuned with hyper parameters min df (min number of times term t has to occur in all sentences),n d = Number of sentences in a train- ing or testing corpus, max df (maximum number of times a term t can occur in all the sentences which is calculated as max df * n d), Encoding(encoding used), sublinear tf (use of term frequency) , use idf (use of inverse document frequency), stopwords(to eliminate stopwords). The Doc2Vec vectorizer used is tuned with hy- per parameters min count(minimum number of times a word has to occur),window size(maximum distance between predicted word and context words used for predic- tion),vector size(size of vector for each sentence),sample( threshold for conﬁguring which higher frequency words are randomly downsampled),negative(negative sam- pling is used for drawing noisy words),workers(number of workers used to extract feature vectors),dm(type of training algorithm used - 0 for distributed bag of words and 1 for distributed model), Epochs(number of iterations for training).Tables 2 and 3 shows hyper paramaters tuned for both the techniques. 5 Performance Analysis To estimate the performance of TF-IDF and Doc2Vec on classiﬁcation algorithms we use accuracy measure as the metric. Accuracy is calculated as the ratio of num- ber of reviews that are correctly predicted to the total number of reviews. Let us assume 2 × 2 matrix as deﬁned in Table 4. Tables 5 and 6 shows the accuracy of all classiﬁers for each feature extraction technique trained and tested on all the datasets. Table 4 Contingency matrix for sentiment analysis Predicted sentiment +ve sentiment -ve sentiment +ve sentiment True Positive(TP) False Negative(FN) Actual Sentiment -ve sentiment False Positive(FP) True Negative(TN) Table 5 Accuracy of classiﬁers Datasets Dataset-1 Dataset-1 Dataset-2 Dataset-2 Dataset-3 Dataset-3 Classiﬁers TF-IDF Doc2Vec TF-IDF Doc2Vec TF-IDF Doc2Vec logistic regression 83.428 86.1428 81.950 77.400 98.864 98.864 SVM-rbf 75.142 86.85714 82.350 77.400 98.887 98.864 SVM-linear 78.000 85.71428 77.400 77.400 98.864 98.864 KNN(n=3) 64.714 74.142 77.400 70.05 98.793 98.816 DT 65.571 70.285 76.950 64.850 97.633 97.065 bernoulli-NB 80.714 83.71428 81.050 76.850 98.840 97.823 Figure 3 shows the plot of accuracy for all classiﬁers using TF-IDF. Figure 4 shows the plot of accuracy for all classiﬁers using Doc2Vec. Based on the accuracy values from Tables 5 and 6, 10 Avinash M and Sivasankar E* Table 6 Accuracy of classiﬁers Datasets Dataset-4 Dataset-4 Dataset-5 Dataset-5 Classiﬁers TF-IDF Doc2Vec TF-IDF Doc2Vec logistic regression 82.633 99.98124 88.460 86.487 SVM-rbf 82.820 99.98124 67.180 86.760 SVM-linear 82.596 99.98124 87.716 86.472 KNN(n=5) 82.417", "meta": {"paper_id": "1906.01573v1", "chunk_index": 5, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_6", "text": "Doc2Vec. Based on the accuracy values from Tables 5 and 6, 10 Avinash M and Sivasankar E* Table 6 Accuracy of classiﬁers Datasets Dataset-4 Dataset-4 Dataset-5 Dataset-5 Classiﬁers TF-IDF Doc2Vec TF-IDF Doc2Vec logistic regression 82.633 99.98124 88.460 86.487 SVM-rbf 82.820 99.98124 67.180 86.760 SVM-linear 82.596 99.98124 87.716 86.472 KNN(n=5) 82.417 99.98124 68.888 74.592 DT 74.137 99.97186 71.896 67.140 bernoulli-NB 79.330 99.98124 82.020 80.279 Fig. 3 TF-IDF performance Fig. 4 Doc2Vec performance 1. For the ﬁrst dataset, Doc2Vec performed better than TF-IDF for all classi- ﬁers.The accuracy is highest for Logistic Regression in case of TF-IDF whereas SVM with rbf kernel achieved highest accuracy in case of Doc2Vec. 2. For the second dataset, TF-IDF performed better than Doc2Vec.The accuracy is highest for SVM with rbf kernel in case of TF-IDF whereas LR,SVM with rbf and linear kernels achieved highest accuracy in case of Doc2Vec. 3. For the third dataset, TF-IDF and Doc2Vec achieved similar accuracies.SVM with rbf kernel achieved highest accuracy in case of TF-IDF whereas LR,SVM with rbf and linear kernels achieved highest accuracy in case of Doc2Vec. 4. For the fourth dataset, Doc2Vec performed better than TF-IDF for all classi- ﬁers.The accuracy is highest for SVM with rbf kernel in case of TF-IDF whereas A Study of Feature Extraction techniques for Sentiment Analysis 11 LR,SVM with rbf,linear kernels,Bernoulli NB achieved highest accuracy in case of Doc2Vec. 5. For the ﬁfth dataset, TF-IDF performed slightly better than Doc2Vec.The accu- racy is highest for LR in case of TF-IDF whereas SVM with rbf kernel achieved highest accuracy for Doc2Vec. Considering the performance analysis of all the classiﬁers on all datasets , we con- clude that Logistic regression, SVM with linear and rbf kernels performs better than all the other classiﬁers. 6 Conclusion: The purpose of this robust analysis is to provide deeper insight about the perfor- mance of feature extraction techniques TF-IDF and Doc2Vec . Based on the accu- racy measures for all the datasets, Doc2vec and TF-IDF has achieved satisfactory performance for most of the datasets. But the accuracy scores of Doc2vec are better when compared to TF-IDF on most of the data sets. This work can be extended to test these techniques on unstructured data to analyze the performance of features extracted from both the methods. References 1. B.Pang,L.Lee,S.Vaithyanathan: Thumbs up?: sentiment classiﬁcation using machine learning techniques,In: Proceedings of the ACL-02 Conference on Empirical Methods in Natural Lan- guage Processing,Vol 10,Series. EMNLP ’02, pp. 79-86, (2002). 2. A. P. Jain and V. D. Katkar : Sentiments analysis of Twitter data using data mining, In : Inter- national Conference on Information Processing (ICIP), pp. 807-810, (2015). 3. Irena Koprinska and Tim O’Keefe: Feature selection and weighting methods in sentiment anal- ysis. In: Proceedings of the 14th Australasian Document Computing Symposium, pp. 67-74, Sydney, Australia, (2009). 4. Albitar S.,Espinasse B, Fournier S.: An Effective TF/IDF-Based Text-to-Text Semantic Simi- larity Measure for Text Classiﬁcation.In: Benatallah B., Bestavros A., Manolopoulos Y,Vakali A,Zhang Y.(eds): Web Information Systems Engineering WISE 2014. Lecture Notes in Com- puter Science, Vol 8786. Springer, Cham,(2014). 5. Quoc V. Le, Tomas Mikolov", "meta": {"paper_id": "1906.01573v1", "chunk_index": 6, "filename": "1906.01573v1.pdf", "chunk_length": 512}}
{"id": "1906.01573v1_chunk_7", "text": "Sydney, Australia, (2009). 4. Albitar S.,Espinasse B, Fournier S.: An Effective TF/IDF-Based Text-to-Text Semantic Simi- larity Measure for Text Classiﬁcation.In: Benatallah B., Bestavros A., Manolopoulos Y,Vakali A,Zhang Y.(eds): Web Information Systems Engineering WISE 2014. Lecture Notes in Com- puter Science, Vol 8786. Springer, Cham,(2014). 5. Quoc V. Le, Tomas Mikolov : Distributed Representations of Sentences and Documents, In: CoRR, Vol. abs/1405.4053, (2014). 6. Parinya Sanguansat: Paragraph2Vec-Based Sentiment Analysis on Social Media for Business in Thailand. In: 8th International Conference on Knowledge and Smart Technology (KST), pp. 175-178, (2016). doi:10.1109/KST.2016.7440526 7. M. Bilgin and I.F.Senturk: Sentiment analysis on Twitter data with semi-supervised Doc2Vec. In:International Conference on Computer Science and Engineering(UBMK), pp. 661-666, (2017). doi:10.1109/UBMK.2017.8093492 8. Andrew L. Maas, Andrew Y. Ng,Christopher Potts,Dan Huang,Peter T. Pham,Raymond E. Daly : Learning Word Vectors for Sentiment Analysis. In: Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Vol. 1, Series. HLT ’11, pp. 142-150, Portland, Oregon ,(2011). 12 Avinash M and Sivasankar E* 9. Dimitrios Kotzias,Misha Denil, Nando de Freitas,Padhraic Smyth : From Group to Individual Labels Using Deep Features, In : Proceedings of the 21th ACM SIGKDD International Confer- ence on Knowledge Discovery and Data Mining, Series. KDD ’15, pp. 597-606, Sydney, NSW, Australia ,(2015). 10. Bo Pang and Lillian Lee: A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts. In: Proceedings of the 42nd Annual Meeting on As- sociation for Computational Linguistics, Series. ACL ’04, articleno. 271, (2004). 11. Bo Pang,Lillian Lee : Seeing Stars: Exploiting Class Relationships for Sentiment Categoriza- tion with Respect to Rating Scales.In :Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, Series. ACL’05, pp. 115-124, (2005).", "meta": {"paper_id": "1906.01573v1", "chunk_index": 7, "filename": "1906.01573v1.pdf", "chunk_length": 280}}
{"id": "1906.01617v1_chunk_0", "text": "Self-Attentional Models for Lattice Inputs Matthias Sperber1, Graham Neubig2, Ngoc-Quan Pham1, Alex Waibel1,2 1Karlsruhe Institute of Technology, Germany 2Carnegie Mellon University, USA {first}.{last}@kit.edu, gneubig@cs.cmu.edu Abstract Lattices are an efﬁcient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recogni- tion hypotheses, or to represent multiple lin- guistic analyses. Previous work has extended recurrent neural networks to model lattice in- puts and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a se- quence modeling technique that relates inputs to one another by computing pairwise simi- larities and has gained popularity for both its strong results and its computational efﬁciency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also pro- pose a method for adapting positional embed- dings to lattice structures. We apply the pro- posed model to a speech translation task and ﬁnd that it outperforms all examined baselines while being much faster to compute than pre- vious neural lattice models during both train- ing and inference. 1 Introduction In many natural language processing tasks, graph- based representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a com- pact form (Figure 1). Example applications in- clude speech recognition lattices that represent al- ternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous de- cisions on word boundaries or morphological al- ternatives (Dyer et al., 2008), word class lattices f a S c E d b e g 1 0.4 1 1 1 0.6 0.2 0.8 1 1 Figure 1: Example of a node-labeled lattice. Nodes are labeled with word tokens and posterior scores. (Navigli and Velardi, 2010), and lattices for alter- native video descriptions (Senina et al., 2014). Prior work has made it possible to handle these through the use of recurrent neural network (RNN) lattice representations (Ladhak et al., 2016; Su et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree struc- tures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured in- puts prevents convenient use of batched compu- tation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Deffer- rard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural lan- guage processing tasks (Bastings et al., 2017; Ce- toli et al., 2017; Vashishth et al., 2018). For linear sequence modeling, self-attention (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017)", "meta": {"paper_id": "1906.01617v1", "chunk_index": 0, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_1", "text": "context and therefore requires combination with slower RNN layers for typical natural lan- guage processing tasks (Bastings et al., 2017; Ce- toli et al., 2017; Vashishth et al., 2018). For linear sequence modeling, self-attention (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017) now provides an alter- native to RNNs. Self-attention encodes sequences by relating sequence items to one another through computation of pairwise similarity, with addition of positional encoding to model positions of words in a linear sequence. Self-attention has gained popularity thanks to strong empirical results and computational efﬁciency afforded by paralleliz- arXiv:1906.01617v1 [cs.CL] 4 Jun 2019 able computations across sequence positions. In this paper, we extend the previously purely sequential self-attentional models to lattice inputs. Our primary goal is to obtain additional modeling ﬂexibility while avoiding the increased cost of pre- vious lattice-RNN-based methods. Our technical contributions are two-fold: First, we incorporate the global lattice structure into the model through reachability masks that mimic the pairwise condi- tioning structure of previous recurrent approaches. These masks can account for lattice scores if avail- able. Second, we propose the use of lattice posi- tional embeddings to model positioning and order- ing of lattice nodes. We evaluate our method on two standard speech translation benchmarks, replacing the encoder component of an attentional encoder-decoder model with our proposed lattice self-attentional encoder. Results show that the proposed model outperforms all tested baselines, including LSTM- based and self-attentional sequential encoders, a LatticeLSTM encoder, and a recently proposed self-attentional model that is able to handle graphs but only considers local context, similar to GCNs. The proposed model performs well without sup- port from RNNs and offers computational advan- tages in both training and inference settings. 2 Background 2.1 Masked Self-Attention We start by introducing self-attentional models for sequential inputs, which we will extend to lattice- structured inputs in § 4. Attentional models in general can be described using the terminology of queries, keys, and val- ues. The input is a sequence of l values, along with a key corresponding to each value. For some given query, the model computes how closely each key matches the query. Here, we assume values, keys, and queries vk, kk, q∈Rd, for some dimen- sionality d and sequence indices k∈{1 . . . l}. Us- ing the computed similarity scores f(q, kk), at- tention computes a weighted average of the values to obtain a ﬁxed-size representation of the whole sequence conditioned on this query. In the self- attentional case, the sequence items themselves are used as queries, yielding a new sequence of same length as output in which each of the orig- inal input elements has been enriched by the re- spectively relevant global context. The following equations formalize this idea. We are given a sequence of input vectors xk ∈Rd. For every query index i, we compute an output vector yi as: eij = f (q (xi) , k (xj)) +mij (∀1≤j≤l) (1) αi = softmax (ei) (2) yi = l X j=1 αijv (xj) . (3) Here, unnormalized pairwise similarities eij are", "meta": {"paper_id": "1906.01617v1", "chunk_index": 1, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_2", "text": "a sequence of input vectors xk ∈Rd. For every query index i, we compute an output vector yi as: eij = f (q (xi) , k (xj)) +mij (∀1≤j≤l) (1) αi = softmax (ei) (2) yi = l X j=1 αijv (xj) . (3) Here, unnormalized pairwise similarities eij are computed through the similarity function f, and then normalized as αij for computation of a weighted sum of value vectors. q, k, v denote parametrized transformations (e.g. afﬁne) of the inputs into queries, keys, and values. Equation 1 also adds an attention masking term mij ∈R that allows adjusting or disabling the in- ﬂuence of context at key position j on the output representation at query position i. Masks have, for example, been used to restrict self-attention to ig- nore future decoder context (Vaswani et al., 2017) by setting mij = −∞for all j>i. We will use this concept in § 4.1 to model reachability structure. 2.2 Lattices We aim to design models for lattice inputs that store a large number of sequences in a compact data structure, as illustrated in Figure 1. We deﬁne lattices as directed acyclic graphs (DAGs) with the additional property that there is exactly one start node (S) and one end node (E). We call the se- quences contained in the lattice complete paths, running from the start node to the end node. Each node is labeled with a word token.1 To make matters precise, let G=(V, E) be a DAG with nodes V and edges E. For k∈V , let R+ G(k) denote all successors (reachable nodes) of node k, and let N+ G(k) denote the neighborhood, deﬁned as the set of all adjacent successor nodes. R– G(k), N– G(k) are deﬁned analogously for prede- cessors. j ≻i indicates that node j is a successor of node i. For arbitrary nodes i, j, let pG (j ≻i | i) be the probability that a complete path in G con- tains j as a successor of i, given that i is con- tained in the path. Note that j /∈R+ G(i) im- plies pG (j ≻i | i) =0. The probability structure 1Edge-labeled lattices can be easily converted to node- labeled lattices using the line-graph algorithm (Hemminger and Beineke, 1978). of the whole lattice can be represented through transition probabilities ptrans k,j :=pG (k ≻j | j) for j ∈N+ G(k). We drop the subscript G when clear from context. 3 Baseline Model Our proposed model builds on established archi- tectures from prior work, described in this section. 3.1 Lattice-Biased Attentional Decoder The common attentional encoder-decoder model (Bahdanau et al., 2015) serves as our starting point. The encoder will be described in § 4. As cross-attention mechanism, we use the lattice- biased variant (Sperber et al., 2017), which adjusts the attention scores αcross ij between encoder posi- tion j and decoder position i according to marginal lattice scores p (j ≻S | S) (§ 4.1.2 describes how to compute these) as follows:2 αcross ij ∝exp (score(•) + log p (j ≻S | S)) . (4) Here, score(•)", "meta": {"paper_id": "1906.01617v1", "chunk_index": 2, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_3", "text": "which adjusts the attention scores αcross ij between encoder posi- tion j and decoder position i according to marginal lattice scores p (j ≻S | S) (§ 4.1.2 describes how to compute these) as follows:2 αcross ij ∝exp (score(•) + log p (j ≻S | S)) . (4) Here, score(•) is the unnormalized attention score. In the decoder, we use long short-term mem- ory (LSTM) networks, although it is straightfor- ward to use alternative decoders in future work, such as the self-attentional decoder proposed by Vaswani et al. (2017). We further use input feed- ing (Luong et al., 2015), variational dropout in the decoder LSTM (Gal and Ghahramani, 2016), and label smoothing (Szegedy et al., 2016). 3.2 Multi-Head Transformer Layers To design our self-attentional encoder, we use Vaswani et al. (2017)’s Transformer layers that combine self-attention with position-wise feed- forward connections, layer norm (Ba et al., 2016), and residual connections (He et al., 2016) to form deeper models. Self-attention is modeled with multiple heads, computing independent self- attentional representations for several separately parametrized attention heads, before concatenat- ing the results to a single representation. This increases model expressiveness and allows using different masks (Equation 1) between different at- tention heads, a feature that we will exploit in § 4.1. Transformer layers are computed as follows: 2We have removed the trainable peakiness coefﬁcient from the original formulation for simplicity and because gains of this additional parameter were unclear according to Sperber et al. (2017). Qk = XW(q) k , Kk=XW(k) k , Vk=XW(v) k (5) Hk = softmax \u0012 1 √ d dropout \u0010 QiK⊤ k +M \u0011\u0013 Vk (6) H = concat(H1, H2, . . . , Hn) (7) L = LN [dropout (H + X)] (8) Y = LN [dropout (FF (L) + L)] (9) Here, X∈Rl×d, Qk, Kk, Vk∈Rl×d/n denote in- puts and their query-, key-, and value transforma- tions for attention heads with index k∈{1, . . . , n}, sequence length l, and hidden dimension d. M∈Rl×l is an attention mask to be deﬁned in § 4.1. Similarity between keys and queries is measured via the dot product. The inputs are word embeddings in the ﬁrst layer, or the out- put of the previous layer in the case of stacked layers. Y∈Rl×d denotes the ﬁnal output of the Transformer layer. W(q) k , W(k) k , W(v) k ∈ Rd×d/n are parameter matrices. FF is a position- wise feed-forward network intended to introduce additional depth and nonlinearities, deﬁned as FF(x)= max (0, xW1 + b1) W2 + b2. LN de- notes layer norm. Note that dropout regularization (Srivastava et al., 2014) is added in three places. Up to now, the model is completely agnostic of sequence positions. However, position informa- tion is crucial in natural language, so a mecha- nism to represent such information in the model is needed. A common approach is to add positional encodings to the word embeddings used as inputs to the ﬁrst layer. We opt to use learned positional embeddings (Gehring et al., 2017), and obtain the following after applying dropout: x′ i", "meta": {"paper_id": "1906.01617v1", "chunk_index": 3, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_4", "text": "mecha- nism to represent such information in the model is needed. A common approach is to add positional encodings to the word embeddings used as inputs to the ﬁrst layer. We opt to use learned positional embeddings (Gehring et al., 2017), and obtain the following after applying dropout: x′ i = dropout (xi + embed [i]) . (10) Here, a position embedding embed [i] of equal di- mension with sequence item xi at position i is added to the input. 4 Self-Attentional Lattice Encoders A simple way to realize self-attentional modeling for lattice inputs would be to linearize the lattice in topological order and then apply the above model. However, such a strategy would ignore the lat- tice structure and relate queries to keys that cannot possibly appear together according to the lattice. We ﬁnd empirically that this naive approach per- forms poorly (§ 5.4). As a remedy, we introduce f b a d E e c f h f a S c i d b f h Figure 2: Example for binary masks in forward- and backward directions. The currently selected query is node f, and the mask prevents all solid black nodes from being attended to. a masking scheme to incorporate lattice structure into the model (§ 4.1), before addressing positional encoding for lattices (§ 4.2). 4.1 Lattice Reachability Masks We draw inspiration from prior works such as the TreeLSTM (Tai et al., 2015) and related works. Consider how the recurrent conditioning of hidden representations in these models is informed by the graph structure of the inputs: Each node is condi- tioned on its direct predecessors in the graph, and via recurrent modeling on all its predecessor nodes up to the root or leaf nodes. 4.1.1 Binary Masks We propose a masking strategy that results in the same conditioning among tokens based on the lattice structure, preventing the self-attentional model from attending to lattice nodes that are not reachable from some given query node i. Figure 2 illustrates the concept of such reachability masks. Formally, we obtain masks in forward and back- ward direction as follows: −→ mbin ij = \u001a 0 if i∈R– (j) ∨i=j −∞ else ←− mbin ij = \u001a 0 if i∈R+ (j) ∨i=j −∞ else The resulting conditioning structure is analo- gous to the conditioning in lattice RNNs (Ladhak et al., 2016) in the backward and forward direc- tions, respectively. These masks can be obtained using standard graph traversal algorithms. 4.1.2 Probabilistic Masks Binary masks capture the graph structure of the in- puts, but do not account for potentially available lattice scores that associate lattice nodes with a probability of being correct. Prior work has found it critical to exploit lattice scores, especially for e a S E c b d 1 0.4 1 1 0.6 0.2 0.8 e a S E c b d 1 0.45 0.88 1 1 0.55 0.12 1 1 → S a b c d e E S 1 0.4 0.6 0.48 0.12 0.88 1 a 0 1 0 1 0 1 1 b 0 0 1 0.8 0.2 0.8", "meta": {"paper_id": "1906.01617v1", "chunk_index": 4, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_5", "text": "0.6 0.2 0.8 e a S E c b d 1 0.45 0.88 1 1 0.55 0.12 1 1 → S a b c d e E S 1 0.4 0.6 0.48 0.12 0.88 1 a 0 1 0 1 0 1 1 b 0 0 1 0.8 0.2 0.8 1 c 0 0 0 1 0 1 1 d 0 0 0 0 1 0 1 e 0 0 0 0 0 1 1 E 0 0 0 0 0 0 1 ← S a b c d e E S 1 0 0 0 0 0 0 a 1 1 0 0 0 0 0 b 1 0 1 0 0 0 0 c 1 0 0 1 0 0 0 d 1 0 1 0 1 0 0 e 1 0.45 0.55 0.55 0 1 0 E 1 0.4 0.6 0.48 0.12 0.88 1 Figure 3: Example for pairwise conditional reaching probabilities for a given lattice, which we logarith- mize to obtain self-attention masks. Rows are queries, columns are keys. noisy inputs such as speech recognition lattices (Sperber et al., 2017). In fact, the previous bi- nary masks place equal weight on all nodes, which will cause the inﬂuence of low-conﬁdence regions (i.e., dense regions with many alternative nodes) on computed representations to be greater than the inﬂuence of high-conﬁdence regions (sparse re- gions with few alternative nodes). It is therefore desirable to make the self- attentional lattice model aware of these scores, so that it can place higher emphasis on conﬁdent con- text and lower emphasis on context with low con- ﬁdence. The probabilistic masks below generalize binary masks according to this intuition: −→ mprob ij = \u001a log pG (j ≻i | i) if i̸=j 0 if i=j ←− mprob ij = \u001a log pG⊤(j ≻i | i) if i̸=j 0 if i=j Here, we set log(0):=−∞. Figure 3 illus- trates the resulting pairwise probability matrix for a given lattice and its reverse, prior to applying the logarithm. Note that the ﬁrst row in the forward matrix and the last row in the backward matrix are the globally normalized scores of Equation 4. Per our convention regarding log(0), the −∞ entries in the mask will occur at exactly the same places as with the binary reachability mask, be- cause the traversal probability is 0 for unreachable Algorithm 1 Computation of logarithmized prob- abilistic masks via dynamic programming. – given: DAG G = (V, E); transition probs ptrans k,j 1: ∀i, j ∈V : qi,j ←0 2: for i ∈V do ▷loop over queries 3: qi,i ←1 4: for k ∈topologic-order (V ) do 5: for next ∈N+ (k) do 6: qi,next ←qi,next + ptrans k,next · qi,k 7: end for 8: end for 9: end for 10: ∀i, j ∈V : mprob ij ←log qi,j nodes. For reachable nodes, the probabilistic mask causes the computed similarity for low-conﬁdent nodes (keys) to be decreased, thus increasing the impact of conﬁdent nodes on the computed hidden representations. The proposed probabilistic masks are further justiﬁed by observing that the result-", "meta": {"paper_id": "1906.01617v1", "chunk_index": 5, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_6", "text": "∀i, j ∈V : mprob ij ←log qi,j nodes. For reachable nodes, the probabilistic mask causes the computed similarity for low-conﬁdent nodes (keys) to be decreased, thus increasing the impact of conﬁdent nodes on the computed hidden representations. The proposed probabilistic masks are further justiﬁed by observing that the result- ing model is invariant to path duplication (see Ap- pendix A), unlike the model with binary masks. The introduced probabilistic masks can be com- puted in O \u0000|V |3\u0001 from the given transition prob- abilities by using the dynamic programming ap- proach described in Algorithm 1. The backward- directed probabilistic mask can be obtained by ap- plying the same algorithm on the reversed graph. 4.1.3 Directional and Non-Directional Masks The above masks are designed to be plugged into each Transformer layer via the masking term M in Equation 6. However, note that we have de- ﬁned two different masks, −→ mij and ←− mij. To em- ploy both we can follow two strategies: (1) Merge both into a single, non-directional mask by using ←→ m ij = max {−→ mij, ←− mij}. (2) Use half of the attention heads in each multi-head Transformer layer (§ 3.2) with forward masks, the other half with backward masks, for a directional strategy. Note that when the input is a sequence (i.e., a lattice with only one complete path), the non- directional strategy reduces to unmasked sequen- tial self-attention. The second strategy, in contrast, reduces to the directional masks proposed by Shen et al. (2018) for sequence modeling. 4.2 Lattice Positional Encoding Encoding positional information in the inputs is a crucial component in self-attentional architectures S a d e f g c E 0 1 2 3 2 2 4 5 b 1 Figure 4: Lattice positions, computed as longest-path distance from the start node S. as explained in § 3.2. To devise a strategy to en- code positions of lattice nodes in a suitable fash- ion, we state a number of desiderata: (1) Positions should be integers, so that positional embeddings (§ 3.2) can be used. (2) Every possible lattice path should be assigned strictly monotonically increas- ing positions, so that relative ordering can be in- ferred from positions. (3) For a compact represen- tation, unnecessary jumps should be avoided. In particular, for at least one complete path the posi- tions should increase by exactly 1 across all adja- cent succeeding lattice nodes. A naive strategy would be to use a topologi- cal order of the nodes to encode positions, but this clearly violates the compactness desideratum. Dyer et al. (2008) used shortest-path distances be- tween lattice nodes to account for distortion, but this violates monotonicity. Instead, we propose using the longest-path distance (ldist) from the start node, replacing Equation 10 with: x′ i = dropout (xi + embed [ldist (S →i)]) . This strategy fulﬁlls all three desiderata, as illus- trated in Figure 4. Longest-path distances from the start node to all other nodes can be computed in O \u0000|V |2\u0001 using e.g. Dijkstra’s shortest-path al- gorithm with edge weights set to −1. 4.3", "meta": {"paper_id": "1906.01617v1", "chunk_index": 6, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_7", "text": "dropout (xi + embed [ldist (S →i)]) . This strategy fulﬁlls all three desiderata, as illus- trated in Figure 4. Longest-path distances from the start node to all other nodes can be computed in O \u0000|V |2\u0001 using e.g. Dijkstra’s shortest-path al- gorithm with edge weights set to −1. 4.3 Computational Complexity The computational complexity in the self- attentional encoder is dominated by generating the masks (O \u0000|V |3\u0001 ), or by the computation of pairwise similarities (O \u0000|V |2\u0001 ) if we assume that masks are precomputed prior to training. Our main baseline model, the LatticeLSTM, can be computed in O (|E|), where |E| ≤|V |2. Never- theless, constant factors and the effect of batched operations lead to considerably faster computa- tions for the self-attentional approach in practice (§ 5.3). 5 Experiments We examine the effectiveness of our method on a speech translation task, in which we directly trans- late decoding lattices from a speech recognizer into a foreign language. 5.1 Settings We conduct experiments on the Fisher–Callhome Spanish–English Speech Translation corpus (Post et al., 2013). This corpus contains translated tele- phone conversations, along with speech recogni- tion transcripts and lattices. The Fisher portion (138k training sentences) contains conversations between strangers, and the smaller Callhome por- tion (15k sentences) contains conversations be- tween family members. Both and especially the latter are acoustically challenging, indicated by speech recognition word error rates of 36.4% and 65.3% on respective test sets for the transcripts contained in the corpus. The included lattices have oracle word error rates of 16.1% and 37.9%. We use XNMT (Neubig et al., 2018) which is based on DyNet (Neubig et al., 2017a), with the provided self-attention example as a starting point.3 Hidden dimensions are set to 512 unless otherwise noted. We use a single-layer LSTM- based decoder with dropout rate 0.5. All self- attentional encoders use three layers with hidden dimension of the FF operation set to 2048, and dropout rate set to 0.1. LSTM-based encoders use 2 layers. We follow Sperber et al. (2017) to tok- enize and lowercase data, remove punctuation, and replace singletons with a special unk token. Beam size is set to 8. For training, we ﬁnd it important to pretrain on sequential data and ﬁnetune on lattice data (§ 5.6). This is in line with prior work (Sperber et al., 2017) and likely owed to the fact that the lattices in this dataset are rather noisy, hampering train- ing especially during the early stages. We use Adam for training (Kingma and Ba, 2014). For se- quential pretraining, we follow the learning sched- ule with warm-up and decay of Vaswani et al. (2017). Finetuning was sometimes unstable, so we ﬁnetune both using the warm-up/decay strat- egy and using a ﬁxed learning rate of 0.0001 and report the better result. We use large-batch train- ing with minibatch size of 1024 sentences, accu- mulated over 16 batched computations of 64 sen- 3Our code is available: http://msperber.com/ research/acl-lattice-selfatt/ Encoder model Inputs Fisher Callh. LSTM4 1-best 35.9 11.8 Seq. SA 1-best 35.71 12.36 Seq. SA (directional) 1-best 37.42 13.00 Graph", "meta": {"paper_id": "1906.01617v1", "chunk_index": 7, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_8", "text": "the better result. We use large-batch train- ing with minibatch size of 1024 sentences, accu- mulated over 16 batched computations of 64 sen- 3Our code is available: http://msperber.com/ research/acl-lattice-selfatt/ Encoder model Inputs Fisher Callh. LSTM4 1-best 35.9 11.8 Seq. SA 1-best 35.71 12.36 Seq. SA (directional) 1-best 37.42 13.00 Graph attention lattice 35.71 11.87 LatticeLSTM4 lattice 38.0 14.1 Lattice SA (proposed) lattice 38.73 14.74 Table 1: BLEU scores on Fisher (4 references) and Callhome (1 reference), for proposed method and sev- eral baselines. tences each, due to memory constraints. Early stopping is applied when the BLEU score on a held-out validation set does not improve over 15 epochs, and the model with the highest validation BLEU score is kept. 5.2 Main Results Table 1 compares our model against several base- lines. Lattice models tested on Callhome are pretrained on Fisher and ﬁnetuned on Callhome lattices (Fisher+Callhome setting), while lattice models tested on Fisher use a Fisher+Fisher train- ing setting. All sequential baselines are trained on the reference transcripts of Fisher. The ﬁrst set of baselines operates on 1-best (sequential) inputs and includes a bidirectional LSTM, an unmasked self-attentional encoder (SA) of otherwise identi- cal architecture with our proposed model, and a variant with directional masks (Shen et al., 2018). Next, we include a graph-attentional model that masks all but adjacent lattice nodes (Veliˇckovi´c et al., 2018) but is otherwise identical to the pro- posed model, and a LatticeLSTM. Note that these lattice models both use the cross-attention lattice- score bias (§ 3.1). Results show that our proposed model outper- forms all examined baselines. Compared to the sequential self-attentional model, our models im- proves by 1.31–1.74 BLEU points. Compared to the LatticeLSTM, our model improves results by 0.64–0.73 BLEU points, while at the same time being more computationally efﬁcient (§ 5.3). Graph attention is not able to improve over the se- quential baselines on our task due to its restriction to local context. Training Inference Encoder Batching Speed Batching Speed Sequential encoder models LSTM M 4629 – 715 SA M 5021 – 796 LatticeLSTM and lattice SA encoders LSTM – 178 – 391 LSTM A 710 A 538 SA M 2963 – 687 SA A 748 A 718 Table 2: Computation speed (words/sec), averaged over 3 runs. Batching is conducted manually (M), through autobatching (A), or disabled (–). The self- attentional lattice model displays superior speed de- spite using 3 encoder layers, compared to 2 layers for the LSTM-based models. 5.3 Computation Speed The self-attentional lattice model was motivated not only by promising model accuracy (as con- ﬁrmed above), but also by potential speed gains. We therefore test computation speed for train- ing and inference, comparing against LSTM- and LatticeLSTM-based models. For fair compari- son, we use a reimplementation of the Lattice- LSTM so that all models are run with the exact same toolkits and identical decoder architectures. Again, LSTM-based models have two encoder layers, while self-attentional models have three layers. LatticeLSTMs are difﬁcult to speed up through manually implemented batched compu- tations, but similar models have been reported to strongly beneﬁt from", "meta": {"paper_id": "1906.01617v1", "chunk_index": 8, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_9", "text": "so that all models are run with the exact same toolkits and identical decoder architectures. Again, LSTM-based models have two encoder layers, while self-attentional models have three layers. LatticeLSTMs are difﬁcult to speed up through manually implemented batched compu- tations, but similar models have been reported to strongly beneﬁt from autobatching (Neubig et al., 2017b) which automatically ﬁnds operations that can be grouped after the computation graph has been deﬁned. Autobatching is implemented in DyNet but not available in many other deep learn- ing toolkits, so we test both with and without au- tobatching. Training computations are manually or automatically batched across 64 parallel sen- tences, while inference speed is tested for sin- gle sentences with forced decoding of gold trans- lations and without beam search. We test with DyNet commit 8260090 on an Nvidia Titan Xp GPU and average results over three runs. Table 2 shows the results. For sequential inputs, the self-attentional model is slightly faster than the LSTM-based model. The difference is perhaps 4BLEU scores taken from Sperber et al. (2017). reachability mask dir. prob. latt. pos. Fisher Callh. 38.73 14.74 38.25 12.45 37.52 14.37 35.49 12.83 30.58 9.41 Table 3: Ablation over proposed features, including reachability masks, directional (vs. non-directional) masking, probabilistic (vs. binary) masking, and lattice positions (vs. topological positions). smaller than expected, which can be explained by the larger number of layers in the self-attentional model, and the relatively short sentences of the Fisher corpus that reduce the positive effect of par- allel computation across sequence positions. For lattice-based inputs, we can see a large speed-up of the self-attentional approach when no autobatch- ing is used. Replacing manual batching with au- tobatching during training for the self-attentional model yields no beneﬁts. Enabling autobatching at inference time provides some speed-up for both models. Overall, the speed advantage of the self- attentional approach is still very visible even with autobatching available. 5.4 Feature Ablation We next conduct a feature ablation to examine the individual effect of the improvements introduced in § 4. Table 3 shows that longest-path position encoding outperforms topological positions, the probabilistic approach outperforms binary reach- ability masks, and modeling forward and reversed lattices with separate attention heads outperforms the non-directional approach. Consistently with the ﬁndings by Sperber et al. (2017), lattice scores are more effectively exploited on Fisher than on Callhome as a result of the poor lattice quality for the latter. The experiment in the last row demon- strates the effect of keeping the lattice contents but removing all structural information, by rearrang- ing nodes in linear, arbitrary topological order, and applying the best sequential model. Results are poor and structural information clearly beneﬁcial. 5.5 Behavior At Test Time To obtain a better understanding of the proposed model, we compare accuracies to the sequential Lattice oracle 1-best Lattice Fisher Sequential SA 47.84 37.42 – Lattice SA 47.69 37.56 38.73 Callhome Sequential SA 17.94 13.00 – Lattice SA 18.54 13.90 14.74 Table 4: Fisher and Callhome models, tested by in- putting lattice oracle paths, 1-best paths, and full lat- tices. self-attentional model when translating either", "meta": {"paper_id": "1906.01617v1", "chunk_index": 9, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_10", "text": "Lattice oracle 1-best Lattice Fisher Sequential SA 47.84 37.42 – Lattice SA 47.69 37.56 38.73 Callhome Sequential SA 17.94 13.00 – Lattice SA 18.54 13.90 14.74 Table 4: Fisher and Callhome models, tested by in- putting lattice oracle paths, 1-best paths, and full lat- tices. self-attentional model when translating either lat- tice oracle paths, 1-best transcripts, or lattices. The lattice model translates sequences by treat- ing them as lattices with only a single complete path and all transition probabilities set to 1. Ta- ble 4 shows the results for the Fisher+Fisher model evaluated on Fisher test data, and for the Fisher+Callhome model evaluated on Callhome test data. We can see that the lattice model out- performs the sequential model even when translat- ing sequential 1-best transcripts, indicating bene- ﬁts perhaps due to more robustness or increased training data size for the lattice model. However, the largest gains stem from using lattices at test time, indicating that our model is able to exploit the actual test-time lattices. Note that there is still a considerable gap to the translation of lattice ora- cles which form a top-line to our experiments. 5.6 Effect of Pretraining and Finetuning Finally, we analyze the importance of our strategy of pretraining on clean sequential data before ﬁne- tuning on lattice data. Table 5 shows the results for several combinations of pretraining and ﬁnetuning data. The ﬁrst thing to notice is that pretraining is critical for good results. Skipping pretraining performs extremely poorly, while pretraining on the much smaller Callhome data yields results no better than the sequential baselines (§ 5.2). We conjecture that pretraining is beneﬁcial mainly due to the rather noisy lattice training data, while for tasks with cleaner training lattices pretraining may play a less critical role. The second observation is that for the ﬁnetuning stage, domain appears more important than data size: Finetuning on Fisher works best when test- ing on Fisher, while ﬁnetuning on Callhome works best when testing on Callhome, despite the Call- Sequential data Lattice data Fisher Callh. – Fisher 1.45 1.78 Callhome Fisher 34.52 13.04 Fisher Callhome 35.47 14.74 Fisher Fisher 38.73 14.59 Table 5: BLEU scores for several combinations of Fisher (138k sentences) and Callhome (15k sentences) training data. home ﬁnetuning data being an order of magnitude smaller. This is encouraging, because the collec- tion of large amounts of training lattices can be difﬁcult in practice. 6 Related Work The translation of lattices rather than sequences has been investigated with traditional machine translation models (Ney, 1999; Casacuberta et al., 2004; Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008; Dyer et al., 2008), but these approaches rely on independence assump- tions in the decoding process that no longer hold for neural encoder-decoder models. Neural lattice- to-sequence models were proposed by Su et al. (2017); Sperber et al. (2017), with promising re- sults but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alter- native to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015;", "meta": {"paper_id": "1906.01617v1", "chunk_index": 10, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_11", "text": "proposed by Su et al. (2017); Sperber et al. (2017), with promising re- sults but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alter- native to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015; Def- ferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), and used for devising tree-to- sequence models (Bastings et al., 2017; Marcheg- giani et al., 2018). We are not aware of any ap- plication of GCNs to lattice modeling. Unlike our approach, GCNs consider only local context, must be combined with slower LSTM layers for good performance, and lack support for lattice scores. Our model builds on previous works on self- attentional models (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017). The idea of masking has been used for various purposes, including occlusion of future informa- tion during training (Vaswani et al., 2017), in- troducing directionality (Shen et al., 2018) with good results for machine translation conﬁrmed by Song et al. (2018), and soft masking (Im and Cho, 2017; Sperber et al., 2018). The only extension of self-attention beyond sequence modeling we are aware of is graph attention (Veliˇckovi´c et al., 2018) which uses only local context and is outper- formed by our model. 7 Conclusion This work extended existing sequential self- attentional models to lattice inputs, which have been useful for various purposes in the past. We achieve this by introducing probabilistic reacha- bility masks and lattice positional encodings. Ex- periments in a speech translation task show that our method outperforms previous approaches and is much faster than RNN-based alternatives in both training and inference settings. Promising future work includes extension to tree-structured inputs and application to other tasks. Acknowledgments The work leading to these results has received funding from the European Union under grant agreement no 825460. References Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffry E. Hin- ton. 2016. Layer Normalization. arXiv:1607.06450. Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In In- ternational Conference on Representation Learning (ICLR), San Diego, USA. Joost Bastings, Ivan Titov, Wilker Aziz, Diego Marcheggiani, and Khalil Sima’an. 2017. Graph Convolutional Encoders for Syntax-aware Neural Machine Translation. In Empirical Methods in Nat- ural Language Processing (EMNLP), Copenhagen, Denmark. Daniel Beck, Gholamreza Haffari, and Trevor Cohn. 2018. Graph-to-Sequence Learning using Gated Graph Neural Networks. In Association for Com- putational Linguistic (ACL), pages 273–283, Mel- bourne, Australia. Francisco Casacuberta, Hermann Ney, Franz Josef Och, Enrique Vidal, J. M. Vilar, S. Barrachina, I. Garc´ıa-Varea, D. Llorens, C. Mart´ınez, S. Mo- lau, F. Nevado, M. Pastor, D. Pic´o, A. Sanchis, and C. Tillmann. 2004. Some approaches to statistical and ﬁnite-state speech-to-speech translation. Com- puter Speech and Language, 18(1):25–47. Alberto Cetoli, Stefano Bragaglia, Andrew D. O’Harney, and Marc Sloan. 2017. Graph Convolu- tional Networks for Named Entity Recognition. In International Workshop on Treebanks and Linguis- tic Theories (TLT16), pages 37–45, Prague, Czech Republic. Jianpeng Cheng, Li Dong, and Mirella", "meta": {"paper_id": "1906.01617v1", "chunk_index": 11, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_12", "text": "and ﬁnite-state speech-to-speech translation. Com- puter Speech and Language, 18(1):25–47. Alberto Cetoli, Stefano Bragaglia, Andrew D. O’Harney, and Marc Sloan. 2017. Graph Convolu- tional Networks for Named Entity Recognition. In International Workshop on Treebanks and Linguis- tic Theories (TLT16), pages 37–45, Prague, Czech Republic. Jianpeng Cheng, Li Dong, and Mirella Lapata. 2016. Long short-term memory-networks for machine reading. In Empirical Methods in Natural Language Processing (EMNLP), Austin, Texas, USA. Micha¨el Defferrard, Xavier Bresson, and Pierre Van- dergheynst. 2016. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In Advances in Neural Information Processing Systems (NIPS), pages 3844–3852, Barcelona, Spain. David Duvenaud, Dougal Maclaurin, Jorge Aguilera- Iparraguirre, Rafael G´omez-Bombarelli, Timothy Hirzel, Al´an Aspuru-Guzik, and Ryan P. Adams. 2015. Convolutional Networks on Graphs for Learning Molecular Fingerprints. In Advances in Neural Information Processing Systems (NIPS), pages 2224–2232, Montr´eal, Canada. Christopher Dyer, Smaranda Muresan, and Philip Resnik. 2008. Generalizing Word Lattice Trans- lation. Technical Report LAMP-TR-149, Univer- sity of Maryland, Institute For Advanced Computer Studies. Yarin Gal and Zoubin Ghahramani. 2016. A Theoreti- cally Grounded Application of Dropout in Recurrent Neural Networks. In Neural Information Process- ing Systems Conference (NIPS), pages 1019–1027, Barcelona, Spain. Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. 2017. Convolutional Sequence to Sequence Learning. In International Conference on Machine Learning (ICML), Sydney, Australia. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog- nition. In Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 770—-778, Las Ve- gas, USA. Robert L. Hemminger and Lowell W. Beineke. 1978. Line graphs and line digraphs. In Selected Topics in Graph Theory, pages 271–305. Academic Press Inc. Jinbae Im and Sungzoon Cho. 2017. Distance-based Self-Attention Network for Natural Language Infer- ence. arXiv:1712.02047. Steven Kearnes, Kevin McCloskey, Marc Berndl, Vi- jay Pande, and Patrick Riley. 2016. Molecu- lar Graph Convolutions: Moving Beyond Finger- prints. Journal of Computer-Aided Molecular De- sign, 30(8):595–608. Diederik P. Kingma and Jimmy L. Ba. 2014. Adam: A Method for Stochastic Optimization. In Inter- national Conference on Learning Representations (ICLR), Banff, Canada. Thomas N. Kipf and Max Welling. 2017. Semi- Supervised Classiﬁcation with Graph Convolutional Networks. International Conference on Learning Representations (ICLR). Faisal Ladhak, Ankur Gandhe, Markus Dreyer, Lam- bert Mathias, Ariya Rastrow, and Bj¨orn Hoffmeis- ter. 2016. LatticeRnn: Recurrent Neural Networks over Lattices. In Annual Conference of the Inter- national Speech Communication Association (Inter- Speech), pages 695–699, San Francisco, USA. Yujia Li, Richard Zemel, Mark Brockschmeidt, and Daniel Tarlow. 2016. Gated Graph Sequence Neural Networks. In International Conference on Learning Representations (ICLR). Zhouhan Lin, Minwei Feng, Cicero Nogueira dos San- tos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A Structured Self-attentive Sentence Embedding. In International Conference on Repre- sentation Learning (ICLR), Toulon, France. Minh-Thang Luong, Hieu Pham, and Christopher D. Manning. 2015. Effective Approaches to Attention- based Neural Machine Translation. In Empir- ical Methods in Natural Language Processing (EMNLP), pages 1412–1421, Lisbon, Portugal. Diego Marcheggiani, Joost Bastings, and Ivan Titov. 2018. Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks. In North American Chapter of the", "meta": {"paper_id": "1906.01617v1", "chunk_index": 12, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_13", "text": "and Christopher D. Manning. 2015. Effective Approaches to Attention- based Neural Machine Translation. In Empir- ical Methods in Natural Language Processing (EMNLP), pages 1412–1421, Lisbon, Portugal. Diego Marcheggiani, Joost Bastings, and Ivan Titov. 2018. Exploiting Semantics in Neural Machine Translation with Graph Convolutional Networks. In North American Chapter of the Association for Computational Linguistics (NAACL), pages 486– 492, New Orleans, USA. Evgeny Matusov, Bj¨orn Hoffmeister, and Hermann Ney. 2008. ASR word lattice translation with ex- haustive reordering is possible. In Annual Con- ference of the International Speech Communication Association (InterSpeech), pages 2342–2345, Bris- bane, Australia. Roberto Navigli and Paola Velardi. 2010. Learning Word-Class Lattices for Deﬁnition and Hypernym Extraction. In Association for Computational Lin- guistic (ACL), pages 1318–1327, Uppsala, Sweden. Graham Neubig, Chris Dyer, Yoav Goldberg, Austin Matthews, Waleed Ammar, Antonios Anastasopou- los, Miguel Ballesteros, David Chiang, Daniel Clothiaux, Trevor Cohn, Kevin Duh, Manaal Faruqui, Cynthia Gan, Dan Garrette, Yangfeng Ji, Lingpeng Kong, Adhiguna Kuncoro, Gaurav Ku- mar, Chaitanya Malaviya, Paul Michel, Yusuke Oda, Matthew Richardson, Naomi Saphra, Swabha Swayamdipta, and Pengcheng Yin. 2017a. DyNet: The Dynamic Neural Network Toolkit. arXiv preprint arXiv:1701.03980. Graham Neubig, Yoav Goldberg, and Chris Dyer. 2017b. On-the-ﬂy Operation Batching in Dynamic Computation Graphs. In Neural Information Pro- cessing Systems Conference (NIPS), Long Beach, USA. Graham Neubig, Matthias Sperber, Xinyi Wang, Matthieu Felix, Austin Matthews, Sarguna Pad- manabhan, Ye Qi, Devendra Singh Sachan, Philip Arthur, Pierre Godard, John Hewitt, Rachid Riad, and Liming Wang. 2018. XNMT: The eXtensible Neural Machine Translation Toolkit. In Conference of the Association for Machine Translation in the Americas (AMTA) Open Source Software Showcase, Boston, USA. Hermann Ney. 1999. Speech Translation: Coupling of Recognition and Translation. In International Con- ference on Acoustics, Speech, and Signal Processing (ICASSP), pages 517–520, Phoenix, USA. Ankur P. Parikh, Oscar T¨ackstr¨om, Dipanjan Das, and Jakob Uszkoreit. 2016. A Decomposable Atten- tion Model for Natural Language Inference. In Em- pirical Methods in Natural Language Processing (EMNLP), pages 2249–2255, Austin, USA. Matt Post, Gaurav Kumar, Adam Lopez, Damianos Karakos, Chris Callison-Burch, and Sanjeev Khu- danpur. 2013. Improved Speech-to-Text Transla- tion with the Fisher and Callhome Spanish–English Speech Translation Corpus. In International Work- shop on Spoken Language Translation (IWSLT), Heidelberg, Germany. Shirin Saleem, Szu-Chen Jou, Stephan Vogel, and Tanja Schultz. 2004. Using Word Lattice Informa- tion for a Tighter Coupling in Speech Translation Systems. In International Conference on Spoken Language Processing (ICSLP), pages 41–44, Jeju Is- land, Korea. Anna Senina, Marcus Rohrbach, Wei Qiu, Annemarie Friedrich, Sikandar Amin, Mykhaylo Andriluka, Manfred Pinkal, and Bernt Schiele. 2014. Coherent multi-sentence video description with variable level of detail. In German Conference on Pattern Recog- nition (GCPR), pages 184–195, M¨unster, Germany. Springer. Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang, Shirui Pan, and Chengqi Zhang. 2018. DiSAN: Directional Self-Attention Network for RNN/CNN- free Language Understanding. In Conference on Ar- tiﬁcial Intelligence (AAAI), New Orleans, USA. Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive Deep Mod- els for Semantic Compositionality Over a Senti- ment Treebank. In Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642,", "meta": {"paper_id": "1906.01617v1", "chunk_index": 13, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_14", "text": "Conference on Ar- tiﬁcial Intelligence (AAAI), New Orleans, USA. Richard Socher, Alex Perelygin, Jean Y. Wu, Jason Chuang, Christopher D. Manning, Andrew Y. Ng, and Christopher Potts. 2013. Recursive Deep Mod- els for Semantic Compositionality Over a Senti- ment Treebank. In Empirical Methods in Natural Language Processing (EMNLP), pages 1631–1642, Seattle, USA. Kaitao Song, Xu Tan, Furong Peng, and Jianfeng Lu. 2018. Hybrid Self-Attention Network for Machine Translation. arXiv:1811.00253v2. Matthias Sperber, Graham Neubig, Jan Niehues, and Alex Waibel. 2017. Neural Lattice-to-Sequence Models for Uncertain Inputs. In Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1380–1389, Copenhagen, Den- mark. Matthias Sperber, Jan Niehues, Graham Neubig, Se- bastian St¨uker, and Alex Waibel. 2018. Self- Attentional Acoustic Models. In Annual Conference of the International Speech Communication Associ- ation (InterSpeech), Hyderabad, India. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting. Journal of Machine Learning Re- search, 15(1):1929–1958. Jinsong Su, Zhixing Tan, Deyi Xiong, Rongrong Ji, Xiaodong Shi, and Yang Liu. 2017. Lattice-Based Recurrent Neural Network Encoders for Neural Ma- chine Translation. In Conference on Artiﬁcial In- telligence (AAAI), pages 3302–3308, San Francisco, USA. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In Computer Vision and Pattern Recognition (CVPR), pages 2818–2826, Las Vegas, USA. Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved Semantic Representa- tions From Tree-Structured Long Short-Term Mem- ory Networks. In Association for Computational Linguistic (ACL), pages 1556–1566, Beijing, China. Shikhar Vashishth, Shib Sankar Dasgupta, Swayambhu Nath Ray, and Partha Talukdar. 2018. Dating Documents using Graph Convolution Networks. In Association for Computational Linguistic (ACL), pages 1605–1615, Melbourne, Australia. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All You Need. In Neural Information Processing Sys- tems Conference (NIPS), pages 5998–6008, Long Beach, USA. Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua Bengio. 2018. Graph Attention Networks. In International Conference on Learning Representations (ICLR), Vancouver, Canada. Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. 2015. Gram- mar as a Foreign Language. In Neural Information Processing Systems Conference (NIPS), Montr´eal, Canada. Ruiqiang Zhang, Genichiro Kikui, Hirofumi Ya- mamoto, and Wai-Kit Lo. 2005. A Decoding Al- gorithm for Word Lattice Translation in Speech Translation. In International Workshop on Spoken Language Translation (IWSLT), pages 23–29, Pitts- burgh, USA. Xiaodan Zhu, Parinaz Sobhani, and Hongyu Guo. 2015. Long Short-Term Memory Over Recursive Structures. In International Conference on Machine Learning (ICML), pages 1604–1612, Lille, France. A Path Duplication Invariance Figure 5 shows a sequential lattice, and a lattice derived from it but with a duplicated path. Seman- tically, both are equivalent, and should therefore result in identical neural representations. Note that while in practice duplicated paths should not oc- cur, paths with partial overlap are quite frequent. It is therefore instructive to consider this hypo- thetical situation. Below, we demonstrate that the binary masking approach (§ 4.1.1) is biased", "meta": {"paper_id": "1906.01617v1", "chunk_index": 14, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_15", "text": "are equivalent, and should therefore result in identical neural representations. Note that while in practice duplicated paths should not oc- cur, paths with partial overlap are quite frequent. It is therefore instructive to consider this hypo- thetical situation. Below, we demonstrate that the binary masking approach (§ 4.1.1) is biased such that computed representations are impacted by path duplication. In contrast, the probabilistic approach (§ 4.1.2) is invariant to path duplication. We consider the example of Figure 5, discussing only the forward direction, because the lattice is symmetric and computations for the backward di- rection are identical. We follow notation of Equa- tions 1 through 3, using ⟨a, b⟩as abbrevation for f (q (xa) , k (xb)) and va to abbreviate v(xa). Let us consider the computed representation for the node S as query. For the sequential lattice with binary mask, it is: yS = 1 C \u0010 e⟨S,S⟩vS + e⟨S,a⟩va + e⟨S,b⟩vb \u0011 (11) Here, C is the softmax normalization term that en- sures that exponentiated similarities sum up to 1. In contrast, the lattice with duplication results in a doubled inﬂuence of va: yS = 1 C \u0010 e⟨S,S⟩vS + e⟨S,a⟩va + e⟨S,a’⟩va’ + e⟨S,E⟩vE \u0011 = 1 C \u0010 e⟨S,S⟩vS + 2e⟨S,a⟩va + e⟨S,E⟩vE \u0011 . The probabilistic approach yields the same re- sult as the binary approach for the sequential lat- tice (Equation 11). For the lattice with path dupli- cation, the representation for the node S is com- a S E a 1 p 1 1-p ‘ a S E 1 1 sequential S a E S 1 1 1 a 0 1 1 E 0 0 1 duplicated S a a’ E S 1 p (1 −p) 1 a 0 1 0 1 a’ 0 0 1 1 E 0 0 0 1 Figure 5: A sequential lattice, and a variant with a du- plicated path, where nodes a and a’ are labeled with the same word token. The matrices contain pairwise reaching probabilities in forward direction, where rows are queries, columns are keys. puted as follows: yS = 1 C \u0010 e⟨S,S⟩vS + e⟨S,a⟩+log pva + e⟨S,a’⟩+log(1−p)va’ + e⟨S,E⟩vE \u0011 = 1 C \u0010 e⟨S,S⟩vS + e⟨S,a⟩elog pva + e⟨S,a’⟩elog(1−p)va’ + e⟨S,E⟩vE \u0011 = 1 C \u0010 e⟨S,S⟩vS + pe⟨S,a⟩va + (1 −p)e⟨S,a’⟩va’ + e⟨S,E⟩vE \u0011 = 1 C \u0010 e⟨S,S⟩vS + e⟨S,a⟩va + e⟨S,E⟩vE \u0011 . The result is the same as in the semantically equivalent sequential case (Equation 11), the com- putation is therefore invariant to path duplica- tion. The same argument can be extended to other queries, to other lattices with duplicated paths, as well as to the lattice-biased encoder-decoder atten- tion. B Qualitative Analysis We conduct a manual inspection and showcase several common patterns in which the lattice in- put helps improve translation quality, as well as one counter example. In particular, we compare the outputs of the sequential and lattice models ac- cording to the 3rd and the last row in Table 1, on Fisher. B.1 Example 1 In this example, the ASR 1-best contains a bad word choice (quedar", "meta": {"paper_id": "1906.01617v1", "chunk_index": 15, "filename": "1906.01617v1.pdf", "chunk_length": 512}}
{"id": "1906.01617v1_chunk_16", "text": "improve translation quality, as well as one counter example. In particular, we compare the outputs of the sequential and lattice models ac- cording to the 3rd and the last row in Table 1, on Fisher. B.1 Example 1 In this example, the ASR 1-best contains a bad word choice (quedar instead of qu tal). The cor- rect word is in the lattice, and can be disam- biguated by exploiting long-range self-attentional encoder context. gold transcript: Qu tal, eh, yo soy Guillermo, Cmo ests? ASR 1-best: quedar eh yo soy guillermo cmo ests seq2seq output: stay eh i ’ m guillermo how are you ASR lattice: quedar S .7 .2 que dar qué tal … … … .1 1 1 lat2seq output: how are you eh i ’ m guillermo how are you B.2 Example 2 Here, the correct word graduar does not appear in the lattice, instead the lattice offers many in- correct alternatives of high uncertainty. The trans- lation model evidently goes with a linguistically plausible guess, ignoring the source side. gold transcript: Claro Es, eh, eh, o sea, yo me, me voy a graduar con un ttulo de esta univer- sidad. ASR 1-best: claro existe eh o sea yo me me puedo habar con un ttulo esta universidad seq2seq output: sure it exists i mean i can talk with a title ASR lattice: puedo voy habar ahora a hablar grabar lavar … … … … … 1 .1 .9 .1 .7 .2 lat2seq output: sure i mean i ’ m going to take a university title B.3 Example 3 In this example, o sea (I mean) appears with slightly lower conﬁdence than saben (they know), but is chosen for a more natural sounding target sentence gold transcript: No, o sea, eso es eh, clarsimo para mi ASR 1-best: no saben eso es eh clarsimo para mi seq2seq output: they don ’ t know that ’ s eh sure for me ASR lattice: o S .34 .37 saben no … sea … … .29 lat2seq output: no i mean that ’ s very clear for me B.4 Counter Example In this counter example, the translation model gets confused from the additional and wrong lattice context and no longer produces the correct output. gold transcript: s ASR 1-best: s seq2seq output: yes ASR lattice: S .107 mhm mm sí .392 E .502 lat2seq output: mm", "meta": {"paper_id": "1906.01617v1", "chunk_index": 16, "filename": "1906.01617v1.pdf", "chunk_length": 395}}
{"id": "1906.01726v1_chunk_0", "text": "An Introduction to a New Text Classification and Visualization for Natural Language Processing Using Topological Data Analysis Naiereh Elyasi Department of Mathematics, Faculty of Mathematics and Computer Science, Kharazmi University, Tehran, Iran, Email: elyasi82@khu.ac.ir Mehdi Hosseini Moghadam Department of Mathematics, Faculty of Mathematics and Computer Science, Kharazmi University, Tehran, Iran, Email:m.h.moghadam1996@gmail.com Abstract: Topological Data Analysis (TDA) is a novel new and fast growing field of data science providing a set of new topological and geometric tools to derive relevant features out of complex high-dimensional data. In this paper we apply two of best methods in topological data analysis, “Persistent Homology” and “Mapper”, in order to classify persian poems which has been composed by two of the best Iranian poets namely ”Ferdowsi” and ”Hafez”. This article has two main parts, in the first part we explain the mathematics behind these two methods which is easy to understand for general audience and in the second part we describe our models and the results of applying TDA tools to NLP. Keywords: TDA · Persistent Homology · NLP · Text Classification · Persian Poems 1 Introduction 1.1 Authorship Attribution These days internet is full of text based information including (news, messages, comments, ...), in order to analyze this huge amount of unstructured data we need to classify them. The process of assigning labels to some text based on their content is called text classification. Authorship attribution is one of the main branches in text classification which tries to identify the author of a given text based on its content. We can divide authorship attribution into two main methods: – Non-Semantic: This method tries to identify the author based on the length of words and sentences and the vocabulary used in a given text[10][16]. – Semantic: This method considers the structure of the language based on its semantic analysis[4][5][26]. In this article we try to do authorship attribution based on a new state-of the-art method called topological data analysis. 1.2 Topological Data Analysis Data is every where, if you just think about our daily life you could find out how much information and data is produced every day by a single person. With ever growing amount of data and advances in technology we face with the world of information overload. In order to make our life easier and having a good sense of what is happening in our world, what the threats and opportunities are, we need to collect and analyze this huge amount of data. Data science is the field of science in which it is grappling with huge and messy amount of data in order to turn it to something useful. One of the main difficulties in data science is to deal with high dimensional data and transform it into data with less dimensionality in order to make it easier for analyzing. Topological data analysis (TDA) is one of the newest and fast growing branches of data science which is trying to analyze data by studying its shape and also reducing the dimensionality of data. TDA is based on two very important branches “Statistics” and", "meta": {"paper_id": "1906.01726v1", "chunk_index": 0, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_1", "text": "order to make it easier for analyzing. Topological data analysis (TDA) is one of the newest and fast growing branches of data science which is trying to analyze data by studying its shape and also reducing the dimensionality of data. TDA is based on two very important branches “Statistics” and “Algebraic Topology”, due to its methodology TDA can solve some serious problems in data science, because it is a clustering method that is robust to noise. The goal of TDA is to reduce the dimensionality of high dimensional data and also analyzing the topological structure or shape of data and finally clustering complex data. Since the nature of data is random, TDA tools has been extended by the statistical and combinatorial concepts. TDA also provides innovative data mining methods that can improve the efficiency of machine learning techniques. TDA has two main methods “Persistent Homology” and “Mapper”. In Persistent homology, a filtration of combinatorial objects, simplicial complexes, is constructed and then main topological structures of data is derived. Some visualization tools such as “Persistent Diagram”,“Barcode” and “ Persistent Landscape” are invented to indicate the main topological features of data. Persistent homology has been previously used in pulse pressure wave analysis[7], analyzing 3D images[9], brain networks[12], image analysis[6], Path Planning[1]. The idea behind Mapper is to reduce a highdimensional and noisy data set into a combinatorial object (simplicial complex). Such an object tries to encapsulate the original shape of the high-dimensional data. TDA Mapper has been previously applied to data in breast-cancer patients[19], Text Representation for Natural Language Processing[29], Text mining[29][8][20], topic detection in twitter[27] and clinical data[19][23][21][18][11]. In this paper first we introduce some mathematical backgrounds about “Group Theory”, “Simplicial Complex” and “Homology”. Next we introduce ”Persistent Homology” algorithm, then we apply this method to authorship attribution (dataset of poems) and analyze the results. Afterwards we introduce a novel method in TDA called “Mapper”. We apply mapper to authorship attribution (dataset of poems) and analyze the result as a simplicial complex which is interactive and can be quantified in several ways using statistics. After wards we introduce ”Persistent Homology” algorithm, next we apply this method to authorship attribution (dataset of poems) and analyze the results. In last section we compare the results of applying persistent homology and mapper on dataset of poems. 2 Preliminaries 2.1 Group Theory Definition 1 A group is a pair (G, *) which * is a binary operation on set a G and an identity element e that satisfies: • (a * b) * c = a * (b * c) for all a,b,c, ∈ G (associativity) • e * a = a for all a ∈ G. • for every a ∈ G, there is an element b ∈ G such that b * a = e (b in the third part of definition is called the inverse of a and is denoted by a−1). Example 2 The integers Z under addition + is a group. From now on we use notation G instead of (G, *). Definition 3 A subset H ⊆ G of a group", "meta": {"paper_id": "1906.01726v1", "chunk_index": 1, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_2", "text": "(b in the third part of definition is called the inverse of a and is denoted by a−1). Example 2 The integers Z under addition + is a group. From now on we use notation G instead of (G, *). Definition 3 A subset H ⊆ G of a group G with operation * is a subgroup of G if (H, *) is itself a group. Definition 4 A group G is abelian if a * b = b * a for all elements a,b ∈ G. Definition 5 Two groups G and H are said to be isomorphic if there is bijective function 𝜃 : H → G such that: 𝜃 (𝑔1𝑔2 ) = 𝜃 (𝑔1) 𝜃 (𝑔2) (1) for all 𝑔1, 𝑔2 ∈ G. The function 𝜃 is called an isomorphism. If 𝜃 satisfies only the equation 1, then 𝜃 is named a homomorphism. Definition 6 If 𝜃 : H → G is a group homomorphism, the kernel of 𝜃 is defined by: Ker(θ) = {a ∈ G | θ(a) = e} and the image of θ is defined by: Im(θ) = { θ(a) | a ∈ G} Definition 7 Let H be a subgroup of a group G. If g ∈ G, the right coset of H generated by g is Hg = {hg | h ∈ H} Similarly the left coset of H generated by g can be defined . Fig. 1: Representation of Ker and Image function h Definition 8 The cosets {aH | a ∈ G} under the operation * form a group, called the quotient group H/G. Definition 9 Let S ⊆ G, the subgroup generated by S denoted by ⟨S⟩ is the subgroup of all elements of G that can expressed as the finite operation of elements in S and their inverses. Definition 10 The rank of a group G is the size of the smallest subset that generates G. 2.2 simplicial Complex and Homology Definition 11 Let 𝑢0, 𝑢1, … , 𝑢𝑘 be points in 𝑅𝑛. A point x=∑ 𝜆𝑖 𝑘 𝑖=1 𝑢𝑖 with each 𝜆𝑖 ∈ R, is an affine combination of the 𝑢𝑖 if ∑ 𝜆𝑖 𝑘 𝑖=1 = 1. Definition 12 The k+1 points are affinely independent if the k vectors 𝑢𝑖−𝑢0, for 1 ≤ i ≤ k, are linearly independent. Definition 13 An affine combination ∑ 𝜆𝑖 𝑘 𝑖=1 is called a convex combination if each 𝜆𝑖 are non-negative. Definition 14 The convex hull of U = {𝑢0, 𝑢1, … , 𝑢𝑘} is the set of all convex combonations of U . Definition 15 A k-simplex σ is the convex hull of k+1 affinely independent points 𝑢0, 𝑢1, … , 𝑢𝑘. So we can see in figure 2 that 0-simplex is a vertex, 1-simplex is an edge, 2-simplex is a triangle, and 3-simplex is a tertrahedron. Now we want to define a special union of some simplexes that is called simplicial complex. The intuitive definition of simplicial complex K is that if a simplex is in K, all of its faces need to be in K, too. In addition,", "meta": {"paper_id": "1906.01726v1", "chunk_index": 2, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_3", "text": "a triangle, and 3-simplex is a tertrahedron. Now we want to define a special union of some simplexes that is called simplicial complex. The intuitive definition of simplicial complex K is that if a simplex is in K, all of its faces need to be in K, too. In addition, the simplexes have to be glued together along whole faces or be separate. The figure 3 showes some useful examples. Fig. 2: The figure shows k-simplexes for k = 0;1;2;3 Fig. 3: figure number 1 showes a simplicial complex but figures in number 2 are not simplicial complexes Definition 16 A face of simplex σ induced by { 𝑢𝑖} is the convex hull of a nonempty subset of the { 𝑢𝑘}, and it is called proper face if the subset is not the entire set. We some times write τ ≤ σ if the τ is a face of s and τ < σ if it is a proper face of σ . Definition 17 A simplicial complex K is a finite collection of simplexes such that first σ ∈ K and τ ≤ σ implies τ∈ K , and second σ, σ0 ∈ K implies σ∩σ0 is either empty or a face of both. Definition 18 The p-th chain group of a simplicial complex K is (𝐶𝑝(K),+), the free Abelian group on the oriented p- simplices, where [σ] = - [t ] if τ = σ furthermore σ and τ have different orientations. An element of 𝐶𝑝(K) is called a p-chain, denoted by ∑ 𝜆𝑖 𝑘 𝑖=1 [σ𝑖] where 𝜆𝑖∈ Z, σ𝑖 ∈ K. Definition 19 Let K be a simplicial complex and σ ∈ K, σ = [𝑣0, 𝑣1, … , 𝑣𝑘]. The boundary homomorphism 𝜕𝑘: 𝐶𝑘 (K) → 𝐶𝑘−1(K) is defined as follows: 𝜕𝑘σ = ∑(−1) 𝑖 𝑖[𝑣0, 𝑣1, … , 𝑣𝑖̂, … , 𝑣𝑛] Where 𝑣𝑖̂ indicates that 𝑣𝑖 is deleted from the sequence. It is easy to check that 𝜕𝑘 is well defined, that is, 𝜕𝑘is the same for every ordering in the same orientation. Example 20 The boundary of the oriented triangle in the figure 4 is as follows: ∂[a,b,c] = [b,c]- [a,c]+[a,b] = [b,c]+[c,a]+[a,b]. Fig. 4: A 3-dimensional simplicial complex Definition 21 The k-th cycle group is 𝑍𝑘= Ker𝜕𝑘. A chain that is an element of 𝑍𝑘is named a k-cycle. The k-th boundary group is 𝐵𝑘 = Im𝜕𝑘+1. A chain that is an element of 𝐵𝑘 is called a k-boundary. Lemma 1 Fundamental lemma of homology: For every (p+1)-chain d we have 𝜕𝑘 𝜕𝑘+1d= 0. From the last lemma, we know that the group of boundaries form a subgroups of the cycles group and then we can take quotients. In other words, we can partition each cycle group into classes of each cycles that differ from each other by boundaries. This leads to the notion of homology groups and their ranks, which we now define and discuss. Definition 22 The p-th homology group is the p-th cycle group modulo the p-th boundary group, denoted by 𝐻𝑝= 𝑍𝑝/𝐵𝑝. The p-th betti number, denoted by, ß𝑝 is", "meta": {"paper_id": "1906.01726v1", "chunk_index": 3, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_4", "text": "from each other by boundaries. This leads to the notion of homology groups and their ranks, which we now define and discuss. Definition 22 The p-th homology group is the p-th cycle group modulo the p-th boundary group, denoted by 𝐻𝑝= 𝑍𝑝/𝐵𝑝. The p-th betti number, denoted by, ß𝑝 is the rank of group 𝐻𝑝. A Vietoris-Rips complex of diameter ε is the simplicial complex that is defined as follows: VR(ε) = { σ | diam(σ) ≤ ε } where diam(σ) is the largest possible distance of points in σ . Definition 24 A sequence of nested simplicial subcomplexes ∅⊆𝐾0 ⊆𝐾1 ⊆𝐾2 ⊆⋯ is called a filtration. Definition 25 Consider the following filteration ∅⊆𝐾0 ⊆𝐾1 ⊆𝐾2 ⊆⋯ . Naturally we will have a sequence of homomorphisms induced by inclusions 𝐻𝑝(𝐾0) →𝐻𝑝(𝐾1) →𝐻𝑝(𝐾2) →⋯ We say that a p-th homology class [c] is born at 𝐾𝑖, if [c] ∈𝐾𝑖 but [c] ∉ 𝐾𝑖−1 and we say [c] dies in 𝐾𝑖, if it merges with a class that is born earlier i.e [c] ∈𝐾𝑖−1 and [c] ∉ 𝐾𝑖 3 Persistent Homology Persistent homology tries to find and track homological groups and holes with the help of filtrations. In this section we try to explain the concept and algorithm of persistent homology and then we apply it to text classification. A filtered simplicial complex with its boundary functions is called a persistent complex. For an increasing sequence of positive real numbers we attain a persistent complex. Our goal is toanalyze topological invariants of a point cloud data set by analyzing its persistent complex. some normal representation for persistent homology are barcode, persistent diagram and lanscape . A barcode represents each persistent generator(classes that generate p-th homology group) with a horizontal line beginning at the first filtration level where it appears, and ending at the filtration level where it disappears, while a persistence diagram plots a point for each generator with its x-coordinate the birth time and its y- coordinate the death time. A visual representation of persistent homology is persistent diagram. Definition 26 The p-persistent diagram D of a filtration ∅⊆𝐾0 ⊆𝐾1 ⊆𝐾2 ⊆⋯ is defined as follows. Let μ𝒑 𝒊𝒋 be the number of independent p- dimensional classes that are Born in 𝐾𝑖 and die entering 𝐾𝑗, then D is obtained by drawing a set of points (i, j) with multiplicity μ𝒑 𝒊𝒋 , where the diagonal is added with infinite multiplicity. For comparing two persistent diagrams some metrics are defined that two of most important of them are bottleneck and wasserstein distances. Definition 27 Let 𝐷1, 𝐷2 be two persistent diagrams and B be the set of all bijective functions φ : 𝐷1 →𝐷2. If ∥ .∥∞be the supremum norm, then the bottleneck distance between two persistent diagrams 𝐷1, 𝐷2 denoted by 𝑊∞ (𝐷1, 𝐷2) is defined as follows. 𝑊∞ (𝐷1, 𝐷2) = inf 𝑠𝑢𝑝φ∈B𝑥∈𝐷1 ∥ x −φ(x) ∥∞ Definition 28 Let 𝐷1, 𝐷2 be two persistent diagrams and B be the set of all bijective functions φ : 𝐷1 →𝐷2, then the Wasserstein distance between two persistent diagrams 𝐷1, 𝐷2 denoted by", "meta": {"paper_id": "1906.01726v1", "chunk_index": 4, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_5", "text": "(𝐷1, 𝐷2) is defined as follows. 𝑊∞ (𝐷1, 𝐷2) = inf 𝑠𝑢𝑝φ∈B𝑥∈𝐷1 ∥ x −φ(x) ∥∞ Definition 28 Let 𝐷1, 𝐷2 be two persistent diagrams and B be the set of all bijective functions φ : 𝐷1 →𝐷2, then the Wasserstein distance between two persistent diagrams 𝐷1, 𝐷2 denoted by 𝑊p (𝐷1, 𝐷2) is defined as follows. 𝑊p (𝐷1, 𝐷2) =[𝑖𝑛𝑓φ∈B ∑ ∥ x −φ(x) ∥∞ 𝑝 𝑥∈𝐷1 ] 1 𝑝 Since it is very hard to analyze the information about homological groups and holes we can use a visualization method called “Barcode”, the idea is as follows: if a hole appears in 𝜀𝑡1 we start to draw a line where the begining of the line is at 𝜀𝑡1 in x-axis and if die in 𝜀𝑡2 , we stop drwing the line and end of the line would be at 𝜀𝑡2 . Persistent landscape is another method introduced by bebunik[2] to visualize persistent homology . Definition 29 The persistence landscape is a function λ: NR →R̅ where R̅ denotes the extended real numbers [-, ]. Alternatively, it may be thought of a sequence of functions 𝜆𝑘 : R →R , where 𝜆𝑘 (𝑡) = 𝜆 (𝑘, 𝑡). Define 𝜆𝑘 (𝑡) = sup{𝑚 ≥0 𝛽(𝑡−𝑚),(𝑡+𝑚) ≥𝑘} where 𝛽𝑖,𝑗is the dimension of group 𝐻𝑖𝐻𝑗 ⁄ . The graph of landscape indicates persistent and non-persistent betti numbers, for example the support of persistent landscape denotes non-persistent betti numbers and the maximum of landscape graph indicate the most persistent betti number. 3.1 Persistant Homology Algorithm Let P be a point cloud data. First we construct the Vietoris-Rips complex for P as follows: consider an increasing sequence of positive real numbers 𝜀1 ≤𝜀2 ≤𝜀3 ≤⋯, then we construct a cover of circles with centers of points in P and diameter 𝜀1, so we have as many circles as the number of data points in point cloud data, next we draw an edge between the center of each two circle which have any intersection and therefore we have a simplicial complex VR(𝜀1). We do the same process for all i = 1,2,3,… as a result we have a filteration of complexes VR(𝜀𝑖). Since it is very hard to analyze the information about homological groups and holes we can use a visualization method called “Barcode”, the idea is as follows: if a hole appears in 𝜀𝑡1 we start to draw a line where the begining of the line is at 𝜀𝑡1 in x-axis and if die in 𝜀𝑡2 , we stop drwing the line and end of the line would be at 𝜀𝑡2 . 3.1 Persistant Homology Algorithm Let P be a point cloud data. First we construct the Vietoris-Rips complex for P as follows: consider an increasing sequence of positive real numbers 𝜀1 ≤𝜀2 ≤𝜀3 ≤⋯, then we construct a cover of circles with centers of points in P and diameter 𝜀1, so we have as many circles as the number of data points in point cloud data, next we draw an edge between the center of each two circle which have any intersection and therefore we have", "meta": {"paper_id": "1906.01726v1", "chunk_index": 5, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_6", "text": "construct a cover of circles with centers of points in P and diameter 𝜀1, so we have as many circles as the number of data points in point cloud data, next we draw an edge between the center of each two circle which have any intersection and therefore we have a simplicial complex VR(𝜀1). We do the same process for all i = 1,2,3,… as a result we have a filteration of complexes VR(𝜀𝑖). Since it is very hard to analyze the information about homological groups and holes we can use a visualization method called “Barcode”, the idea is as follows: if a hole appears in 𝜀𝑡1 we start to draw a line where the begining of the line is a 𝜀𝑡1 in x-axis and if die in 𝜀𝑡2 , we stop drwing the line and end of the line would be at 𝜀𝑡2. 3.2 Results Persistent homology has already been implemented in R packages like “TDA” and “TDAstatus”, in this article we used these two packages for text classification. In this part we used the textual data (poems) of two iranian poets (Hafez and Ferdowsi), the data set gathered from ”Shahnameh”1 and ”Ghazaliat-e-Hafez”. It includes about 8000 hemistich from each book. After preprocessing we fed the data to tf-idf algorithm in order to make document term matrix, next we fed the document term matrix to persistent homology algorithm. First we sketch presistent diagram, barcode and persistent landscapes for a sample of Ferdowsi poems including 1000 hemistich that is shown in figure 5. We also devided 8000 hemistich of “hafez” to 8 parts with lenght 1000, then we computed persistent diagram and first landscape of each part. Finally we sketched the mean landscape of these parts. We did same work for 8000 hemistich of “Ferdowsi”. At last step we computedWasserstein distances between persistent Diagrams of correspondance parts of “hafez” and “Ferdowsi” poems. We bring these results as follows. Fig. 5: Respectively from left to right diagrams show persistent diagram, Barcode and four persistentlandscapes for 1000 hemistich of Ferdowsi 1 An epic book from Ferdowsi Fig. 6: First row shows persistent of land scapes of first there parts of Hafez poems and second row shows persistent of land scapes of first there parts of Ferdowsi poems We bring the results of computing Wasserstein dictances between different partsof poems in tables 1 and 2 as follows. Different parts of poems of Ferdowsi and Hafez Distance of Dim 0 Distance of Dim 1 part1 of Ferdowsi and part1 of Hafez 449.4274 0.6881788 part2 of Ferdowsi and part2 of Hafez 82.16098 0.2928399 part3 of Ferdowsi and part3 of Hafez 82.84308 3.765808 part4 of Ferdowsi and part4 of Hafez 553.3845 2.078418 part5 of Ferdowsi and part5 of Hafez 118.2316 3.312112 part6 of Ferdowsi and part6 of Hafez 141.321 0.5248254 part7 of Ferdowsi and part7 of Hafez 74.28296 3.282468 part8 of Ferdowsi and part8 of Hafez 387.2628 0.4503871 Table 1: Wasserstein0-distances and 1-distances is computed for coresponding parts of Hafez and Ferdowsi poems Different parts of Ferdowsi poems Distance of Dim 0 Distance of Dim 1 part1 and", "meta": {"paper_id": "1906.01726v1", "chunk_index": 6, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_7", "text": "141.321 0.5248254 part7 of Ferdowsi and part7 of Hafez 74.28296 3.282468 part8 of Ferdowsi and part8 of Hafez 387.2628 0.4503871 Table 1: Wasserstein0-distances and 1-distances is computed for coresponding parts of Hafez and Ferdowsi poems Different parts of Ferdowsi poems Distance of Dim 0 Distance of Dim 1 part1 and part2 146.1085 0.3634205 part2 and part3 5.64018 3.405932 part3 and part4 310.8692 1.946319 part4 and part5 278.3973 1.427155 part5 and part6 38.93984 3.003388 part6 and part7 14.5807 3.266145 part7 and part8 16.77798 3.322824 Table 2: Wasserstein distances between consequences parts of Ferdowsi poems 4 Mapper Algorithm The Mapper algorithm was introduced by Singh, Mmoli and Carlsson[25] as a geometrical tool to analyze and visualize datasets. Here we use below notation to explain the mathematics of mapper method. Symbol Explanation 1 𝑋 Underlying space of point cloud data (𝑅𝑛 for some 𝑛∈𝑁) 2 𝑃 Point cloud data ( 𝑃⊆ 𝑋) 3 𝑌 Parameter space (usually 𝑌 is equal to real numbers) 4 𝐹 Filter function ( 𝐹: ρ ⊆ 𝑋 → 𝑌 ) 5 Γ Range of 𝐹 restricted to ρ 6 𝑈 A covering of ρ (ρ⊆ ⋃α∈𝑈 α) 7 ξ Collection of subintervals of Γ which overlap (ξ covers Γ ) 𝑻𝒂𝒃𝒍𝒆 𝟑: 𝑁𝑜𝑡𝑎𝑡𝑖𝑜𝑛 • First we start with a suitable filter function 𝐹: ρ ⊆ 𝑋 → 𝑌 • Then we find the range of 𝐹 restricted to ρ and call it Γ . Then partition Γ into subintervals ξ in order to create a covering of ρ with 𝐹−1in the next step • For every subinterval ξ𝑖∈ ξ we make the following set 𝑋𝑖= {x | 𝐹(𝑥) ∈ξ𝑖}, the set 𝑈={𝑋𝑖} form a covering for 𝑃 (𝑃⊆⋃i𝑋𝑖) • For every element 𝑋𝑖 of 𝑈 we cluster the points of 𝑋𝑖 with a suitable metric i.e for every 𝑋𝑖 we have the set of clusters 𝑋𝑖𝑗; • Every cluster 𝑋𝑖𝑗would be represented as a node in graph and if 𝑋𝑖𝑗∩𝑋𝑟𝑠≠∅ then we draw an edge between the nodes 𝑋𝑖𝑗 and 𝑋𝑟𝑠. The intuitive idea behind Mapper is illustrated in figure7 and can be explained as follows: suppose we have a point cloud data representing a shape, for example a “knot”. First we project the whole data on a coordinate system with less dimensionality in order to reduce complexity via dimensionality reduction (here we project the data on the knot to x-axis). Now we partition the parameter space (x-axis) into several bins with an overlapping percentage. Next, put data into overlapping bins. Afterwards, we use clustering algorithms in order to classify the points of each bin into several clusters. Once the previous stage is done, we can create our interactive graph. (a) (b) (c) (d) (e) (f) Fig. 7: Mapper algorithm on knot shape data cloud: a) First we project the whole data cloud to embedded space (here x-axis). b) Then we partition the embedded space into overlapping bins (here showed as colored intervals). c) Then we put data into overlapping bins. d) Next we use any clustering algorithm to cluster the points in the cloud data. e&f) each cluster of", "meta": {"paper_id": "1906.01726v1", "chunk_index": 7, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_8", "text": "whole data cloud to embedded space (here x-axis). b) Then we partition the embedded space into overlapping bins (here showed as colored intervals). c) Then we put data into overlapping bins. d) Next we use any clustering algorithm to cluster the points in the cloud data. e&f) each cluster of points in every bin would represented as a node of the graph and we draw and edge between two nodes if they share a common data point. 4.1 TDA-Based Mapper Analysis online algorithm and our dataset Implementation of mapper algorithm is already avaliable in python packages like “Mapper” and “ Kepler Mapper”. In this article we used “Kepler Mapper” along side other python packages. In this paper, we used the textual data (poems) of two iranian poets Hafez and Ferdowsi. The data set gathered from ”Shahnameh” (An epic book from Ferdowsi) and ”Ghazaliat-e-Hafez”, that includes about different 9000 hemistich (ranging from epic wars to love and romance) from both books. In our model as a first step we used “ truncatedSVD” and “t-SNE” as filter functions after applying “TF-IDF” on our data. Next we choose resolution and overlap, there are many ways to choose resolution and overlapping percentage. Figure5 illustrates different resolution and overlapping amounts. The more resolution we have, the better data will be partitioned and classified and higher the overlapping percantage is, the more compact our resulting graph would be. For clustering we used “Agglomerative Clustering” avaliable in “sklearn” package with “cosine” similarity and complete linkage. Fig. 8: Comparison of different resolutions and overlapping precentage: As is evident from the above table, the more resolution we have the better our resulting graph will be classified and the more the overlapping percentage is, the more compact the resulting graph would become. 4.2 Results we examined two accuracy tests on our shape graph. First, we partition the whole graph into 3 clusters (”Hafez”,”Ferdowsi”,”Both”), Which in ”Hafez” cluster we have the nodes which include the high percent of Hafezian poems, similarly in ”Ferdowsi” cluster we have the nodes which include the high percent of poems of Ferdowsi and in the ”Both” cluster we have about the same amount of both poems. We examine what percent of poems in ”Hafez” cluster really belong to Hafezian poems and the same method for other clusters. To do this we simply divide the number of Hafezian poems in each node in the ”Hafez” cluster by the number of all poems in each node in the same cluster, and we do the same test to other clusters as well. For example for ”Hafez” cluster we have the following calculations: Fig. 9: TDA Mapper in text classification. A) Gathering and pre-processing the data of poems. B) Using TF-IDF algorithm to turn pre-processed data into a matrix which columns represent a single word in the entire corpus and each row represent a hemistich. The matrix is of the form [9000* 67671] (# hemistich *# words in corpus). C) Zoomed data [100*100]. D) Filteration step, Using truncated SVD and T-SNE algorithms as filter functions. E) Bining: the whole 2-dimensional space divided", "meta": {"paper_id": "1906.01726v1", "chunk_index": 8, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_9", "text": "a single word in the entire corpus and each row represent a hemistich. The matrix is of the form [9000* 67671] (# hemistich *# words in corpus). C) Zoomed data [100*100]. D) Filteration step, Using truncated SVD and T-SNE algorithms as filter functions. E) Bining: the whole 2-dimensional space divided in to smaller bins. F) Clustering: clustering algorithms applied to each bin in order tocreate the nodes in the final graph. G) Output of the algorithm as a graph, every node in the graph is a cluster of the previous step and nodes share an edge if they have a data point in common. Percentage of accuracy = (Number of poems of Hafez in each node of the cluster / Number of all poems in the whole cluster) . So if we have the accuracy percentage of a for a cluster it means that a percent of the poems in that cluster has been labelled correctly. second, we tried to devide some parts of the graph into several clusters based on their semantic subjects and we cteated 5 clusters. To visually analyze each cluster werepresented the text with in each cluster as a word cloud, which is easy to understand in just one sight. (a) Original Graph (b) Clustered for the first test Fig. 10: (a) shows the original output of the Mapper algorithm, while (b) shows the same graph which we partitioned it into three clusters for the first test(Red for ”Hafez” cluster, Orange for “Both” cluster, and Green for “Ferdowsi” cluster). After examining the first test on each cluster, we got the following results: for “Hafez” cluster percentage of accuracy was 80 percent, for“Ferdowsi” cluster percentage of accuracy was about 94 percent ,and for ”Both” cluster percentage of accuracy was 40 percent for Hafez poems and 60 percent for Ferdowsi poems. So for the “Hafez” cluster we can say that 80 percent of the poems in the cluster has the rigth label and so on for other clusters. (a) (b) (c) (d) (e) (f) Fig. 11: semantic clustering of the graph, in image (f) we have examined the second test on our graph shape, we choose some clusters and then examined if they are semantically related to each other, we choosse 5 clusters (“Red”,“Blue”,“Black”,“Yellow”,and “orange”) as apparent in (f) and then draw their word cloud. (a) is word cloud of “Black” and it is clear that it belongs to Hafezian poems because of the frequency of some words and if we want to analyze it semantically it would be the cluster of romantic poems. (b) is word cloud for “Red” and because of the frequency of words in its word cloud we can say it semantically related to worshiping the god. (c) is word cloud for “Yellow”. (d) is word cloud for “Orange”.(e) is word cloud for “Blue” References 1. Bhattacharya, S., Ghrist, R., and Kumar, V. Persistent Homology for Path Planning in Uncertain Environments. IEEE TRANSACTIONS ON ROBOTICS, VOL. 31, NO. 3, JUNE 2015. 2. Bubenik, P., Dotko, P. A persistence landscapes toolbox for topological statistics. J. Symbol. Comput.", "meta": {"paper_id": "1906.01726v1", "chunk_index": 9, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_10", "text": "cloud for “Orange”.(e) is word cloud for “Blue” References 1. Bhattacharya, S., Ghrist, R., and Kumar, V. Persistent Homology for Path Planning in Uncertain Environments. IEEE TRANSACTIONS ON ROBOTICS, VOL. 31, NO. 3, JUNE 2015. 2. Bubenik, P., Dotko, P. A persistence landscapes toolbox for topological statistics. J. Symbol. Comput. 78, 91114 (2016). 3. Carlsson, G. Topology and data. Bull. Amer. Math. Soc, 46, 255308 (2009). 4. Chaski, C. Who’s at the Keyword? Authorship Attribution in Digital Evidence Investigations. International Journal of Digital Evidence, Volume 4, Issue 1(2005). 5. Diederich, J., Kindermann, J., Leopold, E., Paas, G. Authorship Attribution with Support Vector Machines, Applied Intelligence, 19(1):109-123, 2003. 6. Edelsbrunner, H. Persistent Homology In Image processing. Lecture Notes in Computer Science book series.(LNCS, volume 7877), 2013. 7. Emrani, S., Saponas, T., Morris, D., and Krim, H. A. Novel Framework for Pulse Pressure Wave Analysis Using Persistent Homology. IEEE SIGNAL PROCESSING LETTERS, VOL. 22, NO. 11, NOVEMBER 2015. 8. Gholizadeh, S., Seyeditabari, A., and Zadrozny, W. Topological Signature of 19th Century Novelists: Persistent Homology in Text Mining, big data and cognitive computing. 2, 3, 2018, doi:10.3390/bdcc2040033. 9. G¨unther, D., Reininghaus, J., Hotz, I., and Wagner, H. Memory-Efficient Computation of Persistent Homology for 3D Images using Discrete Morse Theory. 24th SIBGRAPI Conference on Graphics, Patterns and Images, 2011. 10. Holmes, D. Authorship Attribution. Computers and the Humanities, 28:87-106. Kluwer Academic Publishers, 1995. 11. Jessica, L., Nielson, Jesse Paquette, Aiwen, W. Liu, Cristian F. Guandique, C., Amy. Tovar et al. Topological data analysis for discovery in preclinical spinal cord injury and traumatic brain injury. Nature Communications, Oct 14, 2015. 12. Lee, H., Chung, M. K., Kang, H., Kim B., Lee, D. DISCRIMINATIVE PERSISTENT HOMOLOGY OF BRAIN NETWORKS. 2011 IEEE International Symposium on Biomedical Imaging: From Nano to Macro. 13. Lee, H., Kang, H., Chung, M.K., Kim, B., and Lee, D. Persistent Brain Network Homology From the Perspective of Dendrogram. IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 31, NO. 12, DECEMBER 2012. 14. Lum, P. Y., Singh. G, Lehman, A., Ishkanov. T., Vejdemo-Johansson, M., Alagappan, M., Carlsson, J., Carlsson, G. Extracting insights from the shape of complex data using topology, Sci. Rep, 3, 1236 (2013). 15. Lum, P. Y., Lehmann, L., Singh, G., Ishkhanov, T., Vejdemo-Johansson, M., Carlsson , G., The topology of politics: voting connectivity in the US House of Representatives. KTH, School of Computer Science and Communication (CSC), Computer Vision and Active Perception, CVAP, 2012. 16. Malyutov, M.B. Authorship Attribution of Texts: a Review. Proceedings of the program ”Information transfer” held in ZIF. University of Bielefeld, Germany, 17 pages, 2004. 17. Maaten, L., Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 9, 25792605 (2008). 18. Mihaela, E. Sardiu., Joshua, M. Gilmore., Groppe, B., Florens, L., Michael, P. Washburn. Identification of Topological Network Modules in Perturbed Protein Interaction Networks. Scientific Reports, 2017. 19. Nicolau, M., Levine, A. J., Carlsson, G. Topology based data analysis identifies a subgroup of breast cancers with a unique mutational profile and excellent survival. Proc Natl Acad Sci U S A, 108, 726570 (2011). 20. NILSSON, D., EKGREN, A., Topology and", "meta": {"paper_id": "1906.01726v1", "chunk_index": 10, "filename": "1906.01726v1.pdf", "chunk_length": 512}}
{"id": "1906.01726v1_chunk_11", "text": "Perturbed Protein Interaction Networks. Scientific Reports, 2017. 19. Nicolau, M., Levine, A. J., Carlsson, G. Topology based data analysis identifies a subgroup of breast cancers with a unique mutational profile and excellent survival. Proc Natl Acad Sci U S A, 108, 726570 (2011). 20. NILSSON, D., EKGREN, A., Topology and Word Spaces, Bachelors Thesis at CSC. 21. Rizvi, A., Camara, P., Kandror, E., Roberts, T., Schieren, I. et al. Single-cell topological RNA-seq analysis reveals insights into cellular differentiation and development, Nature Biotechnology, 2017. 22. Robles, A. , Hajij, M., Paul Rosen. The shape of an image: A study of mapper on images. arXiv:1710.09008v2 ,[cs.CV] 7 Dec 2017. 23. Romano, D., Nicolau, M., Quintin, E. M., Mazaika, P. K., Lightbody, A. A., Hazlett, H. C., Piven, J., Carlsson, G., Reiss, A. L. Topological methods reveal high and low functioning neuro-phenotypes within fragile X syndrome. Hum. Brain Mapp. 35, 49044915 (2014). 24. Saggar, M., Sporns, O., Gonzalez-Castillo, J., Bandettini, P., Carlsson, G. et al. Towards a new approach to reveal dynamical organization of the brain using topological data analysis, Nature Communications, Apr 11, 2018. 25. Singh, G., Mmoli, F., Carlsson, G. Topological methods for the analysis of high dimensional data sets and 3d object recognition. In Eurographics Symposium on Point-Based Graphics (eds Botsch, M., Pajarola, R.) (The Eurographics Association, 2007). 26. Stamatatos, E., Fakotakis, N., Kokkinakis, G. Computer-Based Authorship Attribution Without Lexical Measures. Computers and the Humanities, 35: 193-214, Kluwer Academic Publishers, 2001. 27. Torres P., Hromic H., Heravi B. Topic Detection in Twitter Using Topology Data Analysis. ICWE 2015 Workshops, LNCS 9396, pp. 186197, 2015. 28. Zhang, J., Ziyu, X., and Stan, Z. L. Prime Discriminant Simplicial Complex. . IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS, VOL. 24, NO. 1, JANUARY 2013. 29. Zhu, X. Persistent Homology: An Introduction and a New Text Representation for Natural Language Processing. Twenty-Third International Joint Conference on Artificial Intelligence, 2013.", "meta": {"paper_id": "1906.01726v1", "chunk_index": 11, "filename": "1906.01726v1.pdf", "chunk_length": 316}}
{"id": "2005.13421v1_chunk_0", "text": "arXiv:2005.13421v1 [cs.CL] 27 May 2020 Thirty Musts for Meaning Banking Johan Bos Center for Language and Cognition University of Groningen johan.bos@rug.nl Lasha Abzianidze Center for Language and Cognition University of Groningen l.abzianidze@rug.nl Abstract Meaning banking—creating a semantically annotated corpus for the purpose of semantic parsing or generation—is a challenging task. It is quite simple to come up with a com- plex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This pa- per lists some lessons learned in nearly ten years of meaning annotation during the de- velopment of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper’s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an intro- ductory preface). Instead, its structure is in- spired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4–11), and de- sign of meaning representations (Section 12– 30). We hope this overview will give inspira- tion and guidance in creating improved mean- ing banks in the future. 1 Look at other meaning banks Other semantic annotation projects can be inspiring, help you to ﬁnd solutions to hard annotation problems, or to ﬁnd out where improvements to the state of the art are still needed (Abend and Rappoport, 2017). Good starting points are the English Resource Gram- mar (Flickinger, 2000, 2011), the Groningen Meaning Bank (GMB, Bos et al. 2017), the AMR Bank (Banarescu et al., 2013), the Parallel Meaning Bank (PMB, Abzianidze et al. 2017), Scope Control Theory (Butler and Yoshimoto, 2012), UCCA (Abend and Rappoport, 2013), Prague Semantic Dependencies (Hajiˇc et al., 2017) and the ULF Corpus based on Episodic Logic (Kim and Schubert, 2019). The largest differences between these approaches can be found in the expressive power of the meaning representations used. The sim- plest representations correspond to graphs (Banarescu et al., 2013; Abend and Rappoport, 2013); slightly more expressive ones corre- spond to ﬁrst-order logic (Oepen et al., 2016; Bos et al., 2017; Abzianidze et al., 2017; Butler and Yoshimoto, 2012), whereas others go beyond this (Kim and Schubert, 2019). Generally, an increase of expressive power causes a decrease of efﬁcient reasoning (Blackburn and Bos, 2005). Semantic formalisms based on graphs are at- tractive because of their simplicity, but will face issues when dealing with negation in inference tasks (Section 21). The choice might depend on the application (e.g., if you are not interested in detecting contradictions, coping with negation is less important), but arguably, an open-domain meaning bank ought to be independent of a speciﬁc application. 2 Select public domain corpora Any text could be protected by copyright law and it is not always easy to ﬁnd suitable cor- pora that are free from copyright issues. Indeed, the", "meta": {"paper_id": "2005.13421v1", "chunk_index": 0, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_1", "text": "negation is less important), but arguably, an open-domain meaning bank ought to be independent of a speciﬁc application. 2 Select public domain corpora Any text could be protected by copyright law and it is not always easy to ﬁnd suitable cor- pora that are free from copyright issues. Indeed, the relationship between copyright of texts and their use in natural language processing is com- plex (Eckart de Castilho et al., 2018). Nonethe- less, it pays off to make some effort by search- ing for corpora that are free or in the public do- main (Ide et al., 2010). This makes it easier for other researchers to work with it, in particular those that are employed by institutes with lesser ﬁnancial means. The GMB only includes cor- pora from the public domain (Basile et al., 2012b). Free parallel corpora are also available via OPUS (Skadin¸ˇs et al., 2014). Other researchers take ad- vantage of vague legislation and distribute corpora quoting the right of fair use (Postma et al., 2018). Recently, crowd sourcing platforms such as Figure Eight make datasets available, too (“Data For Ev- eryone”), under appropriate licensing. While tar- geting the public domain corpora, one might need to bear in mind the coverage of the corpora de- pending on the objectives of semantic annotation. 3 Freeze the corpus before you start Once you start your annotation efforts, it is a good idea to freeze the corpora that will comprise your meaning bank.1 In the GMB project (Basile et al., 2012b), the developers were less strict in maintain- ing this principle. During the project they came across new corpora, but after adding them to the GMB they were forced to ﬁx and validate annota- tions on many levels to get the newly added corpus up to date and in sync with the rest. This problems manifests itself especially for corpora that are con- structed via a phenomenon-driven annotation ap- proach (Section 24). 4 Work with raw texts in your corpus Keep the original texts as foundation for annota- tion. Never ever carry out any semantic annota- tion on tokenised texts, but use stand-off annota- tion on character offsets (Section 5). Tokenisa- tion can be done in many different ways, and the atoms of meaning certainly do not correspond di- rectly to words. Most of the current conventions in tokenisation are based on what has been used in (English) syntax-oriented computational linguis- tics and can be misleading when other languages are taken into consideration (Section 29). More- over, if you use an off-the-shelf tokeniser, you will ﬁnd out soon that it makes mistakes—and correct- ing those would break any annotations done at the word token level. More likely, during your anno- tation project, you will ﬁnd the need to change the tokenisation guidelines to deal properly with multi-word expressions (Section 22). In addition, punctuation and spacing carry information that could be useful for deep learning approaches, and their original appearance should therefore in one 1Freezing the corpora already ﬁxes certain data statements for your meaning bank, like curation rationale, language va- riety, and text", "meta": {"paper_id": "2005.13421v1", "chunk_index": 1, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_2", "text": "deal properly with multi-word expressions (Section 22). In addition, punctuation and spacing carry information that could be useful for deep learning approaches, and their original appearance should therefore in one 1Freezing the corpora already ﬁxes certain data statements for your meaning bank, like curation rationale, language va- riety, and text characteristics. Communicating these data statements is important from an application point of view (Bender and Friedman, 2018). way or another should be preserved. An exam- ple: a “New York-based” company could be a new company based in York, but the other interpreta- tion is more likely. In an NLP-processing pipeline, it is too late for syntax to ﬁx this in a compositional way—the tokenisation needs to be improved. 5 Use stand-off annotation Stand-off annotation is a no-brainer as it offers a lot more ﬂexibility. It enables keeping annota- tions separate from the original raw text, where ideally each annotation layer has its own ﬁle (Ide and Romary, 2006; Pustejovsky and Stubbs, 2012). It is best executed with respect to the char- acter offsets of the raw texts in the corpus (Sec- tion 4). A JSON or XML-based annotation ﬁle can always be generated from this, should the demand be there. Stand-off annotation is in particular ad- vantageous in a setting where several layers of an- notation interact with each other (typically in a pipeline architecture). This was extremely helpful in the GMB (Bos et al., 2017) where the document segmentation (sentence and word boundaries) got improved several times during the project, without having any negative effect on annotation occurring later in the semantic processing pipeline (such as part-of-speech tagging and named entity recogni- tion). 6 Consider manual annotation Several meaning banks are created with the help of a grammar. The best example here is the sophisticated English Resource Grammar (Flickinger, 2000, 2011) used to produce the tree- banks, Redwoods (Oepen et al., 2004) and Deep- Bank (Flickinger et al., 2012), annotated with En- glish Resource Semantics (ERS) in a composi- tional way, by letting the annotator pick the correct or most plausible analysis. Similarly, the meaning representations in the GMB are system-produced and partially hand-corrected (Bos et al., 2017), us- ing a CCG parser (Clark and Curran, 2004). Like- wise, the meaning representations in the PMB are system-produced with the help of a CCG parser (Lewis and Steedman, 2014) and some of it is completely hand-corrected. In contrast, the mean- ing representations of the AMR Bank are com- pletely manually manufactured—without the aid of a grammar—with the help of an annotation in- terface and an extensive manual (Banarescu et al., 2013). Bender et al. (2015) argue that grammar- based meaning banking requires less annotation guidelines, that it provides more consistent anal- yses, and that it is more scalable. The downside of grammar-based annotation is that several com- pound expressions are not always compositional (negative and modal concord, postnominal geni- tives (“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive En- glish Resource Grammar took about several years to develop, but it", "meta": {"paper_id": "2005.13421v1", "chunk_index": 2, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_3", "text": "annotation is that several com- pound expressions are not always compositional (negative and modal concord, postnominal geni- tives (“of John’s”), odd punctuation conventions, idioms), and that grammars with high recall and precision are costly to produce (the impressive En- glish Resource Grammar took about several years to develop, but it is restricted to just one language). 7 Make a friendly annotation interface Annotation can be fun (especially if gamiﬁcation is applied, see Section 9), but it can also be te- dious. A good interface helps the annotator to make high-quality annotations, to work efﬁciently, and to be able to focus on particular linguistic phe- nomena. An annotation interface should be web- based (i.e., any browser should support it), sim- ple to use, and personalised.2 The latter grants control over annotations of particular users. The “Explorer” (Basile et al., 2012a) introduced in the GMB and later further developed in the PMB, has various search abilities (searches for phrases, reg- ular expressions, and annotation labels), a statis- tics page, a newsfeed, and a user-friendly way to classify annotations as “gold standard”. The in- clusion of a “sanity checker” helps to identify an- notation mistakes, in particular if there are several annotation layers with dependencies. It is also a good idea to hook the annotation interface up with a professional issue reporting system. 8 Include an issue reporting system Annotators will sooner or later raise issues, have questions about the annotation scheme, or ﬁnd bugs in the processing pipeline. This is valuable information for the annotation project and should not get lost. The proper way to deal with this is to include a sophisticated bug reporting sys- tem in the annotation interface. For the GMB (Bos et al., 2017) and the PMB (Abzianidze et al., 2017), the Mantis Bug Tracker3 was incorporated inside the Explorer (Basile et al., 2012a). Besides Mantis there are many other free and open source web-based bug tracking systems available. A bug tracker enables one to categorize issues, assign 2For more details about web-based collaborative annota- tion tools we refer to Biemann et al. (2017). 3https://www.mantisbt.org/ them to team members, have dedicated discussion thread for each issue, and keep track of all im- provements made in a certain time span (useful for the documentation in data releases). 9 Be careful with the crowd Following the idea of Phrase Detectives (Chamberlain et al., 2008), in the GMB (Bos et al., 2017) a game with a purpose (GWAP) was introduced to annotate parts of speech, antecedents of pronouns, noun compound rela- tions (Bos and Nissim, 2015), and word senses (Venhuizen et al., 2013). The quality of annota- tions harvested from gamiﬁcation was generally high, but the amount of annotations relatively low—it would literally take years to annotate the entire GMB corpus. An additional problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irrespon- sible (Andrade F.R.H., 2016). The alternative, engaging people by ﬁnancially awarding them via crowdsourcing platforms such as Mechanical Turk or Figure Eight, solves the quantity problem", "meta": {"paper_id": "2005.13421v1", "chunk_index": 3, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_4", "text": "problem with GWAPs is recruiting new players: most players play the game only once, and attempts to make the game addictive could be irrespon- sible (Andrade F.R.H., 2016). The alternative, engaging people by ﬁnancially awarding them via crowdsourcing platforms such as Mechanical Turk or Figure Eight, solves the quantity problem (Pustejovsky and Stubbs, 2012), but introduces other issues including the question what a proper wage would be (Fort et al., 2011) and dealing with tricksters and cheaters (Buchholz and Latorre, 2011). 10 Proﬁt from lexicalised grammars A lexicalised grammar gives an advantage in an- notating syntactic structure. In case of the com- positional semantics, this also leads to automatic construction of the phrasal semantics. This is because, in a lexicalised grammar, most of the grammar work is done in the lexicon (there is only a dozen general grammar rules), and anno- tation is just a matter of giving the right infor- mation to a word (rather than selecting the cor- rect interpretation from a possibly large set of parse trees). In the PMB a lexicalised grammar is used: Combinatory Categorial Grammar (CCG, Steedman 2001), and the core annotation layers for each word token are a CCG category, a se- mantic tag (Abzianidze and Bos, 2017), a lemma, and a word sense. Annotating thematic roles (Sec- tion 18) is also convenient in a lexicalised gram- mar environment (Bos et al., 2012). Finally, a lex- icalised grammar coupled with compositional se- mantics facilitates annotation projection for mean- ing preserving translations and opens the door to multilingual meaning banking (Section 29). Pro- jection of meaning representation from one sen- tence to another is reduced to word alignment and word-level annotation transfer. This type of pro- jection is underlying the idea of moving from the monolingual GMB to the multilingual PMB. 11 Try to use language-neutral tools Whenever possible, in machine-assisted annota- tion, get language technology components that are not tailored to speciﬁc languages, because this in- creases portability of meaning processing compo- nents to other languages (Section 29). The statis- tical tokeniser (for word and sentence segmenta- tion) used in the PMB is Elephant (Evang et al., 2013). The current efforts in multi-lingual POS- tagging, semantic tagging (Abzianidze and Bos, 2017) and dependency parsing are promising (White et al., 2016). In the PMB a categorial grammar is used to cover four languages (En- glish, Dutch, German, and Italian), using the same parser and grammar, but with language-speciﬁc statistical models trained for the EasyCCG parser (Lewis and Steedman, 2014). Related are gram- matical frameworks designed for parallel grammar writing (Ranta, 2011; Bender et al., 2010). 12 Apply normalisation to symbols Normalising the format of non-logical symbols (the predicates and individual constants, as op- posed to logical symbols such as negation and con- junction) in meaning representations decreases the need for awkward background knowledge rules that would otherwise be needed to predict cor- rect entailments. Normalisation (van der Goot, 2019) can be applied to date expressions (e.g., the 24th of February 2010 vs. 24-02-2010 or dozens of variations on these), time expressions (2pm, 14:00, two o’clock), and", "meta": {"paper_id": "2005.13421v1", "chunk_index": 4, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_5", "text": "decreases the need for awkward background knowledge rules that would otherwise be needed to predict cor- rect entailments. Normalisation (van der Goot, 2019) can be applied to date expressions (e.g., the 24th of February 2010 vs. 24-02-2010 or dozens of variations on these), time expressions (2pm, 14:00, two o’clock), and numerical ex- pressions (twenty-four, 24, vierundzwanzig; three thousand, 3,000, 3000, 3 000). Compositional attempts to any of the above mentioned classes of expressions are highly ambitious and not rec- ommended. Take, for instance, the Dutch clock time expression “twee voor half vier”, which de- notes 03:28 (or 15:28)—how would you derive this compositionally in a computational straight- forward way? Other normalisations for consider- ation are expansion of abbreviations to their full forms, lowercasing proper names, units of mea- surement, and scores of sports games. To promote inter-operability between annotated corpora, it is a good idea to check whether any standards are pro- posed for normalisation (Pustejovsky and Stubbs, 2012). 13 Limit underspeciﬁcation Underspeciﬁcation is a technique with the aim to free the semantic interpretation component from a disambiguation burden (Reyle, 1993; Bos, 1996; Copestake et al., 2005). In syntactic treebanks, however, the driving force has been to assign the most plausible parse tree to a sentence. This makes sense for the task of statistical (syntac- tic) parsing. The same applies to (statistical) se- mantic parsing: a corpus with the most likely interpretation for sentences is required. More- over, it is not straightforward to draw correct in- ferences with underspeciﬁed meaning representa- tions (Reyle, 1995). So it makes sense, at least from the perspective of semantic annotation, to produce the most plausible interpretation for a given sentence. Consider the following examples. A “sleeping bag” could be a bag that is asleep, but it is very unlikely (even in a Harry Potter set- ting), so should be annotated as a bag designed to be slept in. In the sentence “Tom kissed his mother”, the possessive pronoun could refer to a third party, but by far the most likely interpreta- tion is that Tom’s mother is kissed by Tom, and that reading should be reﬂected in the annotation. Genuine scope ambiguities are relatively rare in ordinary text, and it is questionable whether the representational overhead of underspeciﬁed scope is worth the effort given the low frequency of the phenomenon. Nonetheless, resolving ambiguities is sometimes hard, in particular for sentences in isolation. What is plausible for one annotator is implausible for another. Finally, one needs to be careful, as annotation guidelines that give prefer- ence for one particular reading (based on statisti- cal plausibility) have the danger of introducing or even amplifying bias. 14 Beware of annotation bias Assigning the most likely interpretation to a sen- tence can also give an unfair balance to stereo- types. In the PMB, gender of personal proper names are annotated. In many cases this is a straightforward exercise. But there are some- times cases where the gender of a person is not known. The disturbing distribution of male versus female pronouns (or titles) strongly suggests that a female is the", "meta": {"paper_id": "2005.13421v1", "chunk_index": 5, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_6", "text": "In the PMB, gender of personal proper names are annotated. In many cases this is a straightforward exercise. But there are some- times cases where the gender of a person is not known. The disturbing distribution of male versus female pronouns (or titles) strongly suggests that a female is the least likely choice (Webster et al., 2018). But following this statistical suggestion only causes greater divide. The PMB annotation guidelines for choosing word senses (Section 15) are such that when it is unclear what sense to pick, the higher sense (thus, the most frequent one), must be selected. This is bad, because systems for word sense disambiguation already show a ten- dency towards assigning the most frequent sense (Postma et al., 2016). More efforts are needed to reduce bias (Zhao et al., 2017). 15 Use existing resources for word senses The predicate symbols that one ﬁnds in mean- ing representation are usually based on word lem- mas. But words have no interpretation, and a link to concepts in an existing ontology (Lenat, 1995; Navigli and Ponzetto, 2012) is something that is needed to make the non-logical sym- bols in meaning representations interpretable. In the AMR Bank, verbs are disambiguated by OntoNotes senses (Banarescu et al., 2013). In the PMB, nouns, verbs, adjectives and adverbs are labelled with the senses of (English) Word- Net (Fellbaum, 1998). Picking the right sense is sometimes hard for annotators, sometimes be- cause there is too little context, but also be- cause the deﬁnitions of ﬁne-grained senses are sometimes hard to distinguish from each other (Lopez de Lacalle and Agirre, 2015). Annotation guidelines are needed for ambiguous cases where syntax doesn’t help to disambiguate: “Swimming is great fun.” (swimming.n.01 or perhaps swim.v.01?), “Her words were emphasized.” (emphasized.a.01or emphasize.v.02?). WordNet’s coverage is impressive and substan- tial, but obviously not all words are listed (ex- ample: names of products used as nouns) and sometimes it is inconsistent (for instance, “ap- ple juice” is in WordNet, but “cherry juice” is not). Many WordNets exists for languages other than English (Navigli and Ponzetto, 2012; Bond and Foster, 2013). 16 Apply symbol grounding Symbol grounding helps to connect abstract rep- resentations of meaning with objects in the real world or to unambiguous descriptions of concepts or entities. This happens on the conceptual level with mapping words to WordNet synsets or to a well-deﬁned inventory of relations. Princeton WordNet (Fellbaum, 1998) lists several instances of famous persons but obviously the list is in- complete. The AMR Bank includes links from named entities to wikipedia pages, but obviously not every named entity has a wikipedia entry. To our knowledge, no other meaning banks ap- ply wikiﬁcation. Other interesting applications for symbol grounding are GPS coordinates for to- ponyms (Leidner, 2008), visualisation of concepts or actions (Navigli and Ponzetto, 2012), or creat- ing timelines (Bamman and Smith, 2014). 17 Adopt neo-Davidsonian events It seems that in most (if not all) semantically anno- tated corpora a neo-Davidsonian event semantics is adopted. This means that every event introduces its own entity as a variable, and this variable can", "meta": {"paper_id": "2005.13421v1", "chunk_index": 6, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_7", "text": "(Navigli and Ponzetto, 2012), or creat- ing timelines (Bamman and Smith, 2014). 17 Adopt neo-Davidsonian events It seems that in most (if not all) semantically anno- tated corpora a neo-Davidsonian event semantics is adopted. This means that every event introduces its own entity as a variable, and this variable can be used to connect the event to its thematic roles. In the original Davidsonian approach, an event vari- able was simply added to the predicate introduced by the verb (Davidson, 1967; Kamp and Reyle, 1993) as a way to add modiﬁers (e.g., moving from eat(x,y) to eat(e,x,y) for a transi- tive use of to eat). In most modern meaning repre- sentations thematic roles are introduced to reduce the number of arguments of verbal predicates to one, also known as the neo-Davidsonian tradition (Parsons, 1990) (e.g., moving from eat(e,x,y) to eat(e) AGENT(e,x) PATIENT(e,y)). A direct consequence of a neo-Davidsonian de- sign is the need for an inventory of thematic roles. But there is also an alternative, which is given a ﬁxed arity to event predicates, of which some of them may be unused (Hobbs, 1991) when the context does not provide this information (e.g., for the intransitive usage of to eat, still maintain eat(e,x,y) where y is left unspeciﬁed). 18 Use existing role labelling inventories A neo-Davidsonian approach presupposes a dic- tionary of thematic (or semantic) role names. There are three popular sets available: PropBank, VerbNet, and FrameNet. PropBank (Palmer et al., 2005) proposes a set of just six summarising roles: ARG0 (Agent), ARG1 (Patient), ARG2 (In- strument, Benefactive, Attribute), ARG3 (Start- ing Point), ARG4 (Ending Point), ARGM (Mod- iﬁer). The interpretation of these roles are in many cases speciﬁc to the event in which they participate. The AMR Bank adopts these Prop- Bank roles (Banarescu et al., 2013). VerbNet has a set of about 25 thematic roles independently de- ﬁned from the verb classes (Kipper et al., 2008). A few examples are: Agent, Patient, Theme, In- strument, Experiencer, Stimulus, Attribute, Value, Location, Destination, Source, Result, and Ma- terial. The PMB adopts the thematic roles of VerbNet. FrameNet is organised quite differently. Its starting point is not rooted in linguistics, but rather in real-world situations, classiﬁed as frames (Baker et al., 1998). Frames have frame elements that can be realised by linguistic expressions, and they correspond to the PropBank and VerbNet roles. There are more than a thousand differ- ent frames, and each frame has its own speciﬁc role set (frame elements). For instance, the Buy- Commerce frame has roles Buyer, Goods, Seller, Money, and so on. There are also recent propos- als for comprehensive inventories for roles intro- duced by prepositional and possessive construc- tions (Schneider et al., 2018). In the PMB, we employ a uniﬁed inventory of thematic roles (an extension of the VerbNet roles) that is applicable to verbs, adjectives, prepositions, possessives or noun modiﬁers. 19 Treat agent nouns differently Agent and recipient nouns (nouns that denote per- sons performing or receiving some action, such as employee, victim, teacher, mother, cyclist, victim) are intrinsically relational (Booij, 1986). Mod- elling them", "meta": {"paper_id": "2005.13421v1", "chunk_index": 7, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_8", "text": "of the VerbNet roles) that is applicable to verbs, adjectives, prepositions, possessives or noun modiﬁers. 19 Treat agent nouns differently Agent and recipient nouns (nouns that denote per- sons performing or receiving some action, such as employee, victim, teacher, mother, cyclist, victim) are intrinsically relational (Booij, 1986). Mod- elling them like ordinary nouns, i.e., as one-place predicates, can give rise to contradictions for any individual that has been assigned more than one role, because while you want to be able to state that a violin player is not the same thing as a mother, a person could perfectly be a mother and a violin player at the same time. Moreover, a fast cyclist could be a slow driver. Incorrect modeling can furthermore lead to over-generation of some unmanifested relations (for instance, if Butch is Vincent’s boss and Mia’s husband, a too sim- ple model would predict that Butch is also Vin- cent’s husband and Mia’s boss. In the AMR Bank (Banarescu et al., 2013) agent nouns are decom- posed (e.g., an “investor” is a person that invests). In the PMB agent nouns introduce a mirror entity (e.g. an “investor” is a person with the role of in- vestor). 20 Beware of geopolitical entities Names used to refer to geopolitical entities (GPEs) are a real pain in the neck for semantic annotators. How many times did we change the annotation guidelines for these annoying names! The prob- lem is that expressions like “New York”, “Italy”, or “Africa” can refer to locations, their govern- ments, sport squads that represent them, or the people that live there (and in some case to multi- ple aspects at the same time, as in “Italy produces better wine than France”). This instance of sys- tematic polysemy manifests itself for all classes of GPE, including continents, countries, states, provinces, cities, and so on. Detailed instructions for annotating GPEs can be found in the ACE an- notation guidelines (Doddington et al., 2004). 21 Give scope to negation Sentence meaning is about assigning truth condi- tions to propositions (Section 23). Negation plays a crucial role here—in fact, the core of seman- tics is about negation, identifying whether a state- ment is true of false. Negation is a semantic phe- nomenon that requires scope, in other words, it cannot be modelled by simply applying it as a property of an entity. It should be clear—explicit or implicit—what the scope of any negation oper- ator is, i.e. the parts of the meaning representa- tion that are negated. The GMB, PMB and Deep- Bank (Flickinger et al., 2012) assign proper scope to negation (the latter with the help of underspeci- ﬁcation). In AMR Bank negation is modelled with the help of a relation, and this doesn’t always get the required interpretation (Bos, 2016). Negation can be tricky: negation afﬁxes (Section 23) require special care, negative concord (Section 6) and neg raising (Liu et al., 2018) are challenges for com- positional approaches to meaning construction. 22 Pay attention to compound words In the GMB (Bos et al., 2017) we largely ignored multi-word expressions (MWEs), believing", "meta": {"paper_id": "2005.13421v1", "chunk_index": 8, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_9", "text": "can be tricky: negation afﬁxes (Section 23) require special care, negative concord (Section 6) and neg raising (Liu et al., 2018) are challenges for com- positional approaches to meaning construction. 22 Pay attention to compound words In the GMB (Bos et al., 2017) we largely ignored multi-word expressions (MWEs), believing that compositionality would eventually do away with it. Except it doesn’t. MWEs come in various forms, and require various treatments (Sag et al., 2002). Think about proper names (names of per- sons, companies, locations, events), titles and la- bels (of people, of books, chapters, of songs), compounds, phrasal verbs, particle verbs, ﬁxed phrases, and idioms. Consider for instance “North and South Dakota”, it is quite a challenge to de- rive the representation state(x) & name(x,’North- Dakota’) in a compositional way. And many compounds are not compositional (“peanut but- ter” is not butter, and “athlete’s foot” is not a body part but a nasty infection). It is hard to de- cide where to draw the line between a composi- tional and non-compositional approach to multi- word expressions. Even though “red wine” is written in English with two words, in German it is written in one word (“rotwein”). WordNet (Fellbaum, 1998) lists many multi-word expres- sions and could be used as a resource to decide whether a compounds is analysed compositionally or not. In the PMB, titles of songs or other artistic works are treated as a single token (because they are proper names), which works ﬁne for “Jingle Bells” but becomes a bit awkward and uncomfort- able with longer titles such as Lennon and Mc- Cartney’s “Lucy in the Sky with Diamonds”, or Pink Floyds’s “Several Species of Small Furry An- imals Gathered Together in a Cave and Groov- ing With A Pict”. It is quite unfair and unrealis- tic to expect the tokeniser to recognise this as a multi-word expression. The alternative, applying some reinterpretation after having ﬁrst carried out a compositional analysis, puts a heavier burden on the syntax-semantics interface. The bottom line is that MWEs form a wild bunch of expressions for which a general modelling strategy covering all types does not seem to exist. There also seems to be a connection with quotation (Maier, 2014). 23 Use inference tests in design The driving force to motivate how to shape or what to include in a meaning representation should be textual entailment or contradiction checks (this is a practice borrowed from formal semantics). For instance, when designing a meaning representa- tion for adjectives, the meaning for “ten-year-old boy” should not imply that the boy in question is old. Likewise, the meaning representation for “un- happy” should not be the same as that for “not happy”, because the meanings of these expres- sions are not equivalent (as “Bob is not happy” doesn’t entail “Bob’s unhappy”—Bob can be both not happy and not unhappy—even though the en- tailment holds in the reverse direction: if Bob is unhappy, he is not happy). Similarly, the meaning representation for “Bologna is the cultural capi- tal of Italy” should not lead to the incorrect in- ference", "meta": {"paper_id": "2005.13421v1", "chunk_index": 9, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_10", "text": "doesn’t entail “Bob’s unhappy”—Bob can be both not happy and not unhappy—even though the en- tailment holds in the reverse direction: if Bob is unhappy, he is not happy). Similarly, the meaning representation for “Bologna is the cultural capi- tal of Italy” should not lead to the incorrect in- ference that “Bologna is the capital of Italy”. In addition, or as alternative to inference checks, is applying the method of model-theoretic interpre- tation (Blackburn and Bos, 2005) when designing meaning representations. It should be clear what a representation actually means, in other words, un- der which conditions it is true or false. A formal way of deﬁning this is via models of situation, and a satisfaction deﬁnition that tells us, given a certain situation, whether a statement holds or doesn’t. This method was introduced by the logician Tarski (Tarski and Vaught, 1956). It bears similarities with posing a query to a relational database. The method forces you to make a strict distinction be- tween logical (negation, disjunction, equality) and non-logical symbols (the predicates and individual constants in your meaning representation). 24 Divide and conquer Do not try to do model all semantic phenomena the ﬁrst time around. There are just too many. Some good candidates to put on hold are plurals, tense, aspect, focus, presupposition (see Section 25), and generics (more in Section 27), because a proper treatment of these phenomena requires a lot more than a basic predicate-argument struc- ture. A strict formalisation of plurals quickly leads to complicated representations (Kamp and Reyle, 1993), leading to compromising approximations in the AMR Bank (Banarescu et al., 2013) or PMB (Abzianidze et al., 2017). In the GMB (Bos et al., 2017) and the AMR Bank tense is simply ignored. Annotating aspect is complex—for instance, the use of the perfect differs enormously even between closely related languages such as English, Dutch, and Italian (van der Klis et al., 2017). These com- plications lead to a simple annotation model in the PMB where tense is reduced to a manageable set of three tenses: past, present and future. There are, therefore, a lot of interesting problems left for the second round of semantic annotation! 25 Put complex presuppositions on hold Presuppositions are propositions that are taken for granted. Several natural language expressions in- troduce presuppositions. These expressions are called presuppositions triggers. (For instance, “Mary left, too.” presupposes that someone else besides Mary left. Here “too” is the trigger of this presupposition.) There are many different kinds of triggers, and many do not contribute to the mean- ing of the sentence, but rather put constraints on the context. The question, then, is what to do with them in a meaning banking project. Some classes of presupposition triggers, referring expressions including proper names, possessive phrases, and deﬁnite descriptions, can be treated in a similar way as pronouns, as is done in the GMB and the PMB, following Bos (2003). Yet there are other classes of triggers that are notoriously hard to represent, because they require some ”copying” of large pieces of meaning representation, inter- act with focus, and", "meta": {"paper_id": "2005.13421v1", "chunk_index": 10, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_11", "text": "can be treated in a similar way as pronouns, as is done in the GMB and the PMB, following Bos (2003). Yet there are other classes of triggers that are notoriously hard to represent, because they require some ”copying” of large pieces of meaning representation, inter- act with focus, and require non-trivial semantic composition methods. To these belong implica- tive verbs (manage), focusing adverbs (only, just), and repetition particles (again, still, yet, another). For instance, although in the PMB a sentence like “The crowd applauded again.” is the presuppo- sition trigger, “again” is semantically tagged as a repetition trigger, for now it doesn’t perform any costly operations on the actual meaning rep- resentation. The ﬁrst alternative, a meaning rep- resentation with two different applauding events that are temporally related, is complicated to con- struct. The second alternative, introducing “again” as a predicate, doesn’t make sense semantically (what is the meaning of “again”?), or as an op- erator (again, how will it be deﬁned logically?) isn’t attractive either. There are, currently no good ways to deal with complex presupposition trig- gers, and more research is needed here turning for- mal ideas (Kamp and Rossdeutscher, 2009) into practical solutions. 26 Respect elliptical expressions They are invisible, but omnipresent: elliptical ex- pressions. Comparative ellipsis is present in many languages (“My hair is longer than Tom’s”). In English, verb phrase ellipsis occurs (“Tom eats as- paragus, but his brother doesn’t.”), which is well studied (Dalrymple et al., 1991), and annotated corpora exist as well (Bos and Spenader, 2011). Dutch and German exhibit a large variety of gap- ping cases (“Tom isst Spargel, aber sein Bruder nicht.”). Italian is a language with pro-drop (“Ho fame”, i.e., (I) am hungry). Ellipsis requires a ded- icated component in a pipeline architecture. In the PMB the inclusion of an ellipsis layer has been postponed for the beneﬁt of other components, features, and efforts. As a consequence, a growing number of documents cannot be added to the gold set because there isn’t an adequate way of deal- ing with a missing pronoun, an odd comparison expression, or an elided verb phrase. 27 Think about generics Generic statements and habituals are hard to model straightforwardly in ﬁrst-order logic (Carlson, 1977). The sentence “a lion is strong” or “a dog has four legs” is not about a particular lion or dog, nor is it about all dogs or lions. The inventor of “the typewriter” was not the inventor of a particular typewriter, but of the typewriter concept in general. Such generic concepts are also known as kinds in the literature (Reiter and Frank, 2010). It is not impossible to approximate this in ﬁrst-order logic, but it requires an ontological distinction between entities denoting individuals and entities denoting concepts (kinds). A further question is how tense should be annotated in habitual sentences, as in “Jane used to swim every day” (in some period in the past, Jane swam every day) or “Jane swims every day” (in the current period, Jane swims every day). To our knowl- edge, none of the existing meaning banks have", "meta": {"paper_id": "2005.13421v1", "chunk_index": 11, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_12", "text": "how tense should be annotated in habitual sentences, as in “Jane used to swim every day” (in some period in the past, Jane swam every day) or “Jane swims every day” (in the current period, Jane swims every day). To our knowl- edge, none of the existing meaning banks have a satisfactory treatment of generics, even though techniques have been proposed to detect generics (Reiter and Frank, 2010; Friedrich and Pinkal, 2015). Recent proposals try to change this situation (Donatelli et al., 2018). 28 Don’t try to be clever The English verb “to be” (and its counterpart in other Germanic languages) is a semantic nuisance. When used as an auxiliary verb—including pred- icative usages of adjectives—there isn’t much to worry about it, as it only semantically contributes tense information. However, when used as a cop- ula it can express identity, locative information, or predications involving nouns. From a logical perspective, it might seem attractive to use equal- ity in these cases and interpret “to be” logically rather than lexically, (Blackburn and Bos, 2005), but this makes it impossible to include tense infor- mation, unless equality is (non-standardly) viewed as a three-place relation. There are various senses for “be” in WordNet, and it makes pragmatically sense to use these: “This is a good idea” (sense 1), “John is the teacher” (sense 2), “the book is on the table” (sense 3), and so on. A similar story can be told for “to have” in expressions like “Mary has a son”, where the ﬁrst attempt in the PMB was to analyse “to have” in such possessive constructions as logical, i.e. only introducing tense information, and coerce the relational noun “son” into a posses- sive interpretation. This was soon abandoned due to complications in composition. 29 Don’t focus on just one language Most meaning banks consider just one language, and usually this is English. This is understand- able, as English is the current scientiﬁc language, but it is also risky, because when designing mean- ing representation decisions could be made that work for English but not for other languages. Phenomena such as deﬁnite descriptions, ellipsis, possessives, aspect, and gender, behave even in closely related languages quite differently from each other. Dealing with multiple languages is, without any doubt, harder, but if one takes sev- eral languages into account at the same time the result is more likely to be more language-neutral meaning representations. And that’s what mean- ings should be, they are abstract objects, indepen- dent of the language used to expressed them. Of course, there are concepts that can be expressed in certain languages with a single word that other lan- guages are not capable of, but the core of meaning representations should be agnostic to the source language. A good starting point is to work with typologically-related languages. An efﬁcient an- notation technique to cover multiple languages is annotation projection (Evang and Bos, 2016; Liu et al., 2018). This requires a parallel corpus and automatic word alignment, and existing se- mantic annotations for at least one language. 30 Measure meaning discrepancies A large part", "meta": {"paper_id": "2005.13421v1", "chunk_index": 12, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_13", "text": "work with typologically-related languages. An efﬁcient an- notation technique to cover multiple languages is annotation projection (Evang and Bos, 2016; Liu et al., 2018). This requires a parallel corpus and automatic word alignment, and existing se- mantic annotations for at least one language. 30 Measure meaning discrepancies A large part of the users of semantically an- notated corpora are from the semantic parsing area, and they need to be able to measure and quantify their output with respect to gold stan- dard meanings. The currently accepted methods are based on precision and recall on the compo- nents of the meaning representation by convert- ing them to triples or clauses (Allen et al., 2008; Dridan and Oepen, 2011; Cai and Knight, 2013; Van Noord et al., 2018; Kim and Schubert, 2019). In a parallel corpus setting, such evaluation mea- sures can also be used to compare the meaning representation of a source text and its transla- tion (Saphra and Lopez, 2015). This is done in the PMB, where a non-perfect meaning match between source and target helps the annotator to identify possible culprits. It is important to note that most of these matching techniques check for syntactic equivalence, and don’t take seman- tic equivalence into account—the same meaning could be expressed by syntactically different rep- resentations. The approach by Van Noord et al. (2018) applies normalisation steps for word senses to make matching more semantic. Acknowledgments We would like to thank the two anonymous re- viewers for their comments—they helped to im- prove this paper considerably. Reviewer 1 gave us valuable pointers to the literature that we missed, and spotted many unclear and ambiguous formu- lations. Reviewer 2 was disappointed by the ﬁrst version of this paper— we hope s/he likes this im- proved version better. This work was funded by the NWO-VICI grant Lost in Translation Found in Meaning (288-89-003). References Omri Abend and Ari Rappoport. 2013. Universal conceptual cognitive annotation (ucca). In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 228–238, Soﬁa, Bulgaria. Association for Computational Linguistics. Omri Abend and Ari Rappoport. 2017. The state of the art in semantic representation. In Proceedings of the 55th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 77–89, Vancouver, Canada. Association for Compu- tational Linguistics. Lasha Abzianidze, Johannes Bjerva, Kilian Evang, Hessel Haagsma, Rik van Noord, Pierre Ludmann, Duc-Duy Nguyen, and Johan Bos. 2017. The paral- lel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations. In Proceedings of the 15th Confer- ence of the European Chapter of the Association for Computational Linguistics, pages 242–247, Valen- cia, Spain. Lasha Abzianidze and Johan Bos. 2017. Towards uni- versal semantic tagging. In Proceedings of the 12th International Conference on Computational Seman- tics (IWCS 2017) – Short Papers, pages 1–6, Mont- pellier, France. James F. Allen, Mary Swift, and Will de Beaumont. 2008. Deep Semantic Analysis of Text. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceed- ings, volume 1 of Research", "meta": {"paper_id": "2005.13421v1", "chunk_index": 13, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_14", "text": "Conference on Computational Seman- tics (IWCS 2017) – Short Papers, pages 1–6, Mont- pellier, France. James F. Allen, Mary Swift, and Will de Beaumont. 2008. Deep Semantic Analysis of Text. In Johan Bos and Rodolfo Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceed- ings, volume 1 of Research in Computational Se- mantics, pages 343–354. College Publications. Isotani S. Andrade F.R.H., Mizoguchi R. 2016. The bright and dark sides of gamiﬁcation. Intelligent Tutoring Systems. ITS 2016. Lecture Notes in Com- puter Science, 9684. Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In 36th An- nual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics. Proceedings of the Con- ference, pages 86–90, Universit´e de Montr´eal, Mon- treal, Quebec, Canada. David Bamman and Noah A. Smith. 2014. Unsuper- vised discovery of biographical structure from text. Transactions of the Association for Computational Linguistics, 2:363–376. Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Grifﬁtt, Ulf Her- mjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for Sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178–186, Soﬁa, Bulgaria. Valerio Basile, Johan Bos, Kilian Evang, and Noortje Venhuizen. 2012a. A platform for collaborative se- mantic annotation. In Proceedings of the Demon- strations at the 13th Conference of the European Chapter of the Association for Computational Lin- guistics (EACL), pages 92–96, Avignon, France. Valerio Basile, Johan Bos, Kilian Evang, and Noortje Joost Venhuizen. 2012b. Developing a large semantically annotated corpus. In Proceed- ings of the Eighth International Conference on Lan- guage Resources and Evaluation (LREC’12), Istan- bul, Turkey. European Language Resources Associ- ation (ELRA). Emily M. Bender. 2013. Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax. Synthesis Lectures on Human Language Technolo- gies. Morgan & Claypool Publishers. Emily M. Bender, Scott Drellishak, Antske Fokkens, Laurie Poulson, and Saﬁyyah Saleem. 2010. Gram- mar customization. Research on Language & Com- putation, 8(1):23–72. Emily M. Bender, Dan Flickinger, Stephan Oepen, Woodley Packard, and Ann Copestake. 2015. Layers of interpretation: On grammar and compositionality. In Proceedings of the 11th International Conference on Computational Semantics, pages 239–249, London, UK. Association for Computational Linguistics. Emily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–604. Chris Biemann, Kalina Bontcheva, Richard Eckart de Castilho, Iryna Gurevych, and Seid Muhie Yimam. 2017. Collaborative web-based tools for multi-layer text annotation. In Nancy Ide and James Pustejovsky, editors, The Handbook of Linguistic Annotation, Text, Speech, and Technology book series, pages 229–256. Springer Netherlands. P. Blackburn and J. Bos. 2005. Representation and Inference for Natural Language. A First Course in Computational Semantics. CSLI. Francis Bond and Ryan Foster. 2013. Linking and ex- tending an open multilingual Wordnet. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1352–1362, Soﬁa, Bulgaria. Associa- tion for Computational Linguistics. Geert Booij. 1986. Form and meaning in morphology;", "meta": {"paper_id": "2005.13421v1", "chunk_index": 14, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_15", "text": "Francis Bond and Ryan Foster. 2013. Linking and ex- tending an open multilingual Wordnet. In Proceed- ings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Pa- pers), pages 1352–1362, Soﬁa, Bulgaria. Associa- tion for Computational Linguistics. Geert Booij. 1986. Form and meaning in morphology; the case of dutch agent nouns. Linguistics, 24:503– 518. J. Bos. 2003. Implementing the Binding and Accom- modation Theory for Anaphora Resolution and Pre- supposition Projection. Computational Linguistics, 29(2):179–210. Johan Bos. 1996. Predicate Logic Unplugged. In Pro- ceedings of the Tenth Amsterdam Colloquium, pages 133–143, ILLC/Dept. of Philosophy, University of Amsterdam. Johan Bos. 2016. Expressive power of abstract mean- ing representations. Computational Linguistics, 42(3):527–535. Johan Bos, Valerio Basile, Kilian Evang, Noortje Ven- huizen, and Johannes Bjerva. 2017. The Gronin- gen Meaning Bank. In Nancy Ide and James Puste- jovsky, editors, Handbook of Linguistic Annotation, volume 2, pages 463–496. Springer. Johan Bos, Kilian Evang, and Malvina Nissim. 2012. Annotating semantic roles in a lexicalised grammar environment. In Proceedings of the Eighth Joint ACL-ISO Workshop on Interoperable Semantic An- notation (ISA-8), pages 9–12, Pisa, Italy. Johan Bos and Malvina Nissim. 2015. Uncovering noun-noun compound relations by gamiﬁcation. In Proceedings of the 20th Nordic Conference of Com- putational Linguistics. Johan Bos and Jennifer Spenader. 2011. An annotated corpus for the analysis of vp ellipsis. Language Re- sources and Evaluation, 45(4):463–494. Sabine Buchholz and Javier Latorre. 2011. Crowd- sourcing preference tests, and how to detect cheat- ing. In INTERSPEECH-2011, pages 3053–3056. Alistair Butler and Kei Yoshimoto. 2012. Banking meaning representations from treebanks. Linguistic Issues in Language Technology, 7(6):1–22. Shu Cai and Kevin Knight. 2013. Smatch: an evalua- tion metric for semantic feature structures. In Pro- ceedings of the 51st Annual Meeting of the Associa- tion for Computational Linguistics (Volume 2: Short Papers), pages 748–752, Soﬁa, Bulgaria. Associa- tion for Computational Linguistics. Gregory Norman Carlson. 1977. Reference to Kinds in English. Ph.D. thesis, University of Massachusetts. Richard Eckart de Castilho, Giulia Dore, Thomas Margoni, Penny Labropoulou, and Iryna Gurevych. 2018. A legal perspective on training models for natural language processing. In Proceedings of the 11th Language Resources and Evaluation Con- ference, Miyazaki, Japan. European Language Re- source Association. John Chamberlain, Massimo Poe- sio, and Udo Kruschwitz. 2008. Addressing the Resource Bottleneck to Create Large-Scale Annotated Texts. In Johan Bos and Rodolfo Delmonte, editors, Se- mantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Com- putational Semantics, pages 375–380. College Publications. Stephen Clark and James R. Curran. 2004. Parsing the WSJ using CCG and Log-Linear Models. In Pro- ceedings of the 42nd Annual Meeting of the Associa- tion for Computational Linguistics (ACL ’04), pages 104–111, Barcelona, Spain. Ann Copestake, Dan Flickinger, Ivan Sag, and Carl Pollard. 2005. Minimal recursion semantics: An in- troduction. Journal of Research on Language and Computation, 3(2–3):281–332. Mary Dalrymple, Stuart M. Shieber, and Fer- nando C.N. Pereira. 1991. Ellipsis and Higher- Order Uniﬁcation. Linguistics and Philosophy, 14:399–452. Donald Davidson. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action, pages 81–95. University of", "meta": {"paper_id": "2005.13421v1", "chunk_index": 15, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_16", "text": "of Research on Language and Computation, 3(2–3):281–332. Mary Dalrymple, Stuart M. Shieber, and Fer- nando C.N. Pereira. 1991. Ellipsis and Higher- Order Uniﬁcation. Linguistics and Philosophy, 14:399–452. Donald Davidson. 1967. The logical form of action sentences. In Nicholas Rescher, editor, The Logic of Decision and Action, pages 81–95. University of Pittsburgh Press. George Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extrac- tion (ACE) program – tasks, data, and evaluation. In Proceedings of the Fourth International Conference on Language Resources and Evaluation (LREC’04), Lisbon, Portugal. European Language Resources Association (ELRA). Lucia Donatelli, Michael Regan, William Croft, and Nathan Schneider. 2018. Annotation of tense and aspect semantics for sentential AMR. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Con- structions (LAW-MWE-CxG-2018), pages 96–108, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Rebecca Dridan and Stephan Oepen. 2011. Parser evaluation using elementary dependency matching. In Proceedings of the 12th International Conference on Parsing Technologies, pages 225–230, Dublin, Ireland. Association for Computational Linguistics. Kilian Evang, Valerio Basile, Grzegorz Chrupała, and Johan Bos. 2013. Elephant: Sequence labeling for word and sentence segmentation. In Proceedings of the 2013 Conference on Empirical Methods in Nat- ural Language Processing, pages 1422–1426. Kilian Evang and Johan Bos. 2016. Cross-lingual learning of an open-domain semantic parser. In Pro- ceedings of COLING 2016, the 26th International Conference on Computational Linguistics, pages 579–588, Osaka, Japan. Christiane Fellbaum, editor. 1998. WordNet. An Elec- tronic Lexical Database. The MIT Press. Dan Flickinger. 2000. On building a more efﬁcient grammar by exploiting types. Natural Language Engineering, 6(1):15–28. Dan Flickinger. 2011. Accuracy vs. robustness in grammar engineering. In Emily M. Bender and Jen- nifer E. Arnold, editors, Language from a cogni- tive perspective: Grammar, usage, and processing, pages 31–50. CSLI Publications. Daniel Flickinger, Yi Zhang, and Valia Kordoni. 2012. Deepbank: A dynamically annotated treebank of the wall street journal. In Proceedings of the Eleventh International Workshop on Treebanks and Linguistic Theories, pages 85–96. Edic¸˜oes Colibri. Kar¨en Fort, Gilles Adda, and K. Bretonnel Cohen. 2011. Last words: Amazon mechanical turk: Gold mine or coal mine? Computational Linguistics, 37(2):413–420. Annemarie Friedrich and Manfred Pinkal. 2015. Discourse-sensitive automatic identiﬁcation of generic expressions. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1272–1281, Beijing, China. Association for Computational Linguistics. Jan Hajiˇc, Eva Hajiˇcov´a, Marie Mikulov´a, and Jiˇr´ı M´ırovsk´y. 2017. Prague Dependency Treebank. Springer Verlag, Berlin, Germany. Jerry R. Hobbs. 1991. SRI international’s TACITUS system: MUC-3 test results and analysis. In Pro- ceedings of the 3rd Conference on Message Under- standing, MUC 1991, San Diego, California, USA, May 21-23, 1991, pages 105–107. Nancy Ide, Christiane Fellbaum, Collin Baker, and Rebecca Passonneau. 2010. The manually annotated sub-corpus: a community resource for and by In Proceedings of the ACL 2010 Conference Short Papers, pages 68–73, Stroudsburg, PA, USA. Nancy Ide and Laurent Romary. 2006. Represent- ing linguistic corpora and their annotations. In Proceedings of the Fifth International Conference on Language Resources and", "meta": {"paper_id": "2005.13421v1", "chunk_index": 16, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_17", "text": "Passonneau. 2010. The manually annotated sub-corpus: a community resource for and by In Proceedings of the ACL 2010 Conference Short Papers, pages 68–73, Stroudsburg, PA, USA. Nancy Ide and Laurent Romary. 2006. Represent- ing linguistic corpora and their annotations. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC’06), Genoa, Italy. European Language Resources Asso- ciation (ELRA). Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Seman- tics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht. Hans Kamp and Antje Rossdeutscher. 2009. Drs con- struction and lexically driven inferences. Theoreti- cal Linguistics, 20(2-3):165–236. Gene Louis Kim and Lenhart Schubert. 2019. A type-coherent, expressive representation as an initial step to language understanding. CoRR. Karin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2008. A large-scale classiﬁcation of English verbs. Language Resources and Evaluation, 42(1):21–40. Martijn van der Klis, Bert Le Bruyn, and Henri¨ette de Swart. 2017. Mapping the perfect via translation mining. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Lin- guistics: Volume 2, Short Papers, pages 497–502, Valencia, Spain. Association for Computational Linguistics. Oier Lopez de Lacalle and Eneko Agirre. 2015. Crowdsourced word sense annotations and difﬁcult words and examples. In Proceedings of the 11th International Confer- ence on Computational Semantics, pages 94–100, London, UK. Association for Computational Linguistics. Jochen L. Leidner. 2008. Toponym Resolution in Text : Annotation, Evaluation and Applications of Spatial Grounding of Place Names. Universal Press, Boca Raton, FL, USA. Douglas Lenat. 1995. Cyc: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38:33–38. Mike Lewis and Mark Steedman. 2014. A* ccg parsing with a supertag-factored model. In Proceedings of the 2014 Conference on Em- pirical Methods in Natural Language Processing (EMNLP), pages 990–1000, Doha, Qatar. Associa- tion for Computational Linguistics. Qianchu Liu, Federico Fancellu, and Bonnie Webber. 2018. NegPar: A parallel corpus annotated for negation. In Proceedings of the 11th Language Resources and Evaluation Conference, Miyazaki, Japan. European Language Resource Association. Emar Maier. 2014. Pure quotation. Philosophy Com- pass, 9(9):615–630. Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual se- mantic network. Artiﬁcial Intelligence, 193:217– 250. Stephan Oepen, Dan Flickinger, Kristina Toutanova, and Christopher D. Manning. 2004. Lingo redwoods. Research on Language and Computation, 2(4):575–596. Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinkov´a, Dan Flickinger, Jan Hajiˇc, Angelina Ivanova, and Zdeˇnka Ureˇsov´a. 2016. Towards comparability of linguistic graph banks for semantic parsing. In Proceedings of the 10th International Conference on Language Re- sources and Evaluation, pages 3991–3995, Por- toroˇz, Slovenia. Martha Palmer, Paul Kingsbury, and Daniel Gildea. 2005. The proposition bank: An annotated cor- pus of semantic roles. Computational Linguistics, 31(1):71–106. Terence Parsons. 1990. Events in the semantics of En- glish: A study in subatomic semantics. Cambridge, MA: The MIT Press. M.C. Postma, F. Ilievski, and P.T.J.M. Vossen. 2018. Semeval-2018 task 5: Counting events and participants in the long tai In SemEval-2018 : International Workshop on Se- mantic Evaluation 2018. M.C. Postma, R. Izquierdo, E. Agirre, G. Rigau, and", "meta": {"paper_id": "2005.13421v1", "chunk_index": 17, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_18", "text": "En- glish: A study in subatomic semantics. Cambridge, MA: The MIT Press. M.C. Postma, F. Ilievski, and P.T.J.M. Vossen. 2018. Semeval-2018 task 5: Counting events and participants in the long tai In SemEval-2018 : International Workshop on Se- mantic Evaluation 2018. M.C. Postma, R. Izquierdo, E. Agirre, G. Rigau, and P.T.J.M. Vossen. 2016. Addressing the mfs bias in wsd systems. In LREC 2016, pages 1695–1700. James Pustejovsky and Amber Stubbs. 2012. Nat- ural Language Annotation for Machine Learning - a Guide to Corpus-Building for Applications. O’Reilly. Aarne Ranta. 2011. Grammatical Framework: Pro- gramming with Multilingual Grammars. Center for the Study of Language and Information/SRI. Nils Reiter and Anette Frank. 2010. Identifying generic noun phrases. In Proceedings of the 48th Annual Meeting of the Association for Computa- tional Linguistics, pages 40–49, Uppsala, Sweden. Association for Computational Linguistics. Uwe Reyle. 1993. Dealing with Ambiguities by Un- derspeciﬁcation: Construction, Representation and Deduction. Journal of Semantics, 10:123–179. Uwe Reyle. 1995. On Reasoning with Ambiguities. In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Lin- guistics, pages 1–8, Dublin, Ireland. Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2002. Multiword expressions: A pain in the neck for nlp. In Com- putational Linguistics and Intelligent Text Process- ing, pages 1–15, Berlin, Heidelberg. Springer Berlin Heidelberg. Naomi Saphra and Adam Lopez. 2015. AMRICA: an AMR inspector for cross-language alignments. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Demonstrations, pages 36–40, Denver, Colorado. Association for Computational Linguistics. Nathan Schneider, Jena D. Hwang, Vivek Srikumar, Jakob Prange, Austin Blodgett, Sarah R. Moeller, Aviram Stern, Adi Bitan, and Omri Abend. 2018. Comprehensive supersense disambiguation of En- glish prepositions and possessives. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Australia. Association for Computational Linguistics. Raivis Skadin¸ˇs, J¨org Tiedemann, Roberts Rozis, and Daiga Deksne. 2014. Billions of parallel words for free: Building and using the eu bookshop corpus. In Proceedings of the Ninth International Confer- ence on Language Resources and Evaluation (LREC’14), pages 1850–1855, Reykjavik, Ice- land. European Language Resources Association (ELRA). Mark Steedman. 2001. The Syntactic Process. The MIT Press. A. Tarski and R. Vaught. 1956. Arithmetical exten- sions of relational systems. Compositio Mathemat- ica, 13:81–102. David R. Traum. 2000. 20 questions for dialogue act taxonomies. Journal of Semantics, 17(1):7–30. Rob Matthijs van der Goot. 2019. Normalization and parsing algorithms for uncertain input. Ph.D. thesis, University of Groningen. Rik Van Noord, Lasha Abzianidze, Hessel Haagsma, and Johan Bos. 2018. Evaluating scoped mean- ing representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), pages 1685–1693, Miyazaki, Japan. Noortje Venhuizen, Valerio Basile, Kilian Evang, and Johan Bos. 2013. Gamiﬁcation for word sense label- ing. In Proceedings of the 10th International Con- ference on Computational Semantics (IWCS 2013) – Short Papers, pages 397–403, Potsdam, Germany. Kellie Webster, Marta Recasens, Vera Axelrod, and Ja- son Baldridge. 2018. Mind the gap: A balanced corpus of gendered ambiguous pronouns. Transac- tions of the Association for Computational Linguis-", "meta": {"paper_id": "2005.13421v1", "chunk_index": 18, "filename": "2005.13421v1.pdf", "chunk_length": 512}}
{"id": "2005.13421v1_chunk_19", "text": "ing. In Proceedings of the 10th International Con- ference on Computational Semantics (IWCS 2013) – Short Papers, pages 397–403, Potsdam, Germany. Kellie Webster, Marta Recasens, Vera Axelrod, and Ja- son Baldridge. 2018. Mind the gap: A balanced corpus of gendered ambiguous pronouns. Transac- tions of the Association for Computational Linguis- tics, 6:605–617. Aaron Steven White, Drew Reisinger, Keisuke Sak- aguchi, Tim Vieira, Sheng Zhang, Rachel Rudinger, Kyle Rawlins, and Benjamin Van Durme. 2016. Universal decompositional semantics on universal dependencies. In Proceedings of the 2016 Confer- ence on Empirical Methods in Natural Language Processing, pages 1713–1723, Austin, Texas. Asso- ciation for Computational Linguistics. Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or- donez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias ampliﬁcation using corpus-level constraints. In EMNLP.", "meta": {"paper_id": "2005.13421v1", "chunk_index": 19, "filename": "2005.13421v1.pdf", "chunk_length": 128}}
{"id": "2005.14627v1_chunk_0", "text": "Detection of Bangla Fake News using MNB and SVM Classiﬁer Md Gulzar Hussain1, Md Rashidul Hasan2, Mahmuda Rahman3, Joy Protim4 and Sakib Al Hasan5 Department of CSE, Green University of Bangladesh, Bangladesh1,2,4 Department of CSE, Jahangirnagar University, Dhaka, Bangladesh3 Department of CSE, Huzhou Normal University, Huzhou, China5 gulzar.ace@gmail.com1, rashidulhasan777@gmail.com2, aurthi018@gmail.com3, protimjoy@gmail.com4, sakibgreen29@gmail.com5 Abstract—Fake news has been coming into sight in signiﬁcant numbers for numerous business and political reasons and has become frequent in the online world. People can get contaminated easily by these fake news for its fabricated words which have enormous effects on the ofﬂine community. Thus, interest in research in this area has risen. Signiﬁcant research has been conducted on the detection of fake news from English texts and other languages but a few in Bangla Language. Our work reﬂects the experimental analysis on the detection of Bangla fake news from social media as this ﬁeld still requires much focus. In this research work, we have used two supervised machine learning algorithms, Multinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classiﬁers to detect Bangla fake news with CountVectorizer and Term Frequency - Inverse Document Frequency Vectorizer as feature extraction. Our pro- posed framework detects fake news depending on the polarity of the corresponding article. Finally, our analysis shows SVM with the linear kernel with an accuracy of 96.64% outperform MNB with an accuracy of 93.32%. Keywords—Fake News, Bangla Fake News, Text Classiﬁcation, Bangla News, Natural Language Processing. I. INTRODUCTION Fake news is an oxymoron that undermines the credibility of reporting that actually meets the veriﬁability standard and the public interest-i.e. real news. And in this electronic era, it is one of the biggest challenges to control the spreading of false or misleading news due to the free ﬂow of information through social networking sites such as Facebook, Twitter, YouTube, micro-blogging and others. For instance, a new report says, nearly one in three citizens across the United States, Spain, Germany, United Kingdom, Argentina, and South Korea claim they have seen false or misleading information on social media related to COVID-19 [1]. Because of the abundant spreading of false and unreliable information, some commentators are now referring to the latest wave of misinformation that’s accompanied by the COVID-19 pandemic as a disinfodemic [2]. Since the information shapes the way we perceive the universe, therefore, fake news and other misleading facts can take on different aspects. Research in intelligence shows that the rumors spreading in social media leave a particularly lasting impact on less smart people [3] and keep them from making the right decisions. False news is used to build people’s fears, raise racist ideas, and lead to bullying and violence against innocent people. Even fake news has great democratic impacts. American presidential election showed how it disrupts and fallacy people’s opinions [4]. In the last few years, there have been many tragic incidents in Bangladesh due to rumors. In July 2019, ﬁve people were beaten to death and ten injured by mobs as a result of widespread rumors about the expected human sacriﬁce in the construction of the Padma Bridge", "meta": {"paper_id": "2005.14627v1", "chunk_index": 0, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_1", "text": "opinions [4]. In the last few years, there have been many tragic incidents in Bangladesh due to rumors. In July 2019, ﬁve people were beaten to death and ten injured by mobs as a result of widespread rumors about the expected human sacriﬁce in the construction of the Padma Bridge [5]. Because of the expanding number of clients in web-based life, news can be immediately distributed by anyone, and its credibility stands traded off. As fake news is written to mislead the readers, it makes it a difﬁcult task to detect based on the content of the news only. It becomes essential to bring an efﬁcient system to detect the fake news as the news content is diverse in terms of style and subject in which it is written. Fake news detection has lately received considerable interest from researchers. So far different approaches have been introduced in the detection of fake news [6]. Most of the techniques suggested in the literature to identify fake news deal with the problem as a classiﬁcation task by associating labels such as fake or real, true or false, etc. with a particular text. In most cases, machine learning and deep learning methods are utilized to achieve promising outcomes. Research shows that SVMs have outperformed a number of su- pervised machine learning algorithms for deception detection in text by exploiting content-based features (e.g. linguistic and visual features) [7] [8]. Although a lot of work has been done to identify the fake news, it is still being done in the Bangla Language to a limited extent [9] [10]. With evolving digital technology, the native Bangla speakers are now producing a large amount of Bangla text online. As a result, it will be easier to create a dataset by scraping the text from online sources and categorize them according to polarity of the article by analyzing the text pattern. In this work, detection of fake news is proposed using two different modalities available in an efﬁcient manner using Support Vector Machine (SVM) and Multinomial Naive Bayes (MNB). Content-based features have been extracted from the open source data to classify the fake news. Rest of the paper is organized as follows. In section II we provide an overview of the related works. Section III discusses the methodology i.e. how the data are collected and pre-processed, feature extrac- tion method and detection techniques. Section IV demonstrates arXiv:2005.14627v1 [cs.CL] 29 May 2020 and discusses the result achieved through research and ﬁnally Section V draws the conclusion with possible future research directions. II. RELATED WORKS The rising prevalence and vast amount of fake news draws researchers to analyze problems that internet users suffer from. A. S. Sharma et al. proposed a hybrid extraction technique from text documents, combining Word2Vec and TF-IDF [9] which may detect with standard CNN architecture whether a text document in Bangla is satire or not with a precision of more than 96%. T. Islam et al [10] retrieves comments as data from different social media using comment extractors and removes punctuation marks, numerical meaning, emoti- cons which create a", "meta": {"paper_id": "2005.14627v1", "chunk_index": 1, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_2", "text": "may detect with standard CNN architecture whether a text document in Bangla is satire or not with a precision of more than 96%. T. Islam et al [10] retrieves comments as data from different social media using comment extractors and removes punctuation marks, numerical meaning, emoti- cons which create a perfect text corpus. Using Naive Bayes Classiﬁer, which is commonly used for spam identiﬁcation, they used TF-IDF vectorizer to extract the features from the text corpus and train the processing results. Some researchers have been utilizing graph analysis to better identify origins of fake news. Shu et al. [11] have shown that models of network diffusion can be used to map the provenance nodes and the provenance origins of fake news. Tarek Hamdi et al. [12] suggested an method that incorporates node embedding and user-based functionality to improve Twitter’s analysis of fake news. Using node2vec they retrieved information from twitter followers/followees graph which provides a simpliﬁed way to help identify SOFNs. The ﬁnding that characteristics produced by graph embedding are efﬁcient but node embedding features are powerful SOFN predictors that convey valuable information about a user’s reputation in the social network. Kai Shu et al. [13] proposed a model named Social Article Fusion (SAF) which combines the linguistic features of news content with social context features to identify fake news. They use commonly used autoencoders to identify the news content to represent text content in lower dimensional space. To capture the users temporal interactions with the fake news they used Recurrent Neural Networks (RNN), which efﬁciently performs to capture the temporal relationship. Then they extract useful features after classifying the data, and build various machine learning models to identify fake news. Mohamed Torky et al. [14] suggested a novel blockchain- based algorithm named Proof of Credibility (PoC) to identify and prevent fake news and deceptive social media content. The experimental results gave around 89% accuracy. Eu- genio Tacchini et al. has shown that Facebook posts can be categorized as hoaxes or non-hoaxes with high accuracy depending on the users who ”like” them. Use user IDs as features for classifying posts, they implemented two ML algorithms (logistic regression and harmonic boolean label crowdsourcing) obtained accuracies exceeding 99% even with very limited training sets [15]. But, the method offers difﬁcult- to-beat efﬁciency, its implementation is necessarily limited to situations, because the method uses social interactions (i.e. ”likes”) as signals to help distinguish Facebook posts, it can not be used when a post has no likes, and it is likely to perform worse when a post only receives few social interactions. To detect fake news, Kai Shu et al. [16] introduced a novel system called TriFN (which can isolate useful functionality independently from news provider and user obligations, as well as catch interrelationships simultaneously) in his another research work. Marco L. Della Vedova et al. [17] suggested a novel technique of machine learning that incorporates news content and social signals and implemented their plan inside a Facebook Messenger chatbot and accepted it with a legitimate application [15]. For contrast, they achieve higher accuracies than [15]", "meta": {"paper_id": "2005.14627v1", "chunk_index": 2, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_3", "text": "his another research work. Marco L. Della Vedova et al. [17] suggested a novel technique of machine learning that incorporates news content and social signals and implemented their plan inside a Facebook Messenger chatbot and accepted it with a legitimate application [15]. For contrast, they achieve higher accuracies than [15] and [16] using their respective Facebook and Twitter data sets. III. PROPOSED METHODOLOGY In this work, two classiﬁers SVM and MNB have been used to classify fake news from Bangla news articles. The dataset used in the proposed classiﬁcation system has been built by scraping various Bangla newspapers as it is hard to ﬁnd resources in Bangla language. System ﬂow diagram of the classiﬁcation model is illustrated in Figure 1 and others steps are described below. A. Data Collection For the research purpose, we scraped various news articles from ProthomAlo1, ProthomAlu2, BalerKontho3, Motikontho4 as no previous datasets were found in Bangla language as well as it was hard to gather fake Bangla news. Around 2500 articles were collected for our dataset as all these are public articles. We decided that articles from very popular portals are real news and fake news articles are collected from various sites which are known as portals for satire news such as Motikontho, BalerKnotho, ProthomAlu, etc. Table I provides the percentage of real and fake news in our dataset. TABLE I DATA SET DETAILS INFORMATION Type of Article Total Count Percentage to Total Real 1548 60.92 Fake 993 39.08 B. Data Preprocessing It is very important to apply some preprocessing on the raw text data before feeding them to the classiﬁer. A raw text might contain unnecessary symbols or other things that are not important for our classiﬁcation. Various emoticons like :D, ;) might be helpful for sentiment analysis but not in our case. We also removed various special characters e.g. @, #, ! etc from our text. These elements can reduce or diminish the performance of the classiﬁer. After removing the numerical values, punctuation marks, special symbols, our dataset is 1https://www.prothomalo.com/ 2https://prothom1alu.blogspot.com/ 3http://www.balerkontho.net/ 4https://motikontho.wordpress.com/ Data Cleaning & Preprocessing Feature Extraction Classifier Training Data Test Data Performance Evaluation Count Vectorizer Tf-idf Vectorizer Fig. 1. System Flow Diagram of the Proposed System prepared for the classiﬁer algorithms. Table II shows details characters which were considered removing in preprocessing. TABLE II CHARACTERS CONSIDERED REMOVING IN PREPROCESSING Category Characters Special Charac- ters @, #, $, %, —, ,, ...... Bangla & En- glish Digits 1, 2, 3, 4, .... 0 English Alphabets A, B, C, ...... Z; a, b, c,......z Emoticons :), :D, :(, :o, ..... C. Feature Extraction Extracting the proper feature have a perfect impact on the performance of the machine learning classiﬁer algorithms. Count Vectorizer and TF-IDF Vectorizer (term frequencyin- verse document frequency) are used to extract features from our text before feeding it into the classiﬁcation algorithms. Count Vectorizer generates a vector which has as several dimensions as the speciﬁc word of corpora. Every single word has a particular dimension and contain 1 in that speciﬁc dimension with 0 in others which simply keeps the frequency", "meta": {"paper_id": "2005.14627v1", "chunk_index": 3, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_4", "text": "features from our text before feeding it into the classiﬁcation algorithms. Count Vectorizer generates a vector which has as several dimensions as the speciﬁc word of corpora. Every single word has a particular dimension and contain 1 in that speciﬁc dimension with 0 in others which simply keeps the frequency of every words. TF-IDF vectorizer features numerical repre- sentations of the words whether they are there or not, rather than just featuring a count. Words are measured by frequency, multiplied by the inverse document frequency of them. In simple terms, words that appear a good amount but everywhere should be provided hardly any signiﬁcance or weighting. In Bangla Language words like following, don’t offer a huge deal of interest. If a word occurs quite low, or sometimes occurs, then such words are actually more relevant and should be carefully weighed as such. This would lead to improved performance on classiﬁcation. It is a method intended to describe the signiﬁcance of a keyword inside a text. If the term denoted as ’t’, a particular document as ’d’ and the whole document as ’D’, then the formula [18] is, tf −idf(t, d, D) = tf(t, d) ∗idf(t, D) (1) Here, tf(t, d) is the frequency of ’t’ in ’d’ idf(t, D) is how ’t’ is common or rare across ’D’ D. Classiﬁer The dataset was split into two sets, training-set and test- set to feed into classiﬁer algorithms. The training set con- tains 70% of the dataset and the test set contain 30% of the dataset. Among various classiﬁer, we used two widely used Multinomial Nave Bayes Classiﬁer and Support Vector Machine Classiﬁer with a linear kernel in this research. The Naive Bayes classiﬁer is based on Bayes theorem, a simple probabilistic classiﬁer which is fast and easy to implement. It is a tough task concerning which Nave Bayes version will be implemented. In the context of text classiﬁcation Multinomial Nave Bayes gets better results than the Bernoulli Nave Bayes or Gaussian nave Bayes [19]. Multinomial Nave Bayes is often used in a classiﬁcation problem where the numerous occurrences of the word mean a lot. On the other hand, the support vector machine would be used for regression or classiﬁcation problems. It utilizes the kernel technique to process the data, and determines an appropriate boundary between the potential outputs depending on these transfor- mations. It is robust against over-ﬁtting problems related to high-dimensional space particularly for text datasets [19]. The linear kernel preferred for text classiﬁcation as most of them are linearly separable. IV. EXPERIMENTAL RESULT In this section, performance analysis of multinomial naive bayes and support vector machine classiﬁers on our dataset has been described. The dataset was split into two parts randomly. One part contains 70% of the data which is used to train the classiﬁers and the remaining 30% is to test the performance of the classiﬁers. Multinomial Naive Bayes gave us 93.32% accuracy for our dataset where Support Vector Classiﬁer with linear kernel gave 96.64% accuracy. We observed that SVM performs better than MNB. Table III and IV shows the", "meta": {"paper_id": "2005.14627v1", "chunk_index": 4, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_5", "text": "to train the classiﬁers and the remaining 30% is to test the performance of the classiﬁers. Multinomial Naive Bayes gave us 93.32% accuracy for our dataset where Support Vector Classiﬁer with linear kernel gave 96.64% accuracy. We observed that SVM performs better than MNB. Table III and IV shows the precision, recall and F1- score for MNB and SVM classiﬁer models consecutively. TABLE III RESULTS FOR MNB precision recall f1-score Real 0.90 1.00 0.95 Fake 1.00 0.83 0.90 avg 0.95 0.91 0.93 TABLE IV RESULTS FOR SVM precision recall f1-score Real 0.97 0.99 0.98 Fake 0.99 0.95 0.97 avg 0.98 0.97 0.97 Figure 2 shows the confusion matrix for Multinomial Nave Bayes which shows that 472 news was indicated as real and was actually real news. On the other hand 50 news was predicted as fake which was actually real, that means false negative. Similarly, 240 fake news were predicted as fake and was actually fake but only 1 was predicted as fake and was actually real which new was falsely indicated as real. Figure 3 shows the confusion matrix for Support Vector Classiﬁer which shows that 469 news was indicated as real and was actually real news. On the other hand only 14 news was predicted as fake which was actually real, that means false negative. So we can see SVC gives less false negative. Similarly, 276 fake news were predicted as fake and was actually fake but only 4 was predicted as fake and was actually real which new was falsely indicated as real. V. CONCLUSION This research work concludes that Support Vector Machine with linear kernel is doing marginally better than Multinomial Nave Bayes on our dataset. Since the research with the identiﬁcation of Bangla fake News is new [9] [10], this work will be helpful for any future works on such topic. Very recently a research work has been accepted in a conference Fig. 2. Confusion Matrix for Multinomial Nave Bayes. Fig. 3. Confusion Matrix for Support Vector Machine with Linear Kernel. consisting a dataset around 49 thousand news articles [20] and it is our plan for the future to work with this dataset to expand the number of features and sufﬁcient lexicons. Besides, a stemmer can be applied to minimize corpus size and enhance model efﬁciency. In the future, the rate of success can be improved by doing more research utilizing hybrid-classiﬁers. REFERENCES [1] R. K. Nielsen, R. Fletcher, N. Newman, J. S. Brennen, and P. N. Howard, “Navigating the ’infodemic’: How people in six countries access and rate news and information about coronavirus,” Apr 2020. [2] U. News. During this coronavirus pandemic, fake news is putting lives at risk: Unesco. [Online]. Available: https://news.un.org/en/story/2020/ 04/1061592 [3] J. De keersmaecker and A. Roets, “Fake news: Incorrect, but hard to correct. the role of cognitive ability on the impact of false information on social impressions,” in Intelligence, vol. 65, 2017, pp. 107–110. [4] N. Grinberg, K. Joseph, L. Friedland, B. Swire-Thompson, and D. Lazer, “Fake news on twitter during the 2016 u.s. presidential election,” Science, vol. 363, no. 6425,", "meta": {"paper_id": "2005.14627v1", "chunk_index": 5, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_6", "text": "to correct. the role of cognitive ability on the impact of false information on social impressions,” in Intelligence, vol. 65, 2017, pp. 107–110. [4] N. Grinberg, K. Joseph, L. Friedland, B. Swire-Thompson, and D. Lazer, “Fake news on twitter during the 2016 u.s. presidential election,” Science, vol. 363, no. 6425, pp. 374–378, 2019. [Online]. Available: https://science.sciencemag.org/content/363/6425/374 [5] (2019, July) Mobs beat ﬁve dead for kidnapping, daily star. [Online]. Available: https://www.thedailystar.net/frontpage/news/ mobs-beat-2-dead-kidnapping-1774471 [6] A. Bondielli and F. Marcelloni, “A survey on fake news and rumour detection techniques,” Information Sciences, vol. 497, p. 3855, 2019. [7] V. Rubin, N. Conroy, Y. Chen, and S. Cornwell, “Fake news or truth? using satirical cues to detect potentially misleading news,” in Proceedings of the Second Workshop on Computational Approaches to Deception Detection. San Diego, California: Association for Computational Linguistics, Jun. 2016, pp. 7–17. [Online]. Available: https://www.aclweb.org/anthology/W16-0802 [8] H. Zhang, Z. Fan, J. Zheng, and Q. Liu, “An improving deception detection method in computer-mediated communication,” Journal of Networks, vol. 7, no. 11, Jan 2012. [9] A. S. Sharma, M. A. Mridul, and M. S. Islam, “Automatic detection of satire in bangla documents: A cnn approach based on hybrid feature extraction model,” arXiv preprint arXiv:1911.11062, 2019. [10] T. Islam, S. Latif, and N. Ahmed, “Using social networks to detect malicious bangla text content,” in 2019 1st International Conference on Advances in Science, Engineering and Robotics Technology (ICASERT), 2019, pp. 1–4. [11] K. Shu, H. R. Bernard, and H. Liu, Studying Fake News via Network Analysis: Detection and Mitigation. Cham: Springer International Publishing, 2019, pp. 43–65. [Online]. Available: https://doi.org/10.1007/978-3-319-94105-9 3 [12] T. Hamdi, H. Slimi, I. Bounhas, and Y. Slimani, “A hybrid approach for fake news detection in twitter based on user features and graph embedding,” in Distributed Computing and Internet Technology, D. V. Hung and M. D´Souza, Eds. Cham: Springer International Publishing, 2020, pp. 266–280. [13] K. Shu, D. Mahudeswaran, and H. Liu, “Fakenewstracker: a tool for fake news collection, detection, and visualization,” Computational and Mathematical Organization Theory, vol. 25, no. 1, p. 6071, 2018. [14] M. Torky, E. Nabil, and W. Said, “Proof of credibility: A blockchain approach for detecting and blocking fake news in social networks,” International Journal of Advanced Computer Science and Applications, vol. 10, no. 12, 2019. [Online]. Available: http://dx.doi.org/10.14569/IJACSA.2019.0101243 [15] E. Tacchini, G. Ballarin, M. L. D. Vedova, S. Moret, and L. de Alfaro, “Some like it hoax: Automated fake news detection in social networks,” 2017. [16] K. Shu, S. Wang, and H. Liu, “Exploiting tri-relationship for fake news detection,” ArXiv, vol. abs/1712.07709, 2017. [17] M. L. D. Vedova, E. Tacchini, S. Moret, G. Ballarin, M. D. Pierro, and L. de Alfaro, “Automatic online fake news detection combining content and social signals,” 2018 22nd Conference of Open Innovations Association (FRUCT), pp. 272–279, 2018. [18] Wikipedia. Tfidf - wikipedia. [Online]. Available: https://en.wikipedia. org/wiki/Tf%E2%80%93idf [19] K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and D. Brown, “Text classiﬁcation algorithms: A survey,” Information, vol. 10, no. 4, p. 150, 2019. [20] M. Z. Hossain, M. A. Rahman, M. S. Islam, and S. Kar, “Banfakenews:", "meta": {"paper_id": "2005.14627v1", "chunk_index": 6, "filename": "2005.14627v1.pdf", "chunk_length": 512}}
{"id": "2005.14627v1_chunk_7", "text": "Wikipedia. Tfidf - wikipedia. [Online]. Available: https://en.wikipedia. org/wiki/Tf%E2%80%93idf [19] K. Kowsari, K. Jafari Meimandi, M. Heidarysafa, S. Mendu, L. Barnes, and D. Brown, “Text classiﬁcation algorithms: A survey,” Information, vol. 10, no. 4, p. 150, 2019. [20] M. Z. Hossain, M. A. Rahman, M. S. Islam, and S. Kar, “Banfakenews: A dataset for detecting fake news in bangla,” 2020.", "meta": {"paper_id": "2005.14627v1", "chunk_index": 7, "filename": "2005.14627v1.pdf", "chunk_length": 59}}
{"id": "2006.00459v1_chunk_0", "text": "SANA: Sentiment analysis on newspapers comments in Algeria Hichem Rahab a,b,⇑, Abdelhaﬁd Zitouni b, Mahieddine Djoudi c a ICOSI Laboratory, University of Khenchela, Algeria b LIRE Laboratory, University of Constantine 2, Algeria c TechNE Laboratory, University of Poitiers, France a r t i c l e i n f o Article history: Received 2 February 2019 Revised 27 March 2019 Accepted 24 April 2019 Available online xxxx Keywords: Opinion mining Sentiment analysis Machine learning K-nearest neighbors Naïve Bayes Support vector machines Arabic Comment a b s t r a c t It is very current in today life to seek for tracking the people opinion from their interaction with occurring events. A very common way to do that is comments in articles published in newspapers web sites dealing with contemporary events. Sentiment analysis or opinion mining is an emergent ﬁeld who’s the purpose is ﬁnding the behind phenomenon masked in opinionated texts. We are interested in our work by com- ments in Algerian newspaper websites. For this end, two corpora were used; SANA and OCA. SANA corpus is created by collection of comments from three Algerian newspapers, and annotated by two Algerian Arabic native speakers, while OCA is a freely available corpus for sentiment analysis. For the classiﬁcation we adopt Supports vector machines, naïve Bayes and k-nearest neighbors. Obtained results are very promising and show the different effects of stemming in such domain, also k-nearest neighbors gives important improvement comparing to other classiﬁers unlike similar works where SVM is the most dom- inant. From this study we observe the importance of dedicated resources and methods the newspaper comments sentiment analysis which we look forward in future works. \u0001 2019 Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). Contents 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 2. Background. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "meta": {"paper_id": "2006.00459v1", "chunk_index": 0, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_1", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 2.1. Matter approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 2.2. Validation method. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 2.3. Classifiers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 2.4. Evaluation measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 3. Related works. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "meta": {"paper_id": "2006.00459v1", "chunk_index": 1, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_2", "text": "00 3. Related works. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4. Proposed approach. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.1. Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.2. Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.3. Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "meta": {"paper_id": "2006.00459v1", "chunk_index": 2, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_3", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.4. Train and test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.5. Evaluate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 4.6. Revise. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 5. Experimental study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 https://doi.org/10.1016/j.jksuci.2019.04.012 1319-1578/\u0001 2019 Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an open", "meta": {"paper_id": "2006.00459v1", "chunk_index": 3, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_4", "text": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 https://doi.org/10.1016/j.jksuci.2019.04.012 1319-1578/\u0001 2019 Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/). ⇑Corresponding author at: Laboratoire ICOSI, Faculté des Sciences et de la Technologie, Bloc D, Campus Route Oum El Bouaghi, Université de Khenchela, Khenchela 40000, Algérie. E-mail addresses: rahab.hichem@univ-khenchela.dz (H. Rahab), Abdelhaﬁd.zitouni@univ-constantine2.dz (A. Zitouni), mahieddine.djoudi@univ-poitiers.fr (M. Djoudi). Peer review under responsibility of King Saud University. Production and hosting by Elsevier Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Contents lists available at ScienceDirect Journal of King Saud University – Computer and Information Sciences journal homepage: www.sciencedirect.com Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 5.1. First round . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 5.2. Second round. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 5.3. OCA corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 6. Results discussion . . . . . . . . . . . . . . . . . . . . . . . . . .", "meta": {"paper_id": "2006.00459v1", "chunk_index": 4, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_5", "text": ". . . . . . . . . . . . . . . . . . . . 00 6. Results discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 7. Conclusion and perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 Conflict of interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 00 1. Introduction With the development of the web and its offered services, a huge amount of data is generated (Liu, 2012) and additional needs emerge to take beneﬁt from this information thesaurus. Opinion mining from Political, economic and social data, is a new need to make the huge amount of available information in an easily under- stood form to decision makers in dedicated centers. Sentiment analysis vocation is to classify people opinions into speciﬁc", "meta": {"paper_id": "2006.00459v1", "chunk_index": 5, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_6", "text": "emerge to take beneﬁt from this information thesaurus. Opinion mining from Political, economic and social data, is a new need to make the huge amount of available information in an easily under- stood form to decision makers in dedicated centers. Sentiment analysis vocation is to classify people opinions into speciﬁc cate- gories to facilitate understanding the behind phenomenon. A variety of classiﬁcation approaches are available, some works deal only with positive vs. negatives classes (Rushdi-Saleh et al., 2011; Atia and Shaalan, 2015; Rahab et al., 2018), others deal with more important number of classes (Cherif et al., 2015; Ziani et al., 2013). A very important amount of useful information is available in the comments of newspapers websites visitors around the world and in different languages. A lot of works in this era deal with Eng- lish, and other European languages, but works treating Arabic lan- guage still in their beginning (Alotaibi and Anderson, 2016). Arabic is a Semitic language spoken by about 300 million of people in 22 Arab countries. And the importance of Arabic is also that it is the language of the holy Quran (Cherif et al., 2015) the book of 1.5 billion Muslim in the world. We can ﬁnd three forms of Arabic language, Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. Classical Arabic is the original form of the lan- guage preserved from centuries by the Islamic literature and espe- cially the holy Quran. For Modern Standard Arabic, it takes the role of the ofﬁcial language in almost all Arabic administrations. The effective spoken languages in daily conversations are Arabic dia- lects, which are spoken languages without a standardized writing form. They can be classiﬁed into: Levantine (spoken in Palestine, Jordan, Syrian and Lebanon) Egyptian (in Egypt and Sudan), Magh- rebi (spoken in the Arab Maghreb) and Iraqi (Jarrar et al., 2017), this later one may be also divided into Iraqi versus Gulf classes (Zaidan and Callison-burch, 2011). In these Dialect families, we will ﬁnd also sub-families. In the case of the Algerian dialect, the work of (Harrat et al., 2016) classify Algerian dialects in 4 groups: 1) the dialect of Algiers and its out- skirts, 2) the dialect of the east in Annaba and its outskirts, 3) the dialect Oran and the west of Algeria, and 4) the dialect of the Algerian Sahara. Even the newspaper content is written in MSA and comments follow generally this style, we ﬁnd some visitors that use Algerian Dialects words in their comments. For example the Arabic sentenceﺃﺷﻴﺎﺀﻛﻬﺬﻩﺗﺤﺪﺙﻓﻘﻂﻓﻲﺍﻟﺪﻭﻝﺍﻟﻤﺘﺨﻠﻔﺔȂaš.yAȂkahaðihi taH.duHu faqat ﬁAlduwal almutaxalifa1 (things like this occur only in retarded countries) is written in a commentﺍﺷﻴﺎﺀﻛﻴﻤﺎﻫﺎﺫﻱﺗﺼﺮﻯﻏﻴﺮﻓﻲﺍﻟﺪﻭﻝﺍﻟﻤﺘﺨﻠﻔﺔ Aaš.yA kima hAðy tas.ra cir fy Ald \u0001 uwal almutaxalifa. Also, we found in several cases the use ofﺩd, instead ofﺫð, which is a characteristic of the Dialect of Algiers the capital of Alge- ria (Harrat et al., 2016), as the case in the commentﺷﻜﺮﺍﻳﺎﺣﻔﻴﻆﻫﺪﺍﻫﻮ ﺣﺎﻝﺍﻟﻤﺴﺆﻝﺍﻟﺪﻯﺍﺳﻨﺪﺓﻟﻪﺍﻟﻤﻬﻤﺔﻭﻓﺸﻞﺍﺻﺒﺢﻳﻨﺘﻘﺪﻣﻦﺍﺟﻞﺍﻥﻳﻨﺘﻘﺪﻭﻳﻄﺒﻞﻭﻳﺪﺍﻓﻊﻋﻦﺍﻟﺰﻣﻦﺍﻟﺪﻯ ﻣﺮﻭﻟﻜﻦﻫﻨﺎﻙﺭﺟﺎﻝﻳﺼﻨﻌﻮﻥﺍﻟﻤﺠﺪﺑﺘﺤﺪﻳﻬﻢﺍﻟﻮﻗﺎﺋﻊšukrã ya HaﬁyDˇ hada huwa HaAl Almaswˆ ul Al \u0001 di Ȃus.nidat. lahu Almuhima⁄ wa fašil. Asbaha yan- taqid min Ȃjl Ȃn yantaqid wa yuTab \u0001 il wa yudaﬁ1 1an", "meta": {"paper_id": "2006.00459v1", "chunk_index": 6, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_7", "text": "the Dialect of Algiers the capital of Alge- ria (Harrat et al., 2016), as the case in the commentﺷﻜﺮﺍﻳﺎﺣﻔﻴﻆﻫﺪﺍﻫﻮ ﺣﺎﻝﺍﻟﻤﺴﺆﻝﺍﻟﺪﻯﺍﺳﻨﺪﺓﻟﻪﺍﻟﻤﻬﻤﺔﻭﻓﺸﻞﺍﺻﺒﺢﻳﻨﺘﻘﺪﻣﻦﺍﺟﻞﺍﻥﻳﻨﺘﻘﺪﻭﻳﻄﺒﻞﻭﻳﺪﺍﻓﻊﻋﻦﺍﻟﺰﻣﻦﺍﻟﺪﻯ ﻣﺮﻭﻟﻜﻦﻫﻨﺎﻙﺭﺟﺎﻝﻳﺼﻨﻌﻮﻥﺍﻟﻤﺠﺪﺑﺘﺤﺪﻳﻬﻢﺍﻟﻮﻗﺎﺋﻊšukrã ya HaﬁyDˇ hada huwa HaAl Almaswˆ ul Al \u0001 di Ȃus.nidat. lahu Almuhima⁄ wa fašil. Asbaha yan- taqid min Ȃjl Ȃn yantaqid wa yuTab \u0001 il wa yudaﬁ1 1an Alz \u0001 aman Al \u0001 adi mar wa lakin hunaka rijAl yaSna1un Almajd bitaHad \u0001 iyhim AlwaqaAA˘1 (Thank you haﬁd this is the state of the responsible to whom is affected a mission, and he fails, so he become critic for critic, and he defends the earlier time but there are men making the glory by confronting the realities). We are interested by comments in the Arabic Algerian online press, in the goal of developing an approach to classify these com- ments into positive and negative classes. The paper is organized as follows. In the Section 2 a background of adopted methodology and used parameters are given. In the Sec- tion 3, a literature review is presented. Section 4 is dedicated to the proposed approach. An experimental study is explained and obtained results are in the Section 5. In Section 6 the achieved results are discussed. We ﬁnish by conclusion and perspectives to future works. 2. Background 2.1. Matter approach MATTER is a cyclic approach for natural language texts annota- tion, the approach is based on several iterations to achieve the annotation process (Pustejovsky and Stubbs, 2012). The MATTER approach consists on a cycle of six steps. The model of the phe- nomenon may be revised for further train and test steps (Ide and Pustejovsky, 2017). Model: in the ﬁrst step the studied phenomenon will be modeled. Annotate: an annotation can be seen as a metadata (Matthew and Jessica, 2010). This metadata will be added to our corpus for data classiﬁcation into predeﬁned classes like positive, neg- ative, neutral, etc. The annotation may be integrated in the doc- ument to annotate, in a manner, that when the document is moved, the metadata still integrated, for example the addition of a distinction word in the ﬁle name. It can also take the form of a folder in which the data ﬁles are grouped, in this case a ﬁle extracted out of this folder will lose this metadata (Matthew and Jessica, 2010). The annotation can be done at several levels. o Document level: the whole document take the same label, such as: positive/negative (Rushdi-Saleh et al., 2011) or subjective/ objective, . . .etc. o Sentence level: in this level each sentence in the document may have an independent tag, an example of this level is the tweet’s classiﬁcation (Brahimi et al., 2016) that the tweet cannot exceed 140 words. o Word level: Also known as Part Of speech tagging POS (Tunga, 2010), where each word is tagged according to its position in the text (e.g. noun, verb, and pronoun) (Jarrar et al., 2017). 1 For transliteration we follow in this work the scheme developed by Habash et al. (2007). 2 H. Rahab et al. / Journal of King Saud University – Computer and Information", "meta": {"paper_id": "2006.00459v1", "chunk_index": 7, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_8", "text": "word is tagged according to its position in the text (e.g. noun, verb, and pronoun) (Jarrar et al., 2017). 1 For transliteration we follow in this work the scheme developed by Habash et al. (2007). 2 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 We can ﬁnd several ways to achieve annotation with. Annota- tion by 2–5 persons having some speciﬁed skills (Alotaibi and Anderson, 2016) (Pustejovsky and Stubbs, 2012), Crowdsourcing where the annotation is done by an important number of annota- tors without speciﬁc skills (Bougrine et al., 2017), or Annotation based on rating systems offered by opinion sites (Rushdi-Saleh et al., 2011). The ﬁnal version of the annotated data called the gold standard is the corpus to be used in the classiﬁcation step (Pustejovsky and Stubbs, 2012). Train: a part of the data with their true classes is used to train the classiﬁer. Test: the rest of data (which is not used for training) is submit- ted to classiﬁer for test. Evaluate: evaluation metrics are calculated, to measure the annotation and classiﬁcation performances. Revise: based on evaluation metrics the model may be revised, and additional iteration is to do if needed. 2.2. Validation method In the scope of this work the 10-fold Cross-validation method is used. Cross-Validation is, in machine learning, a method whose objective is to evaluate and compare learning algorithms. It con- sists of dividing the data in two segments: The ﬁrst segment is used to learn or train a model and the second one is used to vali- date the model. In the 10-fold cross validation the corpus is divided into 10 segments of the same size, so in each iteration, 9 segments are used to train the model while the 10th is reported to the test step, this operation will be repeated in a manner that each segment is used both in the train and in the test of the model (Refaeilzadeh et al., 2009). The performance values are taken as a combination of the k performance values (as an average or another combination) to have a single estimation (Mountassir et al., 2013). The authors in (Kohavi, 1995) and (Steven and G, 1997) conclude that 10-fold cross validation is the best alternative to follow in classiﬁcation process, even if computation power allows more folds. 2.3. Classiﬁers Three well-known classiﬁers are used: Support-vector machines: support-vector machines SVM is a rel- atively new machine learning method for binary classiﬁcation problems (Cortes and Vapnik, 1995). To have the best results with SVM, the practitioner needs to well choice and ﬁxed cer- tain parameters: used kernel, gamma, and also well data col- lecting and pre-processing (Ben-Hur and Weston, 2010). Naive Bayes: the well-known Naïve Bayes classiﬁer is based on the ‘‘Bayes assumption” in which the document is assigned to the class in which it belongs with the highest probability", "meta": {"paper_id": "2006.00459v1", "chunk_index": 8, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_9", "text": "and ﬁxed cer- tain parameters: used kernel, gamma, and also well data col- lecting and pre-processing (Ben-Hur and Weston, 2010). Naive Bayes: the well-known Naïve Bayes classiﬁer is based on the ‘‘Bayes assumption” in which the document is assigned to the class in which it belongs with the highest probability (McCallum and Nigam, 1998). K-nearest neighbors: k-nearest neighbors KNN is a simple classi- ﬁer that use an historical values search to ﬁnd the future ones (Wang, 2015). 2.4. Evaluation measures 1. Inter Annotators Agreement: several metrics are used in litera- ture to evaluate the Inter Annotators Agreement (IAA). The kappa coefﬁcient (Jean, 1996a) is the most used in two annota- tors based works (Alotaibi and Anderson, 2016; Pustejovsky and Stubbs, 2012). The coefﬁcient is deﬁned as: k ¼ Pr a ð Þ \u0003 Pr e ð Þ 1 \u0003 Pr e ð Þ where, Pr (a) represent the proportion of the cases where both annotators agree, and Pr(e) is the proportion we search that the two annotators agree by chance (Jean, 1996b). Table 1 gives a proposed interpretation of k parameter (Pustejovsky and Stubbs, 2012). 2. Confusion matrix: confusion matrix or contingency table is a shown in Table 2, Where: o TP counts the correctly assigned comments to the positive category. o FP counts the incorrectly assigned comments to the positive category. o FN counts the incorrectly rejected comments from the posi- tive category. o TN counts the correctly rejected comments from the positive category. 3. Precision and Recall: three performance parameters were used, precision, recall, and accuracy. Precision ¼ TP TP þ FP Recall ¼ TP TP þ FN 4. Accuracy: precision and recall are both complementary one to the other; we combine the two using the Accuracy measure given as: Accuracy ¼ TP þ TN TP þ FP þ TN þ FN 3. Related works Sentiment Analysis is an emergent and challenging ﬁeld of Data Mining and Natural Language Processing (NLP); it is a research issue with the purpose of extract meaningful knowledge from user-generated content, for tracking the mood of people about events, products or topics (G and Chandrasekaran, 2012). It may be considered as a classiﬁcation problem, where the goal is to determine whether a written document, e.g. comments and reviews, express a positive or negative opinion about speciﬁc enti- ties (Korayem et al., 2016), (Alotaibi and Anderson, 2016). It con- sists generally of three main steps: pre-processing, feature selection and sentiment classiﬁcation (Assiri et al., 2015). Table 1 Interpretation of k parameter. K Agreement level < 0 Poor 0.01–0.20 Slight 0.21–0.40 Fair 0.41–0.60 Moderate 0.61–0.80 Substantial 0.81–1.00 Perfect Table 2 Confusion matrix. True class Predictive class Positive Negative Positive True positive (TP) False Positive (FP) Negative False Negative (FN) True Negative (TN) H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx 3 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 In (Rahab et al., 2017)", "meta": {"paper_id": "2006.00459v1", "chunk_index": 9, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_10", "text": "Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx 3 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 In (Rahab et al., 2017) the authors have created ARAACOM, ARAbic Algerian Corpus for Opinion Mining, 92 comments were collected from an Algerian Arabic newspaper website. Support vec- tor machines and Naïve Bayes classiﬁers were used. Both uni-gram and bi-gram word model were tested. The best results are obtained in term of precision and bi-gram model increase results in almost all cases. The authors of Curras (Jarrar et al., 2017) investigate in a corpus creation for Palestinian Arabic dialect. Two annotators are solicited to annotate morphologically Curras at the word level, and Inter Annotators Agreement is calculated using Kappa coefﬁcient. After annotation the two annotators work together to agree in the resul- tant gold standard. The best accuracy among the annotators achieves 98.8%. The work of (Abdul-Mageed and Diab, 2012) presents a multi genre corpus for Modern Standard Arabic, annotated at the sen- tence level. Several annotation methods were adopted, and kappa (k) parameter is used to measure inter annotators agreement (IAA). The authors conclude that a training of annotators is neces- sary to have a consistent annotation. A corpus dedicated to Arabic sentiment analysis is created from tweets in (Gamal et al., 2019), the tweets are annotated (labelled) manually. Five classiﬁcation algorithms are used, Support idge Regression (RR), Vector Machines (SVM), Naive Bayes (NB), Adap- tive Boosting (AdaBoost), and Maximum Entropy (ME), and the best accuracy is obtained when using RR. In (Rushdi-Saleh et al., 2011) the authors create OCA an opinion mining corpus for Arabic with 250 positive documents and 250 negative ones. The corpus is annotated at the document level by using web sites rating systems. Support vector machines and Naïve Bayes classiﬁers were used for evaluation. The corpus documents are mostly related to movie reviews. The OCA corpus is used in addition to an inhouse prepared cor- pus in (Duwairi and El-orfali, 2013) in their study of the prepro- cessing effects on sentiment analysis for arabic language. SVM, NB an KNN classiﬁers are used, and they prove the effect of prepro- cessing in improving classiﬁcation performance. In their work (Tripathy et al., 2017) the authors adopt sentiment analysis at the document level. To evolve their accuracy they used SVM for feature selection and another classiﬁcation method, Artiﬁ- cial neural network (ANN), for sentiment classiﬁcation at docu- ment level. The authors have used IMDb and polarity movie reviewer datasets, and 10 cross-validation method adopted for classiﬁcation. The obtained results are positively inﬂuenced by the number of hidden layers of ANN. In (Ziani et al., 2019) a combination of Support Vector Machines and Random Sub Space algorithms is compared with an hybrid approach where the Genetic Algorithms are adopted for feature selection. The used data set is 1000 reviews collected from two Algerian newspapers and manually annotated by an expert with- out detailing the annotation", "meta": {"paper_id": "2006.00459v1", "chunk_index": 10, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_11", "text": "2019) a combination of Support Vector Machines and Random Sub Space algorithms is compared with an hybrid approach where the Genetic Algorithms are adopted for feature selection. The used data set is 1000 reviews collected from two Algerian newspapers and manually annotated by an expert with- out detailing the annotation process. It is proved that the hybrid approach can improve classiﬁcation results. From this review of literature in opinion mining works and especially works dealing with Arabic language, see Table 3, we can conclude that an important part of work concern movie reviews. So conducting studies with other topics require develop- ing dedicated benchmarks that can be used to validate or revise existing results. Also, publicly available corpora are very sparse which make very necessary the development of dedicated resources to carry out studies is this language. 4. Proposed approach In our research we adopt supervised learning, or corpus based approach for opinion mining or sentiment analysis in Arabic reviews. In this work we have used SANA our proper corpus, in addition to a well known and publically available corpus OCA2 ded- icated for Arabic sentiment analysis. For SANA corpus creation we follow a web search in three Alge- rian Arabic newspaper web sites, in occurrence Echorouk3, Elkhabar4, and Ennahar5. We select articles covering several subjects (news, political, religion, sports, and society). The created corpus is available online6. In this work MATTER approach (Pustejovsky and Stubbs, 2012) for comments annotation is enhanced. We add a processing (PRO- CESS) step to have MApTTER approach. This allows us to give com- ments in the brute form to our annotators. So the processing step is included to the approach to: \u0004 The annotators deal with the original text. \u0004 The new examples can be added to any iteration. The following algorithm summarizes our proposed approach: Algorithm 1: Our proposed approach Algorithm: Enhanced ARAACOM (0) Begin (1) IAA = 0; (2) while (IAA <= 100% ) do (3) read (URL); (4) Page = load (URL); (5) while (there is comments in Page) do (6) Extract the following Comment (7) if (Comment in Data_base) then (8) Delete Comment; (9) Else (10) Add Comment to the Data_base; (11) end if (12) end while (13) MODEL (14) ANNOTATE (15) Calculate New_IAA //the New IAA (16) if New_IAA <= IAA then (17) go to MODEL (18) end if (19) PROCESS (20) TRAIN And TEST (21) EVALUATE (22) if (insufﬁcient results) (23) Break; (24) end if (25) REVISE (26) end while (27) End 4.1. Model The model is deﬁned as the triplet: M= {T,R,I} T = {Comment_classe, Positive, Negative, Neutral} R = {Comment_classe::= Positive|Negative| Neutral} I = {Positive: ‘‘Subjective with positive sentiment”, Negative: ‘‘Subjective with negative sentiment”, Neutral: ‘‘out of topic or without sentiment (objective)”} 2 http://sinai.ujaen.es/wiki/index.php/OCA_Corpus_(English_version). 3 www.echoroukonline.com/ara/. 4 www.elkhabar.com. 5 www.ennaharonline.com. 6 http://rahab.e-monsite.com/medias/ﬁles/corpus.rar. 4 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer", "meta": {"paper_id": "2006.00459v1", "chunk_index": 11, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_12", "text": "5 www.ennaharonline.com. 6 http://rahab.e-monsite.com/medias/ﬁles/corpus.rar. 4 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 In the following DTD the annotation tags and attributes were deﬁned, to have an XML format of comments and annotation: <!ELEMENT comment (#PCDATA)> <!ATTLIST comment Sentiment (Positive | Negative | Neutral) #IMPLIED > <!ELEMENT named_entity (#PCDATA)> <!ATTLIST named_entity role (political_personality | location | events | organization)> 4.2. Annotation Two Arabic native speakers are requested to annotate our cor- pus. In the beginning of each annotation round, a set of guidelines were given to annotators to have the best degree of contingency in obtained results. Annotation Guidelines: Guidelines are orientations we give to annotators to have homogeneous annotation results. In the guidelines, the project must be described with its methodology, outcomes and all information needed to achieve our goals (Ide and Pustejovsky, 2017). In each round of the MApTTER cycle, annotation guidelines will be reﬁned taking into account previ- ous results. Adjudication: In adjudication the annotation from different annotators are merged to have a single corpus called gold stan- dard (Ide and Pustejovsky, 2017). 4.3. Processing To have the best results in stemming and optimizing the word vector, a set of pre-processing steps are conducted: 1. Manual text pre-processing: We found a lot of spelling mistakes in collected comments, also some comments are written in lan- guages other than MSA, such us French and English. First, all comments are translated into Modern Standard Arabic (MSA); we give as samples the comments in Table 4. Second, Repeated letters such asﺍﻟﻴﻮﻣﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻤﻢAlyaw. mmmmmmmmmmmmmmmmmm (today with the last letter repeated) becomeﺍﻟﻴﻮﻡAlyaw.m (today). andﺑﻌﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﻴﺪﺍ ba1iiiiiiiiiiiiiiiiiiiiiiiiiiydã (far with a middle letter repeated) which becomeﺑﻌﻴﺪﺍba1iydã (far). Then Arabizi comments are transformed into their Arabic equivalent as shown in Table 5. Arabizi is an Arabic language used in SMS and tchat on the Internet, it differs from transliteration that there is no standard to adopt in this language. We ﬁnish by character encoding where all texts are resolved to UTF-8 encoding format. 2. Tokenization: In tokenization, words are separated by non- letters characters. 3. Stemming: light stemming is used in this step. Figs. 1 and 2 show light stemming and stemming of the same comment. We remark that the wordﺍﻟﺠﺰﺍﺋﺮAljazAyˆir(Algeria with the deﬁnite article) for example in light stemming is stemmed to ﺟﺰﺍﺋﺮjazAyˆir (Algeria without the deﬁnite article), when in Table 4 Sample of French comments manual pre-processing. Original French comment Translated comment in Arabic English meaning Merci Mr Haﬁd vous avez bien résumé qu est ce qui ce passe dans notre football ﺷﻜﺮﺍﺣﻔﻴﻆﻟﻘﺪﻟﺨﺼﺖ ﺟﻴﺪﺍﻣﺎﻳﺤﺪﺙﻓﻲﻛﺮﺓ ﺍﻟﻘﺪﻡﻋﻨﺪﻧﺎ Thank you haﬁd you resume well what happen in our football Grand MerciﺷﻜﺮﺍﺟﺰﻳﻼThank you very much Table 5 Sample of Arabizi comments. Arabizi comment Arabic equivalence English meaning YA3TIK SAHA KHOYA BARATLI KHATRI FI L3ADYAN VIVE L’ALGÉRIE 1.2.3 ﻳﻌﻄﻴﻚﺍﻟﺼﺤﺔﺃﺧﻲ ﺃﺛﻠﺠﺖﺧﺎﻃﺮﻱﻓﻲ ﺍﻷﻋﺪﺍﺀ، ﺗﺤﻴﺎﺍﻟﺠﺰﺍﺋﺮ Thank you brother you Warmed my heart in the", "meta": {"paper_id": "2006.00459v1", "chunk_index": 12, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_13", "text": "you haﬁd you resume well what happen in our football Grand MerciﺷﻜﺮﺍﺟﺰﻳﻼThank you very much Table 5 Sample of Arabizi comments. Arabizi comment Arabic equivalence English meaning YA3TIK SAHA KHOYA BARATLI KHATRI FI L3ADYAN VIVE L’ALGÉRIE 1.2.3 ﻳﻌﻄﻴﻚﺍﻟﺼﺤﺔﺃﺧﻲ ﺃﺛﻠﺠﺖﺧﺎﻃﺮﻱﻓﻲ ﺍﻷﻋﺪﺍﺀ، ﺗﺤﻴﺎﺍﻟﺠﺰﺍﺋﺮ Thank you brother you Warmed my heart in the enemies, life to Algeria kem anta kabir ya haﬁdhﻛﻢﺃﻧﺖﻛﺒﻴﺮﻳﺎﺣﻔﻴﻆYou are great haﬁd BARAKA ALALHO FIKﺑﺎﺭﻙﺍﻟﻠﻪﻓﻴﻚGod bless you Table 3 Comparison of Related works approaches. Work Classiﬁers/ methods Dataset Level / Classes Best Results Rahab et al. (2018) SVM NB SIAAC (32 positive reviews and 60 negative reviews). Document Pos vs. Neg 88.31% Of F_mesure Rushdi-Saleh et al. (2011) SVM NB OCA (250 positive reviews and 250 negative reviews). Document Pos vs. Neg 95.20% Of Recall Jarrar et al. (2017) IAA Curras Word 99.3% of accuracy Abdul-Mageed and Diab (2012) IAA AWATIF (Penn Arabic Treebank, Wikipedia Talk Pages and Web forums) Sentence Kappa = 0.820 Gamal et al. (2019) NB AdaBoost SVM RR ME 151.500 (75.744 positive and 75.744 negative) Document (tweet) Pos vs. Neg 99.90% of Accuracy Duwairi and El-orfali (2013) SVM NB KNN OCA politics dataset of 322 manually by collecting reviewers opinions from Aljazeera website Document 99.6% of Accuracy in OCA dataset 85.70% of Accuracy in politics dataset Ziani et al. (2019) RSS SVM GA 1000 reviews from Algerian press document 85.99% of Accuracy Tripathy et al. (2017) SVM IMDb (Internet movie database 12,500 positive and 12,500 negative reviews) polarity dataset (1000 positive and 1000 negative reviews) Word/ Document 96.40% of Accuracy H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx 5 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 stemming is stemmed asﺟﺰﺭjazar or jaz.r (carrot or ebb). And the wordﺍﻗﺘﺼﺎﺩﻳﺎA˘q.tiSAdiyã (economically) in light stemming is stemmed as it without changingﺍﻗﺘﺼﺎﺩﻳﺎA˘q.tiSAdiyã, when in stemming it is stemmed asﻗﺼﺪqaSada (intention or meaning). In the scope of this work we have used light stemming, because stemming generate the root of the word which gives a different meaning in several cases (Mountassir et al., 2013). 4. Stop words Removal: a list of stop words is offered by the used toolkit. 5. Word n-gram: Uni-gram, bi-gram and tri-gram word are generated. 6. Word vector: the four vectors are tested (Term frequency TF, Term occurrence TO, Term frequency Inverse document fre- quency TF-IDF and Binary term occurrence BTO). 4.4. Train and test In this step three classiﬁcation methods, Support vector machi- nes (SVM), Naïve Bayes (NB), and k-nearest neighbors (KNN) were used. And the 10-fold cross validation is adopted. For the SVM we use SVM linear, and for KNN we adopt the k = 9 parameter as sug- gested in Brahimi et al. (2016). 4.5. Evaluate For evaluation, Precision Recall and Accuracy are calculated for each classiﬁer and obtained resultants were compared and dis- cussed in each round of the MApTTER cycle. 4.6. Revise In revision step adopted approach is revised in", "meta": {"paper_id": "2006.00459v1", "chunk_index": 13, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_14", "text": "k = 9 parameter as sug- gested in Brahimi et al. (2016). 4.5. Evaluate For evaluation, Precision Recall and Accuracy are calculated for each classiﬁer and obtained resultants were compared and dis- cussed in each round of the MApTTER cycle. 4.6. Revise In revision step adopted approach is revised in the light of eval- uation metrics, and a decision to continue or stop the MApTTER cycle is to take in each step. 5. Experimental study 5.1. First round 1. Model: The above model is conserved as it. M={T,R,I} Fig. 1. Sample of word light stemming. Fig. 2. Sample of word Stemming. 6 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 T = {Comment_classe, Positive, Negative, Neutral} R = {Comment_classe::= Positive| Negative| Neutral} I = {Positive: ‘‘Subjective with positive sentiment”, Negative: ‘‘Subjective with negative sentiment”, Neutral: ‘‘out of topic or without sentiment (objective)”}. 2. Annotate: The Annotation guidelines given to annotators are, to annotate a comment as it presents a positive, negative or neu- tral sentiment regarding the article topic, so for each comment the annotators have the correspondent article, Table 6. o IAA Inter Annotators Agreement: k ¼ 0:6910 \u0003 0:3569 1 \u0003 0:3569 ¼ 0:3341 0:6431 So: k ¼ 0:5195 considered as moderate (see Table 1) o Adjudication: In adjudication step the two annotators are working together in the goal of obtaining a consensus in annotation and in cases when a consensus was not obtained the comment is considered as neutral. So we reach the fol- lowing gold standard (Table 7). We take in this work only the positive and negative classes, to have equilibrium we generate the corpus with the 45 positive com- ments and 45 of the negative ones. 3. Processing: The processing steps describing above are doing again, namely: text pre-processing, UTF-8 encoding, tokeniza- tion, stemming, stop words removal, n-gram word generation, and word vectors creation. 4. Train and Test: For these two steps cross validation method was adopted using the three classiﬁers, support vector machines, naïve Bayes and k-nearest neighbors. 5. Evaluate: To evaluate our model we calculate precision and recall of both negative and positive classes and accuracy of the classiﬁcation at whole. In Table 8 are presented evaluations of ﬁrst round classiﬁcation of SANA corpus, word weighting has different impacts according to classiﬁcation method, and whether the light stemming is used. The SVM method gives best performances using TO and BTO word weighting without stemming, while one using the light stemming, are TF and TF-IDF word weighting the best performing. In the case of NB classiﬁer we have best results using TO and TF-IDF whatever the light stemming is adopted or not. For the KNN the best results are obtained using TF and TF-IDF word weighting. Also, the use of light stemming increases the classiﬁcation per- formance in almost cases for the three classiﬁcation", "meta": {"paper_id": "2006.00459v1", "chunk_index": 14, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_15", "text": "NB classiﬁer we have best results using TO and TF-IDF whatever the light stemming is adopted or not. For the KNN the best results are obtained using TF and TF-IDF word weighting. Also, the use of light stemming increases the classiﬁcation per- formance in almost cases for the three classiﬁcation methods. 6. Revise: After evaluation of the IAA and classiﬁcation scores some suggestions are mentioned to take into account in the next round: o In the annotation guidelines the authors must take into account only the content of the comment no matter what is the article subject. So this recommendation may conduct to have more consensus between annotators, that is in the previous round, when taking into account the article topic, we observe that for the same idea (comment) one annotator can have a positive sentiment and the other have a neutral or even negative sentiment. 5.2. Second round Model: We conserve the above model without any changes. Annotate: In this round we give our annotators instruction to take into account only the content of the comment, as it is rec- ommended in the revision of the above step (see Table 9). o IAA Inter Annotators Agreement: k ¼ 0:7212 \u0003 0:3333 1 \u0003 0:333 ¼ 0:3882 0:667 k ¼ 0:5820 The Inter Annotator Agreement is improved with the new instruction, so it achieves k = 0.5820. o Adjudication: We generate our gold standard by adjudication of annotation step. And this is performed to obtain a single annotated corpus from the two available. We eliminate the neutral class and for equilibrium we take the 194 negative comments and only 194 from the positive comments (Table 10). Processing: The above described processing steps are conducted twice. Train and Test: We always use the three classiﬁers, SVM, NB and KNN. And the 10-fold cross-validation method. Evaluate: In the Table 11 we show the accuracy of the second round classiﬁcation of SANA corpus. As in the ﬁrst round we remark that word weighting has different impacts according to the using algorithm. Also, bi-gram and tri-gram models effect is very weak and give low differences in performance results. Revise: In this point we decide to stop this cycle and report the obtained results. 5.3. OCA corpus In the case of OCA corpus (Table 12) best performance results are obtained using word vector weighting TF and TF-IDF both in Table 6 Confusion matrix of the ﬁrst round. Annotator 02 Total Positive Negative Neutral Annotator 01 Positive 34 2 6 42 Negative 10 65 21 96 Neutral 10 6 24 40 Total 54 73 51 178 Table 7 First round Gold Standard. Positive Negative Neutral Total 45 88 45 178 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx 7 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 SVM and KNN classiﬁers. And the stemming increase accuracy in both methods. When for NB, the best results", "meta": {"paper_id": "2006.00459v1", "chunk_index": 15, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_16", "text": "7 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 SVM and KNN classiﬁers. And the stemming increase accuracy in both methods. When for NB, the best results are obtaining with TO and BTO word weighting. The stemming in the case of NB gives low perfor- mance in almost cases. 6. Results discussion The obtaining results differ from a corpus to another, and this is due to nature of the corpus. In the case of OCA corpus which is con- stituted from movie reviews, and written in well structured MSA. The best results for KNN and SVM are improved with TF and TF- IDF, that this two word weighting methods considers the word in the context of its document and the corpus at a whole. While in the case of NB classiﬁer, the best results are obtained in TO and BTO, that consider the weight of a term independently of the con- text, and NB as classiﬁer consider the independence between fea- tures given the context of the class. The light stemmer does not give an improvement in this case. The bi-gram and tri-gram mod- Table 8 First round accuracy of SANA corpus. Light Stem SVM NB KNN Unigram Bigram Tri-gram Unigram Bigram Tri-gram Unigram Bigram Tri-gram No TO 58.89 58.89 57.78 64.44 61.11 62.22 52.22 51.11 51.11 TF 55.56 54.44 54.44 63.33 58.89 60.00 57.78 56.67 56.67 TF-IDF 52.22 52.22 53.33 65.56 63.33 63.33 63.33 63.33 61.11 BTO 60.00 58.89 57.78 58.89 57.78 58.89 50.00 52.22 52.22 Yes TO 63.33 58.89 60.00 68.89 63.33 64.44 48.89 51.11 52.22 TF 70.00 65.56 66.67 66.67 63.33 63.33 70.00 68.89 68.89 TF-IDF 67.78 67.78 68.89 70.00 65.56 66.67 64.44 64.44 64.44 BTO 62.22 62.22 63.33 66.67 61.11 62.22 53.33 51.11 51.11 Table 9 Second round confusion matrix. Annotator 02 Total Positive Negative Neutral Annotator 1 Positive 161 08 11 180 Negative 34 94 50 178 Neutral 14 26 115 155 Total 209 128 176 513 Table 10 Second round Gold Standard. Positive Negative Neutral Total 236 194 83 513 Table 11 Second round accuracy of SANA corpus. Light Stem SVM NB KNN Unigram Bigram Tri-gram Unigram Bigram Tri-gram Unigram Bigram Tri-gram No TO 71.13 69.85 71.13 70.36 70.62 70.36 64.69 63.92 63.66 TF 70.62 70.88 70.62 70.36 71.13 71.13 61.60 61.86 62.11 TF-IDF 69.33 69.07 70.88 70.36 71.91 72.16 56.19 56.96 57.73 BTO 71.39 70.10 68.56 72.16 71.65 70.88 63.92 62.89 62.37 Yes TO 71.13 69.33 71.91 73.45 72.94 72.42 64.69 65.72 65.72 TF 70.88 72.16 71.39 73.97 75.00 74.74 65.72 67.78 67.53 TF-IDF 69.33 72.16 69.85 73.45 73.97 73.97 65.21 66.49 66.49 BTO 71.65 69.33 72.16 74.74 74.23 73.71 63.66 62.63 62.63 Table 12 Accuracy of OCA Corpus. Light Stem SVM NB KNN Unigram Bigram Tri-gram Unigram Bigram Tri-gram Unigram Bigram Tri-gram No TO 70.00 70.80 71.00 86.00 89,00 89,80 51.40 51.60 51.60 TF 77.00 80.80 80.00 85.20 87.80 88.00 88.00 88.00 88.00 TF-IDF 77.80 80.80 80.40 85.60 87.00 87.20 87.80", "meta": {"paper_id": "2006.00459v1", "chunk_index": 16, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_17", "text": "62.63 Table 12 Accuracy of OCA Corpus. Light Stem SVM NB KNN Unigram Bigram Tri-gram Unigram Bigram Tri-gram Unigram Bigram Tri-gram No TO 70.00 70.80 71.00 86.00 89,00 89,80 51.40 51.60 51.60 TF 77.00 80.80 80.00 85.20 87.80 88.00 88.00 88.00 88.00 TF-IDF 77.80 80.80 80.40 85.60 87.00 87.20 87.80 88.40 88.40 BTO 72.20 72.80 73.40 87.40 88.80 89.20 50.00 50.00 50.00 Yes TO 67.40 68.80 68.80 83.00 87.00 87.60 51.20 50.80 51.00 TF 79.40 82.00 82.80 81.20 86.20 86.80 79.80 83.40 84.80 TF-IDF 78.20 82.00 82.00 80.80 86.40 86.60 80.40 83.20 83.60 BTO 69.20 71.40 70.80 84.40 89.20 88.40 50.00 50.00 50.00 8 H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 els give a considerable amelioration; we report this improvement to result of negation and regular expression used in well written and homogenous documents. When with the SANA (especially second round) we observe that word weighting results are different from a classiﬁer to another. The light stemmer effects are better than OCA corpus and we sug- gest the light stemmer for mixture document (dialectal and MSA). Bi-gram and tri-gram improvement are very weak, so there are no commonly used expression in SANA corpus compared to OCA cor- pus due to the nature of each one. 7. Conclusion and perspectives Available corpora on the web for carried out Arabic sentiment analysis studies are very rare, and those available are generally related to movie and ﬁlm reviews due to the available comments in such websites. The case of newspaper comments is more deli- cate, that it is related to the country in which it is exists, so dealing with such comments must take into account, not only Arabic lan- guage but also related dialects and used languages in this country. In this work we proposed our enhanced approach for opinion min- ing in Algerian Arabic Newspapers comments. We use two corpora, the ﬁrst is SANA which is created in the scope of this work, and the second is the well known and available in the web the OCA corpus. For SANA annotation, MApTTER approach is used to annotate Algerian newspaper comments. MApTTER is based on an existing annotation approach MATTER. Three classiﬁers were used, SVM, NB and KNN. Obtained results are promising, but still to develop. And the most important conclu- sion is about light stemming that give different improvement depending on the nature of the corpus, and it is suggested when dealing with non homogenous and not well written documents. Also, the bigram and trigram models are not always a good repre- sentation regarding the obtained results, their effect depend on the formulation of corpus documents. We aim in the future to develop the approach by taking into account the parts in the comment that enclose the most semantic, which is the ﬁrst and the last parts. Conﬂict of interest None.", "meta": {"paper_id": "2006.00459v1", "chunk_index": 17, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_18", "text": "repre- sentation regarding the obtained results, their effect depend on the formulation of corpus documents. We aim in the future to develop the approach by taking into account the parts in the comment that enclose the most semantic, which is the ﬁrst and the last parts. Conﬂict of interest None. References Abdul-Mageed, M., Diab, M., 2012. AWATIF: a multi-genre corpus for modern standard arabic subjectivity and sentiment analysis. Lang. Resour. Eval. Conf., 3907–3914 Alotaibi, S.S., Anderson, C.W., 2016. Extending the knowledge of the Arabic sentiment classiﬁcation using a foreign external lexical source. Int. J. Nat. Lang. Comput. 5, 1–11. https://doi.org/10.5121/ijnlc.2016.5301. Assiri, A., Emam, A., Aldossari, H., 2015. Arabic sentiment analysis: a Survey. IJACSA). Int. J. Adv. Comput. Sci. Appl. 6, 75–85. https://doi.org/10.1007/978-3- 319-20367-6. Atia, S., Shaalan, K., 2015. Increasing the accuracy of opinion mining in Arabic. Arab. Comput. Ling. 106–113. https://doi.org/10.1109/ACLing. 2015.22. Ben-Hur, A., Weston, J., 2010. A user’s guide to support vector machines. Methods Mol. Biol. 609, 223–239. https://doi.org/10.1007/978-1-60327-241-4_13. Bougrine, S., Cherroun, H., Abdelali, A., 2017. Altruistic crowdsourcing for Arabic speech corpus annotation. ACLing, 133–144. https://doi.org/10.1016/j. procs.2017.10.102. Brahimi, B., Touahria, M., Tari, A., 2016. Data and text mining techniques for classifying Arabic tweet polarity. J. Digit. Inf. Manag. 14, 15–25. Cherif, W., Madani, A., Kissi, M., 2015. Towards an efﬁcient opinion measurement in Arabic comments. Procedia Comput. Sci 73, 122–129. https://doi.org/10.1016/j. procs.2015.12.057. Cortes, C., Vapnik, V., 1995. Support-vector networks. Mach. Learn. 20, 273–297. https://doi.org/10.1023/A:1022627411411. Duwairi, R., El-orfali, M., 2013. A study of the effects of preprocessing strategies on sentiment analysis for arabic text. J. Inf. Sci. 1. https://doi.org/10.1177/ 0165551510000000. Chandrasekaran, G.V.R., 2012. Sentiment analysis and opinion mining: a survey. Int. J. Adv. Res. Comput. Sci. Soft. Eng. 2, 282–292. Gamal, D., Alfonse, M., Salem, A.M., 2019. Twitter benchmark dataset for Arabic sentiment analysis. IJ Mod. Educ. Comput. Sci. 11, 33–38. https://doi.org/ 10.5815/ijmecs.2019.01.04. Habash, N., Soudi, A., Buckwalter, T., 2007. On Arabic transliteration. Arab. Comput. Morphol., 15–22 https://doi.org/10.1007/978-1-4020-6046-5_2. Harrat, S., Meftouh, K., Abbas, M., Hidouci, K., Smaili, K., 2016. An algerian dialect: study and resources 984–396 Int. J. Adv. Comput. Sci. Appl. 7 https://doi.org/10. 14569/IJACSA.2016.070353. Ide, N., Pustejovsky, J., 2017. Handbook of Linguistic Annotation. Springer. 10.1007/ 978-94-024-0881-2. Jarrar, M., Habash, N., Alrimawi, F., 2017. Curras: an annotated corpus for the Palestinian Arabic dialect. Lang. Resour. Eval. 51, 745–775. https://doi.org/ 10.1007/s10579-016-9370-7. Jean, C., 1996. Assessing agreement on classiﬁcation tasks: the kappa statistic. Comput. Linguist. 22, 249–254. Jean, C., 1996. Assessing agreement on classiﬁcation tasks: the kappa statistic. Comput. Linguist. 22, 249–254. Kohavi, R., 1995. A study of cross-validation and bootstrap for accuracy estimation and model selection. Int. Jt. Conf. Artif. Intell. 14, 1137–1143. https://doi.org/ 10.1067/mod.2000.109031. Korayem, M., Aljadda, K., Crandall, D., 2016. Sentiment/subjectivity analysis survey for languages other than English. Soc. Netw. Anal. Min. 6, 75. https://doi.org/ 10.1007/s13278-016-0381-6. Liu, B., 2012. Sentiment analysis and opinion mining. Synth. Lect. Hum. Lang. Technol. 5, 1–167. https://doi.org/10.2200/S00416ED1V01Y201204HLT016. Matthew, P., Jessica, B., 2010. Introduction to manual annotation. Fairview Res. McCallum, A., Nigam, K., 1998. A comparison of event models for naive bayes text classiﬁcation. AAAI/ICML-98 Work Learn Text Categ, 41–48. 10.1.1.46.1529. Mountassir, A., Benbrahim, H., Berraba, I.,", "meta": {"paper_id": "2006.00459v1", "chunk_index": 18, "filename": "2006.00459v1.pdf", "chunk_length": 512}}
{"id": "2006.00459v1_chunk_19", "text": "and opinion mining. Synth. Lect. Hum. Lang. Technol. 5, 1–167. https://doi.org/10.2200/S00416ED1V01Y201204HLT016. Matthew, P., Jessica, B., 2010. Introduction to manual annotation. Fairview Res. McCallum, A., Nigam, K., 1998. A comparison of event models for naive bayes text classiﬁcation. AAAI/ICML-98 Work Learn Text Categ, 41–48. 10.1.1.46.1529. Mountassir, A., Benbrahim, H., Berraba, I., 2013. Sentiment classiﬁcation on Arabic corpora. A preliminary cross-study. Doc numérique 16, 73–96. https://doi.org/ 10.3166/dn.16.1.73-96. Pustejovsky, J., Stubbs, A., 2012. Natural Language Annotation for Machine Learning, First Edit. O’Reilly Media. Rahab, H., Zitouni, A., Djoudi, M., 2018. SIAAC: sentiment polarity identiﬁcation on Arabic algerian newspaper comments. Appl. Comput. Intell. Math. Methods. https://doi.org/10.1007/978-3-319-67621-0. Rahab, H., Zitouni, A., Djoudi, M., 2017. ARAACOM: ARAbic algerian corpus for opinion mining. ACM Int. Conf. Comput. Eng. Sci. 35–39. https://doi.org/ 10.1145/3129186.3129193. Refaeilzadeh, P., Tang, L., Liu, H., 2009. Cross-Validation. In: Liu, L., Özsu, M.T. (Eds.), Encyclopedia of Database Systems. Springer US, Boston, MA, pp. 532–538. 10.1007/978-0-387-39940-9_565. Rushdi-Saleh, M., Martín-Valdivia, M.T., Ureña-López, L.A., Perea-Ortega, J.M., 2011. OCA: opinion corpus for Arabic. J. Am. Soc. Inf. Sci. Technol. 62, 2045–2054. https://doi.org/10.1002/asi.21598. Steven, L.G.S., 1997. On comparing classiﬁers: pitfalls to avoid and a recommended approach. Data Min. Knowl. Discov. 328, 317–328. https://doi.org/10.1023/ A:1009752403260. Tripathy, A., Anand, A., Rath, S.K., 2017. Document-level sentiment classiﬁcation using hybrid machine learning approach. Knowl. Inf. Syst. https://doi.org/ 10.1007/s10115-017-1055-z. Tunga, G., 2010. Part-of-Speech Tagging. In: Nitin, I., Damerau, F.J. (Eds.), Handbook of Natural Language Processing. CRC Press, pp. 205–235. 10.3115/ 1220835.1220877. Wang, X., 2015. Short term prediction of freeway exiting volume based on SVM and KNN. Int. J. Transp. Sci. Technol. 4, 337–352. https://doi.org/10.1260/2046- 0430.4.3.337. Zaidan, O.F., Callison-burch, C., 2011. The arabic online commentary dataset: an annotated dataset of informal arabic with high dialectal content. Comput. Linguist., 37–41 Ziani, A., Tlili Guaissa, Y., Nabiha, A., 2013. Détection de polarité d’opinion dans les forums en langues arabe par fusion de plusieurs SVMs. RECITAL, 83–95. Ziani, A., Zenakhra, D., Cheriguene, S., Aldwairi, M., 2019. Combining RSS-SVM with genetic algorithm for Arabic opinions analysis. Int. J. Intell. Syst. Technol. Appl. 18, 152–178. H. Rahab et al. / Journal of King Saud University – Computer and Information Sciences xxx (xxxx) xxx 9 Please cite this article as: H. Rahab, A. Zitouni and M. Djoudi, SANA: Sentiment analysis on newspapers comments in Algeria, Journal of King Saud University – Computer and Information Sciences, https://doi.org/10.1016/j.jksuci.2019.04.012 View publication stats View publication stats", "meta": {"paper_id": "2006.00459v1", "chunk_index": 19, "filename": "2006.00459v1.pdf", "chunk_length": 390}}
{"id": "2006.00518v1_chunk_0", "text": "Data-driven Detection and Analysis of the Patterns of Creaky Voice Thomas Drugmana,∗, John Kaneb, Christer Goblb aTCTS Lab, University of Mons 31, 7000 Mons, Belgium bPhonetics and Speech Laboratory, School of Linguistic, Speech and Communication Sciences, Trinity College Dublin, Ireland Abstract This paper investigates the temporal excitation patterns of creaky voice. Creaky voice is a voice quality frequently used as a phrase-boundary marker, but also as a means of portraying attitude, aﬀective states and even so- cial status. Consequently, the automatic detection and modelling of creaky voice may have implications for speech technology applications. The acous- tic characteristics of creaky voice are, however, rather distinct from modal phonation. Further, several acoustic patterns can bring about the perception of creaky voice, thereby complicating the strategies used for its automatic detection, analysis and modelling. The present study is carried out using a variety of languages, speakers, and on both read and conversational data and involves a mutual information-based assessment of the various acoustic features proposed in the literature for detecting creaky voice. These features are then exploited in classiﬁcation experiments where we achieve an appre- ciable improvement in detection accuracy compared to the state of the art. Both experiments clearly highlight the presence of several creaky patterns. A subsequent qualitative and quantitative analysis of the identiﬁed patterns is provided, which reveals a considerable speaker-dependent variability in the usage of these creaky patterns. We also investigate how creaky voice detection systems perform across creaky patterns. Keywords: Creaky voice, vocal fry, irregular phonation, glottal source ∗Corresponding author. Tel. +3265374749. Email addresses: thomas.drugman@umons.ac.be (Thomas Drugman), kanejo@tcd.ie (John Kane), cegobl@tcd.ie (Christer Gobl) Preprint submitted to Computer Speech and Language June 2, 2020 arXiv:2006.00518v1 [eess.AS] 31 May 2020 1. Introduction This paper presents an empirical investigation of the temporal excitation patterns related to the voice quality often referred to as creaky voice. Creaky voice is a raspy or croaking quality of the voice generally produced with a very low pitch and often with highly irregular periodicity (Laver, 1980). Creaky voice is used for a variety of functions in spoken communication and, hence, presents both an opportunity and a challenge (because of its distinctive acoustic characteristics) for speech technology. 1.1. Terminology One major diﬃculty with studying creaky voice, and indeed voice qual- ity in general, is the problem of reconciling the variation in the terminology used in the literature. Many studies use the terms irregular phonation (Slifka, 2006; Surana and Slifka, 2006a; Vishnubhotla and Espy-Wilson, 2006; B¨ohm et al., 2010) or glottalisation (Dilley et al., 1996; Redi and Shattuck-Hufnagel, 2001). However, both these terms are rather broad and indeed cover other classes of phonation (or laryngeal activity) aside from what we consider as creaky voice. In this paper, we interpret creaky voice based solely on the au- ditory criterion “a rough quality with the additional sensation of repeating impulses” as is done in Ishi et al. (2008a) (which is close to that in Laver, 1980). Note that the “... sensation of repeating impulses” part clearly dis- criminates the voice quality from harsh voice. Any speech region displaying such a quality", "meta": {"paper_id": "2006.00518v1", "chunk_index": 0, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_1", "text": "rough quality with the additional sensation of repeating impulses” as is done in Ishi et al. (2008a) (which is close to that in Laver, 1980). Note that the “... sensation of repeating impulses” part clearly dis- criminates the voice quality from harsh voice. Any speech region displaying such a quality will be treated in this paper as creaky voice. Note that such a criterion will contain both creak and creaky voice, and, hence, will not apply the discrimination of the two used in Laver (1980). The term vocal fry (perhaps used more by American researchers; Laver, 1980) is often used in the literature (Hollien and Wendahl, 1968; Ishi et al., 2008a; Wolk and Abdelli-Beruh, 2012), and is likely to corresponding closely to our working deﬁnition of creaky voice. 1.2. Creaky voice in speech communication Creaky voice has been studied in relation to various functions in speech communication, and most commonly with phrase or sentence boundary mark- ing (Surana and Slifka, 2006b; Drugman et al., 2013). Similarly creaky voice has been associated with turn-yielding in Finnish (Ogden, 2001). However, 2 creaky voice is likely to be also implicated in a range of speech functions other than boundary marking. It has been studied in relation to hesitations in Swedish (Carlson et al., 2006), and also creaky voice-like properties have been observed as an allophonic variant of word medial oral stops (Zue and Laferriere, 1979; Crystal and House, 1988). Creaky voice has also been inves- tigated in terms of emotion and aﬀectively coloured speech (Yanushevskaya et al., 2005; Gobl and N´ı Chasaide, 2003; Ishi et al., 2008b) and is likely to be a signiﬁcant correlate of subtle variation in levels of activation and formality of the speaking setting (Kane et al., 2011). The use of creaky voice has also recently been shown to be increasingly common for young American females (Wolk and Abdelli-Beruh, 2012) and has also been linked to the portrayal of social status (Yuasa, 2010). 1.3. Creaky voice in speech technology As a consequence of its implication in these various roles, the proper handling of creaky voice, and the distinctive acoustic characteristics associ- ated with it, has a signiﬁcant importance for speech technology. For speech synthesis (see e.g., Silen et al., 2009; Drugman et al., 2012a), this could re- sult in improved naturalness for speakers who use creaky voice and also for the development of expressive speech synthesis. As it has been shown that listeners are sensitive to creaky voice in terms of recognising a speaker’s iden- tity (B¨ohm and Shattuck-Hufnagel, 2007), it is likely beneﬁcial for speaker recognition systems (Espy-Wilson et al., 2006; Elliot, 2002) to exploit infor- mation to do with creaky voice. Detection of creaky voice may also beneﬁt developments in emotion recognition and conversational analysis. 1.4. Physiology Although the focus of this paper is on the acoustic characteristics of creaky voice, we include here a brief outline of some of the physiological characteristics reported as being associated with creaky voice. Laver (1980) provides one of the more comprehensive descriptions of creaky voice. Here he describes creaky voice as involving", "meta": {"paper_id": "2006.00518v1", "chunk_index": 1, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_2", "text": "focus of this paper is on the acoustic characteristics of creaky voice, we include here a brief outline of some of the physiological characteristics reported as being associated with creaky voice. Laver (1980) provides one of the more comprehensive descriptions of creaky voice. Here he describes creaky voice as involving low subglottal pressure, high levels of adductive laryngeal tension (i.e. the muscular tension involved in bringing the vocal folds together) and typically involves low levels of longitudinal vo- cal fold tension (probably the main physiological parameter utilised for pitch variation). Edmondson and Esling (2006) provide some additional physiolog- ical evidence, in particular details on the presence of ventricular incursion. 3 This involves the ventricular folds pushing down and covering the true vo- cal folds, causing an increased mass. This has the consequence of lowering the frequency of vibration and often causing secondary vibrations above the glottis (Moisik and Esling, 2011). 1.5. Acoustic characteristics of creaky voice The above described physiological settings have the eﬀect of generating speech with rather distinct acoustic characteristics from modal voice1. How- ever, rather than displaying a single type of acoustic pattern, speech pertain- ing to the auditory criterion used here can involve more than one pattern. Redi and Shattuck-Hufnagel (2001), for instance, (following Huber, 1988) separate four categories of glottalisation, two of which likely correspond the auditory criterion used in the present study. The ﬁrst category involves a high degree of pulse-to-pulse irregularity, in both duration (jitter) and am- plitude (shimmer). This is likely to be consistent with the “multi-pulsed” pattern reported in Ishi et al. (2008a, 2007). Note, however, that a high de- gree of irregularity in the duration and amplitudes of successive glottal pulses alone will not be suﬃcient to allow the “sensation of repeating impulses”. A certain proportion of the pulses will need to display durations correspond- ing to a very low F0, otherwise the speech will be perceived as harsh voice (Laver, 1980). The second pattern is categorised as having sustained low frequency with little or no superposition of formant oscillations between adjacent glottal pulses. Such a pattern has commonly been reported, with F0 values typ- ically ranging from 40-70 Hz, but at times with glottal periods as long as 100 ms (Blomgren et al., 1998; Hollien and Wendahl, 1968). Previous ex- periments carried out by Titze (1994) found that human listeners begin to perceive individual pulses from around 70 Hz. This suggests that a very low F0, below some auditory threshold in the vicinity of 70 Hz, is suﬃcient to give the auditory criterion used in this paper, even in the absence of irregu- larity in duration or amplitude. What is unclear is the extent to which the two categories used in Redi and Shattuck-Hufnagel (2001) are overlapping. For instance, can the auditory criterion used here be achieved with highly irregular periodicity, even if many of the glottal pulses display a duration corresponding to signiﬁcantly above 70 Hz? 1We interpret modal voice, following Laver (1980), as the case of periodic vocal fold vibration, with full glottal closure and no audible frication. 4", "meta": {"paper_id": "2006.00518v1", "chunk_index": 2, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_3", "text": "auditory criterion used here be achieved with highly irregular periodicity, even if many of the glottal pulses display a duration corresponding to signiﬁcantly above 70 Hz? 1We interpret modal voice, following Laver (1980), as the case of periodic vocal fold vibration, with full glottal closure and no audible frication. 4 Ishi et al. (2007) provide a further subdivision of the second pattern discussed by Redi and Shattuck-Hufnagel (2001), by identifying a single-pulse and a “double-beated” creaky pattern, both in combination with pressed phonation. Interestingly, for the “double-beated” pattern, Ishi et al. (2007) claim that the secondary pulses observed in the speech waveform are likely caused by an abrupt glottal opening, as evidenced by the electroglottographic (EGG) signal. For this pattern, they report a very short glottal open phase and a long glottal closed phase, consistent with a tense or pressed phonation type. The same authors (Ishi et al., 2010) report similar patterns in the phonation type known as “Rikimi” in Japanese. However, in that study creak is also observed within an overall laxer mode of phonation (not consistent with the deﬁnition of “Rikimi”) without the presence of excitations resulting from abrupt glottal openings. Besides the temporal patterning of creaky voice, other work has reported the longest glottal closed phase for creaky voice, compared to a range of other voice qualities (Gobl and N´ı Chasaide, 1992). Furthermore, the same authors reported a negative correlation between the strength of the main glottal excitation with duration of the glottal return phase, for patterns showing a high level of diplophonia. Previous work by the present authors (Drugman et al., 2012a), sought to model the characteristics of the creaky excitation in order to achieve natu- ral rendering of creaky voice for speech synthesis. The study reported the strong presence of secondary residual peaks corresponding to an abrupt glot- tal opening. The two speakers analysed in that study mainly displayed creaky voice patterns corresponding to the second category in Redi and Shattuck- Hufnagel (2001), or the “double-beated” pattern in Ishi et al. (2007). The data-driven excitation model derived in Drugman et al. (2012a) may there- fore not be suited for the system of creaky voice involving a more irregular structure, such as for the ﬁrst category patterns. 1.6. Research aims and outline There are three main aims of the current study: 1. To assess, via mutual information-based measures, the relevance of acoustic features related to creaky voice (proposed by the present au- thors, Drugman et al., 2012b; Kane et al., 2013, as well as by others, Ishi et al., 2008a) for identifying creaky voice regions. 5 2. To integrate these combined features within an eﬃcient creaky voice detection system. 3. To utilise the classiﬁcation system with diﬀerent feature groups to help identify the various temporal excitation patterns of creaky voice, and to analyse them both qualitatively and quantitatively. The paper is structured as follows: next the speech data are described (Section 2), followed by the acoustic features included in the analysis (Section 3), the mutual information of the various features is assessed in Section 4, with the results", "meta": {"paper_id": "2006.00518v1", "chunk_index": 3, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_4", "text": "creaky voice, and to analyse them both qualitatively and quantitatively. The paper is structured as follows: next the speech data are described (Section 2), followed by the acoustic features included in the analysis (Section 3), the mutual information of the various features is assessed in Section 4, with the results from the automatic detection experiment in Section 5, before an examination of the creaky patterns found in the data (Section 6) and providing a summary and a conclusion (Section 7). 2. Databases This section describes the various speech databases used in the present study. The speech data has been selected in order to cover a variety of factors including: read vs conversational speech, diﬀerent recording conditions and a range of languages. 2.1. Text-to-speech database The ﬁrst database consists of studio recorded read speech for the purpose of text-to-speech (TTS) synthesis development. 100 sentences, which were deemed to contain creaky voice, were selected from 3 corpora. The ﬁrst was speech from an American male (BDL) taken from the ARCTIC database, the second was a Finnish Female (HS) and the third was a Finnish male (MV). 2.2. Swedish database Recordings of a male (Swe-M) and a female (Swe-F) speaker were selected from the SPONTAL corpus of Swedish conversational speech (Edlund et al., 2010). Each conversation lasted approximately 30 minutes and audio was captured in a recording studio. Recordings were made of audio, video and motion capture, however the data in the current study is limited to just the audio recorded with a head-mounted Beyerdynamic Opus 54 cardioid which was used to obtain optimal recording quality. 6 2.3. American database The conversations were recorded involving 2 American males (US-M1 and US-M2) and 2 American females (US-F1 and US-F2) engaged in natural dyadic conversations on the topic of food. The conversations, recorded in a quiet room, lasted around 10 minutes and audio was recorded with headset microphones. Similar recordings were used in a recent sociological study on creaky voice (Yuasa, 2010). 2.4. Japanese database The ﬁnal database of audio recordings consisted of conversation speech data of two Japanese female speakers (Jap-F1 and Jap-F2). The two engaged in a 30 minute conversation where they ﬁrst watched some short animated ﬁlms before talking carrying out the conversation. Audio was recorded on AKG C420 III PP MicroMic headset microphones wired through a BeachTek DXA-2S pre-amp connected to the video camera (Sony DCR-TRV38 Mini DV camera). 2.5. Creaky voice annotation It is not generally possible to obtain automatic annotation of creaky voice to the level of precision required to evaluate detection algorithms. Conse- quently, human annotation of creaky voice regions was carried out, closely following the procedure adopted in Ishi et al. (2008a). The binary decision on the presence of creaky voice was made based solely on the auditory criterion mentioned previously, i.e. “a rough quality with the additional sensation of repeating impulses”. Annotation was, however, guided by displays of the speech waveform, spectrogram and F0 contours. Due to the large volume of data included in the present study, the annotation was split and shared between the ﬁrst two authors", "meta": {"paper_id": "2006.00518v1", "chunk_index": 4, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_5", "text": "mentioned previously, i.e. “a rough quality with the additional sensation of repeating impulses”. Annotation was, however, guided by displays of the speech waveform, spectrogram and F0 contours. Due to the large volume of data included in the present study, the annotation was split and shared between the ﬁrst two authors who both adhered to the annotation scheme described here. Note that this annotation was achieved at a very low level, in the sense that regions of creaky regions were labelled at the frame level. We therefore expect our detection algorithms to be accurate at that level. 2.6. Summary of speech data Table 1 provides a summary of the speech data used in the present study. Included in the table is a column displaying the percentage of speaking time which was annotated as involving creaky voice. It can be observed that there is a strong cross-speaker variability in terms of the proportion of speech containing creaky voice. One American speaker used creaky voice in 3.6 % 7 Table 1: Summary of the speech data used in this study. Database ID Gender Country Duration Creak (%) Creak duration (s) TTS BDL Male USA 100 Sentences 7.6 19.39 HS Female Finland 100 Sentences 5.8 36.86 MV Male Finland 100 Sentences 7.1 31.56 Swedish Swe-F Female Sweden 30+ Minutes 5.3 64.13 Swe-M Male Sweden 30+ Minutes 5.8 32.13 US US-F1 Female USA 10+ Minutes 10.5 32.70 US-M1 Female USA 10+ Minutes 7.4 15.87 US-M2 Male USA 10+ Minutes 9.2 16.00 US-F2 Male USA 10+ Minutes 3.6 9.09 Japan Jap-F1 Female Japan 30+ Minutes 4.8 55.68 Jap-F2 Female Japan 30+ Minutes 6.5 40.71 of her speech, while another American female, recorded as part of the same database, used creaky voice in over 10 % of her speech. Also included in Table 1 is the actual duration (in seconds) of creaky productions for each speaker. Note in the TTS data the Finnish sentences are considerably longer than the US English sentences. Also, in the conversational data the duration given is the time they participated in the conversation, where as the percentage of creak is determined from just those times where the person is actually speaking. For instance, speaker Jap-F2 speaks less than Jap-F1 over the course of a 30 minute conversation, but a larger proportion of her speaking time involved creaky phonation. 3. Features for Creaky Voice Characterisation This section describes the set of acoustic features relevant to creaky voice which are examined in the present study. This set consists of features de- veloped by the present authors (Kane et al., 2013; Drugman et al., 2012b) as well as features by Ishi et al. (2008a). A brief description of each of the features is given below. 8 3.1. H2-H1 and F0creak The ﬁrst acoustic feature was originally presented in Drugman et al. (2012b), and is designed to characterise the strong presence of secondary residual peaks often found in creaky voice. The block diagram (in Figure 1) shows that two resonators are applied to the linear prediction (LP) resid- ual. Both resonators have a centre frequency set to the", "meta": {"paper_id": "2006.00518v1", "chunk_index": 5, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_6", "text": "in Drugman et al. (2012b), and is designed to characterise the strong presence of secondary residual peaks often found in creaky voice. The block diagram (in Figure 1) shows that two resonators are applied to the linear prediction (LP) resid- ual. Both resonators have a centre frequency set to the speaker’s mean F0, however each has diﬀerent bandwidth settings. Resonator 1 is set with a bandwidth of around 1 kHz and is used for providing a more robust F0 con- tour, even in creaky voice regions. Note that the F0 contour is derived by calculating a corrected autocorrelation function, r′(τ), from 50-ms Hanning- windowed frames of the Resonator 1 output: r′(τ) = N N −τ · autoCorr(τ) (1) where N is the window length (in samples) and τ is the number of autocorre- lation lags. A correction of N N−τ is applied to compensate for the decreasing properties of the autocorrelation function with increasing τ (as is used in Ishi et al., 2008a). The local fundamental period is then considered as the position of the maximum value in r′(τ) above the peak centred on τ = 0. Figure 1: Block diagram of the H2-H1 parameter, derived from the response of two resonators excited by the LP-residual signal. Both resonators are centred on f0,mean but use diﬀerent bandwidths. For details see text. Resonator 2 is set with a bandwidth to 150 Hz, and is used for calculating the H2-H1 feature. Again applying 50-ms frames Hanning-windowed frames, this time to Resonator 2 output, the amplitude spectrum is derived and the corresponding F0 value (derived from Resonator 1) is used to detect the amplitudes of the ﬁrst two harmonics (H1 and H2). The diﬀerence in amplitude of these two harmonics (i.e. H2-H1) in dB is the ﬁrst acoustic 9 feature used in this study. The H2-H1 contour is smoothed using a 100-ms moving average ﬁlter to remove the eﬀect of outliers. An illustration of the steps involved in the H2-H1 calculation process is given in Figure 2. On left side, for modal phonation, one can observe no strong secondary peaks in the residual signal or in the Resonator 2 output (middle panel). Consequently, its amplitude spectrum (bottom panel) shows a prominent amplitude level at F0 compared to the second harmonic. Con- trastingly, for a creaky voice segment (right side), strong secondary peaks can be observed in the residual signal (middle panel) which aﬀects the Res- onator 2 output, causing a greater harmonicity in the resulting amplitude spectrum (bottom panel). A combination of this increased harmonicity and a considerably lower F0 in creaky voice compared to the speaker’s mean F0, imply that the amplitude of the ﬁrst harmonic is considerably weaker to that of the second. Note that in this study both the H2-H1 and the F0 contour derived from Resonator 1 (named F0creak) are used in this study. 0 0.025 0.05 0.075 0.1 −1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 0.025 0.05 0.075 0.1 −1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 0.025 0.05 0.075 0.1 −0.5 0 0.5 1 Time (seconds)", "meta": {"paper_id": "2006.00518v1", "chunk_index": 6, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_7", "text": "F0 contour derived from Resonator 1 (named F0creak) are used in this study. 0 0.025 0.05 0.075 0.1 −1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 0.025 0.05 0.075 0.1 −1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 0.025 0.05 0.075 0.1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 0.025 0.05 0.075 0.1 −0.5 0 0.5 1 Time (seconds) Amplitude 0 200 400 600 800 1000 −40 −20 Frequency (Hz) Amplitude (dB) 0 200 400 600 800 1000 −40 −20 Frequency (Hz) Amplitude (dB) Figure 2: Example of modal phonation (left column) and creak (right column) uttered by the same speaker, with the speech waveform (top row), the LP residual signal (middle row, solid line) together with the output of Resonator 2 (in dashed line), and the amplitude spectrum (in dB) of a frame of the output of Resonator 2 where the values for F0 and 2 · F0 are indicated by crosses (bottom row). One limitation of the H2-H1 parameter is that for highly irregular pe- 10 riodicity patterns (i.e. the category 1 (Redi and Shattuck-Hufnagel, 2001), multi-pulsed pattern (Ishi et al., 2008a)), the amplitude spectrum may not display any clear harmonics in which case the discriminating power of H2-H1 may be reduced. 3.2. Residual peak prominence - Peak-Prom The second feature (Peak-Prom) was designed to avoid spectral and peri- odicity related measurements, and instead characterise each excitation peak in the time-domain. The Peak-Prom parameter essentially characterises the prominence of LP-residual peaks relative to its immediate neighbourhood. The output of Resonator 1 (see Figure 1) is used, as its low-pass ﬁltering eﬀect makes the prominence measure more robust as opposed to measure- ments directly from the LP-residual. Furthermore, the large bandwidth (1 kHz) ensures a rapid decay in the oscillations which facilitates the promi- nence measurement. Peak-Prom involves the use of a ﬁxed, non-overlapping rectangular window whose duration is set to 30 ms. This roughly corresponds to two periods at 70 Hz. In this method correct polarity of the speech sig- nal is assumed (this can be determined automatically for example using the method described in Drugman, 2013). Although with correct polarity the LP-residual displays positive peaks, the low-pass ﬁltering eﬀect of the res- onator causes corresponding negative peaks in its output. The resonator output is inverted so that it instead displays strong positive peaks. For each frame the absolute maximum peak in the resonator output is identiﬁed and the frame is then shifted to be centred on this peak. By measuring the amplitude diﬀerence between the maximum peak (in the centre of the frame) and the next strongest peak one can obtain a parameter value which diﬀerentiates modal and creaky regions. In order to avoid selecting values in the vicinity of the main centre peak, the search for the next strongest peak is made outside a distance of 6 ms on both sides of the centre of the frame. This corresponds to 40 % of half the frame length which ensures that there is suﬃcient space for peaks to occur from neighbouring glottal pulses. A value is", "meta": {"paper_id": "2006.00518v1", "chunk_index": 7, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_8", "text": "for the next strongest peak is made outside a distance of 6 ms on both sides of the centre of the frame. This corresponds to 40 % of half the frame length which ensures that there is suﬃcient space for peaks to occur from neighbouring glottal pulses. A value is thus obtained for each frame producing the outputted parameter contour. This contour is then ﬁltered with a 3-point median ﬁlter to remove misdetections due to transients in the signal. The Peak-Prom parameter calculation can be summarised in the following steps: 1. Apply a 30 ms rectangular window to the Resonator 1 output 11 2. Invert the windowed frame to ensure positive peaks 3. Identify the maximum peak and shift the frame to be centred on this peak 4. Measure the next strongest peak outside the middle 12 ms of the frame, and calculate the amplitude diﬀerence between it and the strongest peak 5. Repeat with non-overlapping frames 6. Apply 3-point median ﬁlter to the extracted contour −1 0 1 Amplitude Time (seconds) 0 0.1 0.2 0.3 0.4 0.50 0.5 1 Res. peak prom. Figure 3: Linear Prediction residual of an utterance with creaky voice starting from around 0.3 seconds, along with the Peak-Prom contour (green dash-dotted line). Also shown are sample windowing locations (black line) and and exclusion areas (red dashed line) for the parameter calculation in both the modal and creaky region. An illustration of the Peak-Prom parameter is shown in Figure 3. An example of the window (black line) is provided both in the modal voice region (ﬁrst syllable) and in the creaky voice region (second syllable). The dashed red line shows the region within the window from which the second peak measurement is excluded. For the creaky voice window it can be noticed that aside from the prominent excitation peak in the centre of the frame, no other strong peaks can be observed (outside the exclusion region). As a result the Peak-Prom contour (dot-dashed green line) displays high values here. For the modal voice region, one can observe that several strong residual peaks are contained within the rectangular frame and, hence, Peak-Prom displays values close to 0. A limitation of the Peak-Prom feature is that if a given creaky voice pattern (corresponding to the category 1 (Redi and Shattuck- Hufnagel, 2001), multi-pulsed pattern (Ishi et al., 2008a)) contains a high 12 proportion of glottal pulses with a duration signiﬁcantly lower than 15 ms, then the eﬀectiveness of Peak-Prom may be reduced. Note that for the present study the features H2-H1, F0creak and Peak Prom are grouped as the KD (Kane-Drugman) features. 3.3. Power peak parameters - PwP fall & PwP rise The ﬁrst features described in Ishi et al. (2008a) used in this study are the so-called Power-Peak (PwP) parameters. Note that all of the patterns from Ishi et al. (2008a) are calculated from the speech signal bandlimited to 100-1500 Hz. A ‘very short-term’ power contour is measured, with a frame length of 4 ms and shift of 2 ms, in order to highlight the amplitude variation within individual", "meta": {"paper_id": "2006.00518v1", "chunk_index": 8, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_9", "text": "parameters. Note that all of the patterns from Ishi et al. (2008a) are calculated from the speech signal bandlimited to 100-1500 Hz. A ‘very short-term’ power contour is measured, with a frame length of 4 ms and shift of 2 ms, in order to highlight the amplitude variation within individual pulses (see Figure 4). Peaks are then detected in this contour and Power Peak (PwP) parameters are derived for each peak based on the previous (PwP-rising) and following (PwP-falling) 5 frames (i.e. 10 ms) in the contour. The maximum power diﬀerence in each direction is used as the PwP value. In the original implementation a threshold is applied to this parameter to determine whether the peak can be used as a creak candidate location, however in the present study we simply use both PwP parameter values as acoustic features. 0 0.2 0.4 0.6 0.8 1 1.2 0 20 40 60 80 100 120 Power(dB) Time(seconds) −0.2 −0.1 0 0.1 0.2 0.3 0.4 Amplitude Figure 4: Very short-term power contour (blue contour), with power peaks (red stars) and the speech waveform (green) of the utterance “very good” with creaky voice on “good” spoken by an American female during natural conversation. 3.4. Inter-pulse similarity - IPS The inter-pulse similarity measure (Ishi et al., 2008a) is used to discrimi- nate glottal pulses corresponding to creaky voice from unvoiced regions. The parameter is derived using the locations of the peaks measured in the very 13 short-term energy contour (Section 3.3). A cross-correlation function is ap- plied to assess the similarity of adjacent bandlimited speech pulses: IPS = max {CCorr(Fτ1, Fτ2); τ1 −τ2 < Tmax} (2) where CCorr is the cross-correlation function, Fτ1 and Fτ2 are the frames centred on successive candidate peak locations, and Tmax is the maximum allowed distance between adjacent peaks, and is set to 100 ms. Each frame is selected as the range of 5 ms around the peak location. It is assumed that adjacent creaky voice pulses will show a high degree of similarity, as the vocal tract is unlikely to have signiﬁcantly changed in such a short space of time and, hence IPS values should be high. Contrastingly, for unvoiced regions IPS values should be low, indicating a low level of similarity. The IPS parameter is illustrated in Figure 5, where the values are shown as red stems. It can be observed that high IPS values are found in both the ‘modal’ voice region (up to 0.3 second) and also in the creaky voice (0.33 to 0.5 seconds). Note that both the IPS and PwP features are sampled on a glottal syn- chronous basis. These features are interpolated up to a ﬁxed frame basis using a nearest neighbour approach for the current study. 0 0.1 0.2 0.3 0.4 0.5 0.6 −1 −0.5 0 0.5 1 Time (seconds) Amplitude IFP IPS Figure 5: Speech waveform along with the IFP contour (black line) and the IPS values (red stems) of the utterance “... remember it” with creaky voice on the syllable “it” spoken by an American male. 3.5. Intra-frame periodicity - IFP The", "meta": {"paper_id": "2006.00518v1", "chunk_index": 9, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_10", "text": "−0.5 0 0.5 1 Time (seconds) Amplitude IFP IPS Figure 5: Speech waveform along with the IFP contour (black line) and the IPS values (red stems) of the utterance “... remember it” with creaky voice on the syllable “it” spoken by an American male. 3.5. Intra-frame periodicity - IFP The Intra-frame periodicity (IFP) feature (Ishi et al., 2008a) was origi- nally designed to help disambiguate creaky voice from other voiced regions. Unlike the IPS feature, IFP is calculated on a ﬁxed frame basis using: 14 IFP = min \u001a N N −τ · autoCorr(τ); τ = j · τ0; j = 1, 2, ... \u001b (3) where N is the frame length (set to 32 ms, with a 10 ms shift), τ is the auto- correlation lag, autoCorr is the normalised autocorrelation function, and τ0 is the lag of the strongest autocorrelation peak. Note also that the search space for τ is limited to 15 ms and that the factor N N−τ is used to compensate for the decrease in amplitude with increasing τ in the autocorrelation function. Due to irregular periodicity and/or the very low F0 of creaky voice, IFP values in these regions will be close to 0 (see the last syllable in Figure 5). Other voiced regions (for instance the ﬁrst syllable in Figure 5) will display IFP values close to 1. Although in many cases IFP is suitable for discriminating creaky voice from non-creaky voiced regions, its eﬀectiveness for this purpose can be re- duced somewhat when a speech region contains a very low F0 (but not quite suﬃciently low for creaky voice, e.g. around 80 Hz). Such eﬀect was observed in a previous study (Drugman et al., 2012b) and resulted in a high number of false detections for a speaker with an inherently low pitch. Note that in the present study the features PwP-fall, PwP-rise, IPS and IFP are given the group title Ishi’s features, as they were all proposed in (Ishi et al., 2008a). 3.6. Additional acoustic features - Energy norm, Power std & ZeroXrate In addition to the acoustic features (KD) proposed by the present au- thors and those by Ishi et al. (2008a), three further features were included in particular to avoid false positives in unvoiced and silent regions. It was suggested (in a personal communication) by the authors of Ishi et al. (2008a) that creaky voice detected in regions of considerably lower energy (e.g., 20 dB) than the maximum energy of an utterance, be discarded from the de- tection output. Consequently, we include a measure of signal energy (in dB) which has been normalised to the maximum energy of the utterance (Energy Norm). We also include ZeroXrate, which is a measure of the number of zero-crossings per ms. Unvoiced and silent regions are likely to display a signiﬁcantly higher rate of zero-crossings compared with creaky voice regions (Kane et al., 2011). Note that both Energy Norm and ZeroXrate are mea- sured on 32 ms frames, with a 10 ms shift. Finally, the Power Std feature is used as a measure of the variance", "meta": {"paper_id": "2006.00518v1", "chunk_index": 10, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_11", "text": "likely to display a signiﬁcantly higher rate of zero-crossings compared with creaky voice regions (Kane et al., 2011). Note that both Energy Norm and ZeroXrate are mea- sured on 32 ms frames, with a 10 ms shift. Finally, the Power Std feature is used as a measure of the variance in the very short-term power contour 15 used in the calculation of the PwP features (Ishi et al., 2008a). The feature is derived as the standard deviation of the power (in dB), measured on 16 frames (corresponding to 32 ms). As with the other features used in this study, these three features were sampled every 10 ms. 4. Mutual Information-based Relevance of the Creaky Features The goal of this section is to assess the relevance of the features described in Section 3 for the automatic detection of creaky regions. For this, we here make use of measures derived from Information Theory (Shannon, 1948) as they allow a quantiﬁcation of the amount of discriminant information con- veyed by the features. This is done independently of any subsequent classi- ﬁer. Our approach allows for an integrated assessment of the discriminative power of each feature individually and also of the discriminative power in the context of the other features in terms of redundancy and synergy. These mea- sures are ﬁrst presented in Section 4.1. They are then used for an objective assessment of the features in Section 4.2. 4.1. Mutual Information-based Measures The problem of automatic classiﬁcation consists of ﬁnding a set of features Xi such that the uncertainty on the determination of classes C is reduced as much as possible (Huan and Motoda, 1998). For this, Information Theory (Cover and Thomas, 1991) allows the assessment of the relevance of features for a given classiﬁcation problem, by making use of the following measures (where p(.) denotes a probability density function): • The entropy of classes C is expressed as: H(C) = − X c p(c) log2 p(c) (4) where c are the discrete values of the random variable C. H(C) can be interpreted as the amount of uncertainty on the class determination. • The mutual information (MI) between one feature Xi and classes C is deﬁned as: I(Xi; C) = X xi X c p(xi, c) log2 p(xi, c) p(xi)p(c) (5) 16 where xi are the discretised values of feature Xi. I(Xi; C) can be viewed as the information that feature Xi conveys about the considered classiﬁcation problem, i.e. the intrinsic discrimination power of a given feature. • The joint mutual information between two features Xi, Xj, and classes C can be expressed as: I(Xi, Xj; C) = I(Xi; C) + I(Xj; C) −I(Xi; Xj; C) (6) and corresponds to the information that features Xi and Xj, when used together, bring to the classiﬁcation problem. The last term can be written as (Cover and Thomas, 1991): I(Xi;Xj; C) = X xi X xj X c p(xi, xj, c)· log2 p(xi, xj)p(xi, c)p(xj, c) p(xi, xj, c)p(xi)p(xj)p(c) (7) An important remark has to be underlined about the sign of this term. It can be noticed from", "meta": {"paper_id": "2006.00518v1", "chunk_index": 11, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_12", "text": "The last term can be written as (Cover and Thomas, 1991): I(Xi;Xj; C) = X xi X xj X c p(xi, xj, c)· log2 p(xi, xj)p(xi, c)p(xj, c) p(xi, xj, c)p(xi)p(xj)p(c) (7) An important remark has to be underlined about the sign of this term. It can be noticed from Equation 6 that a positive value of I(Xi; Xj; C) im- plies some redundancy between the features, while a negative value means that features exhibit some synergy (depending on whether their association brings respectively less or more than the addition of their own individual information). To evaluate the signiﬁcance of the features described in Section 3, the following measures are computed: • the relative intrinsic information of one individual feature I(Xi;C) H(C) , i.e. the proportion of relevant information conveyed by the feature Xi, • the relative redundancy between two features I(Xi;Xj;C) H(C) , i.e. the rate of their common relevant information, • the relative joint information of two features I(Xi,Xj;C) H(C) , i.e. the propor- tion of relevant information they convey together. For this, Equations 4 to 7 are calculated. Probability density functions are estimated by a histogram approach using bins uniformly distributed between the possible extremum values. The number of bins is set to 50 for each feature 17 dimension, which results in a trade-oﬀbetween an adequately high number for an accurate estimation, while keeping suﬃcient samples per bin. Class labels correspond to the presence (c = 1) or not (c = 0) of a creakiness in the voice, as indicated by the manual annotation. 4.2. Results First, the intrinsic discrimination power of each feature is investigated. Figure 6 displays the relative intrinsic information for the features described in Section 3 as well as for their ﬁrst and second derivatives. Interestingly, the most informative features are H2-H1 (around 0.35) and Peak-Prom (about 0.27), while the best of Ishi’s features is PwP fall with 0.19. The need to develop a F0 tracker speciﬁc to the analysis of creaky voice is clearly emphasised, as F0creak reaches 0.19 when F0 extracted with a standard F0 tracker only reaches 0.07. The F0 tracker used here is the summation of residual harmonics (SRH) algorithm (Drugman and Alwan, 2011). Note that both the low fundamental frequency and irregular temporal characteristics of creaky voice are known to cause trouble for F0 trackers. This was highlighted in a recent study (Raitio et al., 2013) which demonstrated that the majority of F0 trackers incorrectly determined a signiﬁcant proportion of creaky voice regions to be unvoiced. H2−H1 Peak−Prom IFP IPS PwP−fall PwP−rise F0 F0creak Energy norm Power std ZeroXrate 0 0.1 0.2 0.3 0.4 Relative intrinsic info. Static Delta Delta−delta Figure 6: Intrinsic mutual information of the features designed for creaky voice detection. The ﬁrst and second derivatives of these features are considered in addition to their static values. It turns out that the dynamics of the features for creaky voice charac- terisation only conveys a limited amount of information compared to their static version. Finally, it is worth noting that the widely-used Mel-frequency 18 cepstral coeﬃcients (MFCCs) achieve, at", "meta": {"paper_id": "2006.00518v1", "chunk_index": 12, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_13", "text": "features are considered in addition to their static values. It turns out that the dynamics of the features for creaky voice charac- terisation only conveys a limited amount of information compared to their static version. Finally, it is worth noting that the widely-used Mel-frequency 18 cepstral coeﬃcients (MFCCs) achieve, at best, a relative intrinsic informa- tion of only 0.04 and their usefulness for creaky voice detection is therefore negligible. Although Figure 6 gives some answers regarding the discrimination power of each feature independently, nothing is said about their possible comple- mentarity or redundancy. This aspect is now tackled in Table 2 for the KD and Ishi’s features (which were clearly among the most informative in Figure 6). The best combination of two features is H2-H1 with Peak-Prom (0.48). Nonetheless it is of interest to observe that the association of H2-H1 with Ishi’s features (except IFP) carries out comparable results (between 0.46 and 0.47), albeit that these latter features, individually, display less intrinsic in- formation. This is possible thanks to the weak redundancy of Ishi’s features with H2-H1 (varying only between 0.03 and 0.06). This good complemen- tarity can then be turned into an advantage for building up an automatic detection system. One should note that Ishi’s features, as opposed to the KD features, were not developed to be individual detectors of creaky voice, but rather their combination is what is relevant in this regard (i.e. detecting candidate creaky regions, and eliminating normal voiced and unvoiced can- didate regions). This is evident here by the negative value (-0.06) for relative redundancy between IFP and IPS, indicating a degree of synergy between the two parameters. However, aside from this pair features inside a category (i.e. KD or Ishi’s features) are quite redundant with each other while features from two diﬀerent categories exhibit an interesting complementarity. The good complementarity between the two groups of features indicates that they possibly reﬂect diﬀerent characteristics of the creaky production and might therefore be linked to the realisation of diﬀerent creaky patterns. Figure 7 investigates the speaker-dependent variability of the usefulness of both the KD and Ishi’s features. Note that to calculate the total MI for a set of features, we cope with redundancy as suggested in Drugman et al. (2007). Except HS for whom both groups of features seem to provide a comparable amount of information, it seems that speakers tend to predominantly use a creaky production well characterised either by the KD or by Ishi’s features. This observation tends to reinforce the possible existence of at least two creaky patterns as described by the two groups of features. 19 KD ISHI’S H2-H1 Peak-Prom F0creak IFP IPS PwP fall PwP rise KD H2-H1 0.35 0.48 0.38 0.37 0.46 0.47 0.46 Peak-Prom 0.14 0.27 0.37 0.31 0.37 0.37 0.36 F0creak 0.16 0.09 0.19 0.21 0.29 0.30 0.27 ISHI’S IFP 0.03 0.02 0.03 0.05 0.27 0.24 0.21 IPS 0.04 0.06 0.06 -0.06 0.16 0.23 0.22 PwP fall 0.06 0.08 0.08 0.00 0.11 0.19 0.21 PwP rise 0.05 0.06 0.07 0.00 0.09 0.13 0.15 Table 2: Mutual information-based measures for the", "meta": {"paper_id": "2006.00518v1", "chunk_index": 13, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_14", "text": "F0creak 0.16 0.09 0.19 0.21 0.29 0.30 0.27 ISHI’S IFP 0.03 0.02 0.03 0.05 0.27 0.24 0.21 IPS 0.04 0.06 0.06 -0.06 0.16 0.23 0.22 PwP fall 0.06 0.08 0.08 0.00 0.11 0.19 0.21 PwP rise 0.05 0.06 0.07 0.00 0.09 0.13 0.15 Table 2: Mutual information-based measures for the best creaky features. On the diagonal - orange: the relative intrinsic information I(Xi;C) H(C) . In the bottom-left part - light orange: the relative redundancy between the two considered features I(Xi;Xj;C) H(C) . In the top-right part - white: the relative joint information of the two considered features I(Xi,Xj;C) H(C) . 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 0.4 0.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 0.85 0.9 Mutual Information from the KD feature set Mutual Information from Ishi’s Feature Set BDL HS MV Swe-F Swe-M US-F1 US-M1 US-M2 US-F2 Jap-F1 Jap-F2 Figure 7: Mutual Information brought by the two sets of features for the various speakers of the database. 20 5. Automatic Detection of Creaky Voice This section aims at the integration of the acoustic characteristics de- scribed in 3 within a system for the automatic detection of creaky voice. This system is ﬁrst explained in Section 5.1 and the experimental protocol is presented in Section 5.2. The results of classiﬁcation are given in Section 5.3 which further strengthens our ﬁndings from the MI-based analysis described in Section 4.2. 5.1. Proposed System for Automatic Detection of Creaky Voice In order to automatically detect the creaky regions from an audio stream, the features described in Section 3 have been used together with their ﬁrst and second derivatives to account for their dynamics. More precisely, three sets of features have been considered: Ishi’s features, KD features, and the combination of both approaches. In the 3 cases, characteristics informative about audio activity (Energy Norm, Power std and ZeroXrate) have been appended to the feature vector in order to avoid spurious detections during silences or in sudden bursts (which may frequently occur in conversational data). Relying on these features, two types of classiﬁer were trained: a Binary Decision Tree (BDT) and an Artiﬁcial Neural Network (ANN). In the BDT, the separation of the two classes is done using a top-down approach where both classes are initially placed at the root node and then a series of binary questions are asked (to do with the input features) and for each question a new child node is created. This builds up the decision tree, the ends of which are leaf nodes. The commonly used Gini’s Diversity Index (GDI) was employed for the splitting criterion and splits are selected in order to reduce the GDI criterion. The splitting was stopped if the current node was pure (i.e. contained only observations of a single class) or if the branch node contains fewer than 10 observations. Note that the KD features used with the BDT classiﬁer was proposed in our previous work (Kane et al., 2013), where it was shown to outperform the original Ishi’s creaky voice detection system (Ishi et al., 2008a). Regarding the", "meta": {"paper_id": "2006.00518v1", "chunk_index": 14, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_15", "text": "class) or if the branch node contains fewer than 10 observations. Note that the KD features used with the BDT classiﬁer was proposed in our previous work (Kane et al., 2013), where it was shown to outperform the original Ishi’s creaky voice detection system (Ishi et al., 2008a). Regarding the ANN, our implementation relies on the Matlab Neural Network toolbox. The ANN is a feedforward network consisting of a single hidden layer consisting of neurons (ﬁxed to 16 in this work) utilising a tanh transfer function. The output layer is a simple neuron with a logarithmic sigmoid function suited for a binary decision. The training is conducted 21 using a standard error back-propagation algorithm (Bishop, 2006). Although not strictly true the ANN output is treated as a posterior probability in this study. 5.2. Experimental Protocol 5.2.1. Metric To assess the performance of the algorithms, we use the F1 score as a frame-level metric. This measure combines true positives (Tp), false posi- tives (Fp) and false negatives (Fn) into one single metric. This metric is particularly useful when analysing skewed datasets where the feature be- ing considered has a rather sparse occurrence (e.g., for laughter detection, Scherer et al., 2009), and is therefore well suited for assessing the perfor- mance of creak detection techniques. The metric is bound between 0 and 1, with 1 indicating perfect detection: F1 = 2 · Tp 2 · Tp + Fp + Fn ∈[0, 1] (8) 5.2.2. Threshold setting Once the ANN and BDT based classiﬁers have been trained it is then possible to set the decision threshold, α, in order to optimise performance on the training set (the splitting of training and test sets is outlined in Section 5.2.3 below). Note that the classiﬁers output the posterior probability, P1, of a given sample corresponding to class 1 (i.e. creaky). The standard binary decision is class 1 if P1 > α (otherwise class 0) and typically α is set to 0.5. However, for skewed datasets which contain a given class to be detected which displays sparse occurrence, this setting for α may not be optimal. In the present study, we systematically vary α in the range [0, 1], and set it to the value which maximises the F1 score on the training set. This decision threshold is then applied during subsequent testing. Note that we observed this threshold setting to have a very low inter-database sensitivity, as all speakers had their best F1 score for α in the vicinity of 0.3. 5.2.3. Cross-validation In order to evaluate the detection performance of the various methods, analysis was carried out on the speech databases described in Section 2. A leave one speaker out design was used whereby the speech data of a given speaker was held out for testing and the remainder of the speech data was used for training the classiﬁer. This procedure was repeated for each speaker. 22 5.3. Results Our detection results are summarised in Figure 8 for all methods and across all datasets. Note that the KD-BDT system is the one we proposed in a", "meta": {"paper_id": "2006.00518v1", "chunk_index": 15, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_16", "text": "and the remainder of the speech data was used for training the classiﬁer. This procedure was repeated for each speaker. 22 5.3. Results Our detection results are summarised in Figure 8 for all methods and across all datasets. Note that the KD-BDT system is the one we proposed in a previous paper (Kane et al., 2013). Firstly, the inﬂuence of the classiﬁer (BDT vs. ANN) is analysed. Out of the 22 comparisons (11 speakers and two feature sets), it turns out that BDT is outperformed by ANN in all but one occasion. This ﬁnding was supported by evidence from a two-way ANOVA (with F1 score treated as dependent variable, and classiﬁer type and feature group as the two independent variables) which revealed a highly signiﬁcant eﬀect of classiﬁer type [F(1,50) = 23.0, p < 0.001]. As a consequence, when considering the full feature set, only the ANN was used as a classiﬁer. The superiority of the ANN classiﬁer over BDT is likely due to its ability to utilise the interaction of the various features in discriminating creaky voice from other speech regions. One exception where the ANN classiﬁer is actually outperformed is in the case of the Ishi’s features for speaker MV. MV actually has a particularly low- pitched voice, with an average f0 in the vicinity of 80 Hz. The ANN trained classiﬁer with Ishi’s features produces a large number of false positives with many non-creaky low-pitched regions being mistakenly determined as creaky. We speculate that the small number of Ishi’s features are actually over-ﬁt to speakers who are not particularly low-pitched using the ANNs (as this type of speech is not very well represented in the training data) whereas with this setup the decision trees do not over-ﬁt to the same extent. Regarding the relevance of the features, results in Figure 8 corroborate our conclusions from Section 4: the KD features are more informative for the discrimination of creaky phonation. In general, they achieved much higher F1 scores than when using Ishi’s features. This ﬁnding is supported by evidence from the same two-way ANOVA testing mentioned above, where a signiﬁcant eﬀect of feature group was observed [F(2,50) = 11.6, p < 0.001]. Further, a subsequent pairwise comparison using Tukey’s Honestly Signiﬁcant Diﬀer- ence (HSD) test indicated a signiﬁcant diﬀerence (p < 0.001) between the KD and Ishi feature groups. The combination of both feature sets (All-ANN ) leads to the best overall detection results, with the Tukey HSD test revealing a signiﬁcant improve- ment compared to Ishi-ANN (p < 0.05). Note, however, that although the mean F1 score is higher for All-ANN compared with KD-ANN the diﬀerence does not achieve signiﬁcance (p = 0.92). Notice also that compared to our 23 previous method proposed in Kane et al. (2013) (KD BDT), a substantial increase of performance has been achieved. One can observe a certain degree of between speaker varibility in detection performance. In particular, for speaker US-F2, the best classiﬁer (i.e. All- ANN) produces the lowest F1 score (0.57) for any speaker. For this speaker, it was noticed that the", "meta": {"paper_id": "2006.00518v1", "chunk_index": 16, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_17", "text": "BDT), a substantial increase of performance has been achieved. One can observe a certain degree of between speaker varibility in detection performance. In particular, for speaker US-F2, the best classiﬁer (i.e. All- ANN) produces the lowest F1 score (0.57) for any speaker. For this speaker, it was noticed that the audio contained sounds resulting from collisions of the microphone with the speaker’s mouth, and this led to a relatively high volume of false detections. For the interested researcher, we have made the resulting All ANN algo- rithm freely available on the web 2. BDL HS MV Swe−F Swe−M US−F1 US−M1 US−M2 US−F2 Jap−F1 Jap−F2 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 F1 score Ishi BDT Ishi ANN KD BDT KD ANN ALL ANN Figure 8: Automatic detection results, summarised as F1 score, across the various speakers. In order to conﬁrm the possible presence of various patterns in creaky voice, and based on our ﬁndings from Section 4, we inspected whether creaky events have been detected or not using Ishi’s features (Ishi-ANN system) or using our KD features (KD-ANN system). Each event can be assigned to the following categories: i) it is missed by both systems, ii) it is detected using Ishi’s features but not using KD ones, iii) it is detected using KD features but not using Ishi’s ones, iv) it is detected by both systems. Results across all datasets are exhibited in Figure 9. It can be ﬁrst observed that missing rates 2GLOAT toolkit: http://tcts.fpms.ac.be/~drugman/Toolbox/, the Voice Anal- ysis Toolkit: http://www.tcd.ie/slscs/postgraduate/phd-masters-research/ student-pages/johnkane.php, and the COVAREP project: https://github.com/ covarep/covarep 24 are generally low (below 10% for 9 out of the 11 speakers). Note that the false alarm rates are not explicitly reported in this paper (though of course they signiﬁcantly aﬀect the F1 metric), but we noticed that a good trade- oﬀbetween misses and false alarms was obtained for the proposed method. Therefore, the false alarm rates are comparable to the miss rates achieved in Figure 9. Secondly, it is interesting to see how Figure 9 corroborates our results from Figure 7. It can be indeed seen that speakers for whom MI values were high for KD features and lower for Ishi’s features have an important proportion of events detected using KD features but not using Ishi’s ones (while the contrary is not true), and vice versa. This provides a further experimental support which consolidates the existence of at least two patterns that speakers might use to produce creaky voice. An investigation into these patterns will be given in Section 6 through an analysis of the excitation signal. Figure 9: For each speaker, repartition of the events: i) missed when using either KD or Ishi’s features, ii) detected when using Ishi’s features but missed using KD features, iii) detected when using KD features but missed using Ishi’s features, iv) detected when using either KD or Ishi’s features. 6. Creaky Patterns Analysis Our ﬁndings from Sections 4 and 5 tend to indicate that the production of creaky voice does not involve an unique strategy and that speakers use at least", "meta": {"paper_id": "2006.00518v1", "chunk_index": 17, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_18", "text": "when using KD features but missed using Ishi’s features, iv) detected when using either KD or Ishi’s features. 6. Creaky Patterns Analysis Our ﬁndings from Sections 4 and 5 tend to indicate that the production of creaky voice does not involve an unique strategy and that speakers use at least two creaky excitation patterns (corroborating previous ﬁndings in the literature; Redi and Shattuck-Hufnagel, 2001; Ishi et al., 2010, 2008a, 2007). Based on the events exclusively detected using either Ishi’s or KD features, 25 we visually inspected a large number of creaky excitation segments from our 11 speakers. From this analysis, it turned out that creaky excitation signals could be categorised into three patterns: highly irregular temporal character- istics, fairly regular temporal characteristics with strong secondary excitation peaks and fairly regular temporal characteristics without strong secondary excitations. Note that the ﬁrst pattern closely corresponds to the ‘multi- pulsed’ (Ishi et al., 2008a) or ﬁrst category (Redi and Shattuck-Hufnagel, 2001) patterns previously reported. The second and third patterns are es- sentially a subdivision of the ‘single-pulsed’ or second category previously reported patterns. A more complete description of these three patterns follows in Section 6.1. In this description we focus on the LP-residual waveform as this signal, in the present context, can reveal some important characteristics not obvious from the speech signal and also as our research is concerned with the mod- elling of the residual excitation patterns of speech for statistical parametric speech synthesis (Drugman and Dutoit, 2012; Drugman et al., 2012a). A quantitative study of the identiﬁed creaky patterns is eventually provided in Section 6.2. 6.1. Qualitative Analysis of the Identiﬁed Creaky Patterns Three patterns have been identiﬁed across our 11 speakers. There are now thoroughly described. 6.1.1. Pattern A: Highly irregular temporal characteristics An illustration of Pattern A is displayed in Figure 10. During such a creaky production, the LP residual signal exhibits clear discontinuities with a highly irregular temporal structure. These peaks appear sporadically, and the inter-peak duration does not seem to follow a clear deterministic rule as it is the case for regular patterns. Often occuring within this pattern are spells of diplophonia, with amplitude and duration variation in successive pulses. Note, however, that a stable diplophonic pattern (such as that described in Klatt and Klatt, 1990) was not observed to be sustained throughout a creaky voice segment. In the case of such a creaky production, the individual KD features turn out to be somewhat erratic: i) since the pattern is very irregular, the F0 and subsequent H2-H1 parameter extraction becomes less straightforward; ii) excitation peaks are not prominent at the scale used for the calculation of Peak-Prom, and further because often a large proportion of the glottal pulses displays a duration corresponding to an F0 higher than 26 70 Hz. Contrastingly, the combination of the intra-frame periodicity (IFP) and inter-pulse similarity (IPS) parameters proposed by Ishi et al. (2008a) are eﬀective for the detection of such a phonation as IFP values become very close to 0 in such regions, compared to the higher values (closer to 1) in more", "meta": {"paper_id": "2006.00518v1", "chunk_index": 18, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_19", "text": "Hz. Contrastingly, the combination of the intra-frame periodicity (IFP) and inter-pulse similarity (IPS) parameters proposed by Ishi et al. (2008a) are eﬀective for the detection of such a phonation as IFP values become very close to 0 in such regions, compared to the higher values (closer to 1) in more regular phonatory regions, while IPS values stay high in both regions. 0 0.02 0.04 0.06 0.08 0.1 0.12 −0.5 0 0.5 1 Time (seconds) Amplitude Figure 10: LP-residual of a segment of a speech signal containing creaky voice with irreg- ular temporal characteristics. 6.1.2. Pattern B: Fairly regular temporal characteristics with strong sec- ondary excitation peaks An example of Pattern B is displayed in Figure 11. It can be observed that the LP residual signal exhibits fairly regular temporal characteristics, with a stable pattern comprising two clear discontinuities. Note that we are not claiming that these patterns display strictly the same extent of regular temporal characteristics as modal voice, but simply that they are consid- erably more regular than those characteristics in Pattern A. In this ﬁgure, these two peaks have been annotated for each glottal cycle: the peak indi- cated by a red star corresponds to the Glottal Closure Instant (GCI), while the peak indicated by a circle is called the secondary excitation peak. Note that the SE-VQ algorithm (Kane and Gobl, 2013) which is an extension of the SEDREAMS method (Drugman et al., 2012c) is used for detecting GCIs and that secondary peaks are simply measured as the strongest peak between adjacent GCIs. These secondary peaks (black circles) sometimes occur due to secondary laryngeal excitations, but very often they stem from sharp dis- continuities at glottal opening, following a long glottal closed period. This is in fact the case in Figure 11 and this observation was supported by evidence 27 from both the corresponding EGG signal as well as the glottal source deriva- tive estimated by glottal inverse ﬁltering. This appears to be consistent with the “double-beated” pattern observed in Ishi et al. (2007). 0 0.02 0.04 0.06 0.08 0.1 0.12 −0.5 0 0.5 1 Time (seconds) Amplitude GCI GOI Figure 11: LP-residual of a segment of a speech signal containing creaky voice with very long glottal pulses, reasonably regular temporal characteristics and strong secondary peaks. Peaks corresponding to glottal closure instants are marked with a red star while glottal opening instant peaks are marked with a black circle. 5 10 15 20 25 0 10 20 30 Glottal period (ms) Opening period (ms) BDL 10 15 20 25 30 0 10 20 30 Glottal period (ms) Opening period (ms) MV Figure 12: Distributions, for two male speakers who mostly used pattern B, of the glottal opening period as a function of the produced pitch period. The two speakers whose creaky voice regions were detected mainly due to the KD features and not Ishi’s features (refer to Figure 7), BDL and MV, were found to mostly display this creaky voice pattern. For these speakers, 28 considering the duration of the glottal periods against the duration of the glottal opening phase", "meta": {"paper_id": "2006.00518v1", "chunk_index": 19, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_20", "text": "creaky voice regions were detected mainly due to the KD features and not Ishi’s features (refer to Figure 7), BDL and MV, were found to mostly display this creaky voice pattern. For these speakers, 28 considering the duration of the glottal periods against the duration of the glottal opening phase (i.e. duration from a secondary peak to the consecutive GCI) shown in Figure 12, one can observe the striking trend of a relatively stable glottal opening period despite a widely varying glottal period. This indicates that the glottal closed phase is the main determinant of the glottal period (and hence F0), and conﬁrms our observation from (Drugman et al., 2012a). Note that the detection of GCIs and GOIs for this type of phonation is often extremely diﬃcult (Kane and Gobl, 2013) and consequently spuri- ous values will inevitably have occurred in the analysis which would have inﬂuenced these plots (i.e. Figure 12). Both the very prominent secondary excitation peaks and this trend ob- served in Figure 12 likely have a signiﬁcant impact on the perceptual quality of these creaky voice patterns. Indeed we have found that for these two speakers, considering these features in the design of an excitation model brought about a clear improvement in the rendering of the voice quality in the synthesis experiments reported in Drugman et al. (2012a). 6.1.3. Pattern C: Fairly regular temporal characteristics without strong sec- ondary excitation peaks The third pattern of creaky voice observed in the present study (an ex- ample of which is shown in Figure 13), similarly to Pattern B, likely corre- sponds to the ‘single-pulsed’ (Ishi et al., 2008a) or the second category (Redi and Shattuck-Hufnagel, 2001) pattern previously reported in the literature (Redi and Shattuck-Hufnagel, 2001; Ishi et al., 2008a). As with Pattern B, the temporal characteristics observed here are fairly regular, particularly in comparison to Pattern A. The perceptual eﬀect of creaky voice is mainly brought about here by the low F0. However, unlike Pattern B, Pattern C does not display strong secondary excitation peaks. The consequence of this for speech synthesis is that the modelling of this excitation pattern can be carried out in a similar fashion to modal phonation, and its proper rendering is conditioned mainly on the generation of a suitable F0 contour. 6.2. Quantitative Analysis of the Identiﬁed Creaky Patterns In the previous sections, we have identiﬁed three main patterns of creaky voice. These patterns have been described and qualitatively analyzed. We now perform a quantitative study by investigating how frequent these pat- terns are observed, what their speaker dependency is and how eﬀective the 29 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 −0.5 0 0.5 1 Time (seconds) Amplitude Figure 13: LP-residual of a segment of a speech signal containing creaky voice with very long glottal pulses, reasonably regular temporal characteristics and no strong secondary peaks. systems of Section 5 are at detecting these patterns. For this, the same por- tion of the corpus introduced in Section 2 was annotated by the ﬁrst two authors. This dataset consists of 5 minutes", "meta": {"paper_id": "2006.00518v1", "chunk_index": 20, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_21", "text": "voice with very long glottal pulses, reasonably regular temporal characteristics and no strong secondary peaks. systems of Section 5 are at detecting these patterns. For this, the same por- tion of the corpus introduced in Section 2 was annotated by the ﬁrst two authors. This dataset consists of 5 minutes of speech for each of the 11 speakers. The speech and residual waveforms of each creaky segment (which was manually labelled as described in Section 2.5) were presented to the an- notator. Based on this visualization, the annotator had to assign the creaky segment one (and only one) of the three identiﬁed creaky patterns, strictly following the criteria described in Section 6.1. The inter-annotator agreement rate was estimated using the Krippen- dorﬀ’s alpha (Krippendorﬀ, 2012). Our results report a Krippendorﬀ’s alpha value of 0.718 which indicates a reasonably high level of inter-rater agree- ment. The confusion matrix between the 3 patterns is shown in Table 3. Thre greater risk of confusion concerns the pairs (A,B) and (A,C). This is because some cases were particularly ambiguous as their degree of regularity was neither totally regular nor totally irregular, which inherently induces to some subjectivity issues and might explain the discrepancies across annota- tors. Based on these new annotations, we have ﬁrst investigated how frequent do speakers use the diﬀerent patterns. For this, we only consider the manual labels for which both annotators agreed. The repartition of the creaky usage per speaker is displayed in Figure 14. First of all, a considerable speaker dependency can be observed. One ﬁrst conclusion is that pattern A is the most frequently used pattern, except for male speakers BDL and MV whose 30 Pattern A Pattern B Pattern C Pattern A 97.66 1.87 0.47 Pattern B 33.22 65.37 1.41 Pattern C 19.42 2.16 78.42 Table 3: Confusion matrix (in %) for the three identiﬁed creaky patterns across the two annotators. dominant pattern is clearly B. This is an important observation as the creaky vocoder system we have developed in (Drugman et al., 2012a), and which was further integrated into HMM-based speech synthesis in (Raitio et al., 2013), is based on the modeling of pattern B. This is because in these latter studies, only voices from the TTS set have been considered (BDL, HS and MV) and pattern B appeared to be the most frequent characteristic of creaky voice. A posteriori, this seems to be biased as pattern A turns out to be ubiquitously used in 9 out of our 11 speakers. BDL HS MV Swe−F Swe−M US−F1 US−M1 US−M2 US−F2 Jap−F1 Jap−F2 0 10 20 30 40 50 60 70 80 90 Proportion((%) Pattern(A Pattern(B Pattern(C Figure 14: Repartition (in %) of the creaky pattern usage for our 11 speakers. A second observation is that although a great inter-speaker variability is noticed in Figure 14, speakers could be categorized into similar groups: • BDL and MV who predominantly use pattern B, • HS, Swe-F, US-M1, Jap-F1 and Jap-F2 who use pattern A with more than 75%, the rest being shared between B and C (B being sometimes even", "meta": {"paper_id": "2006.00518v1", "chunk_index": 21, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_22", "text": "inter-speaker variability is noticed in Figure 14, speakers could be categorized into similar groups: • BDL and MV who predominantly use pattern B, • HS, Swe-F, US-M1, Jap-F1 and Jap-F2 who use pattern A with more than 75%, the rest being shared between B and C (B being sometimes even not used at all), • Swe-M, US-F1, US-M2 and US-F2 who use pattern C between 33 and 42%, and pattern A almost exclusively in other cases. 31 As a consequence, it is essential that a parametric speech synthesizer of creaky voice incorporates a proper modeling of the irregularities of pattern A. Note that the modeling of pattern B was already proposed in (Drug- man et al., 2012a) and that the excitation in pattern C is very similar to the excitation signals in modal voice but is merely characterized by much lower F0 values. This latter could therefore be synthesized using any existing excitation modeling. Finally, we address how the performance of the creaky voice detection systems developed in Section 5 is aﬀected across the three identiﬁed creaky patterns. For this, we run classiﬁcation experiments over the patterns where both annotators agreed. It is worth emphasizing at this point that our goal is not to build up classiﬁcation systems able to discriminate between the diﬀerent creaky patterns, but rather to investigate how the systems of Section 5 perform for each creaky pattern. As a performance measure, we here use miss rate (also called false negative error rate). Indeed, metrics based on false alarm measurements (and also for F1 score) would be meaningless here, since these false alarms could not be assigned to a speciﬁc pattern. Results are shown in Figure 15 for the 3 ANN-based systems using either Ishi, KD or All feature sets. Across all three patterns there is a consistent trend, with All features achieving the lowest classiﬁcation miss rate, KD fea- tures with the second lowest and Ishi features with the highest. There are also considerable inter-pattern diﬀerences. Interestingly for pattern A, which involves a highly irregular temporal excitation pattern, KD features result in a lower miss rate compared to the Ishi features classiﬁer. Despite the in- dividual KD features displaying relatively erratic contours in these regions, their combination within the ANN classiﬁer achieves eﬀective detection of this pattern. Pattern B, which contains excitation patterns with strong sec- ondary peaks, is rarely missed by the three classiﬁers. The highest overall miss rates are observed for Pattern C. 7. Conclusion This paper addressed the automatic analysis of the excitation patterns used during the production of creaky voice. The goal was threefold: i) to as- sess the relevance of acoustic features speciﬁcally designed to characterise creaky voice; ii) to integrate these combined features within an eﬃcient creaky voice detection system; iii) based on these results, to categorize the 32 Pattern A Pattern B Pattern C 0 20 40 60 Event level results Miss rate (%) KD ISHI ALL Figure 15: Classiﬁcation miss rate results across the 3 identiﬁed creaky patterns. observed temporal creaky patterns, and to analyse them both qualitatively and quantitatively.", "meta": {"paper_id": "2006.00518v1", "chunk_index": 22, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_23", "text": "on these results, to categorize the 32 Pattern A Pattern B Pattern C 0 20 40 60 Event level results Miss rate (%) KD ISHI ALL Figure 15: Classiﬁcation miss rate results across the 3 identiﬁed creaky patterns. observed temporal creaky patterns, and to analyse them both qualitatively and quantitatively. This study was led on a large amount of manually-annotated data with a variety of languages, speakers and recording conditions (read vs conver- sational speech). Our analysis was based on the use of acoustic features which were previously speciﬁcally designed for the characterisation of creaky voice. Our approach consisted of three consecutive stages. In the ﬁrst one, the relevance of each feature individually as well as their complementar- ity/redundancy were assessed relying on mutual information-based measures. Two groups of features were interestingly noticed to be complementary: fea- tures proposed by Ishi and in one of our previous studies. This is initial evidence supporting the notion of the presence of several creaky patterns as it shows that these features describe diﬀerent characteristics of creaky voice. In the second step, these features were the input of two classiﬁers (a binary decision tree and an artiﬁcial neural network) with the goal of automatic creaky voice detection. Two conclusions were drawn from our classiﬁcation experiments. First the use of the combined feature set with neural networks led to an appreciable improvement over the state of the art across our 11 speakers. Secondly, an analysis of the detected creaky events strengthened our ﬁndings about the evident existence of several creaky patterns. Finally, in the last stage of our approach, we inspected a large number of creaky excitation signals detected exclusively using one of the two sets of features, as supported by the two ﬁrst experiments. Our analysis revealed 3 distinct creaky excitation patterns. In pattern A, the LP-residual signal exhibits important discontinuities occurring spo- 33 radically and whose temporal structure does not seem to follow any obvious deterministic rule. Contrastingly, the temporal characteristics in patterns B and C is much more regular and often highly periodic. In both patterns, the discontinuity at the glottal closure instants (GCIs) is well marked, and the glottal period (which separates two consecutive GCIs) is much larger than in modal speech. In the case of pattern B, the LP-residual displays one ex- tra peak per glottal cycle in addition to the discontinuity at GCIs. These secondary excitation peaks are often likely due to the sudden opening of the glottis. Interestingly, we observed for two speakers who mostly used pattern B, that the opening period remained almost constant independently of the produced pitch. Finally, these secondary peaks were not observed in the case of pattern C whose characteristics are similar to the excitation used in modal voiced phonation, but with F0 values falling below 50 Hz. A quantitative analysis of the identiﬁed creaky patterns was carried out. Three main conclusions could be drawn from our experiments: i) a Krip- pendorﬀ’s alpha value of 0.718 was obtained when annotating the patterns, which indicates a reasonably high level of inter-rater agreement; ii) a consid- erable", "meta": {"paper_id": "2006.00518v1", "chunk_index": 23, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_24", "text": "below 50 Hz. A quantitative analysis of the identiﬁed creaky patterns was carried out. Three main conclusions could be drawn from our experiments: i) a Krip- pendorﬀ’s alpha value of 0.718 was obtained when annotating the patterns, which indicates a reasonably high level of inter-rater agreement; ii) a consid- erable inter-speaker variability was observed in the way the creaky patterns are used; iii) the ANN-based classiﬁer using both Ishi’s and our proposed features misses creaky events in the following proportion: 18.5% for pattern A, 4.4% for pattern B and 25% for pattern C. For future work, it would be worth further investigating the eﬀect of diﬀerent classiﬁer settings. Such analysis could involve optimising classiﬁer parameters (e.g., number of neurons in the hidden layer), assessing diﬀerent classiﬁer types and also determining the value of fusing multiple classiﬁers. We intend to utilise the new detection algorithm as part of our on-going developments incorporating creaky voice in speech synthesis and voice quality transformation of synthetic vocies. Indeed, the vocoder we have developed in (Drugman et al., 2012a) models only pattern B. However, our ﬁndings in the present study, in particular that pattern A is dominantly used by 9 out of the 11 speakers, suggest that further eﬀort needs to be invested into modelling the highly irregular excitation characteristics of Pattern A. It is therefore essential to develop an excitation model able to model the temporal irregularities as found in pattern A. It is also envisaged that the algorithm will be used to study the use of creaky voice in conversational and expressive speech. Furthermore, as creaky voice is thought to be associated with certain aﬀective states, its automatic detection may bring signiﬁcant beneﬁt to applications like emotion recognition. Another direction of future 34 work could involve carrying out analysis-synthesis experiments to help better determine the acoustic characteristics required for the perception of creaky voice. 8. Acknowledgements The ﬁrst author is supported by FNRS. The second and third authors are supported by the Science Foundation Ireland Grant 09/IN.1/I2631 (FAST- NET) and the Irish Department of Arts, Heritage and the Gaeltacht (ABAIR project). The authors would like to thank Jens Edlund for providing us with the Spontal corpus, Ikuko Patricia Yuasa for providing us with the US dataset and Thomas Magnuson for sharing the Japanese data with us. References Bishop, C. M., 2006. Pattern Recognition and Machine Learning (Informa- tion Science and Statistics). Springer. Blomgren, M., Chen, Y., Ng, M., Gilbert, H., 1998. Acoustic, aerodynamic, physiologic, and perceptual properties of modal and vocal fry registers. Journal of the Acoustical Society of America 103 (5), 2649–2658. B¨ohm, T., Both, Z., N´emeth, G., 2010. Automatic classiﬁcation of regular vs. irregular phonation types. Advances in nonlinear speech processing, 43–50. B¨ohm, T., Shattuck-Hufnagel, S., 2007. Listeners recognize speakers’ habit- ual utterance-ﬁnal voice quality. Proceedings of ParaLing07, 29–34. Carlson, R., Gustafson, K., Strangert, E., 2006. Cues for hesitation in speech synthesis. Proceedings of Interspeech, Pittsburgh, USA, 1300–1303. Cover, T., Thomas, J., 1991. Elements of information theory. In: Wiley Series in Telecommunications, New York. Crystal, T., House, A., 1988. The duration of american", "meta": {"paper_id": "2006.00518v1", "chunk_index": 24, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_25", "text": "voice quality. Proceedings of ParaLing07, 29–34. Carlson, R., Gustafson, K., Strangert, E., 2006. Cues for hesitation in speech synthesis. Proceedings of Interspeech, Pittsburgh, USA, 1300–1303. Cover, T., Thomas, J., 1991. Elements of information theory. In: Wiley Series in Telecommunications, New York. Crystal, T., House, A., 1988. The duration of american english stop conso- nants: an overview. Journal of Phonetics 16, 285–294. Dilley, L., Shattuck-Hufnagel, S., Ostendorf, M., 1996. Glottalization of word-initial vowels as a function of prosodic structure. Journal of Pho- netics 24, 423–444. 35 Drugman, T., 2013. Residual Excitation Skewness for Automatic Speech Po- larity Detection. IEEE Signal Processing Letters 20, 387–390. Drugman, T., Alwan, A., 2011. Joint Robust Voicing Detection and Pitch Estimation Based on Residual Harmonics. Proceedings of Interspeech, Flo- rence, Italy, 1973–1976. Drugman, T., Dutoit, T., 2012. The deterministic plus stochastic model of the residual signal and its applications. IEEE Transactions on Audio Speech and Language processing, 20 (3), 968–981. Drugman, T., Gurban, M., Thiran, J.-P., 2007. Relevant feature selection for audio-visual speech recognition. IEEE International Workshop on Multi- media Signal Processing, 179–182. Drugman, T., Kane, J., Gobl, C., 2012a. Modeling the creaky excitation for parametric speech synthesis. Proceedings of Interspeech, Portland, Oregon, USA. Drugman, T., Kane, J., Gobl, C., 2012b. Resonator-based creaky voice de- tection. Proceedings of Interspeech, Portland, Oregon, USA. Drugman, T., Kane, J., Raitio, T., Gobl, C., 2013. Prediction of Creaky Voice from Contextual Factors. Proceedings of ICASSP. Drugman, T., Thomas, M., Gudnason, J., Naylor, P., Dutoit, T., 2012c. Detection of glottal closure instants from speech signals: a quantitative re- view. IEEE Transactions on Audio Speech and Language processing 20 (3), 994–1006. Edlund, J., Beskow, J., Elenius, K., Hellmer, K., Str¨ombergsson, S., House, D., 2010. Spontal: a Swedish spontaneous dialogue corpus of audio, video and motion capture. Proceedings of LREC, Malta, 2992–2995. Edmondson, J., Esling, J., 2006. The valves of the throat and their function- ing in tone, vocal register and stress: laryngoscopic case studies. Phonology 23 (2), 157. Elliot, J. R., 2002. The application of a Bayesian approach to auditory anal- ysis in forensic speaker identiﬁcation. Proceedings of the 9th Australian International Conference on Speech Science and Technology, 315–320. 36 Espy-Wilson, C., Manocha, S., Vishnubhotla, S., 2006. A new set of features for text-independent speaker identiﬁcation. Proceedings of Interspeech (IC- SLP), Pittsburgh, Pennsylvania, USA, 1475–1478. Gobl, C., N´ı Chasaide, A., 1992. Acoustic characteristics of voice quality. Speech Communication 11, 481–490. Gobl, C., N´ı Chasaide, A., 2003. The role of voice quality in communicating emotion, mood and attitude. Speech Communication 40, 189–212. Hollien, H., Wendahl, R. W., 1968. Perceptual study of vocal fry. Journal of the Acoustical Society of America 47 (3), 506–509. Huan, L., Motoda, H., 1998. Feature selection for knowledge discovery and data mining. In: The Springer International Series in Engineering and Computer Science. Vol. 454. Huber, D., 1988. Aspects of the communicative function of voice in text intonation. Ph.D. thesis, University of Goteborg/Lund. Ishi, C. T., Ishiguro, H., Hagita, N., 2010. Acoustic, electroglottographic and paralinguistic analysis of “Rikimi” in expressive speech. Proceedings of Speech Prosody, Chicago, USA, 1–4. Ishi, C., Sakakibara, K.,", "meta": {"paper_id": "2006.00518v1", "chunk_index": 25, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_26", "text": "Science. Vol. 454. Huber, D., 1988. Aspects of the communicative function of voice in text intonation. Ph.D. thesis, University of Goteborg/Lund. Ishi, C. T., Ishiguro, H., Hagita, N., 2010. Acoustic, electroglottographic and paralinguistic analysis of “Rikimi” in expressive speech. Proceedings of Speech Prosody, Chicago, USA, 1–4. Ishi, C., Sakakibara, K., Ishiguro, H., Hagita, N., 2008a. A method for au- tomatic detection of vocal fry. IEEE Transactions on Audio, Speech, and Language Processing 16 (1), 47–56. Ishi, C. T., Ishiguro, H., Hagita, N., 2008b. Automatic extraction of par- alinguistic information using prosodic features related to F0, duration and voice quality. Speech communication 50 (6), 531–543. Ishi, C. T., Ishiguro, H., Hagita, N., 2007. Acoustic and EGG analysis of pressed phonation. Proceedings of ICPhS, Saarbrucken, Germany, 2057– 2060. Kane, J., Drugman, T., Gobl, C., 2013. Improved automatic detection of creak. Computer Speech and Language 27(4), 1028–1047. Kane, J., Gobl, C., 2013. Evaluation of glottal closure instant detection in a range of voice qualities. Speech Communication 55(3), 397–414. 37 Kane, J., Papay, K., Hunyadi, L., Gobl, C., 2011. On the use of creak in hungarian spontaneous speech. Proceedings of ICPhS, 1014–1017. Klatt, D., Klatt, L., 1990. Analysis, synthesis, and perception of voice quality variations among female and male talkers. Journal of the Acoustical Society of America 87 (2), 820–857. Krippendorﬀ, K., 2012. Content Analysis, An Introduction to its Methodol- ogy. Thousand Oaks, CA: Sage Publications, third edition. Laver, J., 1980. The Phonetic Description of Voice Quality. Cambridge Uni- versity Press. Moisik, S., Esling, J., 2011. The ‘whole’ larynx approach to laryngeal fea- tures. Proceedings of ICPhS, Hong Kong, 1406–1409. Ogden, R., 2001. Turn transition, creak and glottal stop in Finnish talk- in-interaction. Journal of the International Phonetic Association 31 (1), 139–152. Raitio, T., Kane, J., Drugman, T., 2013. HMM-based synthesis of creaky voice. Proceedings of Interspeech. Redi, L., Shattuck-Hufnagel, S., 2001. Variation in the realization of glottal- ization in normal speakers. Journal of Phonetics 29, 407–429. Scherer, S., Schwenker, F., Campbell, N., Palm, G., 2009. Multimodal laugh- ter detection in natural discourses. Human centered robot systems, Cog- nitive Systems Monographs 6, 111–120. Shannon, C., 1948. A mathematical theory of communication. Bell System Technical Journal 27, 379–423. Silen, H., Helander, E., Nurminen, J., Gabbouj, M., 2009. Parameterization of vocal fry in HMM based speech synthesis. Proceedings of Interspeech 2009, Brighton, UK, 1775–1778. Slifka, J., 2006. Some physiological correlates to regular and irregular phona- tion at the end of an utterance. Journal of Voice 20 (2), 171–186. 38 Surana, K., Slifka, J., 2006a. Acoustic cues for the classiﬁcation of regular and irregular phonation. Proceedings of Interspeech (ICSLP), Pittsburgh, Pennsylvania, USA, 693–699. Surana, K., Slifka, J., 2006b. Is irregular phonation a reliable cue towards the segmentation of continuous speech in American English. Proceedings of Speech Prosody, Dresden, Germany, Paper 177. Titze, I., 1994. Vocal registers. In: Principles of Voice Production. Englewood Cliﬀs, MJ: Prentice Hall, pp. 252–259. Vishnubhotla, S., Espy-Wilson, C., 2006. Automatic detection of irregular phonation in continuous speech. Proceedings of Interspeech, Pittsburgh, USA, 949–952. Wolk, L., Abdelli-Beruh, N., 2012. Habitual use of vocal fry in young", "meta": {"paper_id": "2006.00518v1", "chunk_index": 26, "filename": "2006.00518v1.pdf", "chunk_length": 512}}
{"id": "2006.00518v1_chunk_27", "text": "Paper 177. Titze, I., 1994. Vocal registers. In: Principles of Voice Production. Englewood Cliﬀs, MJ: Prentice Hall, pp. 252–259. Vishnubhotla, S., Espy-Wilson, C., 2006. Automatic detection of irregular phonation in continuous speech. Proceedings of Interspeech, Pittsburgh, USA, 949–952. Wolk, L., Abdelli-Beruh, N., 2012. Habitual use of vocal fry in young adult female speakers. Journal of Voice 26 (3), 111–116. Yanushevskaya, I., Gobl, C., N´ı Chasaide, A., 2005. Voice quality and f0 cues for aﬀect expression. Proceedings of Interspeech, Lisbon, Portugal, 1849–1852. Yuasa, I. K., 2010. Creaky voice: A new feminine voice quality for young urban-oriented upwardly mobile American women? American Speech 85 (3), 315–337. Zue, V., Laferriere, M., 1979. Acoustic study of medial /t, d/ in american english. Journal of the Acoustical Society of America 66(4), 1039–1050. 39", "meta": {"paper_id": "2006.00518v1", "chunk_index": 27, "filename": "2006.00518v1.pdf", "chunk_length": 128}}
{"id": "2006.00533v1_chunk_0", "text": "Benchmarking BioRelEx for Entity Tagging and Relation Extraction Abhinav Bhatt Amelia Science RnD, IPsoft Bengaluru, KA 560066 abhatt@ipsoft.com Kaustubh D. Dhole Amelia Science RnD, IPsoft New York, NY 10004 kdhole@ipsoft.com Abstract Extracting relationships and interactions be- tween different biological entities is still an extremely challenging problem but has not re- ceived much attention as much as extraction in other generic domains. In addition to the lack of annotated data, low benchmarking is still a major reason for slow progress. In or- der to ﬁll this gap, we compare multiple exist- ing entity and relation extraction models over a recently introduced public dataset, BioRelEx of sentences annotated with biological entities and relations. Our straightforward benchmark- ing shows that span-based multi-task architec- tures like DYGIE show 4.9% and 6% absolute improvements in entity tagging and relation ex- traction respectively over the previous state-of- art and that incorporating domain-speciﬁc in- formation like embeddings pre-trained over re- lated domains boosts performance. 1 Introduction Extracting biological entities and determining rela- tionships between them is particularly challenging due to the heterogeneous nomenclature of such entities. Besides, alongwith the rapid changes in molecular biology, there is a lack of consensus over a standardized dataset. While Khachatrian et al. (2019) attempt to address the issue of stan- dardization by introducing BioRelEx, there is still plenty of room for improving the tagging and rela- tion extraction of these entities. Especially when there has been tremendous progress achieved by multi-task span-based archi- tectures to extract entities and relations in various datasets within domains like news and scientiﬁc articles (Luan et al., 2019; Wadden et al., 2019), we attempt to seek if this success can be emulated to recent standardized biological literature, like BioRelEx. We compare a range of classiﬁers; previously common generic ones like CRF and BiLSTM as Figure 1: BioRelEx has 33 types of entities including annotations of multiple overlapping entities. (Entity Tagging) Figure 2: A sample Binding Interaction in BioRelEx between two BioRelEx entities (Relation Extraction) well as recent state-of-the-art (SOA) multi-task span based architectures like Dynamic Graph IE (DYGIE) (Luan et al., 2018, 2019) in order to pro- vide a stronger benchmark over BioRelEx (Khacha- trian et al., 2019), a recently published dataset of 2010 sentences consisting annotations of biological entities and binding interactions between those en- tities. We also check the impacts of using DYGIE’s updated span representations obtained from rela- tion and coreference graph propagation. Given the domain speciﬁcity of BioRelEx, we assess the im- pact of utilizing embeddings pre-trained on related domains. 2 Related Work Most information extraction systems have looked at entity tagging as a ﬂat sequence labeling problem and have employed feature-based CRFs and later neural architectures (Ma and Hovy, 2016; Peters et al., 2018). Lample et al. (2016) take inspiration from shift-reduce parsers to employ a transition based approach. Yu and Lam (2010); Miwa and Sasaki (2014); Li and Ji (2014); Zheng et al. (2017) arXiv:2006.00533v1 [cs.CL] 31 May 2020 have jointly modelled entity tagging and relation extraction. Fu et al. (2019) use relation-weighted graph convolution networks for considering the interaction between named", "meta": {"paper_id": "2006.00533v1", "chunk_index": 0, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_1", "text": "to employ a transition based approach. Yu and Lam (2010); Miwa and Sasaki (2014); Li and Ji (2014); Zheng et al. (2017) arXiv:2006.00533v1 [cs.CL] 31 May 2020 have jointly modelled entity tagging and relation extraction. Fu et al. (2019) use relation-weighted graph convolution networks for considering the interaction between named entities and relations. To incorporate the ability to tag overlapping spans, Katiyar and Cardie (2018); Wang and Lu (2018, 2019) devised neural methods applying hy- per graph based representations on top of sequence labelling. Fei et al. (2019); Dai et al. (2020) tag nested and discontinuous mentions in biomedical text. Lee et al. (2017, 2018) enumerate over all possible spans and apply bean pruning to reduce the num- ber of candidate spans used for coreference res- olution. Luan et al. (2018, 2019) further apply a multi-task graph based architecture incorporating relation, coreference and event based propagation to enhance span representations. Lin et al. (2020) test how well do current NER models generalize in the absence of strong name regularity, high mention coverage and sufﬁcient context diversity. Khachatrian et al. (2019) introduce BioRelEx by annotating bio-medical texts with entities grounded with other databases, focusing on delineating bio- logical and experimental entities and distinguishing between direct regulatory and indirect physical in- teractions. 3 Dataset & Evaluation Khachatrian et al. (2019) have made publicly avail- able 2010 annotated sentences (1405 for training and 201 for validation) consisting of annotations for 33 types of entities and relation annotations for binding interactions. We gauge the performance of all architectures over the validation dataset using the evaluation script of Khachatrian et al. (2019) namely measuring the micro-averaged precision, recall and F-score. 4 Entity Tagging Entity Tagging (Figure 1) is the task of predicting the best entity type label for every span of words in a given piece of text. And Relation Extraction (Figure 2) involves predicting the best relation type for such entity spans. 4.1 Traditional Architectures We evaluate various architectures traditionally com- mon for the task of entity tagging which have dis- played strong performances on other entity tag- ging datasets. As a baseline, we train a BiLSTM- CRF (Huang et al., 2015), a BiGRU-CRF and a CNN-CRF (Collobert et al., 2011) with 300-d Glove embeddings (Pennington et al., 2014) as in- put. BioRelEx, like other bio-medical texts, belong to a knowledge intensive domain and hence we expect a considerable amount of domain-speciﬁc terms to appear. In accordance with that, we incorporate sub-word embeddings by taking advantage of • Flair’s (Akbik et al., 2019) implementation of Contextual String Embeddings (Akbik et al., 2018) which use a character-language model • Byte Pair Embeddings (Heinzerling and Strube, 2018) • FastText Embeddings (Bojanowski et al., 2017) as inputs to a BiLSTM. Table 1 compares the per- formance of these traditional architectures along- with the various embeddings on BioRelEx’s val- idation dataset. Our experiments show that Flair embeddings provide a clear gain of 14% over other embeddings. However, ﬂat architectures like BiL- STM unarguably lack the ability to model over- lapping spans and hence in the next section, we attempt to", "meta": {"paper_id": "2006.00533v1", "chunk_index": 1, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_2", "text": "architectures along- with the various embeddings on BioRelEx’s val- idation dataset. Our experiments show that Flair embeddings provide a clear gain of 14% over other embeddings. However, ﬂat architectures like BiL- STM unarguably lack the ability to model over- lapping spans and hence in the next section, we attempt to use recently successful architectures like DYGIE to check for performance gains. 4.2 Multi-Task Architectures We choose to evaluate DYGIEPP (Wadden et al., 2019) which has shown strong results on many entity tagging datasets and emphasized the impor- tance of pre-training with text from similar domains like SciBERT (Beltagy et al., 2019). In Table 2, we present different conﬁgurations of DYGIEPP alongwith their results. Surprisingly, concatenation of ELMo Embeddings (Peters et al., 2018), Glove and Character Embeddings performs better than SciBERT for the BioRelEx dataset. Al- though, relation propagation do not seem to help much, coreference propagation shows an increase in the scores. Moreover, we observe that increasing the number of hidden units and layers boosts the scores. 1 DYGIEPP’s best conﬁguration performs 3.2% better than the current SOA, SciERC (Beltagy et al., 2019; Khachatrian et al., 2019). 1We ﬁnd that keeping 400 hidden units and 2 layers in the BiLSTM works best for our case. So, we use this conﬁguration in all our future experiments. System Precision Recall F-score BiLSTM-CRF (Glove) 0.516 0.525 0.520 BiGRU-CRF (Glove) 0.508 0.483 0.495 CNN-CRF (Glove) 0.541 0.500 0.520 News-forward + News-backward + Glove 0.723 0.604 0.658 Pubmed-forward + Pubmed-backward + BytePairEmbeddings 0.743 0.596 0.662 Pubmed-forward + Pubmed-backward + BytePairEmbeddings + FastText 0.748 0.591 0.660 Table 1: Performance of Flat Architectures on Entity Tagging. News-* and Pubmed-* are character level embed- dings (Akbik et al., 2018, 2019) used as inputs to a BiLSTM-CRF. System Precision Recall F-score DYGIEPP (SciBERT) 0.587 0.630 0.600 (-0.208) DYGIEPP (BERT-base) 0.567 0.660 0.610 (-0.198) DYGIEPP 0.832 0.786 0.808 +Relation Propagation (RP) 0.845 0.766 0.803 (-0.005) +Coreference Propagation (CP) 0.837 0.787 0.811 (+0.003) +RP, CP 0.832 0.767 0.798 (-0.010) DYGIEPP (200 BiLSTM units 1 layer) 0.836 0.769 0.801 (-0.007) DYGIEPP (400 BiLSTM units 2 layers) 0.826 0.773 0.800 (-0.008) DYGIEPP (512 BiLSTM units 3 layers) 0.852 0.771 0.810 (+0.002) SciERC 0.864 0.710 0.779 Table 2: Performance of Multi-Task Architectures on Entity Tagging. DYGIEPP mentioned in the third row uses ELMo embeddings but without graph propagation. Also it has 400 BiLSTM units and 3 BiLSTM layers. System Precision Recall F-score +BioELMo 0.858 0.790 0.823 (+0.015) +BioELMo, Relation Propagation (RP) 0.862 0.794 0.827 (+0.019) +BioELMo, Coreference Propagation (CP) 0.858 0.792 0.824 (+0.016) +BioELMo, RP, CP 0.851 0.805 0.828 (+0.020) +BioELMo, 200 BiLSTM units 1 layer 0.836 0.807 0.821 (+0.013) +BioELMo, 400 BiLSTM units 2 layers 0.859 0.790 0.822 (+0.014) +BioELMo, BioWord2Vec 0.844 0.793 0.818 (+0.010) Table 3: Impact of using Domain Speciﬁc Embeddings on Entity Tagging. We perform the experiments on DY- GIEPP with the same base conﬁguration as that of the Section 4.2 (Table 2) System Precision Recall F-score DYGIEPP (SciBERT) 0.241 0.139 0.176 (-0.379) DYGIEPP (BERT-base) 0.239 0.109 0.150 (-0.405) DYGIEPP 0.592 0.523 0.555 +Relation Propagation (RP) 0.628 0.500 0.556 (+0.001) +Coreference Propagation", "meta": {"paper_id": "2006.00533v1", "chunk_index": 2, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_3", "text": "Tagging. We perform the experiments on DY- GIEPP with the same base conﬁguration as that of the Section 4.2 (Table 2) System Precision Recall F-score DYGIEPP (SciBERT) 0.241 0.139 0.176 (-0.379) DYGIEPP (BERT-base) 0.239 0.109 0.150 (-0.405) DYGIEPP 0.592 0.523 0.555 +Relation Propagation (RP) 0.628 0.500 0.556 (+0.001) +Coreference Propagation (CP) 0.450 0.520 0.482 (-0.073) +RP, CP 0.479 0.527 0.502 (-0.053) DYGIEPP (200 BiLSTM units 1 layer) 0.569 0.459 0.508 (-0.047) DYGIEPP (400 BiLSTM units 2 layers) 0.570 0.452 0.504 (-0.051) DYGIEPP (512 BiLSTM units 3 layers) 0.594 0.503 0.545 (-0.010) SciERC 0.490 0.503 0.496 Table 4: Relation Extraction Performance of Multi-Task Architectures. We use the same base conﬁguration of DYGIEPP as the one in Section 4.2 (Table 2) System Precision Recall F-score +BioELMo 0.584 0.517 0.548 (-0.007) +BioELMo, Relation Propagation (RP) 0.571 0.520 0.544 (-0.011) +BioELMo, Coreference Propagation (CP) 0.498 0.568 0.531 (-0.024) +BioELMo, RP, CP 0.424 0.557 0.482 (-0.073) +BioELMo, 200 BiLSTM units 1 layer 0.469 0.557 0.510 (-0.045) +BioELMo, 400 BiLSTM units 2 layers 0.582 0.503 0.540 (-0.015) +BioELMo, BioWord2Vec 0.521 0.493 0.506 (-0.049) Table 5: Relation Extraction Performance of Domain Speciﬁc Embeddings over DYGIEPP 4.3 Domain-Speciﬁc Embeddings Domain-speciﬁc embeddings have been essential to achieve high performances for entity tagging tasks especially in knowledge intensive domains. (Ro- manov and Shivade, 2018; Zhu et al., 2018). Since BioRelEx is a domain speciﬁc dataset, we exper- imented with embeddings trained on related do- mains as well as with embeddings pre-trained on BioRelEx. We replaced the original ELMo embeddings in Wadden et al. (2019) with BioELMo (Jin et al., 2019) which have been pre-trained on 10M PubMed abstracts. We use Word2Vec embeddings trained on PubMed data (Pyysalo et al., 2013) too, which we will refer to as BioWord2Vec and also train Glove embeddings on the BioRelEx dataset. We present the results in Table 3. We observe that incorporating BioELMo boosts performance of DY- GIEPP by an absolute 1.7% over ELMo embed- dings. Using coreference propagation and relation propagation reveal slight improvement as well. 5 Relation Extraction 5.1 Multi-Task Architectures Since the multi-task architecture proposed by Luan et al. (2018); Wadden et al. (2019) have at- tempted to leverage context across multiple sen- tences as well as incorporate coreference, relation and event information to enhance span representa- tions, we verify if this success would easily apply to BioRelEx since this dataset contains a notable amount of coreferences and biologically nested en- tities like shown in Figure 1. We summarise the results for relation extraction in Table 4. Here, the combination of ELMo, Glove and Character Embeddings performs remarkably better than SciBERT. Contrary to entity tagging, for relation extraction, enabling relation propagation gives us the best scores. Further increasing the layers and the number of units in the BiLSTM also helps a lot in this case. 5.2 Domain-Speciﬁc Embeddings We follow the same steps as followed for entity tagging. We present the results for relation extrac- tion in Table 5. We observe that whereas in case of entity tagging, using domain speciﬁc embeddings, speciﬁcally BioELMo shows a good improvement, for relation extraction, they show", "meta": {"paper_id": "2006.00533v1", "chunk_index": 3, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_4", "text": "this case. 5.2 Domain-Speciﬁc Embeddings We follow the same steps as followed for entity tagging. We present the results for relation extrac- tion in Table 5. We observe that whereas in case of entity tagging, using domain speciﬁc embeddings, speciﬁcally BioELMo shows a good improvement, for relation extraction, they show a slight decrease in the F-score. Comparing Table 5 to Table 4, we see a small reduction when using domain speciﬁc embeddings. Similar to entity tagging, here too, BioWord2Vec reduces the F-score. 6 Conclusion We evaluate a number of architectures for entity tag- ging and relation extraction and perform extensive hyperparameter tuning over the BioRelEx dataset. Our experiments improve BioRelEx’s SOA for en- tity tagging and relation extraction by 4.9% and 6% F-score respectively. Our evaluations show that using ELMo, Glove and character embeddings im- proves the performance compared to BERT and SciBERT on multi-task architectures. Likewise, utilizing domain speciﬁc information in the form of pre-trained embeddings also leads to an improve- ment in performance for entity tagging. 7 Discussion Multi-task architectures like DYGIE, by and large with domain-speciﬁc embeddings improve the pre- dictions of BioRelEx. Interestingly, we ﬁnd that contextual embeddings like ELMo and BioELMo exceed the performance of BERT (Devlin et al., 2018) and SciBERT by a large margin which is tangential to ﬁndings on other benchmarks like GLUE (Wang et al., 2018). Besides, we observe that in a few cases, certain span annotations of entities are not being taken into account while converting to SciERC’s input format by the provided conversion script, but were being expected to be predicted during the evalua- tion process making it impossible for any model to go beyond a training set F-score of 91.2%.2 While this minor incorporation in BioRelEx might drasti- cally improve F-scores for all of the architectures, relative differences of performances between the architectures might hardly change. We would still expect multi-task architectures to predict BioRelEx entities and relations better vis--vis traditional ar- chitectures as the F-scores display an appreciable leap of 15% to 30% already (Table 1 vs Table 2). References Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. Flair: An easy-to-use framework for state-of-the-art 2When we convert the dev set using the provided conver- sion script, the F-score from the evaluation script gives us 0.915 F-score for entity tagging and 0.834 for relation ex- traction, while on the training set it goes to 0.912 for entity tagging and 0.850 for relation extraction. nlp. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Com- putational Linguistics (Demonstrations), pages 54– 59. Alan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In COLING 2018, 27th International Con- ference on Computational Linguistics, pages 1638– 1649. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: Pretrained language model for scientiﬁc text. In EMNLP. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135–146. Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen,", "meta": {"paper_id": "2006.00533v1", "chunk_index": 4, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_5", "text": "Beltagy, Kyle Lo, and Arman Cohan. 2019. Scib- ert: Pretrained language model for scientiﬁc text. In EMNLP. Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Associa- tion for Computational Linguistics, 5:135–146. Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:24932537. Xiang Dai, Sarvnaz Karimi, Ben Hachey, and Cecile Paris. 2020. An effective transition-based model for discontinuous ner. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand- ing. H. Fei, Y. Ren, and D. Ji. 2019. Recognizing nested named entity in biomedical texts: A neural net- work model with multi-task learning. In 2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 376–381. Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. 2019. Graphrel: Modeling text as relational graphs for joint entity and relation extraction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1409–1418. Benjamin Heinzerling and Michael Strube. 2018. BPEmb: Tokenization-free pre-trained subword em- beddings in 275 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC-2018), Miyazaki, Japan. European Languages Resources Association (ELRA). Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec- tional lstm-crf models for sequence tagging. Qiao Jin, Bhuwan Dhingra, William Cohen, and Xinghua Lu. 2019. Probing biomedical embeddings from language models. In Proceedings of the 3rd Workshop on Evaluating Vector Space Representa- tions for NLP, pages 82–89. Arzoo Katiyar and Claire Cardie. 2018. Nested named entity recognition revisited. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, Volume 1 (Long Pa- pers), pages 861–871. Hrant Khachatrian, Lilit Nersisyan, Karen Ham- bardzumyan, Tigran Galstyan, Anna Hakobyan, Ar- sen Arakelyan, Andrey Rzhetsky, and Aram Gal- styan. 2019. BioRelEx 1.0: Biological relation ex- traction benchmark. In Proceedings of the 18th BioNLP Workshop and Shared Task, pages 176–190, Florence, Italy. Association for Computational Lin- guistics. Guillaume Lample, Miguel Ballesteros, Sandeep Sub- ramanian, Kazuya Kawakami, and Chris Dyer. 2016. Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computa- tional Linguistics: Human Language Technologies, pages 260–270, San Diego, California. Association for Computational Linguistics. Kenton Lee, Luheng He, Mike Lewis, and Luke Zettle- moyer. 2017. End-to-end neural coreference resolu- tion. Proceedings of the 2017 Conference on Empir- ical Methods in Natural Language Processing. Kenton Lee, Luheng He, and Luke Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to- ﬁne inference. In Proceedings of the 2018 Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies, Volume 2 (Short Papers), pages 687–692, New Orleans, Louisiana. Association for Computational Linguistics. Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402–412, Baltimore, Maryland. Association for Computational Linguistics. Hongyu Lin,", "meta": {"paper_id": "2006.00533v1", "chunk_index": 5, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_6", "text": "pages 687–692, New Orleans, Louisiana. Association for Computational Linguistics. Qi Li and Heng Ji. 2014. Incremental joint extraction of entity mentions and relations. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 402–412, Baltimore, Maryland. Association for Computational Linguistics. Hongyu Lin, Yaojie Lu, Xianpei Han, and Le Sun. 2020. A rigourous study on named entity recogni- tion: Can ﬁne-tuning pretrained model lead to the promised land? arXiv preprint arXiv:2004.12126. Yi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identiﬁcation of enti- ties, relations, and coreference for scientiﬁc knowl- edge graph construction. Proceedings of the 2018 Conference on Empirical Methods in Natural Lan- guage Processing. Yi Luan, Dave Wadden, Luheng He, Amy Shah, Mari Ostendorf, and Hannaneh Hajishirzi. 2019. A gen- eral framework for information extraction using dy- namic span graphs. Proceedings of the 2019 Confer- ence of the North. Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional LSTM-CNNs- CRF. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), pages 1064–1074, Berlin, Ger- many. Association for Computational Linguistics. Makoto Miwa and Yutaka Sasaki. 2014. Modeling joint entity and relation extraction with table repre- sentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1858–1869. Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 1532–1543, Doha, Qatar. Asso- ciation for Computational Linguistics. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word rep- resentations. In Proceedings of NAACL-HLT, pages 2227–2237. Sampo Pyysalo, Filip Ginter, Hans Moen, Tapio Salakoski, and Sophia Ananiadou. 2013. Distribu- tional semantics resources for biomedical text pro- cessing. Alexey Romanov and Chaitanya Shivade. 2018. Lessons from natural language inference in the clin- ical domain. David Wadden, Ulme Wennberg, Yi Luan, and Han- naneh Hajishirzi. 2019. Entity, relation, and event extraction with contextualized span representations. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan- guage Processing (EMNLP-IJCNLP), pages 5784– 5789, Hong Kong, China. Association for Computa- tional Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Pro- ceedings of the 2018 EMNLP Workshop Black- boxNLP: Analyzing and Interpreting Neural Net- works for NLP, pages 353–355, Brussels, Belgium. Association for Computational Linguistics. Bailin Wang and Wei Lu. 2018. Neural segmental hypergraphs for overlapping mention recognition. arXiv preprint arXiv:1810.01817. Bailin Wang and Wei Lu. 2019. Combining spans into entities: A neural two-stage approach for rec- ognizing discontiguous entities. Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia text via a graphical", "meta": {"paper_id": "2006.00533v1", "chunk_index": 6, "filename": "2006.00533v1.pdf", "chunk_length": 512}}
{"id": "2006.00533v1_chunk_7", "text": "neural two-stage approach for rec- ognizing discontiguous entities. Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Xiaofeng Yu and Wai Lam. 2010. Jointly identifying entities and extracting relations in encyclopedia text via a graphical model approach. In Proceedings of the 23rd International Conference on Computa- tional Linguistics: Posters, pages 1399–1407. Asso- ciation for Computational Linguistics. Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, and Bo Xu. 2017. Joint extrac- tion of entities and relations based on a novel tagging scheme. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers). Henghui Zhu, Ioannis Ch. Paschalidis, and Amir Tah- masebi. 2018. Clinical concept extraction with con- textual word embedding.", "meta": {"paper_id": "2006.00533v1", "chunk_index": 7, "filename": "2006.00533v1.pdf", "chunk_length": 133}}
{"id": "2012.10055v2_chunk_0", "text": "END-TO-END SPEAKER DIARIZATION AS POST-PROCESSING Shota Horiguchi1 Paola Garc´ıa2 Yusuke Fujita1 Shinji Watanabe2 Kenji Nagamatsu1 1 Hitachi, Ltd. Research & Development Group, Japan 2 Center for Language and Speech Processing, Johns Hopkins University, USA ABSTRACT This paper investigates the utilization of an end-to-end diarization model as post-processing of conventional clustering-based diariza- tion. Clustering-based diarization methods partition frames into clusters of the number of speakers; thus, they typically cannot handle overlapping speech because each frame is assigned to one speaker. On the other hand, some end-to-end diarization methods can handle overlapping speech by treating the problem as multi-label classiﬁcation. Although some methods can treat a ﬂexible number of speakers, they do not perform well when the number of speakers is large. To compensate for each other’s weakness, we propose to use a two-speaker end-to-end diarization method as post-processing of the results obtained by a clustering-based method. We iteratively select two speakers from the results and update the results of the two speakers to improve the overlapped region. Experimental results show that the proposed algorithm consistently improved the perfor- mance of the state-of-the-art methods across CALLHOME, AMI, and DIHARD II datasets. Index Terms— Speaker diarization, EEND 1. INTRODUCTION Speaker diarization, which is sometimes referred to as “who spoke when”, has important roles in many speech-related applications. It is sometimes used to enrich transcriptions by adding speaker attributes [1], and at the other times, it is used to improve the performance of speech separation and recognition [2, 3]. Speaker diarization methods can be classiﬁed roughly into two: clustering-based methods and end-to-end methods. Typical clustering-based methods i) ﬁrst classify frames into speech and non-speech, ii) then extract an embedding which describes speaker characteristics from each speech frame, and iii) ﬁnally apply clus- tering to the extracted embeddings. Most methods employ hard clustering such as agglomerative hierarchical clustering (AHC) and k-means clustering; as a result, each frame belongs either to one of the speaker clusters or to the non-speech cluster. The assumption that underlies these clustering-based methods is that each frame contains at most one speaker, i.e., they treat speaker diarization as a set partitioning problem. Thus, they fundamentally cannot deal with overlapping speech. Despite the assumption, they are still strong baselines over end-to-end methods on datasets of a large number of speakers, e.g., DIHARD II dataset [4]. This is because they handle multiple speaker problems based on unsupervised clustering without using any speech mixtures as training data. Thus, the methods do not suffer from overtraining due to the lack of the overlap speech especially for a large number of speakers. On the other hand, some end-to-end methods called EEND treat speaker diarization as a multi-label classiﬁcation problem. They pre- dict whether each speaker is active or not at each frame; thus, they can deal with speaker overlap. Evaluation of the early models ﬁxed the number of speakers to two [5, 6, 7]. Some extensions are pro- posed recently to handle unknown number of speaker cases, e.g., encoder-decoder-based attractor calculation [8] and one-by-one pre- diction using speaker-conditioned model [9]. However, these meth- ods still perform poorly when", "meta": {"paper_id": "2012.10055v2", "chunk_index": 0, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_1", "text": "overlap. Evaluation of the early models ﬁxed the number of speakers to two [5, 6, 7]. Some extensions are pro- posed recently to handle unknown number of speaker cases, e.g., encoder-decoder-based attractor calculation [8] and one-by-one pre- diction using speaker-conditioned model [9]. However, these meth- ods still perform poorly when the number of speakers is large. One reason is the training datasets. Mixtures of a large number of speak- ers are often rare in various datasets; thus, end-to-end models cannot produce diarization results for large number of speakers because they are overtrained on mixtures of a few number of speakers. Even if the issue on the number of mixtures is solved, the EEND depends on the permutation invariant training [10] so that it is still hard to train the model on a large number of mixtures in terms of the calculation cost. For these reasons, how to handle mixtures that contain overlapping speech of a large number of speakers is still an open problem for both clustering and end-to-end diarization methods. In this paper, we propose to combine both clustering-based and end-to-end methods effectively to deal with overlapping speech re- gardless of the number of speakers. We ﬁrst obtain the initial diariza- tion result using x-vector clustering, which does not produce over- lapping results in most cases. We then apply the following steps it- eratively: i) frame selection to contain only two speakers and silence and ii) overlap estimation using a two-speaker EEND model. The frame selection is also used to adapt the EEND model to a dataset which contains mixtures of more than two speakers. We evaluate our method using various datasets including CALLHOME, AMI, and DIHARD II datasets. 2. RELATED WORK 2.1. Clustering-based diarization While some methods provide supervised clustering of speaker em- beddings [11], the most common approach is an x-vector clustering in an unsupervised manner (See the systems submitted to DIHARD II Challenge, e.g., [12, 13]). Since naive x-vector clustering results in poor performance, various techniques to improve the performance have been proposed, e.g., probabilistic linear discriminate analysis (PLDA) rescoring [14] and Variational Bayes (VB) hidden Markov model (HMM) resegmentation [15]. In terms of overlap processing, most methods ﬁrst detect overlapped frames and then assign the sec- ond speaker for the detected frames based on heuristics [12, 16] or the results of VB resegmentation [17]. Another direction is based on clustering of overlapped segments [18]. It ﬁrst extracts overlapped segments using a region proposal network, and then applies clustering for embeddings extracted from each of them. It fundamentally solved the issue of embedding extrac- tion using a sliding window, but its accuracies are not comparable to end-to-end methods described in Section 2.2. arXiv:2012.10055v2 [eess.AS] 23 Dec 2020 2.2. End-to-end diarization for overlapping speech One end-to-end approach is called EEND [5, 6, 7]. They calculate multiple speaker activities, each corresponding to a single speaker. Recent models can output a ﬂexible number of speakers’ activi- ties by using encoder-decoder-based attractor calculation modules (EDA) [8] or speaker-conditional EEND (SC-EEND) [9]. Another approach is called RSAN, which are based on residual masks", "meta": {"paper_id": "2012.10055v2", "chunk_index": 1, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_2", "text": "EEND [5, 6, 7]. They calculate multiple speaker activities, each corresponding to a single speaker. Recent models can output a ﬂexible number of speakers’ activi- ties by using encoder-decoder-based attractor calculation modules (EDA) [8] or speaker-conditional EEND (SC-EEND) [9]. Another approach is called RSAN, which are based on residual masks in the time-frequency domain to extract speakers one by one [19, 20]. While EEND and RSAN take only acoustic features as input, a variant of these methods also accepts a speaker embedding as in- put to determine the target-speaker and output his/her speech activi- ties. For example, target-speaker voice activity detection (TS-VAD) uses i-vectors to output the corresponding speakers’ voice activities [21], but the number of speakers is ﬁxed by the model architec- ture. Personal VAD [22] and VoiceFilter-Lite [23], which are based on d-vectors, have not such a limitation, but they assume that each speaker’s d-vector is stored in the database in advance; thus they are not suited for speaker-independent diarization. 3. PROPOSED METHOD 3.1. Overview Given acoustic features {xt}T t=1, where t ∈{1, . . . , T} =: [T] denotes a frame index, diarization is a problem to predict a set of active frames Tk ⊆[T] for each speaker k ∈{1, . . . , K} =: [K]. K is the estimated number of speakers. For simplicity, we use XT := {xt | t ∈T } to denote the features of selected frames T ⊆[T]. Clustering-based methods assume that input recordings do not contain speaker overlap. It formulates diarization as a set partition- ing problem, i.e., Tk for k ∈[K] are predicted to be disjoint, i.e., ∀{i, j} ∈ \u0000[K] 2 \u0001 , Ti ∩Tj = ∅. In EEND, on the other hand, di- arization is formulated as a multi-label classiﬁcation to handle over- lapping speech; thus, they do not have to be disjoint. The formula- tion of EEND is appropriate for real conversations in which speakers sometimes utter simultaneously. However, it makes the problem too difﬁcult to be solved; when K is large (e.g. 10), it rarely happens that K speakers speak together. Therefore, we assume that at most K′(< K) speakers speak simultaneously, and reﬁne the clustering- based results using an end-to-end model that is trained to process at most K′ speakers. In this study, we set K′ = 2. The detailed algorithm is explained in Section 3.2. 3.2. Algorithm Given initial diarization results {Tk | ∅̸= Tk ⊆[T]}K k=1, we iteratively select two speakers among K and update the diariza- tion results of the two speakers using an EEND model. The EEND model fEEND : RD×L →(0, 1)2×L was trained to estimate pos- teriors probabilities of two speakers from an L-length sequence of D-dimensional acoustic features. Figure 1 show the ﬂow of the pro- posed method when K = 3. 3.2.1. Processing order determination To apply the iterative reﬁnement to each pair of speakers, the pro- cessing order inﬂuences the accuracy of ﬁnal diarization results. This is because we cannot select frames to include only two speak- ers based on estimated diarization results because they include", "meta": {"paper_id": "2012.10055v2", "chunk_index": 2, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_3", "text": "when K = 3. 3.2.1. Processing order determination To apply the iterative reﬁnement to each pair of speakers, the pro- cessing order inﬂuences the accuracy of ﬁnal diarization results. This is because we cannot select frames to include only two speak- ers based on estimated diarization results because they include diarization errors. For example, if we select frames not containing Speaker 1 in Figure 1, the fourth frame contains Speaker 1 according to the ﬁnal results. If the ratio of such impurities among the selected frames is high, the reﬁnement using EEND may not perform well. We found that this problem is simply solved by processing the pairs of speakers in decreasing order of the number of selected frames (Figure 1(i)). For each speaker pair {i, j} ∈ \u0000[K] 2 \u0001 , we ﬁrst select a set of frames Pi,j not containing speakers other than i and j as follows: Pi,j = [T] \\ [ k∈[K]\\{i,j} Tk. (1) We then apply the reﬁnement described below for each speaker pair in descending order of |Pi,j| as in Figure 1 (ii-a)–(ii-c). 3.2.2. Iterative update of diarization results To update the diarization results of speakers i and j, we ﬁrst reselect a set of frames Pi,j using (1) . This is because the diarization results {Tk}K k=1 are updated at each reﬁnement step so that we cannot reuse the one that is calculated to decide the processing order. Then the corresponding features XPi,j are input to the EEND model to obtain posteriors of two speakers (a) and (b) by \u0012h q(a) t , q(b) t iT t ∈Pi,j \u0013 = f \u0000XPi,j \u0001 ∈(0, 1)2×|Pi,j| , (2) where q(a) t and q(b) t denote posteriors of the ﬁrst and second speakers at frame index t, respectively, and (·)T denotes the matrix transpose. We simply apply the threshold value of 0.5 to obtain the indexes of active frames of the two speakers as Q(a) = n t ∈Pi,j q(a) t > 0.5 o , Q(b) = n t ∈Pi,j q(b) t > 0.5 o . (3) Note that we have speaker permutation ambiguity between (a)–(b) and i–j, and we solve permutation to ﬁnd the optimal correspon- dence between (Ti, Tj) and (Q(a), Q(b)) as follows: \u0010 ˆTi, ˆTj \u0011 = arg max (u,v)∈{(a,b),(b,a))} s \u0010 Q(u), Ti \u0011 + s \u0010 Q(v), Tj \u0011 , (4) where s(U, V) is a function to calculate similarity between speech and non-speech activities described by two sets U and V deﬁned as s(U, V) := |U ∩V| | {z } speech similarity + |([T] \\ U) ∩([T] \\ V)| | {z } non-speech similarity . (5) Finally, we update the diarization results of speakers i and j. To conﬁrm that the new results ˆTi and ˆTj are calculated for speaker i and j, we check whether they satisfy the following conditions: ˆTi ∩(Ti ∩Pi,j) |Ti ∩Pi,j| > α, ˆTj ∩(Tj ∩Pi,j) |Tj ∩Pi,j| > α, (6) where α is a lower limit of the ratio of the intersection between the new results ˆTi (or ˆTj) and the previous results", "meta": {"paper_id": "2012.10055v2", "chunk_index": 3, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_4", "text": "speaker i and j, we check whether they satisfy the following conditions: ˆTi ∩(Ti ∩Pi,j) |Ti ∩Pi,j| > α, ˆTj ∩(Tj ∩Pi,j) |Tj ∩Pi,j| > α, (6) where α is a lower limit of the ratio of the intersection between the new results ˆTi (or ˆTj) and the previous results Ti∩Pi,j (or Tj∩Pi,j). In this study, we set α = 0.5. Only if the conditions in (6) are satisﬁed, we update the results of speakers i and j. When K = 2, we simply update the results with the new ones as Ti ←ˆTi ∪([T] \\ Pi,j) , Tj ←ˆTj ∪([T] \\ Pi,j) . (7) On the other hand, when K ≥3, such fully-update strategy causes a performance drop due to impurities in the selected frames. Thus, we use the following instead of (7) to update only overlapped frames: Ti ←Ti ∪ \u0010 ˆTi ∩ˆTj \u0011 , Ti ←Tj ∪ \u0010 ˆTj ∩ˆTj \u0011 . (8) 1 2 12 10 11 Initial results Speaker 1 Speaker 2 Speaker 3 4 7 6 5 3 8 9 After update #1 (ii-a) Update #1 (Speakers 2&3) frame index After update #2 After update #3 (Final results) (ii-b) Update #2 (Speakers 1&3) 1 2 12 10 11 4 7 6 5 3 8 9 Select frames not Corresponding results & features Solve permutation New results for Selected frames Speaker 2 Speaker 3 1 2 12 10 11 4 7 6 5 3 8 9 1 2 12 10 11 4 7 6 5 3 8 9 Corresponding results & features Solve permutation New results for Selected frames Speaker 1 Speaker 3 Corresponding results & features Solve permutation New results for Selected frames Speaker 1 Speaker 2 containing speaker 1 Selected frames of each speaker pair Processing order (ii-c) Update #3 (Speakers 1&2) (i) Decide processing order Speaker 2 Speaker 3 Speaker 1 Speaker 3 Speaker 1 Speaker 2 Speakers 2&3 1&3 1&2 Overwrite by the new results Select frames not containing speaker 2 Overwrite by the new results Select frames not containing speaker 3 Overwrite by the new results 1 12 10 11 4 7 6 5 8 9 1 2 12 10 11 3 8 9 10 11 4 5 3 1 12 10 11 4 7 6 5 8 9 1 2 12 10 11 3 8 9 10 11 4 5 3 Fig. 1: The ﬂow of the proposed method when the number of speakers is three. Given initial diarization results (top left), our method (i) picks up a pair of speakers iteratively in decreasing order of the number of frames and (ii) reﬁnes the diarization results of the two speakers using an end-to-end speaker diarization model. For the end-to-end model fEEND, we use the self-attentive EEND model with an encoder-decoder attractor calculation module (SA- EEND-EDA) [8]. It consists of a four-layer-stacked Transformer encoder to extract embeddings for each frame and the EDA mod- ule to calculate attractors from the extracted embeddings. The EDA includes long short-term memories but we shufﬂed the order of em- beddings just before they are fed into", "meta": {"paper_id": "2012.10055v2", "chunk_index": 4, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_5", "text": "calculation module (SA- EEND-EDA) [8]. It consists of a four-layer-stacked Transformer encoder to extract embeddings for each frame and the EDA mod- ule to calculate attractors from the extracted embeddings. The EDA includes long short-term memories but we shufﬂed the order of em- beddings just before they are fed into the EDA, which improves the diarization performance. Thus, we can consider that all the com- ponents of fEEND are independent of the order of embeddings and therefore the model can treat input features of selected frames even if they are not continuous in time. 3.3. Training strategy of the SA-EEND-EDA model In the original EEND and its derived methods [6, 8, 9] used matched dataset for model adaptation, i.e., only two-speaker subset of the original dataset (e.g. CALLHOME1) was used to ﬁnetune the mod- els in two-speaker evaluations. This strategy cannot be used to ﬁne- tune two-speaker models when the dataset does not contain two- speaker mixtures (e.g. AMI [24]). Even if two-speaker mixtures are included in the dataset, it does not make full use of the datasets, which may cause performance degradation. To cope with this situation, we adopt the frame-selection tech- nique used in (1) for model adaptation. If the input chunk contains more than two speakers, we ﬁrst choose two dominant speakers and then eliminate frames in which the other speakers are active as in (1). The model is trained only using the selected frames to output speech activities of the two speakers. This makes it possible to ﬁne- tune two-speaker models from any kind of multi-speaker datasets without mixture-wise selection. 4. EXPERIMENT 4.1. Settings Table 1 shows the datasets used for our evaluation. The model was pretrained using simulated two-speaker mixtures Sim2spk for 100 epochs. Each mixture was simulated from two single-speaker audios derived from Switchboard-2 (Phase I & II & III), Switchboard Cel- lular (Part 1 & 2), or NIST Speaker Recognition Evaluation (2004 1https://catalog.ldc.upenn.edu/LDC2001S97 Table 1: Dataset to train and test our diarization models. Dataset #Speakers #Mixtures Overlap ratio (%) Pretrain Sim2spk 2 100,000 34.1 Adaptation CALLHOME-2spk 2 155 14.0 CALLHOME 2–7 249 17.0 AMI train [24] 3–5 118 19.4 DIHARD dev [4] 1–10 192 9.8 Test CALLHOME-2spk 2 148 13.1 CALLHOME 2–6 250 16.7 AMI eval [24] 3–4 24 18.6 DIHARD eval [4] 1–9 194 8.9 & 2005 & 2006 & 2008). Noise sources from MUSAN corpus [25] and simulated room impulse responses [26] are also used to sim- ulate noisy and reverberant environments. The detailed simulation protocol is in our previous paper [6]. For the pretraining, Adam op- timizer with the learning rate scheduler proposed in [27] was used. The number of warm-up steps was set to 100,000 following [8]. After the pretraining, the model was adapted on CALLHOME, AMI [24] and DIHARD II [4] datasets for another 100 epochs, re- spectively. Adam optimizer was also used in the adaptations but its learning rate was ﬁxed to 1 × 10−5 following [8]. We used diarization error rate (DER) and Jaccard error rate (JER) for evaluation. While some studies excluded overlapped re- gions from evaluation", "meta": {"paper_id": "2012.10055v2", "chunk_index": 5, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_6", "text": "[4] datasets for another 100 epochs, re- spectively. Adam optimizer was also used in the adaptations but its learning rate was ﬁxed to 1 × 10−5 following [8]. We used diarization error rate (DER) and Jaccard error rate (JER) for evaluation. While some studies excluded overlapped re- gions from evaluation [14, 11], this study scored overlapped region. We also note that our evaluations are based on estimated speech activity detection (SAD), while some studies used oracle segments [17] or only reported confusion errors [11]. 4.2. Preliminary evaluation of the training using frame selection Before the evaluation of the proposed post-processing method, we ﬁrst evaluated the training strategy explained in Section 3.3 using two transformer-based two-speaker EEND models: SA-EEND [6] and SA-EEND-EDA [8]. They were trained on CALLHOME-2spk in the original papers, but we utilized mixtures that contain more than two speakers in the CALLHOME dataset. Table 2 shows DERs on the CALLHOME-2spk test set. Using the full CALLHOME im- proved DER of SA-EEND from 9.54 % to 9.00 % and that of SA- EEND-EDA from 8.07 % to 7.84 %. According to these results, Table 2: DERs (%) on CALLHOME-2spk. Collar tolerance of 0.25 s is allowed. Model Adaptation DER SA-EEND [7] CALLHOME-2spk 9.54 SA-EEND CALLHOME + frame selection 9.00 SA-EEND-EDA [8] CALLHOME-2spk 8.07 SA-EEND-EDA CALLHOME + frame selection 7.84 Table 3: DERs (%) on CALLHOME. All the results include over- lapped regions and are NOT based on oracle SAD. Collar tolerance of 0.25 s is allowed. #Speakers Method 2 3 4 5 6 All SA-EEND-EDA [8] 8.50 13.24 21.46 33.16 40.29 15.29 x-vector AHC 15.45 18.01 22.68 31.40 34.27 19.43 x-vector AHC + Proposed 13.85 14.72 18.61 28.63 29.02 16.79 x-vector AHC + VB 12.62 16.82 21.27 31.14 31.80 17.61 x-vector AHC + VB + Proposed 9.87 13.11 16.52 28.65 27.83 14.06 we show the effectiveness of our training strategy described in Sec- tion 3.3 based on the frame selection with (1). 4.3. Results 4.3.1. CALLHOME We ﬁrst evaluated the proposed method on CALLHOME dataset, which is composed of telephone conversations. As a clustering- based baseline, x-vectors with AHC and PLDA2 was used with TDNN-based speech activity detection3. We also prepared the re- sults for which VB-HMM resegmentation [15] was applied. All the components were implemented in Kaldi recipe. Table 3 shows the evaluation results. X-vector clustering with- out and with VB achieved 19.43 % and 17.61 % DERs, respectively, but they didn’t outperform the 15.29 % DER scored by SA-EEND- EDA trained to output diarization results on ﬂexible number of speakers. However, we can also observe that the clustering-based methods are better when the number of speakers is larger than four. Applying the proposed post-processing for x-vector clustering base- lines achieved 16.79 % and 14.06 % without and with VB, and the latter is 1.23 % better than the SA-EEND-EDA model. In terms of the number of speakers, the proposed method performed well on both large and small number of speakers. 4.3.2. AMI Second, we evaluated our method on AMI dataset, consisting of meeting recordings. While it includes various types", "meta": {"paper_id": "2012.10055v2", "chunk_index": 6, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_7", "text": "and the latter is 1.23 % better than the SA-EEND-EDA model. In terms of the number of speakers, the proposed method performed well on both large and small number of speakers. 4.3.2. AMI Second, we evaluated our method on AMI dataset, consisting of meeting recordings. While it includes various types of recordings, we used Headset mix recordings for this experiment. We chose the system developed during JSALT 2019 [28] as a baseline. It is based on x-vector clustering followed by VB resegmentation and overlap detection and assignment for the second speaker candidate [29]. Table 4 shows DERs and JERs on AMI eval set. The proposed method reduced DERs of 3.07 %, 3.14 %, and 0.18 % of absolute improvement from the three baselines. Surprisingly, our method im- 2https://github.com/kaldi-asr/kaldi/tree/master/ egs/callhome_diarization/v2 3https://github.com/kaldi-asr/kaldi/tree/master/ egs/aspire/s5 Table 4: DERs and JERs (%) on AMI eval. VB: Variational Bayes resegmentation, OVL: Overlap detection and speaker assignment [29]. All the results include overlapped regions and are NOT based on oracle SAD. No collar tolerance is allowed. Method DER JER x-vector AHC [28] 33.75 45.68 x-vector AHC + Proposed 30.64 43.78 x-vector AHC + VB [28] 32.80 43.72 x-vector AHC + VB + Proposed 29.66 42.63 x-vector AHC + VB + OVL [28] 28.15 41.00 x-vector AHC + VB + OVL + Proposed 27.97 40.57 Table 5: DERs and JERs (%) on DIHARD II eval. All the results include overlapped regions and are NOT based on oracle SAD. No collar torelance is allowed. Method DER JER DIHARD II baseline [30] 40.86 66.60 DIHARD II baseline + Proposed 37.90 63.79 BUT system (w/o OVL) [12, 16] 27.26 49.15 BUT system (w/o OVL) + Proposed 26.91 48.49 BUT system (w/ OVL) [12, 16] 27.11 49.07 BUT system (w/ OVL) + Proposed 26.88 48.43 proved DER and JER of the results in which the overlap detection [29] was already applied. 4.3.3. DIHARD II Finally, we evaluated the proposed method on DIHARD II dataset, which includes recordings from 10 different domains. We used the ofﬁcial baseline system [4] and the BUT system [12, 16], which is the winning system of the second DIHARD Challenge, to obtain initial diarization results. Both are based on the x-vector clustering, but the BUT system is more polished in that it extracts x-vectors in shorter intervals and uses VB resegmentation and overlap detection and assignment based on heuristics. The results are shown in Table 5. The proposed method re- duced DER and JER of the baseline system by 2.96 % and 2.91 %, respectively. Our method also improved DER and JER of 0.35 % and 0.66 % from the BUT system without overlap assignment and 0.38 % and 0.64 % from that with overlap assignment, respectively. These improvements are small, but it is far better than the heuristic- based overlap assignment in [12], which improved DER by 0.15 % (= 27.26 −27.11) and JER by 0.08 % (= 49.15 −49.07). 5. CONCLUSION In this paper, we proposed a post-processing method for clustering- based diarization using an end-to-end diarization model. We itera- tively selected two speakers, picked up frames that", "meta": {"paper_id": "2012.10055v2", "chunk_index": 7, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_8", "text": "overlap assignment in [12], which improved DER by 0.15 % (= 27.26 −27.11) and JER by 0.08 % (= 49.15 −49.07). 5. CONCLUSION In this paper, we proposed a post-processing method for clustering- based diarization using an end-to-end diarization model. We itera- tively selected two speakers, picked up frames that contain the two speakers, and process the frames by the end-to-end model to up- date diarization results. Evaluations on CALLHOME, AMI, and DI- HARD II datasets showed that our proposed method improves vari- ous types of clustering-based diarization results. 6. ACKNOWLEDGMENT We would like to thank Federico Landini for providing the results of the winning system [12, 16] of the second DIHARD Challenge. 7. REFERENCES [1] X. Anguera, S. Bozonnet, N. Evans, C. Fredouille, G. Fried- land, and O. Vinyals, “Speaker diarization: A review of recent research,” IEEE TASLP, vol. 20, no. 2, pp. 356–370, 2012. [2] C. Boeddeker, J. Heitkaemper, J. Schmalenstoeer, L. Drude, J. Heymann, and R. Haeb-Umbach, “Front-end processing for the CHiME-5 dinner party scenario,” in CHiME-5, 2018. [3] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, I. Podluzhny, A. Laptev, and A. Romanenko, “The STC system for the CHiME-6 Challenge,” in CHiME- 6, 2020. [4] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and M. Liberman, “The Second DIHARD Diarization Chal- lenge: Dataset, task, and baselines,” in INTERSPEECH, 2019, pp. 978–982. [5] Y. Fujita, N. Kanda, S. Horiguchi, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with permutation-free objectives,” in INTERSPEECH, 2019, pp. 4300–4304. [6] Y. Fujita, N. Kanda, S. Horiguchi, Y. Xue, K. Nagamatsu, and S. Watanabe, “End-to-end neural speaker diarization with self- attention,” in ASRU, 2019, pp. 296–303. [7] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, and K. Nagamatsu, “End-to-end neural diarization: Reformulat- ing speaker diarization as simple multi-label classiﬁcation,” arXiv:2003.02966, 2020. [8] S. Horiguchi, Y. Fujita, S. Wananabe, Y. Xue, and K. Naga- matsu, “End-to-end speaker diarization for an unknown num- ber of speakers with encoder-decoder based attractors,” in IN- TERSPEECH, 2020, pp. 269–273. [9] Y. Fujita, S. Watanabe, S. Horiguchi, Y. Xue, J. Shi, and K. Nagamatsu, “Neural speaker diarization with speaker-wise chain rule,” arXiv:2006.01796, 2020. [10] D. Yu, M. Kolbæk, Z.-H. Tan, and J. Jensen, “Permutation in- variant training of deep models for speaker-independent multi- talker speech separation,” in ICASSP, 2017, pp. 241–245. [11] A. Zhang, Q. Wang, Z. Zhu, J. Paisley, and C. Wang, “Fully supervised speaker diarization,” in ICASSP, 2019, pp. 6301– 6305. [12] F. Landini, S. Wang, M. Diez, L. Burget, P. Matˇejka, K. ˇZmol´ıkov´a, L. Moˇsner, A. Silnova, O. Plchot, O. Novotn`y, H. Zeinali, and J. Rohdin, “BUT system for the Second DIHARD Speech Diarization Challenge,” in ICASSP, 2020, pp. 6529– 6533. [13] Q. Lin, W. Cai, L. Yang, J. Wang, J. Zhang, and M. Li, “DI- HARD II is still hard: Experimental results and discussions from the DKU-LENOVO team,” in Odyssey, 2020, pp. 102– 109. [14] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in SLT,", "meta": {"paper_id": "2012.10055v2", "chunk_index": 8, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_9", "text": "Q. Lin, W. Cai, L. Yang, J. Wang, J. Zhang, and M. Li, “DI- HARD II is still hard: Experimental results and discussions from the DKU-LENOVO team,” in Odyssey, 2020, pp. 102– 109. [14] G. Sell and D. Garcia-Romero, “Speaker diarization with PLDA i-vector scoring and unsupervised calibration,” in SLT, 2014, pp. 413–417. [15] M. Diez, L. Burget, and P. Matejka, “Speaker diarization based on Bayesian HMM with eigenvoice priors,” in Odyssey, 2018, pp. 102–109. [16] F. Landini, S. Wang, M. Diez, L. Burget, P. Matˇejka, K. ˇZmol´ıkov´a, L. Moˇsner, O. Plchot, O. Novotn`y, H. Zeinali, and J. Rohdin, “BUT system description for DIHARD Speech Di- arization Challenge 2019,” arXiv:1910.08847, 2019. [17] M. Diez, L. Burget, S. Wang, J. Rohdin, and J. ˇCernock`y, “Bayesian HMM based x-vector clustering for speaker diariza- tion,” in INTERSPEECH, 2019, pp. 346–350. [18] Z. Huang, S. Watanabe, Y. Fujita, P. Garc´ıa, Y. Shao, D. Povey, and S. Khudanpur, “Speaker diarization with region proposal network,” in ICASSP, 2020, pp. 6514–6518. [19] K. Kinoshita, L. Drude, M. Delcroix, and T. Nakatani, “Listen- ing to each speaker one by one with recurrent selective hearing networks,” in ICASSP, 2018, pp. 5064–5068. [20] K. Kinoshita, M. Delcroix, S. Araki, and T. Nakatani, “Tack- ling real noisy reverberant meetings with all-neural source sep- aration, counting, and diarization system,” in ICASSP, 2020, pp. 381–385. [21] I. Medennikov, M. Korenevsky, T. Prisyach, Y. Khokhlov, M. Korenevskaya, I. Sorokin, T. Timofeeva, A. Mitrofanov, A. Andrusenko, Podluzhny, et al., “Target-speaker voice activity detection: a novel approach for multi-speaker diarization in a dinner party scenario,” in INTERSPEECH, 2020, pp. 274–278. [22] S. Ding, Q. Wang, S.-y. Chang, L. Wan, and I. L. Moreno, “Personal VAD: Speaker-conditioned voice activity detection,” in Odyssey, 2020, pp. 433–439. [23] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A. Chiao, R. Liu, Y. He, W. Li, J. Pelecanos, M. Nika, and A. Gruen- stein, “VoiceFilter-Lite: Streaming targeted voice separation for on-device speech recognition,” in INTERSPEECH, 2020, pp. 2677–2681. [24] J. Carletta, “Unleashing the killer corpus: experiences in cre- ating the multi-everything AMI Meeting Corpus,” Language Resources and Evaluation, vol. 41, no. 2, pp. 181–190, 2007. [25] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech, and noise corpus,” arXiv:1510.08484, 2015. [26] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, “A study on data augmentation of reverberant speech for robust speech recognition,” in ICASSP, 2017, pp. 5220–5224. [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polosukhin, “Attention is all you need,” in NeurIPS, 2017, pp. 5998–6008. [28] P. Garcia, J. Villalba, H. Bredin, J. Du, D. Castan, A. Cristia, L. Bullock, L. Guo, K. Okabe, P. S. Nidadavolu, et al., “Speaker detection in the wild: Lessons learned from JSALT 2019,” in Odyssey, 2020, pp. 415–422. [29] L. Bullock, H. Bredin, and L. P. Garcia-Perera, “Overlap- aware diarization: Resegmentation using neural end-to-end overlapped speech detection,” in ICASSP, 2020, pp. 7114– 7118. [30] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N.", "meta": {"paper_id": "2012.10055v2", "chunk_index": 9, "filename": "2012.10055v2.pdf", "chunk_length": 512}}
{"id": "2012.10055v2_chunk_10", "text": "learned from JSALT 2019,” in Odyssey, 2020, pp. 415–422. [29] L. Bullock, H. Bredin, and L. P. Garcia-Perera, “Overlap- aware diarization: Resegmentation using neural end-to-end overlapped speech detection,” in ICASSP, 2020, pp. 7114– 7118. [30] G. Sell, D. Snyder, A. McCree, D. Garcia-Romero, J. Villalba, M. Maciejewski, V. Manohar, N. Dehak, D. Povey, S. Watan- abe, and S. Khudanpur, “Diarization is hard: Some experi- ences and lessons learned for the JHU team in the inaugural DI- HARD challenge,” in INTERSPEECH, 2018, pp. 2808–2812.", "meta": {"paper_id": "2012.10055v2", "chunk_index": 10, "filename": "2012.10055v2.pdf", "chunk_length": 83}}
{"id": "2012.10187v1_chunk_0", "text": "Regularized Attentive Capsule Network for Overlapped Relation Extraction Tianyi Liu1†, Xiangyu Lin2†, Weijia Jia3,1∗, Mingliang Zhou4, Wei Zhao5 1Department of Computer Science and Engineering, Shanghai Jiao Tong University 2Department of Computer and Information Science, University of Macau 3BNU-UIC Institute of AI and Future Networks, Beijing Normal University (Zhuhai) 4School of Computer Science, Chongqing University 5American University of Sharjah, Sharjah United Arab Emirates Abstract Distantly supervised relation extraction has been widely applied in knowledge base construction due to its less requirement of human efforts. However, the automatically established training datasets in distant supervision contain low-quality instances with noisy words and overlapped relations, introducing great challenges to the accurate extraction of relations. To address this problem, we propose a novel Regularized Attentive Capsule Network (RA-CapNet) to better identify highly overlapped relations in each informal sentence. To discover multiple relation features in an instance, we embed multi-head attention into the capsule network as the low-level capsules, where the subtraction of two entities acts as a new form of relation query to select salient features regardless of their positions. To further discriminate overlapped relation features, we devise disagreement regularization to explicitly encourage the diversity among both multiple attention heads and low-level capsules. Extensive experiments conducted on widely used datasets show that our model achieves signiﬁcant improvements in relation extraction. 1 Introduction Relation extraction aims to extract relations between entities in text, where distant supervision proposed by (Mintz et al., 2009) automatically establishes training datasets by assigning relation labels to instances that mention entities within knowledge bases. However, the wrong labeling problem can occur and var- ious multi-instance learning methods (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) have been proposed to address it. Despite the wrong labeling problem, each instance in distant super- vision is crawled from web pages, which is informal with many noisy words and can express multiple similar relations. This problem is not well-handled by previous approaches and severely hampers the performance of conventional neural relation extractors. To handle this problem, we have to address two challenges: (1) Identifying and gathering spotted relation information from low-quality instances; (2) Distinguishing multiple overlapped relation features from each instance. First, a few signiﬁcant relation words are distributed dispersedly in the sentence, as shown in Figure 1, where words marked in red brackets represent entities, and italic words are key to expressing the rela- tions. For instance, the clause “evan bayh son of birch bayh” in S1 is sufﬁcient to express the relation /people/person/children of evan bayh and birch bayh. Salient relation words are few in number and dis- persedly in S1, while others excluded from the clause can be regarded as noise. Traditional neural models have difﬁculty gathering spotted relation features at different positions along the sequence because they use Convolutional Neural Network (CNN) or Recurrent Neural Network (RNN) as basic relation en- coders (Zeng et al., 2015; Liu et al., 2018; Ye and Ling, 2019), which model each sequence word by word and lose rich non-local information for modeling the dependencies of semantic salience. Thus, a well-behaved relation extractor is needed", "meta": {"paper_id": "2012.10187v1", "chunk_index": 0, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_1", "text": "Network (CNN) or Recurrent Neural Network (RNN) as basic relation en- coders (Zeng et al., 2015; Liu et al., 2018; Ye and Ling, 2019), which model each sequence word by word and lose rich non-local information for modeling the dependencies of semantic salience. Thus, a well-behaved relation extractor is needed to extract scattered relation features from informal instances. Second, each instance can express multiple similar relations of two entities. As shown in Figure 1, Changsha and Hunan possess the relations /location/location/contains and /location/province/capital in S2, which have similar semantics, introducing great challenges for neural extractors in discriminating †make equal contributions ∗Corresponding author: jiawj@sjtu.edu.cn arXiv:2012.10187v1 [cs.CL] 18 Dec 2020 ID Instances Relations S1 senator [evan_bayh], son of former senator [birch_bayh] of indiana, is organizing and testing the waters for a possible presidential bid in 2008. /people/person/children S2 that is one reason that [hunan]'s fast-growing provincial capital, [changsha], is beginning to siphon some workers. /location/location/contains /location/province/capital S3 the land is near calgary; while that is one of [alberta]'s largest cities, the capital is [edmonton]. /location/location/contains /location/province/capital /location/country/capital … … … Figure 1: Example of instances from the New York Times (NYT). them clearly. Conventional neural methods are not effective at extracting overlapped relation features, because they mix different relation semantics into a single vector by max-pooling (Zeng et al., 2014) or self-attention (Lin et al., 2016). Although (Zhang et al., 2019) ﬁrst propose an attentive capsule network for multi-labeled relation extraction, it treats the CNN/RNN as low-level capsules without the diversity encouragement, which poses the difﬁculty of distinguishing different and overlapped relation features from a single type of semantic capsule. Therefore, a well-behaved relation extractor is needed to discriminate diverse overlapped relation features from different semantic spaces. To address the above problem, we propose a novel Regularized Attentive Capsule Network (RA- CapNet) to identify highly overlapped relations in the low-quality distant supervision corpus. First, we propose to embed multi-head attention into the capsule network, where attention vectors from each head are encapsulated as a low-level capsule, discovering relation features in an unique semantic space. Then, to improve multi-head attention in extracting spotted relation features, we devise relation query multi-head attention, which selects salient relation words regardless of their positions. This mechanism assigns proper attention scores to salient relation words by calculating the logit similarity of each relation representation and word representation. Furthermore, we apply disagreement regularization to multi- head attention and low-level capsules, which encourages each head or capsule to discriminate different relation features from different semantic spaces. Finally, the dynamic routing algorithm and sliding- margin loss are employed to gather diverse relation features and predict multiple speciﬁc relations. We evaluate RA-CapNet using two benchmarks. The experimental results show that our model achieves satisfactory performance over the baselines. Our contributions are summarized as follows: • We ﬁrst propose to embed multi-head attention as low-level capsules into the capsule network for distantly supervised relation extraction. • To improve the ability of multi-head attention in extracting scattered relation features, we design relation query multi-head attention. • To discriminate overlapped relation features, we devise disagreement regularization on", "meta": {"paper_id": "2012.10187v1", "chunk_index": 1, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_2", "text": "• We ﬁrst propose to embed multi-head attention as low-level capsules into the capsule network for distantly supervised relation extraction. • To improve the ability of multi-head attention in extracting scattered relation features, we design relation query multi-head attention. • To discriminate overlapped relation features, we devise disagreement regularization on multi-head attention and low-level capsules. • RA-CapNet achieves signiﬁcant improvements for distantly supervised relation extraction. 2 Related Work Distantly supervised relation extraction has been essential for knowledge base construction since (Mintz et al., 2009) propose it. To address the wrong labeling problem in distant supervision, multi-instance and multi-label approaches are proposed (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). With the renaissance of neural networks, increasing researches in distant supervision have been pro- posed to extract precise relation features. Piecewise CNNs with various attention mechanisms are pro- posed (Zeng et al., 2015; Lin et al., 2016; Ji et al., 2017). Reinforcement learning and adversarial training are proposed to select valid instances to train relation extractors (Feng et al., 2018; Qin et al., 2018b; Qin et al., 2018a). Recently, multi-level noise reduction is designed by (Ye and Ling, 2019; Jia et al., 2019). Nevertheless, the above approaches ignore the effect of noisy words and overlapped relation features in each instance. To reduce the impact of noisy words, tree-based methods attempt to obtain the rele- vant sub-structure of an instance for relation extraction (Xu et al., 2015; Miwa and Bansal, 2016; Liu et al., 2018). To discriminate overlapped relation features, (Zhang et al., 2019) apply the capsule net- work (Sabour et al., 2017) for multi-labeled relation extraction. Inspired by the ability of multi-head attention in modeling the long-term dependency (Vaswani et al., 2017), (Zhang et al., 2020) attempt to reduce multi-granularity noise via multi-head attention in relation extraction. senator evan_bahy , son of former birch_bahy of indiana x1 x2 x3 x4 x5 x6 x7 x8 x9 Word Encoding h1 h2 h3 h4 h5 h6 h7 h8 h9 BLSTM Encoding Regularized Relation Query Multi-Head Attention Low-Level Regularized Capsules ... u11 u1k ... u21 u2k ... u31 u3k ... ut1 utk ... High-Level Capsules Dynamic Routing r2 r1 ... rm Head1 Head2 diff. h2-h7 h1 h2 h3 h4 h5 h6 h7 h8 h9 query Headi h1 h2 h3 h4 h5 h6 h7 h8 h9 (a) Relation Query Attention (b) Disagreement Regularization y2 y1 ... ym Output yNA diff. diff. diff. Figure 2: Overall architecture of RA-CapNet, expressing the process of handling an instance. 3 Methodology As shown in Figure 2, we will introduce the three-layer RA-CapNet: (1) The Feature Encoding Layer primarily contains the word encoding layer and BLSTM encoding layer. (2) The Feature Extracting Layer chieﬂy includes relation query multi-head attention and disagreement regularization. (3) The Relation Gathering Layer mainly consists of a regularized capsule network and dynamic routing. 3.1 Feature Encoding Layer Each instance is ﬁrst input into the encoding layer to be transformed to the distributed representations for the convenience of calculation and extraction by neural networks. Word Encoding Layer As mentioned in (Zeng et al., 2014), the inputs", "meta": {"paper_id": "2012.10187v1", "chunk_index": 2, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_3", "text": "of a regularized capsule network and dynamic routing. 3.1 Feature Encoding Layer Each instance is ﬁrst input into the encoding layer to be transformed to the distributed representations for the convenience of calculation and extraction by neural networks. Word Encoding Layer As mentioned in (Zeng et al., 2014), the inputs of the relation extractor are word and position tokens, which are encoded by word embeddings and position embeddings at ﬁrst. Then, the jth input word xij in the ith instance, is concatenated by one word embedded vector xw ij ∈Rk and two position embedded vectors xp1 ij and xp2 ij ∈Rp, xij = [xw ij; xp1 ij ; xp2 ij ], where k and p represent the dimensions of word vectors and position vectors respectively, and ; denotes the vertical concatenating operation. To simplify the mathematical expression, we denote xij as xj. BLSTM Encoding Layer To further encode relation features inside the context, we adopt the Bidirectional Long-Short Term net- work (BLSTM) (Graves, 2013) as our basic relation encoder, which can access the future context as well as the past. The encoding feature vector hi of the ith word is calculated as follows: −→ hi = −−−−→ LSTM(xi, −−→ hi−1) (1) ←− hi = ←−−−− LSTM(xi, ←−− hi+1) (2) hi = −→ hi + ←− hi (3) where −→ hi and ←− hi ∈Rd are hidden state vectors of the LSTM. Finally, we obtain the sentence encoding vector H = [h1, h2, · · · , hl], where l represents the instance length. 3.2 Feature Extracting Layer First, relation query multi-head attention is devised to emphasize spotted relation features from different semantic spaces. Then, disagreement regularization is applied to encouraging the diversity of relation features that each head discovers. Relation Query Multi-Head Attention Multi-head attention is useful for modeling the long-term dependency of salient information in the con- text (Vaswani et al., 2017). Based on this mechanism, we propose relation query multi-head attention to improve the ability of multi-head attention in extracting spotted and salient relation features regardless of their irregular positions in the instance. Formally, given an encoding instance H, we use the subtraction of two entities’ states hen1 and hen2 as the relation representation, as inspired by (Bordes et al., 2013). The relation representation acts as a query vector as follows: Qrel = (hen1 −hen2)W Q (4) where W Q ∈Rd×d is a weight matrix. The corresponding key K and value V vectors are deﬁned: K = HW K V = HW V (5) where W K and W V ∈Rd×d are weight matrices. Afterward, we calculate the logit similarity of the relation query vector and word representation vectors as attention scores: energy = QrelKT √ d (6) where the energy can measure the importance of each word to relation extraction, which is leveraged to select salient and spotted relation features along the sequence: ATT = softmax(energy)V (7) To extract diverse relation features, we employ relation query attention into multi-head attention: headi = ATT(Qrel i , Ki, Vi) (8) Em = [head1; head2; · · · ; headn]W o (9)", "meta": {"paper_id": "2012.10187v1", "chunk_index": 3, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_4", "text": "extraction, which is leveraged to select salient and spotted relation features along the sequence: ATT = softmax(energy)V (7) To extract diverse relation features, we employ relation query attention into multi-head attention: headi = ATT(Qrel i , Ki, Vi) (8) Em = [head1; head2; · · · ; headn]W o (9) where W o ∈Rd×d is the weight matrix. Multiple heads can capture various semantic features. After we acquire the output Em of multi-head attention, a Feed-Forward Network (FFN) is applied: Hr = max(0, EmW f 1 + bf 1)W f 2 + bf 2 (10) where W f 1 ∈Rd×d, W f 2 ∈Rd×d′, bf 1 ∈Rd and bf 2 ∈Rd′ are parameters. Disagreement Regularization on Multi-Head Attention To further discriminate overlapped relation features from different heads in multi-head attention, we introduce the disagreement regularization based on (Yang et al., 2018). Formally, given n heads Head = [head1, head2, · · · , headn] as calculated in Eq. (8), we calculate the cosine similarity cos(.) between the vector pair headi and headj in different value subspaces: Dsub ij = cos(headi, headj) = headi · headj ∥headi∥∥headj∥ (11) where ∥∗∥represents the L2 norm of vectors. The average cosine distance among all heads is obtained: Dsub = P ij Dsub ij n2 (12) Our goal is to minimize Dsub, which encourages the heads to be different from each other, improving the diversity of subspaces among multiple heads. Accordingly, each head can discriminate overlapped relation features more clearly. 3.3 Relation Gathering Layer To form relation-speciﬁc features, the relation gathering layer gathers scattered relation features from diverse low-level capsules using a dynamic routing algorithm. Low-Level Capsules with Disagreement Regularization The capsule network has been proven effective in discriminating overlapped features (Sabour et al., 2017; Zhang et al., 2019). In our application, a capsule is a group of neural vectors within one-head attention and regularized by a disagreement term. Thus, each capsule can capture relation features in an unique semantic space. In detail, the orientation of the attention vector inside one head indicates one certain factor of a speciﬁc relation, while its length means the probability that this relational factor exists. We reorganize each attention head of Hr to form a low-level capsule denoted as u ∈Rdu, where each capsule captures information in a speciﬁc semantic space. Formally, the above process is as follows: Hr = [hr 1; ...; hr t] (13) uk = g(hr k) = ∥hr k∥2 1 + hr k 2 hr k hr k (14) where t is the number of low-level capsules, which equals the quantity of heads. Eq. (14) is a squash function, shrinking the length of vectors from 0 to 1 to express the probability. To encourage the diversity of these capsules, disagreement regularization is applied to them: Dcap ij = ui · uj ∥ui∥∥uj∥ (15) Dcap = P ij Dcap ij t2 (16) To minimize Dcap, we can encourage the capsules to be different from each other, improving the diversity of subspaces among multiple capsules and discriminating overlapped relation features more clearly. The ﬁnal disagreement regularization term is the", "meta": {"paper_id": "2012.10187v1", "chunk_index": 4, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_5", "text": "= ui · uj ∥ui∥∥uj∥ (15) Dcap = P ij Dcap ij t2 (16) To minimize Dcap, we can encourage the capsules to be different from each other, improving the diversity of subspaces among multiple capsules and discriminating overlapped relation features more clearly. The ﬁnal disagreement regularization term is the average of multi-head and capsule disagreement: D = Dsub + Dcap 2 (17) where D is the ﬁnal disagreement regularization term which only works for the training process. High-Level Capsules with Dynamic Routing After the low-level capsules capturing the different aspects of semantic information, the high-level cap- sules r ∈Rdr are produced from them to gather scattered information and form speciﬁc relation features, which are calculated as follows: rj = g( X cijW h j ui) (18) where W h j ∈Rdu×dr are parameters for high-level capsules and cij are coupling coefﬁcients that are determined by the dynamic routing process described in (Sabour et al., 2017). Loss Function The sliding-margin loss function used in the capsule network enables the prediction of multiple over- lapped relations, which sums up the loss for both the relations present and absent from the instances. This margin loss function is integrated into our model as follows: Lj =Yjmax(0, (S + γ) −∥rj∥)2+ λ(1 −Yj)max(0, ∥rj∥−(S −γ))2 (19) where γ is the width of the margin, S is a learnable threshold for “no relation” (NA), and λ is the down- weighting of the loss for absent relations. Yj = 1 if the relation corresponding to rj is present in the sentence and Yj = 0 otherwise. Afterward, the ﬁnal loss is deﬁned as follows: loss = X j Lj + βD + β′∥θ∥2 (20) where β and β′ are hyperparameters used to restrict the disagreement regularization and L2 regularization of all parameters θ. In this paper, we use the Adam (Kingma and Ba, 2014) to minimize the ﬁnal loss. 4 Experiments Our experiments are devised to demonstrate that RA-CapNet can identify highly overlapped relations of informal instances in distant supervision. In this section, we ﬁrst introduce the dataset and experimental setup. Then, we evaluate the overall performance of RA-CapNet and the effects of different parts of RA-CapNet. Finally, we present the case study. 4.1 Dataset and Experimental Setup Dataset. To evaluate the effects of RA-CapNet, we conduct experiments on two datasets. NYT-10 is a standard dataset constructed by (Riedel et al., 2010), which aligns relational tuples in Freebase (Bol- lacker et al., 2008) with the corpus of New York Times. Sentences from 2005-2006 are used as the training set, while sentences from 2007 are used for testing. NYT-18 is a larger dataset constructed by (Zhang et al., 2020) with the same creation method as NYT-10, which crawls 2008-2017 contexts from the NYT. All the sentences are divided into ﬁve parts with the same relation distribution for ﬁve-fold cross-validation. The details of the datasets are illustrated in Table 1. Datasets Training (k) Testing (k) Rel. Sen. Ent. Sen. Ent. NYT-10 523 281 172 97 53 NYT-18 2446 1234 611 394 503 Table 1: The dataset information. Sen., Ent. and", "meta": {"paper_id": "2012.10187v1", "chunk_index": 5, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_6", "text": "ﬁve parts with the same relation distribution for ﬁve-fold cross-validation. The details of the datasets are illustrated in Table 1. Datasets Training (k) Testing (k) Rel. Sen. Ent. Sen. Ent. NYT-10 523 281 172 97 53 NYT-18 2446 1234 611 394 503 Table 1: The dataset information. Sen., Ent. and Rel. indicate numbers of sentences, entity pairs and relations (including NA). Evaluation Metric. As mentioned in (Mintz et al., 2009), we use the held-out metrics to evaluate RA-CapNet. The held-out evaluation offers an automatic way to assess models with the PR curve and Precision at top 100 or 10k predictions (P@100 on NYT-10 or P@10k on NYT-18) at all numbers of instances under each entity pair, which indicates that all instances under the entity pair are used to represent the relation. Parameter. In our work, we use the Skip-Gram (Mikolov et al., 2013) to pretrain our word embedding matrices. The words of an entity are concatenated when it has multiple words. The grid search and cross- validation are used to adjust important hyperparameters of the networks. Our ﬁnal parameter settings are illustrated in Table 2. Parameter NYT-10 NYT-18 Batch size b 50 50 Word embedding size k 50 360 Position embedding size p 5 5 Sentence length l 100 100 LSTM hidden size d 256 256 Multi-head number n 16 16 FFN hidden size d′ 512 512 Capsule dimensions [du, dr] [16,16] [16,16] Low-level capsule number t 16 16 Valid relation class m 52 502 Sliding margin γ 0.4 0.4 Down-weighting λ 1.0 1.0 Learning rate lr 0.0001 0.0001 Dropout probability p 0.5 0.5 Weight of disagreement β 0.001 0.001 L2 penalty β′ 1e-08 0.0 Table 2: Parameter settings. 4.2 Overall Performance 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Precision PCNN BGRU+ATT PCNN+ATT BGRU+SET ATT-CAPNET PCNN+INTRA+INTER QARE+ATT RA-CAPNET Figure 4: Precision-recall curves on NYT-10. 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00 Precision PCNN PCNN+ATT BGRU+SET PCNN+INTRA+INTER BGRU+ATT ATT+CAPNET QARE+ATT RA-CAPNET Figure 5: Precision-recall curves on NYT-18. To evaluate our model, we select the following methods for comparison: PCNN (Zeng et al., 2015) present a piecewise CNN for relation extraction. PCNN+ATT (Lin et al., 2016) propose the selective attention mechanism with PCNN. BGRU+ATT (Zhou et al., 2016) present a BGRU-based model with word-level attention. BGRU+SET (Liu et al., 2018) propose a BGRU-based approach to reduce inner-sentence noise. PCNN+INTRA+INTER (Ye and Ling, 2019) propose to emphasize true labeled sentences and bags. ATT+CAPNET (Zhang et al., 2019) put forward an attentive capsule network for relation extraction. QARE+ATT (Zhang et al., 2020) propose improved multi-head attention with transfer learning. We compare our method with baselines on two datasets. For both datasets, the PR curves on NYT-10 and NYT-18 are shown in Figure 4 and Figure 5. We ﬁnd that: (1) BGRU+SET performs well on NYT- 10 but poorly on NYT-18. This demonstrates that BGRU+SET is not well-handled on highly informal instances because the complex instances in NYT-18 are difﬁcult to be parsed precisely by the conven- tional", "meta": {"paper_id": "2012.10187v1", "chunk_index": 6, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_7", "text": "NYT-18 are shown in Figure 4 and Figure 5. We ﬁnd that: (1) BGRU+SET performs well on NYT- 10 but poorly on NYT-18. This demonstrates that BGRU+SET is not well-handled on highly informal instances because the complex instances in NYT-18 are difﬁcult to be parsed precisely by the conven- tional parser. (2) RA-CapNet achieves the best PR curve among all baselines on both datasets, which improves the PR curve signiﬁcantly. This veriﬁes that our model is effective in capturing overlapped and scattered relation features. (3) RA-CapNet outperforms ATT+CAPNET, which indicates that the relation query multi-head attention and disagreement regularization are useful for overlapped relation extraction. Model PR curve area NYT-10 NYT-18 BGRU+ATT 0.337 0.596 PCNN+ATT 0.356 0.511 BGRU+SET 0.392 0.290 ATT-CAPNET 0.415 0.647 PCNN+INTRA+INTER 0.423 0.617 QARE+ATT 0.428 0.645 RA-CAPNET 0.526 0.780 Table 3: Precision-recall curve areas. Model P@100 P@10k PCNN 72.3 81.0 PCNN+ATT 82.0 82.2 BGRU+ATT 74.0 88.1 BGRU+SET 87.0 67.4 PCNN+INTRA+INTER 91.8 - ATT+CAPNET 84.0 - QARE+ATT 93.0 91.6 RA-CAPNET 98.0 96.6 Table 4: P@100 and P@10k. A detailed comparison of all approaches, the areas of the PR curves, P@100 and P@10k on NYT-10 or NYT-18, are illustrated in Table 3 and Table 4. From the tables, we ﬁnd that: (1) RA-CapNet is the ﬁrst method to increase the PR curve area over 0.5 on NYT-10 while improving it on NYT-18 to 0.7. In P@100 and P@10k, our model also achieves superior performance. This result further demonstrates the effectiveness of RA-CapNet with multi-instance learning on overlapped relation extraction. (2) Capnet- based models achieve better performance on the highly complex NYT-18 dataset, which results from their capability of handling overlapped relations and complex sentences. 4.3 Ablation Study 0.0 0.1 0.2 0.3 0.4 0.5 0.6 Recall 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 Precision ATT-CAPNET A-CAPNET HR-CAPNET CR-CAPNET RA-CAPNET R-MHATT Figure 6: PR curves of our models. Model PR curve area R-MHATT 0.383 ATT-CAPNET 0.415 A-CAPNET 0.449 HR-CAPNET 0.493 CR-CAPNET 0.501 RA-CAPNET 0.526 Figure 7: PR curve areas of our models. To further evaluate the impacts of different parts on RA-CapNet, we compare the performance on the NYT-10 dataset of RA-CapNet with ﬁve settings: R-MHATT: Two multi-head attention layers with relation query attention. ATT-CAPNET: The same as above. A-CAPNET: RA-CapNet without disagreement regularization. HR-CAPNET: RA-CapNet without capsule disagreement regularization. CR-CAPNET: RA-CapNet without multi-head disagreement regularization. RA-CAPNET: Our model. In Figure 6 and 7, the result indicates that: (1) ATT-CAPNET improves the performance of R-MHATT by incorporating the capsule network for handling the multiple relations. (2) Compared with ATT- CAPNET, A-CAPNET improves the PR curve area from 0.415 to 0.449. This proves that relation query multi-head attention helps the capsule network extract salient relation features from different represen- tations at different positions. (3) HR-CAPNET further increases the PR curve area to 0.493, which proves the effectiveness of our disagreement regularization on multiple heads in discriminating the di- verse overlapped relation features. (4) Compared with A-CAPNET, CR-CAPNET achieves 0.501 of the PR curve area. This demonstrates that disagreement regularization on the capsules helps models distin- guish multiple relation features more clearly. (5) Our", "meta": {"paper_id": "2012.10187v1", "chunk_index": 7, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_8", "text": "which proves the effectiveness of our disagreement regularization on multiple heads in discriminating the di- verse overlapped relation features. (4) Compared with A-CAPNET, CR-CAPNET achieves 0.501 of the PR curve area. This demonstrates that disagreement regularization on the capsules helps models distin- guish multiple relation features more clearly. (5) Our complete model, RA-CAPNET, achieves the best performance, showing that the relation query multi-head attention and disagreement regularization term are both effective for relation extraction. 4.4 Case Study ID Instances S1 that is one reason that [hunan]'s fast-growing provincial capital, [changsha], is beginning to siphon some workers. Model PCNN+ATT BGRU+ATT ATT+CAPNET QARE+ATT RA-CAPNET LLC ✗ ✗ ✓ ✗ ✓ LPC ✓ ✓ ✓ ✓ ✓ S2 the land is near calgary; while that is one of [alberta]’s largest cities, the capital is [edmonton]. Model PCNN+ATT BGRU+ATT ATT+CAPNET QARE+ATT RA-CAPNET LLC ✗ ✗ ✗ ✗ ✓ LPC ✗ ✗ ✓ ✗ ✓ LCC ✓ ✓ ✓ ✓ ✓ Figure 7: Prediction results of different models on some samples. “LLC”, “LPC” and “LCC” represent relation labels of “/location/location/contains”, ”/location/province/capital” and ”/loca- tion/country/capital”. In Figure 7, we randomly select two samples from NYT-10 to analyze the prediction performance of different models, where the entities are labeled in the red and bold brackets. From the ﬁgure, we ﬁnd the following: (1) In S1 and S2, compared with CNN/RNN/Attention-based methods, the capsule-based approaches can predict multiple similar relations. (2) In S2, only RA-CapNet predicts the correct relation of “/location/location/contains”. This result demonstrates that by incorporating relation query multi-head attention and disagreement regularization in the capsule network, RA-CapNet makes further progress in discriminating overlapped relations. 5 Conclusion and Future Work In this paper, we propose a novel regularized attentive capsule network for overlapped relation extrac- tion. RA-CapNet embeds relation query multi-head attention into the capsule network and uses a novel disagreement regularization term to encourage the diversity among heads and capsules, making it capa- ble of gathering salient information from diverse semantic spaces. Our model is resistant to the noise of distant supervision and achieves signiﬁcant improvements on both standard and complex datasets. In the future, we will experiment with different forms of regularization terms and their application to other components of our model. Acknowledgements This work is partially supported by Chinese National Research Fund (NSFC) Key Project No. 61532013 and No. 61872239; BNU-UIC Institute of Artiﬁcial Intelligence and Future Networks funded by Beijing Normal University (Zhuhai) and AI and data Science Hub, United International College (UIC), Zhuhai, Guangdong, China. References Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the ACM SIGMOD. Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translat- ing embeddings for modeling multi-relational data. In Proceedings of the NeurIPS. Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. 2018. Reinforcement learning for relation classiﬁcation from noisy data. In Proceedings of the AAAI. Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and", "meta": {"paper_id": "2012.10187v1", "chunk_index": 8, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_9", "text": "Proceedings of the NeurIPS. Jun Feng, Minlie Huang, Li Zhao, Yang Yang, and Xiaoyan Zhu. 2018. Reinforcement learning for relation classiﬁcation from noisy data. In Proceedings of the AAAI. Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850. Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the ACL. Guoliang Ji, Kang Liu, Shizhu He, and Jun Zhao. 2017. Distant supervision for relation extraction with sentence- level attention and entity descriptions. In Proceedings of the AAAI. Wei Jia, Dai Dai, Xinyan Xiao, and Hua Wu. 2019. Arnor: Attention regularization based noise reduction for distant supervision relation classiﬁcation. In Proceedings of the ACL. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural relation extraction with selective attention over instances. In Proceedings of the ACL. Tianyi Liu, Xinsong Zhang, Wanhao Zhou, and Weijia Jia. 2018. Neural relation extraction via inner-sentence noise reduction and transfer learning. In Proceedings of the EMNLP. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations in vector space. Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the ACL and the AFNLP. Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using lstms on sequences and tree struc- tures. In Proceedings of the ACL. Pengda Qin, XU Weiran, and William Yang Wang. 2018a. Dsgan: Generative adversarial training for distant supervision relation extraction. In Proceedings of the ACL. Pengda Qin, XU Weiran, and William Yang Wang. 2018b. Robust distant supervision relation extraction via deep reinforcement learning. In Proceedings of the ACL. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In Proceedings of the ECML and PKDD. Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. 2017. Dynamic routing between capsules. In Proceedings of the NeurIPS. Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning. 2012. Multi-instance multi- label learning for relation extraction. In Proceedings of the 2012 Joint Conference on EMNLP and CoNLL. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of the NeurIPS. Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying relations via long short term memory networks along shortest dependency paths. In Proceedings of the EMNLP. Baosong Yang, Zhaopeng Tu, Derek F Wong, Fandong Meng, Lidia S Chao, and Tong Zhang. 2018. Modeling localness for self-attention networks. In Proceedings of the EMNLP. Zhi-Xiu Ye and Zhen-Hua Ling. 2019. Distant supervision relation extraction with intra-bag and inter-bag atten- tions. In Proceedings of the ACL. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classiﬁcation via convolu- tional deep neural network. In Proceedings of COLING. Daojian Zeng, Kang Liu, Yubo Chen, and Jun", "meta": {"paper_id": "2012.10187v1", "chunk_index": 9, "filename": "2012.10187v1.pdf", "chunk_length": 512}}
{"id": "2012.10187v1_chunk_10", "text": "Zhen-Hua Ling. 2019. Distant supervision relation extraction with intra-bag and inter-bag atten- tions. In Proceedings of the ACL. Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation classiﬁcation via convolu- tional deep neural network. In Proceedings of COLING. Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the EMNLP. Xinsong Zhang, Pengshuai Li, Weijia Jia, and Hai Zhao. 2019. Multi-labeled relation extraction with attentive capsule network. In Proceedings of the AAAI. X. Zhang, T. Liu, P. Li, W. Jia, and H. Zhao. 2020. Robust neural relation extraction via multi-granularity noises reduction. IEEE Transactions on Knowledge and Data Engineering. Peng Zhou, Wei Shi, Jun Tian, Zhenyu Qi, Bingchen Li, Hongwei Hao, and Bo Xu. 2016. Attention-based bidirectional long short-term memory networks for relation classiﬁcation. In Proceedings of the ACL.", "meta": {"paper_id": "2012.10187v1", "chunk_index": 10, "filename": "2012.10187v1.pdf", "chunk_length": 146}}
{"id": "2012.10226v1_chunk_0", "text": "arXiv:2012.10226v1 [cs.IR] 18 Dec 2020 Should I visit this place? Inclusion and Exclusion Phrase Mining from Reviews Omkar Gurjar1 and Manish Gupta1,2 1IIIT-Hyderabad, India 2Microsoft, India omkar.gurjar@students.iiit.ac.in, manish.gupta@iiit.ac.in, gmanish@microsoft.com Abstract. Although several automatic itinerary generation services have made travel planning easy, often times travellers ﬁnd themselves in unique situations where they cannot make the best out of their trip. Visitors diﬀer in terms of many factors such as suﬀering from a disability, being of a particular dietary preference, travelling with a toddler, etc. While most tourist spots are universal, others may not be inclusive for all. In this paper, we focus on the problem of mining inclusion and exclusion phrases associated with 11 such factors, from reviews related to a tourist spot. While existing work on tourism data mining mainly focuses on structured extraction of trip related information, personalized sentiment analysis, and automatic itinerary generation, to the best of our knowl- edge this is the ﬁrst work on inclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000 reviews related to 1000 tourist spots, our broad level classiﬁer provides a binary overlap F1 of ∼80 and ∼82 to classify a phrase as inclusion or exclusion respectively. Further, our inclusion/exclusion classiﬁer provides an F1 of ∼98 and ∼97 for 11- class inclusion and exclusion classiﬁcation respectively. We believe that our work can signiﬁcantly improve the quality of an automatic itinerary generation service. 1 Introduction Hundreds of millions of visitors travel across the globe every year resulting into trillions of dollars of spending. Number of international tourist arrivals has seen a steady increase over the past few decades1. Thanks to the availability of multiple online services like web maps, travel and stay booking, and automatic planning, tourism has become a lot comfortable in recent years. Automated itinerary planning systems2 provide a holistic solution enabling transportation, lodging, sights, and food recommendations. However such rec- ommendation systems cannot incorporate subtle user constraints like a claustro- phobic user, visitors travelling with a toddler, visitors of a particular ethnicity with visa restrictions, etc. Indeed, many of them, do not even incorporate tourist spot speciﬁc properties like what time of day is best to visit, temporary ad hoc 1 https://data.worldbank.org/indicator/ST.INT.ARVL 2 http://itineree.com/top-online-travel-planners/ 2 Gurjar and Gupta closures due to local vacations or maintenance work, visitor height/gender re- strictions, vegetarian friendly or not, etc. Tourist review websites are a gold mine of data related to very subtle re- strictions (or exclusions) associated with a tourist spot. In this work, we focus on the following 11 diﬀerent factors regarding inclusion or exclusion nature of tourist spots. (1)Age/Height: Disallow visitors of a particular age/height group: too old, or too young, too short. (2) Claustrophobia: Some spots consist of a lot of conﬁned spaces and hence unsuitable for claustrophobic visitors. (3) Cou- ples/Family: Some spots are family/kids friendly versus not. (4) Crowd: Some spots are often heavily crowded, which may be repulsive to some visitors. (5) Food: Some spots may serve low quality food, non-vegetarian food only, may not serve any food, may not allow any external food, may not allow alcoholic", "meta": {"paper_id": "2012.10226v1", "chunk_index": 0, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_1", "text": "ples/Family: Some spots are family/kids friendly versus not. (4) Crowd: Some spots are often heavily crowded, which may be repulsive to some visitors. (5) Food: Some spots may serve low quality food, non-vegetarian food only, may not serve any food, may not allow any external food, may not allow alcoholic drinks, etc. (6) Handicap: Some spots may not allow facilities for disabled folks like lifts, ramps, etc. The terrain may not be wheel-chair or stroller friendly. (7) Hygiene: Some spots may be ﬁlthy, e.g., unclean toilets, littered beaches, etc. (8) Parking: Unavailability and ease of parking. (9) Price: Some spots may be very expensive for tourists. (10) Queues: Some spots may exhibit large queues leading to long wait times. Visitors on a tight schedule may want to avoid such places, or visit them in low wait time durations. (11) Time: Various spots have a preferred visit timings, such as early morning, late evening, on Wednesdays, from Sep-Dec, etc. This category also includes ad hoc closures due to maintenance or other reasons. In this paper, we focus on two related tasks: (1) Task 1 pertains to mining in- clusion/exclusion phrases from tourism reviews. A phrase which pertains to any of the exclusions as mentioned above is labeled as an exclusion phrase, while a phrase related to inclusion of the above factors is labeled as an inclusion phrase. (2) Task 2 is about ﬁne-grained classiﬁcation of inclusion/exclusion phrases into one of the above 11 categories. “I had my kids who loved this museum” and “ele- vators for those whom stairs are problematic” are examples of age and handicap inclusion phrases. “place was very crowded”, “would not recommend the area for young children” are examples of crowd and age exclusion phrases. We are the ﬁrst to propose the problem of extracting inclusion/exclusion phrases from tourism review data. The problem is challenging: (1) There can be many types of exclusions as discussed above. (2) These factors can be expressed in lots of diﬀerent ways. (3) There could be multiple indirect references (e.g. if the place allows gambling, likely kids are not allowed) or unrelated references (e.g., a review talking about a tour guide’s “family” rather than if “families” are allowed at the spot). Overall, we make the following contributions in this paper: (1) We propose a novel task of mining inclusion/exclusion phrases from online tourist reviews, and their ﬁne-grained classiﬁcation. (2) We model the ﬁrst task as a sequence label- ing problem, and the second one as a multi-class classiﬁcation. We investigate the eﬀectiveness of CRFs (Conditional Random Fields), BiLSTMs (Bidirectional Long Short-Term Memory networks), and Transformer models like BERT (Bidi- rectional Encoder Representations from Transformers). (3) We make the code and the manually labeled dataset (2303 phrases mined from ∼2000 reviews) Inclusion and Exclusion Phrase Mining from Reviews 3 publicly available3. Our experiments show that the proposed models lead to practically usable classiﬁers. 2 Related Work Tourism Data Mining: Work on tourism data mining has mostly focused on structured extraction of trip related information [18], mining reviews (per- sonalized sentiment analysis of tourist reviews [15],", "meta": {"paper_id": "2012.10226v1", "chunk_index": 1, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_2", "text": "Mining from Reviews 3 publicly available3. Our experiments show that the proposed models lead to practically usable classiﬁers. 2 Related Work Tourism Data Mining: Work on tourism data mining has mostly focused on structured extraction of trip related information [18], mining reviews (per- sonalized sentiment analysis of tourist reviews [15], establishing review credibil- ity [1,7]), and automatic itinerary generation [2,3,5,8]. Popescu et al. [18] extract visit durations or information like “what can I visit in one day in this city?” from Flickr data. Pantano et al. [15] predict tourists’ future preferences from reviews. Ayeh et al. [1] examine the credibility perceptions and online travelers’ attitude towards using user-generated content (UGC). Filieri et al. [7] study the impact of source credibility, information quality, website quality, customer satisfaction, user experience on users’ trust towards UGC. The automatic itinerary generation problem has been studied extensively from multiple perspectives. Friggstad et al. [8] model the problem as an orienteering problem on a graph of tourist spots. Chang et al. [2] weigh diﬀerent factors like spot name, popularity, isRestau- rant, isAccomodation, etc. based on user interactions to optimize the process of trip planning. De et al. [5] aggregate across geo-temporal breadcrumbs data for multiple users to construct itineraries. Clearly, our system can be an important sub-module to generate automated itineraries which are exclusion-sensitive. Sequence Labeling: Sequence labeling involves predicting an output label se- quence given an input text sequence. A label is generated per input token. Pop- ular sequence labeling models include CRFs [13], LSTMs [9], LSTM-CRFs [11], and Transformer models like BERT [6]. Many NLP tasks can be modeled as sequence labeling tasks including opinion mining [12], part-of-speech tagging, etc. The labels for such tasks are typically encoded using BIO (begin, inside, outside) labeling. In this paper, we investigate the eﬀectiveness of such sequence labeling approaches for the inclusion/exclusion phrase mining task. Aspect Extraction: Aspect extraction has been studied widely in the past decade, mainly for product reviews, using supervised [19], semi-supervised [14] as well as unsupervised [10] methods. In this work, we study aspect extraction for reviews in the tourism domain. 3 Proposed Approach 3.1 Dataset We ﬁrst obtained a list of top 1000 tourist spots from lonelyplanet.com (a popu- lar tourist website). Next, we obtained a maximum of 2000 reviews correspond- ing to each of these spots from tripadvisor.com. Further, we broadly ﬁltered out reviews (and then sentences) that could be potentially related to the eleven 3 https://github.com/omkar2810/Inclusion_Exclusion_Phrase_Mining 4 Gurjar and Gupta factors mentioned in Sec. 1 using a manually produced keyword list for each category. We provide the full keyword list per category as part of the dataset. These ∼2000 reviews were then manually labeled for inclusion/exclusion phrases using the BIO tagging, as well as their ﬁne categorization into one of the 11 cat- egories. A total of 2303 phrases were labeled with one of the 11 categories. The distribution across the categories is as follows: Age/Height: 324, Claustrophobia: 217, Couples/Family: 151, Crowd: 307, Food: 313, Handicap: 204, Hygiene: 95, Parking: 65, Price: 351, Queues: 185, and Time: 91. For the inclusion/exclusion phrase", "meta": {"paper_id": "2012.10226v1", "chunk_index": 2, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_3", "text": "cat- egories. A total of 2303 phrases were labeled with one of the 11 categories. The distribution across the categories is as follows: Age/Height: 324, Claustrophobia: 217, Couples/Family: 151, Crowd: 307, Food: 313, Handicap: 204, Hygiene: 95, Parking: 65, Price: 351, Queues: 185, and Time: 91. For the inclusion/exclusion phrase mining task, a total of 2303 phrases from 2154 sentences were labeled. Phrases in these sentences which are not inclusion/exclusion are marked as oth- ers. Across these phrases, the word label distribution is as follows: B EXC: 1176, B INC: 1223, EXC: 5713, INC: 5455, O: 29976, where INC and EXC denote in- clusion and exclusion respectively. We make the code and the manually labeled dataset publicly available3. On a small set of 115 instances, we measured the inter-annotator agreement and found the Cohen’s Kappa to be 0.804 and 0.931 for the ﬁrst and the second tasks respectively, which is considered as very good. 3.2 Methods We experiment with two diﬀerent word embedding methods: GloVe (Global Vectors for Word Representation) [16] and ELMo (Embeddings from Language Models) [17]. We use CRFs, BiLSTMs, BiLSTM-CRFs and BERT for the ﬁrst sequence labeling task. We use traditional machine learning (ML) classiﬁers like XGBoost and Support Vector Machines (SVMs) and deep learning (DL) models like BiLSTMs, LSTM-CNN and BERT for the multi-class classiﬁcation task. CRFs [13]: Conditional Random Fields (CRFs) are prediction models for tasks where contextual information or state of the neighbors aﬀect the current predic- tion. They are a type of discriminative undirected probabilistic graphical model. BiLSTMs [9]: Bidirectional LSTMs are the most popular traditional deep learn- ing models for sequence modeling. They model text sequences using recurrence and gate-controlled explicit memory logic. Bidirectionality helps propagate in- formation across both directions leading to improved accuracies compared to unidirectional LSTMs. BiLSTM-CNNs [4]: BiLSTM-CNNs use character-based CNNs to ﬁrst gener- ate the word embeddings. These word embeddings are further used by the LSTM to generate the embedding for the text sequence. This is then connected to a dense layer and then ﬁnally to the output softmax layer. BiLSTM-CRFs [11]: We combine a BiLSTM network and a CRF network to form a BiLSTM-CRF model. This network can eﬃciently use past input features via a LSTM layer and sentence level tag information via a CRF layer. BERT [6]: BERT is a Transformer-encoder model trained in a bidirectional way. BERT has been shown to provide very high accuracies across a large number of NLP tasks. For the sequence labeling task, we connect the semantic output for each position to an output softmax layer. For multi-class classiﬁcation, we connect semantic representation of CLS token to the output softmax layer. Inclusion and Exclusion Phrase Mining from Reviews 5 4 Experiments For BiLSTM experiments, we used three layers, ReLU activation for hidden layers and softmax for output, SGD optimizer (with momentum=0.7, learning rate=1e-5, batch size=8), and cross-entropy loss. We trained for 50 epochs. We used GloVe 200D word vectors. For BERT, we used the pretrained BERT BASE model with 12 Transformer layers, Adam optimizer with learning rate=3e-5, max sequence length=128, batch size=8,", "meta": {"paper_id": "2012.10226v1", "chunk_index": 3, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_4", "text": "layers and softmax for output, SGD optimizer (with momentum=0.7, learning rate=1e-5, batch size=8), and cross-entropy loss. We trained for 50 epochs. We used GloVe 200D word vectors. For BERT, we used the pretrained BERT BASE model with 12 Transformer layers, Adam optimizer with learning rate=3e-5, max sequence length=128, batch size=8, and categorical cross entropy loss. 4.1 Results Table 1 shows results for the inclusion/exclusion phrase mining task. As dis- cussed in [12], we use two metrics: (1) Binary Overlap which counts every overlapping match between a predicted and true expression as correct, and (2) Proportional Overlap which imparts a partial correctness, proportional to the overlapping amount, to each match. BERT based method outperforms all other methods. This is because the 12 layers of self-attention help signiﬁcantly in dis- covering the right inclusion/exclusion label for each word. Also, precision values are typically lower than recall, which means that our models can detect that the text implies some inclusion or exclusion but ﬁnd it diﬃcult to diﬀerentiate between the two. Model Inclusion Exclusion Precision Recall F1 Precision Recall F1 Prop Bin Prop Bin Prop Bin Prop Bin Prop Bin Prop Bin CRF + GloVe 0.354 0.417 0.531 0.758 0.425 0.538 0.372 0.392 0.524 0.728 0.435 0.512 BiLSTM + GloVe 0.456 0.590 0.573 0.643 0.508 0.615 0.506 0.638 0.570 0.668 0.536 0.650 BiLSTM CRF + GloVe 0.490 0.625 0.613 0.714 0.545 0.666 0.516 0.649 0.654 0.788 0.577 0.712 BiLSTM + ELMo 0.580 0.645 0.604 0.770 0.590 0.701 0.602 0.678 0.566 0.738 0.579 0.703 BERT 0.677 0.748 0.765 0.869 0.718 0.804 0.664 0.756 0.801 0.908 0.726 0.825 Table 1. Inclusion/Exclusion Phrase Mining Accuracy Results We present the results of our 11-class phrase classiﬁcation in Table 2. We observe that typically the accuracy is better for inclusion phrases rather than exclusion phrases. Deep learning based methods like LSTMs and BERT are better than traditional ML classiﬁers. BERT outperforms all other methods by a large margin for both the inclusion and exclusion phrases. Further, we performed an end-to-end evaluation of our system. For each sentence in the test set, we ﬁrst obtained BIO predictions using our phrase mining system. Then, we perform 11-class classiﬁcation on these mined phrases. Golden label for our predicted inclusion/exclusion phrase is set to the ground truth label for the phrase with maximum intersection. For predicted phrases which have no intersection with any golden phrase, we assume them to belong to a special “sink” class, and they count towards loss in precision. Golden phrases not detected by our system count towards loss in recall. Such an evaluation leads to an overall F1 of 0.748 (P=0.695, R=0.812), inclusion F1 of 0.739 (P=0.691, R=0.795) and an exclusion F1 of 0.759 (P=0.700, R=0.830). 6 Gurjar and Gupta Model Total Inclusion Exclusion Precision Recall F1 Precision Recall F1 Precision Recall F1 SVM 0.725 0.631 0.626 0.759 0.635 0.649 0.665 0.626 0.604 XGBoost 0.802 0.796 0.797 0.802 0.785 0.786 0.817 0.806 0.806 BiLSTM + GloVe 0.890 0.885 0.884 0.921 0.917 0.916 0.862 0.852 0.853 BiLSTM-CNN + GloVe 0.895 0.892 0.891 0.903 0.900 0.900 0.889 0.883 0.883 BiLSTM Attn +", "meta": {"paper_id": "2012.10226v1", "chunk_index": 4, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_5", "text": "Precision Recall F1 SVM 0.725 0.631 0.626 0.759 0.635 0.649 0.665 0.626 0.604 XGBoost 0.802 0.796 0.797 0.802 0.785 0.786 0.817 0.806 0.806 BiLSTM + GloVe 0.890 0.885 0.884 0.921 0.917 0.916 0.862 0.852 0.853 BiLSTM-CNN + GloVe 0.895 0.892 0.891 0.903 0.900 0.900 0.889 0.883 0.883 BiLSTM Attn + GloVe 0.914 0.911 0.911 0.938 0.934 0.934 0.894 0.887 0.889 BERT 0.978 0.978 0.978 0.983 0.982 0.982 0.975 0.973 0.973 Table 2. 11-class Categorization Accuracy Results Next, we present two examples of the output from our system. Consider the sentence: “The wheelchair wouldn’t go through the turnstile which was disappointing”. Our inclusion/exclusion phrase mining BERT classiﬁer outputs “B EXC EXC EXC EXC EXC EXC EXC O O O” while our 11-class classiﬁer labels this as “Handicap”. Our system was able to smartly associate “wheelchair wouldn’t go through” with “handicap” category. Consider another example, “We came to Eiﬀel Tower to celebrate twenty ﬁve years of togetherness”. Our two clas- siﬁers predict “O O O O O O O INC INC INC INC INC” and “Couples/Family”. Interestingly, it can relate “togetherness” with “Couples/Family”. 4.2 Error Analysis We performed a manual analysis of some of the errors made by our best model. We found the following interesting patterns. (1) It is diﬃcult to predict the right label when the phrase can be provided multiple labels. E.g. “If you don’t like crowds or feel claustrophobic being on narrow walkways full of groups of people ...” can be labeled into either of the Crowd or Claustrophobia categories. (2) Conﬂicting opinions mentioned in same review. “... Well worth the $25 ... The cost of the day was very expensive compared to Australian water parks.” In this review, from a price perspective, it is diﬃcult to ﬁgure out whether the spot is cheap or expensive. Similarly, consider another review: “Wednesday night is bike night in Beale Street so a lot of noise from at least 1000 bikes many highly decorated. It was fun and the usual bar street of many cities.” Can’t really make out whether one should visit during the night or not. (3) References to other unrelated things: Consider this review: “... I was lucky enough to have a descendant who gave the garden tour and tell about the family (more than you might usually get) ...” The word “family” here does not indicate anything about inclusion/exclusion wrt families for the spot. 5 Conclusion In this paper, we proposed a novel task for mining of inclusion/exclusion phrases and their detailed categorization. We investigated the eﬀectiveness of various deep learning methods for the task. We found that BERT based methods lead to a binary overlap F1 of ∼80 and ∼82 for the sequence labeling task, and an F1 of ∼98 and ∼97 for 11-class inclusion and exclusion classiﬁcation respectively. In the future, we plan to integrate this module as a part of a personalized automated itinerary recommendation system. Inclusion and Exclusion Phrase Mining from Reviews 7 References 1. Ayeh, J.K., Au, N., Law, R.: “Do we believe in TripAdvisor?” Examining credibility perceptions and online travelers’ attitude toward using", "meta": {"paper_id": "2012.10226v1", "chunk_index": 5, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_6", "text": "respectively. In the future, we plan to integrate this module as a part of a personalized automated itinerary recommendation system. Inclusion and Exclusion Phrase Mining from Reviews 7 References 1. Ayeh, J.K., Au, N., Law, R.: “Do we believe in TripAdvisor?” Examining credibility perceptions and online travelers’ attitude toward using user-generated content. Journal of Travel Research 52(4), 437–452 (2013) 2. Chang, H.T., Chang, Y.M., Tsai, M.T.: ATIPS: automatic travel itinerary planning system for domestic areas. Computational intelligence and neuroscience (2016) 3. Chen, G., Wu, S., Zhou, J., Tung, A.K.: Automatic itinerary planning for traveling services. IEEE transactions on knowledge and data engineering 26(3), 514–527 (2013) 4. Chiu, J.P., Nichols, E.: Named entity recognition with bidirectional lstm-cnns. Transactions of the Association for Computational Linguistics 4, 357–370 (2016) 5. De Choudhury, M., Feldman, M., Amer-Yahia, S., Golbandi, N., Lempel, R., Yu, C.: Automatic construction of travel itineraries using social breadcrumbs. In: Pro- ceedings of the 21st ACM conference on Hypertext and hypermedia. pp. 35–44 (2010) 6. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirec- tional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018) 7. Filieri, R., Alguezaui, S., McLeay, F.: Why do travelers trust tripadvisor? an- tecedents of trust towards consumer-generated media and its inﬂuence on recom- mendation adoption and word of mouth. Tourism management 51, 174–185 (2015) 8. Friggstad, Z., Gollapudi, S., Kollias, K., Sarlos, T., Swamy, C., Tomkins, A.: Orien- teering algorithms for generating travel itineraries. In: Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining. pp. 180–188 (2018) 9. Graves, A., Jaitly, N., Mohamed, A.r.: Hybrid speech recognition with deep bidi- rectional lstm. In: 2013 IEEE workshop on automatic speech recognition and un- derstanding. pp. 273–278. IEEE (2013) 10. He, R., Lee, W.S., Ng, H.T., Dahlmeier, D.: An unsupervised neural attention model for aspect extraction. In: Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 388–397 (2017) 11. Huang, Z., Xu, W., Yu, K.: Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991 (2015) 12. Irsoy, O., Cardie, C.: Opinion mining with deep recurrent neural networks. In: Proceedings of the 2014 conference on empirical methods in natural language pro- cessing (EMNLP). pp. 720–728 (2014) 13. Laﬀerty, J.D., McCallum, A., Pereira, F.C.: Conditional random ﬁelds: Proba- bilistic models for segmenting and labeling sequence data. In: Proceedings of the Eighteenth International Conference on Machine Learning. pp. 282–289 (2001) 14. Mukherjee, A., Liu, B.: Aspect extraction through semi-supervised modeling. In: Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). pp. 339–348 (2012) 15. Pantano, E., Priporas, C.V., Stylos, N.: ‘you will like it!’ using open data to predict tourists’ response to a tourist attraction. Tourism Management 60, 430–438 (2017) 16. Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre- sentation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pp. 1532–1543 (2014) 8 Gurjar and Gupta 17. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep", "meta": {"paper_id": "2012.10226v1", "chunk_index": 6, "filename": "2012.10226v1.pdf", "chunk_length": 512}}
{"id": "2012.10226v1_chunk_7", "text": "Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word repre- sentation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pp. 1532–1543 (2014) 8 Gurjar and Gupta 17. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.: Deep contextualized word representations. arXiv preprint arXiv:1802.05365 (2018) 18. Popescu, A., Grefenstette, G.: Deducing trip related information from ﬂickr. In: Proceedings of the 18th international conference on World wide web. pp. 1183–1184 (2009) 19. Wang, W., Pan, S.J., Dahlmeier, D., Xiao, X.: Recursive neural conditional random ﬁelds for aspect-based sentiment analysis. arXiv preprint arXiv:1603.06679 (2016)", "meta": {"paper_id": "2012.10226v1", "chunk_index": 7, "filename": "2012.10226v1.pdf", "chunk_length": 104}}
{"id": "2012.13004v1_chunk_0", "text": "arXiv:2012.13004v1 [cs.CL] 23 Dec 2020 SPEECH SYNTHESIS AS AUGMENTATION FOR LOW-RESOURCE ASR Deblin Bagchi*, Shannon Wotherspoon*, Zhuolin Jiang, Prasanna Muthukumar† Raytheon BBN, Cambridge, MA-02138, USA {dbagchi, swothers, zjiang, pmuthuku}@bbn.com ABSTRACT Speech synthesis might hold the key to low-resource speech recognition. Data augmentation techniques have become an essential part of modern speech recognition training. Yet, they are simple, naive, and rarely reﬂect real-world conditions. Meanwhile, speech synthesis techniques have been rapidly getting closer to the goal of achieving human-like speech. In this paper, we investigate the possibility of using synthesized speech as a form of data augmentation to lower the resources necessary to build a speech recognizer. We experiment with three different kinds of synthesizers: statistical parametric, neural, and adversarial. Our ﬁndings are interesting and point to new research directions for the future. Index Terms— speech synthesis, data augmentation, speech recognition, low resource languages 1. INTRODUCTION The modern speech processing system has a voracious ap- petite for data. The general trend in automatic speech recog- nition (ASR) systems today has been to create algorithms that can make use of increasingly larger datasets. These efforts have undoubtedly yielded impressive results. The ability to learn effectively from large datasets has directly corresponded with recognition accuracy. Far from being unique to speech recognition, ASR’s counterpart, speech synthesis has also fol- lowed a similar trend. The original Blizzard challenge dataset [1] consisted of a one-hour corpus. The most recent version [2] uses corpora several times larger. While this trend of ever larger datasets has led to great ad- vances in speech processing abilities, it has also ignored the needs of several end-users. Large sized corpora are simply not available for a large variety of tasks. The most common use case with this constraint is speech recognition when deal- ing with non-mainstream languages. Igbo, for instance, is spoken by 47 million speakers but no large corpus exists, and creating a several thousand hour Igbo corpus for ASR will be prohibitively expensive. Arguably, older techniques like Hidden Markov Models [3] would still be applicable when dealing with smaller corpora. However, it is important that we do not exclude poorly resourced languages from enjoying the advances of modern speech processing technologies. In this paper, we will therefore pursue a goal directly opposite the general trend of papers today. We will attempt ∗Equal contribution †Corresponding author: Prasanna Muthukumar, pmuthuku@bbn.com to reduce the amount of data required to build a viable speech recognizer while still using modern neural network based ASR techniques. We attempt this using speech synthesis techniques [4] as a form of data augmentation. The reason- ing behind this decision is that synthesizers typically require orders of magnitude less data than recognizers do. After all, statistical parametric speech synthesizers have even been built on as little as 30 minutes of speech data [5]. Speech syn- thesizers also offer a means of data augmentation that is less naive compared to standard augmentation techniques. For instance, a common form of data augmentation is to speed up or slow down existing speech. While this technique might improve accuracy, it does not reﬂect the", "meta": {"paper_id": "2012.13004v1", "chunk_index": 0, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_1", "text": "speech data [5]. Speech syn- thesizers also offer a means of data augmentation that is less naive compared to standard augmentation techniques. For instance, a common form of data augmentation is to speed up or slow down existing speech. While this technique might improve accuracy, it does not reﬂect the true way that hu- mans speed up or slow down their speech. A synthesis-based augmentation technique, on the other hand, is more likely to reﬂect rate changes more accurately. We therefore believe that synthesis-based augmentation should yield even better results. We will investigate three different forms of speech syn- thesis and their performance when used as a means of data augmentation for ASR. The ﬁrst is the classic statistical para- metric speech synthesis [4]. The second is the popular neural synthesizer, Tacotron2 [6]. The third is the adversarial syn- thesizer, WGANSing [7]. We report our experiences with all three synthesizers. 2. PRIOR WORK Low-resource ASR and TTS have a rich and varied history. Technically, any ASR paper from 10 years ago or earlier can be considered a viable approach for low-resource ASR. How- ever, the closest related work to ours is the paper from Rosen- berg et al [8]. The authors describe an approach where the Tacotron speech synthesizer is used to augment real speech with the goal of increasing lexical and acoustic diversity. The major difference between Rosenberg et al. and our work is in the quantity of data used. Keeping in mind our goal of using the least amount of data possible, we use substantially smaller subsets of the Librispeech corpus [9]. Because we restricted ourselves to using as little data as possible, we also had to resort to synthesizers beyond Tacotron that Rosenberg et al. use. Another closely related work is LRSpeech by Xu et al [10]. The authors target low-resource scenarios like we do but use a high-resource language to bootstrap ASR and TTS systems for the low-resource language. We, on the other hand, assume that no corpus is available apart from that of the low- resource language. We see our approach as complementary to that of Xu et al., and hope our approach is an interesting alternative. 3. EXPERIMENTS 3.1. Speech recognizer Our speech recognition models are trained using the BBN speech recognition system, Sage [11]. Sage is an extension of the popular Kaldi speech recognition toolkit [12]. Our ASR systems are multilingual initialized [13] hybrid TDNN- F models [14], trained for one epoch of lattice-free MMI (LF- MMI) and two epochs of sMBR. For language modeling, we train word-level trigram models. All our experiments in this paper will focus on the acoustic model since text data is typi- cally easier to obtain than labeled audio. We use 100 hours of Librispeech transcripts for language model data in all cases, and vary the amount of audio data as required by the experi- ment. 3.2. Statistical Parametric synthesis For our experiments using statistical parametric speech syn- thesis, we use the Clustergen synthesizer [15]. Clustergen uses the Festival system [16] to convert text into a sequence of phonemes, and", "meta": {"paper_id": "2012.13004v1", "chunk_index": 1, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_2", "text": "in all cases, and vary the amount of audio data as required by the experi- ment. 3.2. Statistical Parametric synthesis For our experiments using statistical parametric speech syn- thesis, we use the Clustergen synthesizer [15]. Clustergen uses the Festival system [16] to convert text into a sequence of phonemes, and uses random forests to predict mel-frequency cepstral co-efﬁcients that correspond to the phonemes. While Clustergen generally produces good quality speech, we use it differently from its original intent. Clustergen is heavily optimized towards a single speaker corpus. How- ever, most ASR corpora are multi-speaker, and only possess a few minutes of speech for each speaker. It is impossible to build a synthesis model for each speaker with such little data. We therefore attempted to build average models for groups of speakers. We identiﬁed similar speakers by clus- tering i-vectors [17]. We pretended that all the speech from a particular cluster was from the same speaker, and built a Clustergen model for it. These average models did not pro- duce high quality synthesis but resulted in some interesting ﬁndings anyway. They pointed out several ﬂaws in Mel Cep- stral Distortion (MCD), the objective metric we were using to assess synthesis quality. We discuss these ﬂaws in section 4. Table 1 shows the results of using Clustergen as a data augmentation system. Here, Clustergen was trained on the speciﬁed amount of real speech and then used to synthesize the speciﬁed amount of synthetic speech. The ‘MCD < 5’ and ‘MCD < 6’ rows correspond to experiments where we left out synthetic voices that had MCD scores higher than the number. Table 2 shows the results of an experiment where we trained on either purely real speech or purely synthetic speech. The results in the second row were generated by building a separate Clustergen model for each speaker no matter how little the available data. The third row in the table is the result of clustering similar speakers using i-vectors. The results in both tables indicate that Clustergen’s speech Table 1. ASR performance when using synthetic speech as additional training data Training data WER 20h real 12.7 20h real + 20h synth 12.6 20h real + 20h synth (MCD < 5) 12.7 20h real + 20h synth (MCD < 6) 12.7 20h real + 60h synth 13.0 20h real + 80h synth 12.8 40h real 12.0 40h real + 60h synth 13.0 80h real 11.4 80h real + 20h synth 11.5 Table 2. ASR performance when training on either purely real or purely synthetic speech Training data WER 80h real 11.4 80h synth, unclustered, 50 voices 36.2 80h synth, clustered, 5 voices 47.1 quality was not sufﬁcient to be useful as a data augmentation method. For neural TTS and adversarial TTS, we used Clustergen as a baseline. Since ASR training is computationally expen- sive, we did not train the ASR system whenever any synthe- sizer performed worse than the parametric synthesizer. 3.3. Neural TTS Tacotron2 [6] is a purely neural speech synthesizer that con- sists of a recurrent sequence-to-sequence neural network that takes as input", "meta": {"paper_id": "2012.13004v1", "chunk_index": 2, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_3", "text": "a baseline. Since ASR training is computationally expen- sive, we did not train the ASR system whenever any synthe- sizer performed worse than the parametric synthesizer. 3.3. Neural TTS Tacotron2 [6] is a purely neural speech synthesizer that con- sists of a recurrent sequence-to-sequence neural network that takes as input character embeddings and predicts mel spec- trograms. We use Waveglow [18] as a vocoder to convert the mel spectrograms to raw speech. Tacotron2 has been shown to produce excellent quality speech in the past, but our pri- mary reason for choosing this synthesizer and waveglow was because there was open-source code available. We were successful in producing high quality synthesis from Tacotron2 by training on the 24 hour LJ Speech dataset [19], but had difﬁculty achieving similar results on any 1 hour CMU Arctic dataset [20]. Despite our best efforts, the best Tacotron2 could do on the one-hour corpus was babble in a voice that strongly resembled the original speaker. This test is important because CMU Arctic is still larger and cleaner than any single voice we had for our ASR datasets. We there- fore had to rule out any possibility of training a Tacotron2 model exclusive for each voice in an ASR corpus. (Building an exclusive model for each voice in an ASR corpus would have been terribly expensive computationally anyway). To overcome this difﬁculty, we extended Tacotron2 to support multi-speaker training by incorporating speaker em- beddings as input in addition to the character embeddings. We experimented with both one-hot embedddings as well as i-vectors. One-hot embeddings gave us marginally better results than i-vectors but neither could match up to even the quality of Clustergen. Nor was there an easy way of quantify- ing the difference in quality between the i-vectors and one-hot embeddings. Since the quality of synthesis on our corpus was signiﬁcantly worse than using classical parametric synthesis, we did not attempt our augmentation experiments. 3.4. Adversarial TTS In addition to the classical parametric and neural speech synthesizers, we also tested an adversarial synthesizer based on Generative Adversarial Networks (GANs). Adversarial learning is a new machine learning technique that has had tremendous success in generating high-dimensional data. These techniques are still in the nascent stages for speech synthesis, but nevertheless show great promise because of the success they have had in related ﬁelds such as image generation. The toolkit we used for our experiments was actually a GAN-based singing synthesizer called WGANSing [7]. We chose WGANSing because it was the only open-source toolkit we could ﬁnd capable of open-vocabulary speech synthesis. The architecture of this system is the same as DCGANs [21] and is trained using the Wasserstein GAN algorithm. WGANSing takes as input a block of frame-wise linguis- tic features and singer identity features, and outputs vocoder features that correspond to the block. The most problematic for us among the input features were the precise phone du- rations and the pitch contours required. These features make sense for synthesizing singing, but are very difﬁcult to gener- ate for regular speech. We were therefore forced to modify the toolkit", "meta": {"paper_id": "2012.13004v1", "chunk_index": 3, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_4", "text": "that correspond to the block. The most problematic for us among the input features were the precise phone du- rations and the pitch contours required. These features make sense for synthesizing singing, but are very difﬁcult to gener- ate for regular speech. We were therefore forced to modify the toolkit to avoid conditioning on these troublesome features. However, the resulting synthesis quality without these fea- tures was poor compared to the other synthesizers discussed in this paper. We suspect that this sub-par performance is caused by the low amounts of training data we use. Low- resources are a requirement for our task. Unfortunately, this requirement might also be the Achilles heel of adversarial ML. 4. DISCUSSION One major setback we faced in our series of experiments is the failure of Mel Cepstral Distortion (MCD) as a metric. In standard speech synthesis experiments, MCD has issues but is generally reliable. For instance, a rule of thumb is that an MCD reduction of 0.12 is equivalent to doubling the amount of training data [22]. Unfortunately, intuitive rules and beliefs such as these only ended up being true for the standard speech synthesis training scenario of clean, single- speaker data. When training on noisy, multi-speaker data, not only was MCD a poor reﬂection of quality, but it was also extremely misleading. In informal tests, we found several in- stances where the metric indicated high MCD but the synthe- sized speech was understandable. We also found several ex- amples where the synthesized speech was unintelligible but the measured MCD was low. Clearly, single-speaker objec- tive metrics do not extrapolate well to multi-speaker training scenarios. Another major pain point for us was the gordian knot that was Tacotron2. We faced tremendous engineering and re- search challenges in getting sensible performance from this synthesizer. The engineering challenges were primarily re- lated to the amount of compute required (at least the imple- mentation we used required this). Both Tacotron2 and Wave- glow could only be run on the fastest GPUs we had available, and then still took between days to weeks to converge. While we managed to overcome these through a lot of work, the re- search challenges proved more insurmountable. We were able to successfully replicate the default experiments in the open- source Tacotron2 toolkit, but we had little success when run- ning the codebase on our own smaller dataset. The inscrutable nature of modern neural networks also made it difﬁcult to ex- tend the system to support multiple speakers. Despite being experts in speech synthesis and neural networks, progress was extremely difﬁcult. We hope that future work by us and oth- ers relieves researchers of this burden, and makes it easier to achieve positive results. 5. CONCLUSION The results of our experiments have mostly been disappoint- ing. It appears that modern speech synthesis has not yet ad- vanced to be of sufﬁcient use in training low-resource speech recognizers. Nevertheless we still believe that as speech syn- thesizers increase in quality, this approach will eventually be- come viable. We also hope that our experiments point to in- teresting", "meta": {"paper_id": "2012.13004v1", "chunk_index": 4, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_5", "text": "It appears that modern speech synthesis has not yet ad- vanced to be of sufﬁcient use in training low-resource speech recognizers. Nevertheless we still believe that as speech syn- thesizers increase in quality, this approach will eventually be- come viable. We also hope that our experiments point to in- teresting future directions with a higher potential for success. For instance, every synthesizer we explored in our paper has been heavily tuned for the human ear. It is highly likely that new synthesizers will need to be designed and built with the explicit goal of acting as sources of augmented data. 6. ACKNOWLEDGEMENT We thank Sean Colbath, Ilana Heintz, and Ron Coleman for all their support in this endeavor. 7. REFERENCES [1] Alan W Black and Keiichi Tokuda, “The blizzard challenge-2005: Evaluating corpus-based speech synthesis on common datasets,” in Ninth European Conference on Speech Communication and Technol- ogy, 2005. [2] Zhenhua Ling and Simon King, “The blizzard challenge 2020: Evaluating corpus-based speech synthesis on common datasets,” https://www.synsig.org/images/e/ee/Blizzard2020-CallforParticipation.pdf. [3] Lawrence Rabiner and B Juang, “An introduction to hidden markov models,” ieee assp magazine, vol. 3, no. 1, pp. 4–16, 1986. [4] Heiga Zen, Keiichi Tokuda, and Alan W Black, “Statistical parametric speech synthesis,” speech communication, vol. 51, no. 11, pp. 1039– 1064, 2009. [5] Alan W Black and Prasanna Kumar Muthukumar, “Random forests for statistical speech synthesis,” in Sixteenth Annual Conference of the International Speech Communication Association, 2015. [6] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly, Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al., “Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions,” in 2018 IEEE International Con- ference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4779–4783. [7] Pritish Chandna, Merlijn Blaauw, Jordi Bonada, and Emilia G´omez, “WGANSing: A multi-voice singing voice synthesizer based on the wasserstein-gan,” in 2019 27th European Signal Processing Confer- ence (EUSIPCO). IEEE, 2019, pp. 1–5. [8] Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye Jia, Pedro Moreno, Yonghui Wu, and Zelin Wu, “Speech recognition with aug- mented synthesized speech,” in 2019 IEEE Automatic Speech Recogni- tion and Understanding Workshop (ASRU). IEEE, 2019, pp. 996–1002. [9] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudan- pur, “Librispeech: an ASR corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210. [10] Jin Xu, Xu Tan, Yi Ren, Tao Qin, Jian Li, Sheng Zhao, and Tie- Yan Liu, “LRSpeech: Extremely low-resource speech synthesis and recognition,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 2802– 2812. [11] Roger Hsiao, Ralf Meermeier, Tim Ng, Zhongqiang Huang, Maxwell Jordan, Enoch Kan, Tanel Alum¨ae, Jan Silovsk´y, William Hartmann, Francis Keith, et al., “Sage: The new BBN speech processing plat- form,” in Proc. INTERSPEECH 2016, San Francisco, California, USA, 2016, pp. 3022–3026. [12] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in IEEE 2011 workshop", "meta": {"paper_id": "2012.13004v1", "chunk_index": 5, "filename": "2012.13004v1.pdf", "chunk_length": 512}}
{"id": "2012.13004v1_chunk_6", "text": "The new BBN speech processing plat- form,” in Proc. INTERSPEECH 2016, San Francisco, California, USA, 2016, pp. 3022–3026. [12] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, et al., “The Kaldi speech recognition toolkit,” in IEEE 2011 workshop on automatic speech recognition and understand- ing. IEEE Signal Processing Society, 2011, number CONF. [13] J.Z. Ma, F. Keith, T. Ng, M.-h. Siu, and O. Kimball, “Improving deliv- erable speech-to-text systems with multilingual knowledge transfer,” in Proc. INTERSPEECH 2017, Stockholm, Sweden, 2017, pp. 127–131. [14] Daniel Povey, Gaofeng Cheng, Yiming Wang, Ke Li, Hainan Xu, Mahsa Yarmohammadi, and Sanjeev Khudanpur, “Semi-orthogonal low-rank matrix factorization for deep neural networks.,” in Inter- speech, 2018, pp. 3743–3747. [15] Alan W Black, “CLUSTERGEN: A statistical parametric synthesizer using trajectory modeling,” in Ninth International Conference on Spo- ken Language Processing, 2006. [16] Alan Black, Paul Taylor, and Richard Caley, “The festival speech syn- thesis system,” . [17] Najim Dehak, Patrick J Kenny, R´eda Dehak, Pierre Dumouchel, and Pierre Ouellet, “Front-end factor analysis for speaker veriﬁcation,” IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788–798, 2010. [18] Ryan Prenger, Rafael Valle, and Bryan Catanzaro, “Waveglow: A ﬂow- based generative network for speech synthesis,” in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP). IEEE, 2019, pp. 3617–3621. [19] Keith Ito and Linda Johnson, “The LJ speech dataset,” https://keithito.com/LJ-Speech-Dataset, 2017. [20] John Kominek and Alan W Black, “The cmu arctic speech databases,” in Fifth ISCA workshop on speech synthesis, 2004. [21] Merlijn Blaauw, Jordi Bonada, and Ryunosuke Daido, “Data efﬁcient voice cloning for neural singing synthesis,” in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP). IEEE, 2019, pp. 6840–6844. [22] John Kominek, Tanja Schultz, and Alan W Black, “Synthesizer voice quality of new languages calibrated with mean mel cepstral distortion,” in Spoken Languages Technologies for Under-Resourced Languages, 2008.", "meta": {"paper_id": "2012.13004v1", "chunk_index": 6, "filename": "2012.13004v1.pdf", "chunk_length": 325}}
{"id": "2012.13190v2_chunk_0", "text": "QUACKIE: A NLP Classiﬁcation Task With Ground Truth Explanations Yves Rychener1,2, Xavier Renard2, Djam´e Seddah3, Pascal Frossard1, Marcin Detyniecki2,4,5 1EPFL, Lausanne, Switzerland 2AXA, Paris, France 3Inria, Paris, France 4Sorbonne Universit´e, Paris, France 5Polish Academy of Science, Warsaw, Poland Abstract NLP Interpretability aims to increase trust in model predictions. This makes evaluating interpretability approaches a pressing issue. There are multiple datasets for evaluating NLP interpretability, but their dependence on hu- man provided ground truths raises questions about their unbiasedness. In this work, we take a different approach and formulate a spe- ciﬁc classiﬁcation task by diverting question- answering datasets. For this custom classi- ﬁcation task, the interpretability ground-truth arises directly from the deﬁnition of the classi- ﬁcation problem. We use this method to pro- pose a benchmark and lay the groundwork for future research in NLP interpretability by eval- uating a wide range of current state of the art methods. 1 Introduction In recent years, we have seen an increase in the performance of NLP models in a variety of tasks. It has, to a large extent, been fuelled by the us- age of progressively complex neural network archi- tectures (Devlin et al., 2019; Brown et al., 2020). While this allows to model increasingly complex input-output relations, it is virtually impossible to understand the reasoning behind a prediction. To address this issue, algorithms have been proposed to explain the reasoning of models in the ﬁeld of interpretability. While there has been a number of interpretability approaches published which are applicable to NLP, (Simonyan et al., 2013; Smilkov et al., 2017; Sundararajan et al., 2017; Ribeiro et al., 2016; Lundberg and Lee, 2017), deﬁning what con- stitutes a good explanation and appropriate evalua- tion methods are largely unanswered problems. This work considers the case of post-hoc inter- pretability for classiﬁcation, where the reasoning of an existing classiﬁcation model is explained. How- ever, the evaluation technique could be adopted to also test the explanation performance of inherently interpretable classiﬁcation models. Interpretability and explanation are very subjec- tive terms, with different deﬁnitions for different needs (Liao et al., 2020). However, to our knowl- edge, the ﬁeld agrees that faithfulness is a desired property of a good interpretability approach. Faith- fulness means that an explanation well replicates the actual reasoning of the model to explain. How- ever, since the actual reasoning of a black-box model is unknown, the task of evaluating faith- fulness is difﬁcult. Many works proposing inter- pretability algorithms have thus resorted (often without explicitly acknowledging) to plausibility for evaluating faithfulness, evaluating how convinc- ing an explanation is to a human. Recently, Jacovi and Goldberg (2020) pointed out the drawbacks of this approach, coming to the conclusion that many current evaluation approaches in the NLP interpretability literature may be biased towards matching human’s intuition instead of explaining the black-box model. In this work, we propose a novel evaluation approach for faithfulness. It is based on the ob- servation that a good interpretability algorithm should be able to well distinguish between impor- tant and unimportant parts of the text. Our evalua- tion methodology can compare any", "meta": {"paper_id": "2012.13190v2", "chunk_index": 0, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_1", "text": "of explaining the black-box model. In this work, we propose a novel evaluation approach for faithfulness. It is based on the ob- servation that a good interpretability algorithm should be able to well distinguish between impor- tant and unimportant parts of the text. Our evalua- tion methodology can compare any interpretability algorithm assigning importance for parts of the in- puts. By building a classiﬁcation task on top of the NLP question-answering problem, we are able to construct a ground truth label for important and unimportant parts of the input text with respect to the classiﬁcation problem. We further show how these labels can be leveraged for more de- tailed metrics. The methodology is applied to two question-answering datasets in order to propose a new benchmark, QUACKIE, for interpretability evaluation. This work is structured as follows: Section 2 mo- arXiv:2012.13190v2 [cs.CL] 27 Dec 2020 tivates our approach. In Section 3, we present the classiﬁcation task, construction of ground truths and the metrics used. This is followed by apply- ing the methods to propose the QUACKIE bench- mark and analysing its properties in Section 4. Sec- tion 5.1 presents a selection of interpretability al- gorithms, which aims to reﬂect the current state of post-hoc Black-Box and White-Box interpretabil- ity research. The algorithms are compared in Sec- tion 5.2 using QUACKIE. In Section 6, we discuss the work and its results.1 The main contributions of this work are the construction of interpretability ground truth values which do not need human an- notation and its application to compare state of the art post-hoc interpretability methods. 2 Evaluating Discriminative Power of Interpretability Methods No matter the explanation needs or the interpretabil- ity deﬁnition, a key property of a good interpretabil- ity method is its ﬁdelity to the model to explain. To evaluate this property, we propose an approach to assess interpretability methods’ ﬁdelity. We provide an evaluation method for a universal re- quirement of NLP interpretability: The ability to distinguish between relevant and irrelevant text seg- ments for the classiﬁcation. In certain applications, an even ﬁner separation may be useful, however the desideratum stated here builds a common ground. Some works exist to assess interpretability meth- ods: their quality is evaluated using datasets built on human generated ground-truths on top of com- mon classiﬁcation tasks (DeYoung et al., 2019). As stated by Jacovi and Goldberg (2020), such hu- man generated gold labels are built on assumptions about what a model should do according to humans. This can lead to problems, as the actual evaluation of the interpretability method shifts from creating a faithful explanation for the reasoning of the model, to giving an ”explanation” that best agrees with human reasoning for the text-prediction pair. The result is that high scoring interpretability methods could suffer from the disastrous problem of giving convincing explanations which do not match the reasoning of the classiﬁcation model. As noted by Jacovi and Goldberg (2020), a plausible but unfaithful explanation may be the worst-case sce- nario, promoting trust in a prediction even if it is 1The code for this work is available on", "meta": {"paper_id": "2012.13190v2", "chunk_index": 1, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_2", "text": "problem of giving convincing explanations which do not match the reasoning of the classiﬁcation model. As noted by Jacovi and Goldberg (2020), a plausible but unfaithful explanation may be the worst-case sce- nario, promoting trust in a prediction even if it is 1The code for this work is available on GitHub and de- signed such that researchers can easily apply it to assess their own interpretability algorithms. Appendix A gives an overview of the code structure. not warranted. Further, since they do not provide a model to interpret, the benchmark results may become pol- luted: If wrong model predictions are not excluded from the benchmark, advances in classiﬁcation ac- curacy lead to advances in interpretation perfor- mance. This is because we cannot expect the ratio- nale for a faulty prediction to be the same as for a correct one, which is used as ground truth. If this problem is addressed by excluding samples with wrong model predictions, then different samples are excluded for different classiﬁers, which leads to different versions of the dataset being used for different interpretability methods, calling compara- bility into question. We use a different approach to combat the issue of ground truth interpretability labels. Instead of annotating ground truth interpretability labels for an existing classiﬁcation task, we develop a clas- siﬁcation task for which ground-truth importance labels arise directly from the problem deﬁnition. If the model is successful in classiﬁcation, it must base its prediction on these rationales. We also ad- dress the problem of benchmark pollution by pro- viding trained classiﬁcation models, which achieve good classiﬁcation performance and ensure a level playing ﬁeld. In the next section, we will provide an descrip- tion of how this classiﬁcation task is created as well as explain the metrics used. 3 Approach We will now present our interpretability evalua- tion approach for NLP. It is based on the question- answering (QA) task, where an answer span to a given question has to be identiﬁed in a text. Note that while we will later use this approach on two speciﬁc datasets for QUACKIE, the approach is general and applicable for any QA dataset, pro- vided it contains the answers’ location in the text and single sentence reasoning can be assumed. 3.1 Ground Truth Construction 3.1.1 The Question-Answering Task We base our evaluation procedure on an extractive question answering task, where the answer needs not necessarily be in the context. A model is trained to extract the answer A to a question Q from a given textual context C. However, it is also possible that there is no answer in the context, a fact which models also have to identify. In other words, the Context The further decline of Byzantine state-of-affairs paved the road to a third attack in 1185, when a large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine ofﬁcials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands. Some Questions When did the Normans attack Dyrrachium? 1185 Where was Dyrrachium located? the Adriatic Who attacked Dyrrachium in the", "meta": {"paper_id": "2012.13190v2", "chunk_index": 2, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_3", "text": "large Norman army invaded Dyrrachium, owing to the betrayal of high Byzantine ofﬁcials. Some time later, Dyrrachium—one of the most important naval bases of the Adriatic—fell again to Byzantine hands. Some Questions When did the Normans attack Dyrrachium? 1185 Where was Dyrrachium located? the Adriatic Who attacked Dyrrachium in the 11th century? No Answer Figure 1: An sample of questions-answers from SQuAD 2.0 (Rajpurkar et al., 2018), Normans from val- idation set QA model f is trained to predict f(Q, C) = \u001a A A ⊆C ∅ otherwise . (1) Due to the nature of the extractive question- answering task, the ground truth answer is not only supplied as text, but also its span in the context is given in most datasets (Examples Figure 1). This can be leveraged to identify the sentence containing the answer to the question. 3.1.2 Classiﬁcation Task Based on Question-Answering The answer to the question and its surroundings must certainly be some of the most important parts for correctly detecting the answer and consequently the presence of an answer in the context. This suggests that a model trained on predicting if an answer is present in the context must place its focus on the sentence containing the answer. In other words, the most important sentence for knowing that an answer is in the context is the sentence containing the answer. While this seems trivial, this intuition can be leveraged to construct a classiﬁcation task with ground truth interpretability labels. We build a classiﬁcation task g on top of an existing question- answering dataset, where models have to predict if the answer to the question is in the context or not. For this, we view the question as ﬁxed for each question-context pair, thus building a classiﬁcation task with only the context as input. An already Question (Q) Context (C) Q&A Model Answer (A) A ̸= 0? Figure 2: Illustration of the SQuAD experiment: Black/Grey: The Question-Answering Problem with the pretrained Model f, Blue: Our proposed classiﬁ- cation task for evaluating NLP interpretability methods (g). trained QA model is used, which means no training is needed: g(C) = 1{f(Q, C) ̸= ∅} (2) where f is according to the deﬁnition in Equation 1 and 1{·} is the indicator function. A visual depic- tion of this can be seen in Figure 2. Then, interpretability is performed to identify the most important sentence for samples where the answer is in the context. This prediction is com- pared to the ground truth: the sentence containing the answer. Note that only the correctly classiﬁed samples are used (True Positive). This is because, looking at the confusion matrix, True Negative and False Positive cannot be considered since no ground truth can be derived from the underlying dataset (since no answer exists). We also exclude False Negative samples, since the obtained ground truth is for a different prediction than the one being made. One may think that this evaluation procedure is in fact a case of What should the model do? eval- uation. However, in the original SQuAD paper (Rajpurkar et al., 2016), authors", "meta": {"paper_id": "2012.13190v2", "chunk_index": 3, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_4", "text": "exclude False Negative samples, since the obtained ground truth is for a different prediction than the one being made. One may think that this evaluation procedure is in fact a case of What should the model do? eval- uation. However, in the original SQuAD paper (Rajpurkar et al., 2016), authors ﬁnd that in the ma- jority of cases (86.4%), single sentence reasoning is sufﬁcient to ﬁnd the answer. Further, employing an erasure based view, removing the sentence with the answer will remove the answer, resulting in the answer no longer being present in the text. The question is thus no longer answerable, leading to a change in prediction. This implies that the sentence containing the answer must be the most important one for a positive prediction. A quantitative test of this is given in Section 4 for the datasets used in QUACKIE. 3.1.3 Sentence as Ground Truth Many interpretability methods provide word-based explanations. It would thus seem natural to di- rectly take the answer itself as ground truth and not the whole sentence containing it. However, we argue that the answer itself is not an appropriate interpretability ground truth. Consider the question ”What is the capital of France?” and a context con- taining the sentence ”[...] The capital of France is Paris. [...]”. We cannot expect a classiﬁer to know that the question is answerable with just ”Paris” as the rationale, since this requires external knowl- edge. However, it would seem that a language model, which only argues based on the text, can know that the answer to the question is in the con- text by looking at the rationale ”capital of France is”: The reasoning is not based on the answer itself, but on its surrounding. To account for this, we use sentences as ground truth. Interpretability methods producing explanations of other granularity (e.g. words) should aggregate them to the sentence level. For the baselines examined in our benchmark, it is not clear how the token scores should be combined into a sentence score, arguments can be made for both taking the maximum score and summing the scores in each sentence. We thus test both methods in our benchmark. 3.2 Metrics Interpretability algorithms may attribute impor- tance to inputs in a variety of ways: By attributing scores to input tokens, by ranking tokens accord- ing to their importance or by selecting important vs unimportant tokens. Of course, scores can be used to produce a ranking, and the ranking can be used to perform selection. However, we believe it is important to use metrics that accommodate for the nuances in attributions, which is why we propose three metrics for the three levels of infor- mation: selection, ranking and scoring. All of them are aggregated across samples by reporting their mean. 3.2.1 IoU for Evaluating Selections Given a ground truth sentence and one or more se- lected sentences, we use the IoU (Intersection over Union) between them as score. If the interpreter selects only one sentence, then the IoU is either 0 or 1, thus after aggregation reﬂecting the accuracy of detection. Given", "meta": {"paper_id": "2012.13190v2", "chunk_index": 4, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_5", "text": "for Evaluating Selections Given a ground truth sentence and one or more se- lected sentences, we use the IoU (Intersection over Union) between them as score. If the interpreter selects only one sentence, then the IoU is either 0 or 1, thus after aggregation reﬂecting the accuracy of detection. Given binary vectors of selection and ground truth (s and g) respectively, the score is: IoU = P(s ∧g) P(s ∨g), where P signiﬁes summing over all entries, with True being 1 and False being 0. This metric reﬂects how well the ground truth is detected. 3.2.2 HPD for Evaluating Rankings If a ranking of sentences is provided by the inter- preter, then we may leverage this for more detailed scoring: Even if the ground truth is not detected as the most important sentence, it should be among the highest in the ranking. To measure this property, we propose HPD (Highest Precision for Detection). Instead of selecting the most important sentence as prediction, we select the K-most important ones, where K is the rank of the ground truth. For this selection, the precision is reported. Given a rank- ing of sentences in decreasing importance r and ground truth sentence GT: HPD = min k 1 k s.t GT ∈rk 1 This metric measures how highly ranked the ground truth is. 3.2.3 SNR for Evaluating Scores When scores are provided, we cannot simply com- pare them to a common target, since scores from different interpretability methods mean different things: Saliency scores reﬂect the aggregated local gradient, while Integrated Gradient scores reﬂect the aggregated effects on a the transition from a baseline to the text sample. However, since we know that only one sentence is important, while the rest is not, we would like this sentence to have a signiﬁcantly higher score than the rest. This can be measured with an adapted SNR (signal to noise ratio), which we deﬁne as follows: Denote s the scores of the sentences and GT the index of the ground truth sentence. Denote sGT the score of the ground truth sentence and s\\GT the scores of all other sentences. Using mean and std, the mean and standard deviation, we deﬁne: SNR = (sGT −mean(s\\GT ))2 std(s\\GT )2 This metric measures the selectivity of the interpre- tation. Note that while the other metrics directly reﬂect performance, this metric could in theory be optimized for, by decreasing the standard devi- ation of sentences predicted as non-important in post-processing. Nonetheless, we believe that inter- esting conclusions can be drawn from this metric. Since IoU and HPD both measure how well the explanation reﬂects model behaviour, while SNR reﬂects selectiveness of the scores, we see IoU and HPD as primary metrics, which SNR as an additional, secondary metric. 4 QUACKIE MODEL SQUAD NEW WIKI NYT REDDIT AMAZON QA 5066 6857 8317 5121 5910 CLASSIF 5597 7494 9444 7882 8321 Number of Samples Used in each Dataset MODEL SQUAD NEW WIKI NYT REDDIT AMAZON QA 5.32 5.06 5.86 8.75 7.64 CLASSIF 5.32 5.05 5.92 8.81 7.74 Average Number of Sentences MODEL SQUAD NEW WIKI", "meta": {"paper_id": "2012.13190v2", "chunk_index": 5, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_6", "text": "WIKI NYT REDDIT AMAZON QA 5066 6857 8317 5121 5910 CLASSIF 5597 7494 9444 7882 8321 Number of Samples Used in each Dataset MODEL SQUAD NEW WIKI NYT REDDIT AMAZON QA 5.32 5.06 5.86 8.75 7.64 CLASSIF 5.32 5.05 5.92 8.81 7.74 Average Number of Sentences MODEL SQUAD NEW WIKI NYT REDDIT AMAZON QA 0.79 0.86 0.91 0.56 0.75 CLASSIF 0.82 0.97 0.96 0.88 0.88 Classiﬁer Accuracy Table 1: Descriptive Statistics of Datasets Used So far we have presented the methodology for extracting ground-truths for classiﬁcation inter- pretability evaluation from QA datasets and intro- duced the metrics. We apply this methodology to two datasets in order to propose a new bench- mark for interpretability evaluation; QUACKIE2. We further provide two pretrained models to inter- pret. Finally, we perform a dataset analysis as well as examining the accuracy of the ground truth. 4.1 Datasets We use two datasets in our benchmark: SQuAD 2.0 (Rajpurkar et al., 2018) is a ques- tion answering dataset based on a selection of Wikipedia articles. It has the exact task explained in Section 3.1. SQuADShifts (Miller et al., 2020) gives 4 new test sets from different domains for SQuAD: New Wiki (other Wikipedia Contexts), NYT (New York Times Articles), Reddit (Reddit Posts) and Amazon (Amazon Reviews). The aim is to provide different semantic contexts for SQuAD. However, in this dataset the answer is always in the context. Since we use pretrained models from SQuAD 2.0, this is not a problem. 2QUACKIE stands for QUestion and Answering for Classiﬁcation tasK Interpretability Evaluation 4.2 Classiﬁcation Models We use two pretrained models which are available in the HuggingFace Transformers library (Wolf et al., 2020) and were trained on SQuAD 2.0. The ﬁrst model is a RoBERTa (Liu et al., 2019) based classiﬁer3 which was directly trained on the classiﬁcation problem g(·) from Equation 2. It is hereafter named ”Classif” model. The second model is an ALBERT (Lan et al., 2019) question answering model 4, trained on the task f(·, ·) from Equation 1. From the output scores, we perform a softmax to get probabili- ties for start- and end-tokens. From these, we can deduct the probability of no answer being present (invalid span or answer in question or CLS token). It is hereafter named ”QA” model. 4.3 Descriptive Statistics As mentioned before, we only use True Positive predictions for interpretability analysis. Further, some samples are discarded because their tokeniza- tion is too long for the models (Question + Context > 512 tokens). This results in a different number of samples usable for the different classiﬁers and datasets. To get a sense of the datasets used, we give the number of samples and average number of sentences for each dataset (after ﬁltering). We also report the accuracy of the models over the whole dataset (positive and negative samples). Since the SQuADShifts datasets only contain positives, the recall (fraction of correctly predicted positive sam- ples) is equal to the accuracy. For SQuAD, the re- call is 0.91 and 0.82 for the Classif and QA models respectively. See Table 1 for the full results.", "meta": {"paper_id": "2012.13190v2", "chunk_index": 6, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_7", "text": "whole dataset (positive and negative samples). Since the SQuADShifts datasets only contain positives, the recall (fraction of correctly predicted positive sam- ples) is equal to the accuracy. For SQuAD, the re- call is 0.91 and 0.82 for the Classif and QA models respectively. See Table 1 for the full results. With the exception of the QA model on Reddit (marked in bold), accuracy and recall are consistently high, suggesting that model predictions can be trusted, making them good candidates for interpretation benchmarking. 4.4 Ground Truth Veriﬁcation In the original SQuAD paper (Rajpurkar et al., 2016), the authors perform an analysis into the type of reasoning required for ﬁnding the answer and ﬁnd that in the majority of cases (86.4%), only single sentence reasoning is necessary. This moti- vated us to use the sentence containing the answer 3https://huggingface.co/a-ware/ roberta-large-squad-classification 4https://huggingface.co/twmkn9/ albert-base-v2-squad2 as ground-truth. We will however now evaluate this ground truth with two experiments: The experiments used are similar to the metrics comprehensiveness and sufﬁciency used by DeY- oung et al. (2019). In the ﬁrst experiment, we remove the sentence containing the ground truth and observe the change in probability output of the model. In the second experiment, we remove the rest of the text and only keep the sentence contain- ing the answer. We again observe the change in probability output. The results are given in Table 2, the probability output without removing anything can be seen in Appendix B. Note that for the classi- ﬁcation model, predictions are very discriminative, which results in the change in probability being either 0 or 1. Nonetheless, we observe for both datasets that removing the ground truth results in a much bigger decrease in probability than removing all the rest. This suggests that our ground-truth la- bels are accurate, since the ground truth rationales are necessary (comprehensiveness) and sufﬁcient (sufﬁciency) for prediction. 5 Baselines Having formulated our benchmark, we will now give a selection of interpretability algorithms which we compare in order to lay a foundation for the comparison of interpretability approaches with QUACKIE. 5.1 Interpretability Algorithms There is not an agreement on the problem formula- tion of NLP interpretability. This means that many approaches use different deﬁnitions, resulting in differing types of outputs and assumptions. As it was already stated in Section 1, we consider algo- rithms that map parts of the inputs to their impor- tances, essentially creating a heatmap (sometimes referred to as the attribution problem). We split the considered interpretability algorithms into 2 groups, based on the assumptions of model access: • White-Box approaches assume complete ac- cess to the model and its internals, notably access to gradients. • Black-Box approaches do not need access to model internals, the model is viewed as a black box. We do however allow access to probability outputs, a condition that is not al- ways necessary. We will now present the interpretability algorithms we considered. For a more detailed explanation of the approaches, refer to their original papers. 5.1.1 White-Box Sailency Maps (Simonyan et al., 2013) report the gradient of the output with respect to", "meta": {"paper_id": "2012.13190v2", "chunk_index": 7, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_8", "text": "to probability outputs, a condition that is not al- ways necessary. We will now present the interpretability algorithms we considered. For a more detailed explanation of the approaches, refer to their original papers. 5.1.1 White-Box Sailency Maps (Simonyan et al., 2013) report the gradient of the output with respect to each input to- ken as the importance of the token. While saliency maps are a straight-forward solution to the attribu- tion problem, they have been observed to create noisy attributions (Smilkov et al., 2017). The gradi- ent is aggregated for each token by summing over all the dimensions. Saliency maps are the simplest attribution method based on gradients and often used as baseline for gradient-based methods. SmoothGrad/NoiseTunnel (Smilkov et al., 2017) aims to address the issue of noise by aver- aging the attributions from multiple similar inputs, which are created by adding small amounts of noise to the original input. The gradient is aggregated for each token by summing over all the dimensions. SmootGrad is a widely used approach, as its ap- proach of removing noise by adding noise has been shown to reduce noise in attributions, making it a viable candidate for benchmarking. Integrated Gradients (Sundararajan et al., 2017) follow an axiomatic approach to the attri- bution problem by approximating the path integral of the gradients between a baseline reference and the input to explain. Multiplication by the inputs is used, the ﬁnal scores are aggregated across token di- mensions taking the sum. The axiomatic approach makes Integrated Gradients an interesting addition to our benchmark. For all Black-Box methods, we use the proba- bility of the answerable class as target for gradient computation. 5.1.2 Black-Box LIME (Ribeiro et al., 2016) uses a weighted lin- ear regression to locally approximate the decision function. A local dataset is created by randomly removing some words from the text. After ﬁtting the weighted linear regression, the coefﬁcients are used as word-attributions. We use LIME in our benchmark since it is widely used and a staple in black-box interpretability. SHAP (Lundberg and Lee, 2017) follows the same approach as LIME and constructs a local linear approximation of the decision function. It utilizes methods from cooperative game-theory to derive a kernel for the weighted regression. Since Model SQuAD New Wiki NYT Reddit Amazon QA -0.70 -0.67 -0.73 -0.63 -0.62 Classif -0.62 -0.60 -0.68 -0.54 -0.56 Remove Ground Truth (Comprehensiveness) Model SQuAD New Wiki NYT Reddit Amazon QA -0.04 -0.04 -0.05 -0.04 -0.05 Classif -0.03 -0.03 -0.04 -0.18 -0.11 Only Keep Ground Truth (Sufﬁciency) Table 2: Average Change in Probability Output SHAP provides guarantees under certain assump- tions, it is interesting to look at it as a baseline. 5.1.3 Baseline The algorithms are compared to a random baseline, which creates a random ordering of sentences, by attributing them with evenly spaced scores from 0 to nwords −1. 5.2 Results Figure 3 displays the results on the SQuAD 2.0 dataset in visual form. Complete Results in tabu- lar form can be found in Appdendix C. We com- pare the interpretation approaches with different aggregation methods (sum and max, as", "meta": {"paper_id": "2012.13190v2", "chunk_index": 8, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_9", "text": "with evenly spaced scores from 0 to nwords −1. 5.2 Results Figure 3 displays the results on the SQuAD 2.0 dataset in visual form. Complete Results in tabu- lar form can be found in Appdendix C. We com- pare the interpretation approaches with different aggregation methods (sum and max, as stated in Section 3.1.3). First, we observe that model access can greatly improve the explanation accuracy, reﬂected by the consistently higher scores of Saliency and Smooth- grad. Further, the performance of Black-Box mod- els LIME and SHAP increases signiﬁcantly with the number of samples used. Evaluation with the default number of samples was not possible due to computational limitations: Evaluation with a neigh- borhood of 100 samples took around 10h for LIME on only SQuAD 2.0 for 1 classiﬁer5, using the de- fault value of 5000 samples seems infeasible with reasonable computational resources, as we expect it to take close to 200 days only for LIME on a single GPU instance. We further note, surprisingly, that SmoothGrad does not seem to bring much improve- ment over standard Saliency using gradients. We attribute this to the fact that noise reduction is al- ready occurring by averaging the scores over token dimensions and over the sentences. Finally, note that both LIME and SHAP use LASSO regression as a surrogate, which results in the attribution of a score of 0 to unimportant sentences, thus reducing the noise, which increases the SNR. 5Experiments were carried out on an AWS SageMaker instance with 1/2 Nvidia Tesla K80 6 Related Work and Discussion Most work in evaluating rationales has been done by comparing rationales to human judge- ments (Doshi-Velez and Kim, 2017; Strout et al., 2019) or by letting humans evaluate the expla- nations (Ribeiro et al., 2016; Lundberg and Lee, 2017). While this approach measures the plausi- bility of interpretations, it does not guarantee that explanations actually reﬂect model behaviour. Ja- covi and Goldberg (2020) performs an in-depth analysis on the state of current NLP interpretability evaluation approaches and desired future proper- ties, notably the need to evaluate faithfulness, as plausible but unfaithful explanations are arguably the worst-case scenario. However, it would seem that all evaluation methodology based on human- annotated ground truth labels is biased towards plausibility by construction. DeYoung et al. (2019) address this problem by using the additional met- rics comprehensiveness and sufﬁciency, which mea- sure change in model prediction when removing the rationale and only keeping the rationale. Our approach also aims to address the bias towards plau- sibility, but instead of addressing the problem with metrics, we build a classiﬁcation task for which interpretability ground truth labels arise directly without human annotation. To our knowledge, it is the ﬁrst approach to propose unbiased ground truths for rationales. The take-away from our benchmark results is two-fold. First, for practitioners, with the current state of the art we see that model access should be used if possible, as it is able to signiﬁcantly increase the ﬁdelity of explanations. However this access is not always possible, in particular with complex text processing pipelines. Thus, for", "meta": {"paper_id": "2012.13190v2", "chunk_index": 9, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_10", "text": "results is two-fold. First, for practitioners, with the current state of the art we see that model access should be used if possible, as it is able to signiﬁcantly increase the ﬁdelity of explanations. However this access is not always possible, in particular with complex text processing pipelines. Thus, for researchers, as Black-Box models lag behind gradient-based methods by a substantial amount, especially with limited computational capacity (10 samples), it seems that there is still a large potential for advances in black-box interpretability. Finally, we note that the results and descriptive statistics for the Reddit and Amazon datasets are Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.0 0.2 0.4 0.6 0.8 1.0 QA Intersection Over Union (IoU) Aggregation sum max Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.0 0.2 0.4 0.6 0.8 1.0 Highest Precision for Detection (HPD) Aggregation sum max Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 Signal to Noise Ratio (SNR) Aggregation sum max Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.0 0.2 0.4 0.6 0.8 1.0 Classif Intersection Over Union (IoU) Aggregation sum max Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.0 0.2 0.4 0.6 0.8 1.0 Highest Precision for Detection (HPD) Aggregation sum max Gradient 5 SmoothGrad 50 IntegratedGrad 10 LIME 100 10 SHAP 100 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Signal to Noise Ratio (SNR) Aggregation sum max Figure 3: Benchmark Results on SQuAD 2.0 quite different from the other datasets (Table 1, 2). We hypothesize that this is because they contain user-generated content, which has different proper- ties that standard text (Eisenstein, 2013). 7 Conclusion In this work, we proposed a novel model-agnostic NLP interpretability evaluation methodology for text classiﬁcation. Our methodology allows for a ground truth that arises directly from the deﬁnition of the classiﬁcation task to be interpreted. Further, it avoids biases of current state of the art evalu- ation methodologies based on human’s intuition. We used this methodology to develop a bench- mark based on two different Question-Answering datasets with a variety of semantic contexts. We publish our code, data and models to facili- tate testing of future methods, which will hopefully lead to the continuous comparison of interpretabil- ity approaches. By providing results on a range of currently available interpretability methods, we provide an easy way for researchers to compare their approach to the state of the art. The publica- tion of these results can also aid NLP practitioners in selecting a suitable interpretability approach. We encourage researchers to submit their results6, as it helps the community keep track of the advances in NLP interpretability. 6Researchers may make a pull request on GitHub with their results. Website: https://axa-rev-research. github.io/quackie/ Acknowledgements We would like to thank Travis McGuire (hugging- face ID TWMKN9) and A-Ware UG (huggingface ID A-WARE) for publishing their pretrained models, which we use for our benchmark. Djam´e Seddah was party founded by the French National Research Agency via the ANR ParSiTi", "meta": {"paper_id": "2012.13190v2", "chunk_index": 10, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_11", "text": "with their results. Website: https://axa-rev-research. github.io/quackie/ Acknowledgements We would like to thank Travis McGuire (hugging- face ID TWMKN9) and A-Ware UG (huggingface ID A-WARE) for publishing their pretrained models, which we use for our benchmark. Djam´e Seddah was party founded by the French National Research Agency via the ANR ParSiTi project (ANR-16- CE33-002). References Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pages 4171–4186. Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace. 2019. Eraser: A benchmark to evaluate rationalized nlp models. arXiv preprint arXiv:1911.03429. Finale Doshi-Velez and Been Kim. 2017. Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. Jacob Eisenstein. 2013. What to do about bad language on the internet. In Proceedings of the 2013 confer- ence of the North American Chapter of the associa- tion for computational linguistics: Human language technologies, pages 359–369. Alon Jacovi and Yoav Goldberg. 2020. Towards faith- fully interpretable nlp systems: How should we deﬁne and evaluate faithfulness? arXiv preprint arXiv:2004.03685. Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learn- ing of language representations. arXiv preprint arXiv:1909.11942. Q. Vera Liao, Daniel Gruen, and Sarah Miller. 2020. Questioning the AI: Informing Design Practices for Explainable AI User Experiences. Conference on Human Factors in Computing Systems - Proceed- ings. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692. Scott M Lundberg and Su-In Lee. 2017. A uniﬁed approach to interpreting model predictions. In Ad- vances in neural information processing systems, pages 4765–4774. John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. 2020. The effect of natural distribution shift on question answering models. arXiv preprint arXiv:2004.14444. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable ques- tions for squad. pages 784–789. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383–2392. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. ” why should i trust you?” explain- ing the predictions of any classiﬁer. In Proceed- ings of the 22nd ACM SIGKDD international con- ference on knowledge discovery and data mining, pages 1135–1144. Karen Simonyan, Andrea Vedaldi, and Andrew Zisser- man. 2013. Deep inside convolutional networks: Vi- sualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. 2017. Smoothgrad: removing noise", "meta": {"paper_id": "2012.13190v2", "chunk_index": 11, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_12", "text": "international con- ference on knowledge discovery and data mining, pages 1135–1144. Karen Simonyan, Andrea Vedaldi, and Andrew Zisser- man. 2013. Deep inside convolutional networks: Vi- sualising image classiﬁcation models and saliency maps. arXiv preprint arXiv:1312.6034. Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi´egas, and Martin Wattenberg. 2017. Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825. Julia Strout, Ye Zhang, and Raymond J Mooney. 2019. Do human rationales improve machine expla- nations? arXiv preprint arXiv:1905.13714. Mukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017. Axiomatic attribution for deep networks. In Inter- national Conference on Machine Learning, pages 3319–3328. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R´emi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro- cessing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38–45, Online. Asso- ciation for Computational Linguistics. A Testing Custom Interpretability algorithms Figure 4 gives the UML class diagram of the ex- perimentation framework. The central part of the framework is the QAExperimenter. It can also be instantiated with SQuADExperimenter or SQuAD- ShiftsExperimenter, which automatically load the corresponding NLP dataset. The user should provide the instantiated classiﬁcation model (from Model QA or Model Classiﬁcation) and interpreter func- tion. The interpreter function receives the model, question and context as arguments. It should produce a numpy array of importances, of the same length as the number of sentences in the context (using nltk.tokenize.sent tokenize). For information on how to use the model for other interpreters, one may refer to the implemented interpreters in the interpreters folder. The ﬁle run experiment.py can be used to run the experiment with custom interpreters, run sota.py was used to produce the results published here. Model -network: transformers model -tokenizer: transformers tokenizer -device: torch device +predict() +predict_proba() +predict_proba_torch() QAExperimenter -model: Model -interpreter: fctn handle -dataset: datasets Dataset +analyze() +describe() +experiment() +load() +save() +test_gt() SQuADExperimenter SQuADShiftsExperimenter Figure 4: Class Diagram of the Experimentation Suite B Mean Probability Output To complement the results from Table 2, we give the average probability output with the complete context present in Table 3. MODEL SQUAD NEW WIKI NYT REDDIT AMAZON QA 0.94 0.95 0.94 0.88 0.89 CLASSIF 1.00 1.00 1.00 1.00 1.00 Table 3: Average Probability Output for Question- Context Pairs, round to two digits C Complete Results in Tabular Form Tables 4, 5 and 6 give the complete results on the SQuAD 2.0 and SQuADShifts datasets. The results on SQuADShifts are in line with the ﬁndings on SQuAD 2.0. SQuAD New Wiki NYT Reddit Amazon Interpreter Aggregation Samples Classif QA Classif QA Classif QA Classif QA Classif QA LIME† sum 10 37.70 38.47 40.72 41.40 40.22 41.98 31.35 35.34 32.69 35.27 100 58.04 69.90 60.74 70.82 62.02 73.50 50.33 69.02 53.57 67.58 max 10 34.06 35.36 36.98 37.77 36.19 36.43 26.52 28.89 29.80 32.12 100 57.86 68.30 59.65 68.43 61.57 72.00 48.22 65.23", "meta": {"paper_id": "2012.13190v2", "chunk_index": 12, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_13", "text": "QA Classif QA Classif QA LIME† sum 10 37.70 38.47 40.72 41.40 40.22 41.98 31.35 35.34 32.69 35.27 100 58.04 69.90 60.74 70.82 62.02 73.50 50.33 69.02 53.57 67.58 max 10 34.06 35.36 36.98 37.77 36.19 36.43 26.52 28.89 29.80 32.12 100 57.86 68.30 59.65 68.43 61.57 72.00 48.22 65.23 53.09 66.24 SHAP† sum 10 30.48 32.90 31.66 32.84 29.26 31.03 22.13 23.75 24.59 25.43 100 54.85 65.92 57.53 65.79 56.38 67.70 49.35 65.02 54.03 67.68 max 10 29.69 30.81 30.72 31.68 28.32 30.00 21.17 22.58 22.72 23.84 100 52.45 62.34 54.56 63.18 53.19 64.78 45.79 60.03 49.54 63.35 Saliency sum - 74.74 91.12 72.19 91.18 68.87 88.46 57.57 85.26 64.82 85.91 max - 66.27 80.79 65.04 80.78 58.95 76.07 48.41 77.33 59.52 79.70 Integrated Gradients sum 50 66.73 85.93 65.00 85.93 65.44 85.20 51.62 79.73 51.96 78.21 max 50 62.73 87.05 60.70 86.73 61.63 85.92 50.24 82.35 49.09 82.45 SmoothGrad sum 5 60.98 91.28 60.29 90.56 60.25 88.29 50.32 84.51 52.34 84.40 max 5 59.48 82.16 61.45 82.38 56.93 78.33 45.95 77.72 53.03 78.26 Random - - 24.64 25.38 26.86 27.39 24.53 24.36 16.51 16.09 18.71 19.17 Table 4: IoU Results (†Note that evaluation with the default number of samples was not possible due to computa- tional limitations (100 samples already took 7h and 14h for Classif and QA model on SQuAD)) SQuAD New Wiki NYT Reddit Amazon Interpreter Aggregation Samples Classif QA Classif QA Classif QA Classif QA Classif QA LIME† sum 10 50.29 50.83 53.32 53.76 51.62 53.12 39.99 43.56 42.31 44.64 100 66.50 75.98 68.93 76.93 69.17 78.58 56.60 72.97 60.12 72.25 max 10 45.12 46.19 47.85 48.62 46.60 47.11 34.47 36.74 38.43 40.63 100 63.74 71.33 65.47 71.89 67.28 75.25 53.23 67.65 58.41 69.21 SHAP† sum 10 41.22 44.09 42.87 44.57 39.06 41.38 28.94 31.26 32.97 34.63 100 63.93 72.75 66.18 72.91 64.39 73.74 55.59 69.44 60.48 72.29 max 10 37.74 39.28 39.29 40.68 36.40 38.54 27.30 29.13 30.24 32.05 100 59.85 67.47 61.80 68.35 60.80 69.98 51.19 63.41 55.32 66.86 Saliency sum - 79.91 93.01 78.20 93.06 74.97 90.71 63.05 87.21 69.96 88.01 max - 72.99 84.83 72.40 84.81 66.86 80.94 55.10 80.32 65.31 82.74 Integrated Gradients sum 50 73.52 88.85 72.39 88.93 71.99 88.00 57.84 82.46 58.93 81.56 max 50 70.15 89.67 68.93 89.48 68.73 88.52 56.51 84.70 56.35 85.10 SmoothGrad sum 5 69.03 93.08 68.92 92.59 68.05 90.52 56.77 86.58 59.36 86.72 max 5 67.83 85.77 69.64 86.12 65.32 82.65 53.00 80.64 59.84 81.43 Random - - 40.28 40.71 42.66 42.92 39.23 39.18 27.41 27.17 30.79 31.34 Table 5: HPD Results (†Note that evaluation with the default number of samples was not possible due to computa- tional limitations (100 samples already took 7h and 14h for Classif and QA model on SQuAD)) SQuAD New Wiki NYT Reddit Amazon Interpreter Aggregation Samples Classif QA Classif QA Classif QA Classif QA Classif QA LIME† sum 10 39.23 38.20 41.82 40.66 36.98 37.26 25.89 22.84 27.87 27.08 100 39.30 40.91 42.30 43.96 39.41 39.38 32.90 46.71 27.42 32.52 max 10 91.76 91.54 94.38 88.83 93.24 85.01 107.89", "meta": {"paper_id": "2012.13190v2", "chunk_index": 13, "filename": "2012.13190v2.pdf", "chunk_length": 512}}
{"id": "2012.13190v2_chunk_14", "text": "Wiki NYT Reddit Amazon Interpreter Aggregation Samples Classif QA Classif QA Classif QA Classif QA Classif QA LIME† sum 10 39.23 38.20 41.82 40.66 36.98 37.26 25.89 22.84 27.87 27.08 100 39.30 40.91 42.30 43.96 39.41 39.38 32.90 46.71 27.42 32.52 max 10 91.76 91.54 94.38 88.83 93.24 85.01 107.89 110.55 93.77 95.83 100 125.98 176.07 124.96 162.51 133.54 184.66 171.42 305.21 151.52 232.94 SHAP† sum 10 73.24 67.24 74.17 67.85 71.42 68.34 91.28 83.95 68.02 60.68 100 42.27 42.09 44.80 45.60 37.31 43.69 34.30 40.05 28.76 34.64 max 10 99.16 102.31 97.42 101.44 97.10 92.66 130.98 127.01 99.87 102.16 100 107.51 137.77 102.01 132.57 94.43 132.64 149.95 240.47 135.62 207.62 Saliency sum - 37.29 39.92 40.88 40.14 35.14 34.23 19.10 19.77 22.90 23.34 max - 35.58 38.20 39.75 39.81 34.08 36.35 19.15 20.04 22.17 23.78 Integrated Gradients sum 50 37.28 37.32 39.16 40.19 33.30 33.04 18.96 20.60 22.50 24.60 max 50 35.71 34.99 38.74 38.09 32.55 32.80 18.41 20.57 21.80 23.69 SmoothGrad sum 5 38.13 37.22 41.29 40.15 35.16 33.04 19.40 20.33 23.34 23.11 max 5 37.55 36.61 40.29 40.04 34.85 35.98 19.23 19.62 22.45 22.69 Random - - 37.70 37.34 40.63 40.52 34.87 35.06 19.24 19.79 23.21 23.70 Table 6: SNR Results (†Note that evaluation with the default number of samples was not possible due to compu- tational limitations (100 samples already took 7h and 14h for Classif and QA model on SQuAD)) Examples for which noise cannot be estimated are omitted.", "meta": {"paper_id": "2012.13190v2", "chunk_index": 14, "filename": "2012.13190v2.pdf", "chunk_length": 244}}
{"id": "2012.13391v2_chunk_0", "text": "I like ﬁsh , especially dolphins :∗ Addressing Contradictions in Dialogue Modeling Yixin Nie1, Mary Williamson2, Mohit Bansal1, Douwe Kiela2, Jason Weston2 1UNC Chapel Hill 2Facebook AI Research Abstract To quantify how well natural language un- derstanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DE- CODE) and a new conversational dataset con- taining both human-human and human-bot contradictory dialogues. We then compare a structured utterance-based approach of using pre-trained Transformer models for contradic- tion detection with the typical unstructured ap- proach. Results reveal that: (i) our newly col- lected dataset is notably more effective at pro- viding supervision for the dialogue contradic- tion detection task than existing NLI data in- cluding those aimed to cover the dialogue do- main; (ii) the structured utterance-based ap- proach is more robust and transferable on both analysis and out-of-distribution dialogues than its unstructured counterpart. We also show that our best contradiction detection model corre- lates well with human judgements and further provide evidence for its usage in both auto- matically evaluating and improving the consis- tency of state-of-the-art generative chatbots. 1 Introduction Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020a) have triggered a resurgent inter- est on building intelligent open-domain chatbots. Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their prede- cessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019b; Adiwardana et al., 2020) that aim to give a more accurate measure of what ∗* Dolphins are mammals, not ﬁsh. makes a good conversation. While the success is indisputable, there is still a long way to go be- fore we arrive at human-like open-domain chatbots. For example, it has been shown that open-domain chatbots frequently generate annoying errors (Adi- wardana et al., 2020; Roller et al., 2020) and a noto- rious one among these is the class of contradiction, or consistency, errors. When interacting with chatbots, people carry over many of the same expectations as when in- teracting with humans (Nass and Moon, 2000). Self-contradictions (see examples in Figure 1) by these bots are often jarring, immediately disrupt the conversational ﬂow, and help support arguments about whether generative models could ever really understand what they are saying at all (Marcus, 2018). From a listener’s perspective, such inconsis- tent bots fail to gain user trust and their long-term communication conﬁdence. From a speaker’s per- spective, it violates the maxim of quality in the Grice’s cooperative principle (Grice, 1975) —”Do not say what you believe to be false.” Hence, efforts on reducing contradicting or inconsistent conversa- tions by open-domain chatbots are imperative. Historically, modularizing dialogue systems, i.e., assigning an aspect of conversational modeling to a speciﬁc component and then integrating it back into the dialogue system, can often help improve overall system satisfaction (Fang et al., 2017; Chen", "meta": {"paper_id": "2012.13391v2", "chunk_index": 0, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_1", "text": "Hence, efforts on reducing contradicting or inconsistent conversa- tions by open-domain chatbots are imperative. Historically, modularizing dialogue systems, i.e., assigning an aspect of conversational modeling to a speciﬁc component and then integrating it back into the dialogue system, can often help improve overall system satisfaction (Fang et al., 2017; Chen et al., 2018). Prior works (Welleck et al., 2019) character- ized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan et al., 2005; Bowman et al., 2015), constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of such techniques. Overall, the challenge re- mains that we are still unable to answer the simple yet important question—“how well can a natural language understanding module model the consis- arXiv:2012.13391v2 [cs.CL] 28 Dec 2020 Human BlenderBot 2.7B Figure 1: Two dialogue examples demonstrating a state-of-the-art chatbot (B) (Roller et al., 2020) contra- dicting itself when talking to a human (A). tency (including persona, logic, causality, etc) in a general conversation?”. The lack of an ability to measure this obscures to what degree building new modules or techniques can in turn help prevent contradicting responses during generation. Seeking to answer this question, we introduce the DialoguE COntradiction DEtection task (DE- CODE)1 and collect a new conversational dataset containing human written dialogues where one of the speakers deliberately contradicts what they have previously said at a certain point during the conversation. We also collect an out-of-distribution (OOD) set of dialogues in human-bot interac- tive settings which contain human-labeled self- contradictions made by different chatbots. We then compare a set of state-of-the-art sys- tems, including a standard unstructured approach and a proposed structured approach for utilizing NLI models to detect contradictions. In the unstruc- tured approach, a Transformer NLI model directly takes in the concatenation of all utterances of the in- put dialogue for prediction, following the paradigm of NLU modeling. In the structured approach, ut- terances are paired separately before being fed into Transformer NLI models, explicitly taking account 1Our DECODE dataset is publicly available at https: //parl.ai/projects/contradiction. of the natural dialogue structure. Results reveal that: (1) our newly collected dataset is notably more effective at providing su- pervision for the contradiction detection task than existing NLI data including those aimed at covering the dialogue domain; (2) the structured utterance- based approach for dialogue consistency modeling is more robust in our analysis and more transfer- able to OOD human-bot conversation than the un- structured approach. This ﬁnding challenges the mainstream unstructured approach of simply apply- ing pre-trained Transformer models and expecting them to learn the structure, especially for OOD sce- narios which are often the case when incorporating NLU modules into NLG systems, since intermedi- ate in-domain data are scarce. Finally, with such improvements on the contra- diction detection task, we show that our best re- sultant contradiction detector correlates well with human judgements and can be suitable for use as an automatic metric for checking dialogue consis- tency. We further provide evidence for", "meta": {"paper_id": "2012.13391v2", "chunk_index": 1, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_2", "text": "intermedi- ate in-domain data are scarce. Finally, with such improvements on the contra- diction detection task, we show that our best re- sultant contradiction detector correlates well with human judgements and can be suitable for use as an automatic metric for checking dialogue consis- tency. We further provide evidence for its usage in improving the consistency of state-of-the-art gener- ative chatbots. 2 Related Work Several prior works on improving dialogue con- sistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are main- tained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memo- ries (Bang et al., 2015), hierarchical neural archi- tecture (Serban et al., 2016), latent variables (Ser- ban et al., 2017), topical attention (Dziri et al., 2019b), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded gen- eration models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response gen- eration based on the consistency modeling, the is- sue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Fur- ther, simply scaling models has not made the prob- lem go away, as is evident in the largest chatbots trained such as BlenderBot with up to 9.4B param- eter Transformers (Roller et al., 2020). More similar to our work is utilizing NLI mod- els in dialogue consistency. Dziri et al. (2019a) attempted to use entailment models trained on syn- thetic datasets for dialogue topic coherence eval- uation. Particularly, Welleck et al. (2019) con- structed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contra- diction rates. We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across diverse domains. Our task also involves logical and context-related reasoning beyond per- sonal facts, for example the dialogue at the bottom of Figure 1 shows a non-persona-related contradic- tion. We show in our experiments that transfer of DECODE is subsequently more robust than dia- logue NLI on both human-human and human-bot chats. 3 Task and Data 3.1 Dialogue Contradiction Detection We formalize dialogue contradiction detection as a supervised classiﬁcation task. The input of the task is a list of utterances x = {u0, u1, u2, ..., un} representing a dialogue or a dialogue snippet. The output is y, indicating whether the last utterance un contradicts any previously conversed information contained in the dialogue{u0, u1, u2, ..., un−1}, where y can be 0 or 1 corresponding to the non- contradiction and the contradiction label respec- tively. Preferably, the output should also include a set of indices I ⊆{0, 1, ..., n", "meta": {"paper_id": "2012.13391v2", "chunk_index": 2, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_3", "text": "the last utterance un contradicts any previously conversed information contained in the dialogue{u0, u1, u2, ..., un−1}, where y can be 0 or 1 corresponding to the non- contradiction and the contradiction label respec- tively. Preferably, the output should also include a set of indices I ⊆{0, 1, ..., n −1} representing a subset of {u0, u1, u2, ..., un−1} which contain in- formation that is actually contradicted by the last utterance un. The extra indices I output require models to pinpoint the evidence for the contradic- tion, providing an extra layer of explainability. 3.2 Data Collection Annotation Design Our goal is ﬁrst to collect training and evaluation data for this task. We thus collect dialogues in which the last utterance con- tradicts some previous utterances in the dialogue history. To obtain such dialogues, we give annota- tors dialogue snippets from pre-selected dialogue corpora, and then ask them to continue the con- versation by writing one or two utterances such that the last utterance by the last speaker contra- dicts the dialogue history. We also ask annotators to mark all the utterances in the dialogue history that are involved in the contradiction as supporting evidence. Figure 2 shows the annotation user in- terface. We ask annotators to write contradicting utterances based partly on existing dialogues rather than collecting new dialogue from scratch because the provided dialogues can often convey semantic- rich contexts from different domains and inspire annotators to write more diverse examples. We crowdsource the continuation and annotation data with Amazon Mechanical Turk and the collection is based on the ParlAI2 framework. Quality Control We apply the following mecha- nism to ensure the quality of collected data: • Onboarding Test: Every annotator needs to pass an onboarding test before they can actu- ally contribute dialogue examples. The test is the same dialogue contradiction detection task as in the actual collection procedure, including 5 dialogues where 3 of them have an ending utterance that contradicts the dialogue history. The annotator needs to select the correct label (contradiction or non-contradiction) for all ﬁve dialogues to pass the test. This mechanism tests whether an annotator understands the task. • Maximum Annotation Count Limit: The maximum number of examples one annotator can create is 20. This mechanism helps further diversify the dialogue examples by reducing sim- ilar patterns that appear in one or a group of annotators (Geva et al., 2019). • Veriﬁcation: This subtask ensures that the dia- logue examples indeed contain an ending utter- ance that contradicts the dialogue history. We ask 3 additional annotators to verify some of the collected examples and select the ones where all three veriﬁers agreed on the contradiction label, and use these for our resulting validation and tests sets. This mechanism ensures that there is a clear, agreed-upon contradiction in the dialogue, preventing the subjectivity and ambiguity issues in some NLU tasks (Nie et al., 2020b). See the appendix for statistics about the data veriﬁcation. 3.3 Dataset We collected 17,713 human-written contradicting dialogues in which 4,121 are veriﬁed by 3 anno- tators. The pre-selected dialogue source corpora", "meta": {"paper_id": "2012.13391v2", "chunk_index": 3, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_4", "text": "agreed-upon contradiction in the dialogue, preventing the subjectivity and ambiguity issues in some NLU tasks (Nie et al., 2020b). See the appendix for statistics about the data veriﬁcation. 3.3 Dataset We collected 17,713 human-written contradicting dialogues in which 4,121 are veriﬁed by 3 anno- tators. The pre-selected dialogue source corpora 2https://parl.ai (Miller et al., 2017) Figure 2: The collection interface. The task preview box (top right) gives a short description of the task before the annotator will work on the writing. The collection consists of two steps. In Step 1 (on the left), the annotators are asked to write one or two utterances such that the last utterance will contradict some previous utterances in the conversation. In Step 2 (on the right), the annotators are asked to pick the utterances in the conversation that are involved in the contradiction. We use a casual term “message” instead of “utterance” in the instructions. are Wizard of Wikipedia (Dinan et al., 2018), EMPATHETICDIALOGUES (Rashkin et al., 2019), Blended Skill Talk (Smith et al., 2020a), and Con- vAI2 (Dinan et al., 2020), covering various con- versational topics. To facilitate the evaluation of consistency modeling on the dialogue contradic- tion detection classiﬁcation task, we sample an equal number of non-contradicting dialogues ac- cording to the same dialogue length distribution as the contradicting ones from the same dialogue corpus.3 Then, we make the split such that the train split contains unveriﬁed examples, and dev and test splits only contain veriﬁed examples. Each split has balanced labels between contradiction and non-contradiction dialogues. Table 1 shows the breakdown of each of the dataset sources and data splits. Auxiliary (Checklist) Test Sets We further create two auxiliary checklist evaluation sets by trans- forming the contradiction examples in the original test in two ways such that the ground truth label is either invariant or expected to change. The two re- sultant sets serve as diagnostic tests on the behavior, generalization and transferability of our models. The transformations are described below: 3We balance the labels in the dataset following the standard NLI evaluation (Bowman et al., 2015; Welleck et al., 2019). Train Dev Test Wizard of Wikipedia 6,234 1,208 1,160 EMPATHETICDIALOGUES 6,182 1,046 1,050 Blended Skill Talk 8,554 1,200 1,310 ConvAI2 6,214 572 696 Total 27,184 4,026 4,216 Table 1: Our DECODE Main Dataset source statistics. The labels in each split are balanced. There are a to- tal of 2,013+2,108 contradicting examples in the dev and test sets which are the collected 4,121 veriﬁed ex- amples. The ﬁrst column indicates the source of the dialogue. • Add Two Turns (A2T) We insert a pair of ran- domly sampled utterances into the dialogue such that the inserted utterances are between the two original contradicting utterances. This gives a new contradicting dialogue with a longer dia- logue history. • Remove Contradicting Turns (RCT) We re- move all the turns (all pairs of utterances)4 marked as supporting evidence for the contra- 4All the dialogues in the dataset involved two speakers that takes turns in speaking. To maintain this structure, for each marked utterance we remove a", "meta": {"paper_id": "2012.13391v2", "chunk_index": 4, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_5", "text": "dia- logue history. • Remove Contradicting Turns (RCT) We re- move all the turns (all pairs of utterances)4 marked as supporting evidence for the contra- 4All the dialogues in the dataset involved two speakers that takes turns in speaking. To maintain this structure, for each marked utterance we remove a pair of utterance that represents a turn of conversation. This also helps remove the information that was involved in the contradiction such that the resultant label should be “non-contradiction”. Count Label Main (Train) 27,184 balanced Main (Dev) 4,026 balanced Main (Test) 4,216 balanced Human-Bot (Test) 764 balanced A2T (Test) 2,079 contradiction RCT (Test) 2,011 non-contradiction Table 2: DECODE Dataset summary. The ﬁrst column presents the different dataset types. “Main” is the col- lected human-written dialogues. “balanced” indicates that the contradiction and non-contradiction labels in that part of the dataset are balanced. A2T and RCT are the auxiliary test sets described in Sec. 3.3. . diction in the dialogue except the last utterance. This results in a new non-contradiction dialogue. Notice that the two data transformations we used were based on utterance-level evidence annotations and therefore are not applicable for DNLI and other NLI data. Human-Bot Test Set Our main collected dataset involves human-written dialogues containing con- tradicting utterances based on human-human dia- logues from existing corpora. In practice, to evalu- ate the response quality of a machine rather than a human in terms of its consistent responses, we care about how well a contradiction detector can per- form in human-bot interactive conversations. To that end, we further collect human-bot dialogue data by employing workers on Amazon Mechan- ical Turk to interact with a diverse set of open- domain bots. These include Poly-encoder (Humeau et al., 2019) based retrieval models, generative models (Roller et al., 2020), unlikelihood trained models (Li et al., 2019a), retrieve-and-reﬁne mod- els (Weston et al., 2018; Roller et al., 2020), mod- els either pre-trained on a previously existing Red- dit dataset extracted and obtained by a third party that was hosted by pushshift.io (Baumgartner et al., 2020) or ﬁne-tuned on the Blended Skill Talk (BST) dialogue tasks (Smith et al., 2020b) – that is, all the dialogue models that are compared in the study in (Roller et al., 2020). During the collection, if the bot generates an utterance that contradicts itself, we ask the worker to mark the utterance. In some of the dialogues, workers are explicitly instructed to goad the bots into making contradicting utterances. The ﬁnal human-bot test set we derive contains 764 dialogues, half of which ends with a contradicting utterance by the bot. All the dialogues in the set, Main (Human-Human) Human1: I recently had a long weekend with some old friends. It was fantastic. Human2: It must have been fun to catch up with them. Human1: it was. We’ve spent a lot of time together and apart now, so it was good to catchup. Human2: Well I hope you guys continue to stay in touch. Human1: Sometimes friends grow apart over time though. Human2: Yeah, that’s true. You should forget about them and go", "meta": {"paper_id": "2012.13391v2", "chunk_index": 5, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_6", "text": "them. Human1: it was. We’ve spent a lot of time together and apart now, so it was good to catchup. Human2: Well I hope you guys continue to stay in touch. Human1: Sometimes friends grow apart over time though. Human2: Yeah, that’s true. You should forget about them and go on with your life. Human-Bot: Human1: It’s a beautiful day. I’m going to take a walk later. Bot: What do you do for a living? Human1: I work in a doctor’s ofﬁce. How about yourself? Bot: I am a stay at home mom. Human1: That’s great! How many kids do you have? Bot: I don’t have any kids yet, but I plan on getting married soon. Add Two Turns (A2T): Human1: i hate when ash from my cigarette drops in my work pants Human2: oof that sucks really bad Human1: yeah, i haave to wait till i get home to get the stain off, it is really embarras- ing Human2: yea i can imagine it is Human1: Every time I look at it I remember the good times we had together. Human2: well thats nice Human1: I will have to wash the stain with soap and water. Human2: Ash stains on your pants is not a big deal though. Remove Contradicting Turns (RCT): Human1: I was disgusted when I noticed the food on the table Human2: What kind of food? Human1: It was brussel sprouts and Liver Human2: Oh, disgusting. Human1: I couldn’t even bear to take a single bite Human2: Brussel sprouts and liver sounds delicious to me! Table 3: Dialogue examples for different dataset types. Underline indicates that the pair of utterances is ran- domly added. Strikethrough text indicates that the pair of utterances is removed. Dialogue examples for Human-Human, Human-Bot, and A2T end with a con- tradicting utterance whereas the example for RCT has an ending utterance whereby the original contradicting pair of utterances in the dialogue history are removed. with either contradiction or non-contradiction la- bels, are veriﬁed by 3 additional annotators, beside the human who actually talked to the bot. The auxiliary and human-bot test sets are aimed to test models’ robustness and generalizability be- yond accuracy on the collected human-written test set (Ribeiro et al., 2020; Gardner et al., 2020), and give a more comprehensive analysis of the task. Ta- ble 2 summarizes the ﬁnal overall dataset. Table 3 gives one example for each dataset type. 4 Models To model the dialogue consistency task, we ﬁrst em- ploy some of the techniques used in NLI sequence- to-label modeling, where the input is a pair of tex- tual sequences and the output is a label. The beneﬁt of such modeling is that we can directly make use of existing NLI datasets during training. However, unlike previous work (Welleck et al., 2019) that directly utilized NLI models giving a 3-way output among “entailment”, “contradiction”, and “neu- tral”, we modify the model with a 2-way output between “contradiction” and “non-contradiction” labels. This is because the task is, in its essence, centered around the detection of inconsistency. More formally, we", "meta": {"paper_id": "2012.13391v2", "chunk_index": 6, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_7", "text": "(Welleck et al., 2019) that directly utilized NLI models giving a 3-way output among “entailment”, “contradiction”, and “neu- tral”, we modify the model with a 2-way output between “contradiction” and “non-contradiction” labels. This is because the task is, in its essence, centered around the detection of inconsistency. More formally, we denote the model as ˆypred = fθ(C, u), where ˆypred is the prediction of the label y, i.e. whether the textual response u contradicts some textual context C, and where θ are the param- eters of the model. We then explore two different approaches to utilize fθ for dialogue contradiction detection. 4.1 Dialogue Contradiction Detectors As described in subsection 3.1, a detector is asked to determine whether the last utterance of the di- alogue un contradicts the previous dialogue his- tory {u0, u1, u2, ..., un−1}. In what follows, we describe two approaches that propose differing fθ for the detection prediction problem. Unstructured Approach. In this approach, we simply concatenate all the previous utterances in the dialogue history to form a single textual con- text. Then, we apply fθ to the context and the last utterance to infer the probability of contradiction. ˆypred = fθ([u0, u1, u2, ..., un−1], un) (1) When concatenating the utterances, we insert spe- cial tokens before each utterance to indicate the speaker of that utterance. This is aimed to provide a signal of the dialogue structure to the models. Still, this approach assumes that the model can use these features adequately to learn the underlying structure of the dialogue implicitly during training. Structured Utterance-based Approach. Since the reasoning crucially depends on the last utter- ance, in this method we ﬁrst choose all the utter- ances by the last speaker to form a set S. We then pair every utterance in the set with the last utter- ance and feed them one by one into fUB θ . The ﬁnal contradiction probability is the maximum over all the outputs. ˆypred = max \b fUB θ (ui, un) : ui ∈S (2) Additionally, the utterance-based approach is able to give a set of utterances as supporting evidence for a contradiction decision by choosing the pairs having contradiction probability higher than a threshold ηe: I = \b i : fUB θ (ui, un) > ηe (3) This not only gives explanations for its predic- tion but can also help diagnose the model itself, e.g. we can measure metrics of the model’s ability to provide these explanations by comparing them against gold supporting evidence annotations from DECODE. One downside of this modeling approach is that it will not be able to capture reasoning between speakers. A case for that would be a pronoun by one speaker might refer to something initiated by the other speaker. Nevertheless, the utterance- based approach explicitly adds an inductive struc- ture bias to learning and inference which we will see can aid its generalization capability. Thresholding. For both the unstructured and utterance-based approaches, the detection of contra- diction is made by comparing ˆypred with a thresh- old τ and by default τ is 0.5. 4.2 Experimental", "meta": {"paper_id": "2012.13391v2", "chunk_index": 7, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_8", "text": "adds an inductive struc- ture bias to learning and inference which we will see can aid its generalization capability. Thresholding. For both the unstructured and utterance-based approaches, the detection of contra- diction is made by comparing ˆypred with a thresh- old τ and by default τ is 0.5. 4.2 Experimental Setup We study four base pre-trained models variants for fθ: BERT (Devlin et al., 2019), Electra (Clark et al., 2019), RoBERTa (Liu et al., 2019), and BART (Lewis et al., 2020). They represent the start-of-the-art language representation models and have yielded successes in many NLU tasks. The in- put format of fθ follows how these models handle sequence-pairs (C and u) classiﬁcation task with padding, separator and other special tokens such as position embeddings and segment features inserted at designated locations accordingly. We ﬁne-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), ANLI- R3 (Nie et al., 2020a)5, DNLI (Welleck et al., 2019), as well as our DECODE Main training set. We convert the 3-way labels of the examples in existing NLI datasets to 2-way6 and θ is optimized using cross-entropy loss. When training fUB θ in the utterance-based approach using the DECODE training set, the input sequences are sampled utter- ance pairs from the DECODE dialogue. In other scenarios, fθ or fUB θ are trained with data treated as in normal NLI training. The models are evaluated on the test sets de- scribed in Sec. 3.3. For the utterance-based ap- proach, which additionally provides supporting evi- dence utterances (Equation 3), we report Precision, 5ANLI data is collected in three rounds resulting in three subsets (R1, R2, R3). We only used training data in R3 since it contains some dialogue-related examples. 6The 3-way “entailment” and “neutral” label is converted to “non-contradiction” while 3-way “contradiction” is kept the same. Pre-trained Model Training Data Main (Test) Main (Test-Strict) Human-Bot SE (Precision / Recall / F1) Unstructured Approach RoBERTa All 97.46 - 77.09 - All - DNLI 97.44 - 73.17 - All - ANLI-R3 98.04 - 73.56 - All - DECODE 84.42 - 61.91 - DNLI 57.19 - 60.34 - ANLI-R3 82.21 - 59.69 - DECODE 96.85 - 70.03 - Utterance-based Approach RoBERTa SNLI + MNLI 77.40 47.70 73.17 63.3 / 84.6 / 72.4 All 94.19 80.08 83.64 85.9 / 91.2 / 88.5 All - DNLI 94.38 80.93 81.68 86.7 / 90.1 / 88.4 All - ANLI-R3 94.07 79.32 82.85 85.2 / 91.8 / 88.4 All - DECODE 86.67 66.95 77.36 78.0 / 83.4 / 80.6 DNLI 76.54 63.09 75.26 85.1 / 61.2 / 71.2 ANLI-R3 81.59 69.11 70.52 88.2 / 64.3 / 74.3 DECODE 93.19 80.86 84.69 87.9 / 87.2 / 87.5 BERT DECODE 88.88 74.14 75.52 84.9 / 83.7 / 84.3 Electra DECODE 93.17 81.19 80.76 87.9 / 87.1 / 87.5 BART DECODE 94.47 80.10 79.19 85.8 / 90.7 / 88.2 Majority - - 50.00 50.00 50.00 50.4 / 47.1 / 48.7 Table 4: Test performance of different models and approaches. “All” in the “Training Data” column stands for a combination of", "meta": {"paper_id": "2012.13391v2", "chunk_index": 8, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_9", "text": "DECODE 93.17 81.19 80.76 87.9 / 87.1 / 87.5 BART DECODE 94.47 80.10 79.19 85.8 / 90.7 / 88.2 Majority - - 50.00 50.00 50.00 50.4 / 47.1 / 48.7 Table 4: Test performance of different models and approaches. “All” in the “Training Data” column stands for a combination of SNLI, MNLI, DNLI, ANLI-R3, DECODE. “All - DNLI” denotes all the datasets with DNLI removed. “SE” stands for supporting evidence. The “Main (Test-Strict)” column indicates the performance where both the 2-way contradiction detection and the supporting evidence retrieval exactly match with the ground truth. Recall, and F1 on these evidence predictions. We also report a stricter score which evaluates whether both 2-way contradiction detection and supporting evidence retrieval exactly match with the ground truth on our DECODE Main test set. 5 Results and Analysis 5.1 Performance on Constructed Dataset We test different pre-trained models with both the unstructured and the structured utterance-based ap- proaches. We explicitly investigate the model per- formance when trained on DNLI or ANLI-R3 and compare it with DECODE because these are re- cently published NLI datasets that contain exam- ples in a dialogue setting. However, we do also provide results comparing to other NLI datasets as well as multi-tasking all datasets at once, in addi- tion to various ablations. The results are shown in Table 4. We now describe our key observations. DECODE is notably more effective than other existing NLI data in providing supervision for contradiction detection in dialogue. We found that models trained on DECODE achieve higher ac- curacy than that of those trained on DNLI or ANLI- R3, on all evaluation sets in both the unstructured and utterance-based approach. On the DECODE Main test set, the utterance-based RoBERTa model trained (ﬁne-tuned) on DECODE achieves 93.19% accuracy, which is a 12-point jump from the same model training on ANLI-R3 and a 16-point jump from training on DNLI. The best model on human- bot data is utterance-based RoBERTa trained on DECODE with 84.69%, while the same model trained on DNLI can only get 75.26% accuracy, and ANLI-R3 is even worse with 70.52%. While training on “All” datasets (SNLI, MNLI, ANLI- R3, DNLI & DECODE) is effective, the removal of DECODE from the training data induces a con- sequential downgrade on the performance on all evaluation sets. In particular, removing DECODE training data for unstructured RoBERTa causes a 15-point loss of accuracy on the human-bot data from (77.09% to 61.91%). Further, training on DE- CODE is also more helpful than DNLI or ANLI-R3 for supporting evidence retrieval. These ﬁndings indicate that existing NLI data has limited transfer- ability to the dialogue contradiction detection task despite their coverage of the dialogue domain in addition to other domains. Training on NLI data which does not cover examples with dialogue struc- tures, e.g., SNLI+MNLI is even worse, only achiev- ing 77.4% on DECODE Main (Test) vs. 93.19% for DECODE and cannot even reach the majority base- line on the “Main (Test-Strict)”. Hence overall, this empirically demonstrates that our DECODE data provides a valuable resource for modeling dialogue consistency and developing data-driven", "meta": {"paper_id": "2012.13391v2", "chunk_index": 9, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_10", "text": "tures, e.g., SNLI+MNLI is even worse, only achiev- ing 77.4% on DECODE Main (Test) vs. 93.19% for DECODE and cannot even reach the majority base- line on the “Main (Test-Strict)”. Hence overall, this empirically demonstrates that our DECODE data provides a valuable resource for modeling dialogue consistency and developing data-driven approaches for contradiction detection. Different pre-training models that perform sim- ilarly on the in-domain test set can have very different performance on OOD human-bot dia- logue. The last four rows of the table show the results of utterance-based RoBERTa, BERT, Elec- tra, and BART trained on DECODE. We can see that RoBERTa, Electra, and BART got similar in- domain accuracy on DECODE, around 93%-94%. RoBERTa stands out when comparing their perfor- mance on the human-bot test set with the highest score of 84.69% across the column (compared to 75.52, 79.19 and 80.76 for the other methods) and with better performance on supporting evidence retrieval as well. We speculate that this is due to the fact that RoBERTa pre-training data has a broader coverage than Electra and BART. We hope future work on dialogue contradiction detec- tion could explore pre-training models on more dialogue-focused corpora. The unstructured approach gets higher accu- racy on the in-domain test set. A direct compar- ison between unstructured RoBERTa and utterance- based RoBERTa trained on DECODE reveals that the unstructured approach more often than not gets a higher accuracy than its corresponding utterance- based approach when other experiential setups are kept identical. Noticeably, unstructured RoBERTa trained on all NLI data got a 97.46% score, whereas utterance-based yielded 94.19%. This seemingly indicates that training an unstructured model is able to yield a good representation of the consis- tency of the dialogue. However, further analysis on the human-bot and auxiliary test sets shows that such high accuracy is an over-ampliﬁcation of the model’s real understanding ability, as we discuss next. The structured utterance-based approach is more robust, and more transferable. Figure 3 gives a comparison between utterance-based and unstructured RoBERTa on each of the evaluation sets. We can see that the utterance-based model is Utterance-based Unstructured 0.0 20.0 40.0 60.0 80.0 100.0 Accuracy 93.2 96.8 84.7 70.0 91.5 97.5 78.4 34.4 DECODE Main (Test) Human-Bot A2T RCT Figure 3: Comparison between utterance-based and unstructured approaches of RoBERTa pre-trained, DE- CODE ﬁne-tuned models on DECODE Main (Test), Human-bot, and auxiliary test sets. able to maintain satisfactory performance across all the sets whereas the unstructured model under- performs at the human-bot and RCT auxiliary test sets with a 34.4% accuracy on RCT compared to 78.4% for utterance-based, in stark contrast to the high performance of the unstructured method on the in-domain DECODE Main test set. This result indicates the unstructured approach overﬁts on su- perﬁcial patterns in the DECODE Main training data which are still present due to RCT’s construc- tion process.7 The fact that the utterance-based ap- proach has good transferability to the OOD human- bot test set indicates that injecting the correct in- ductive structure bias is beneﬁcial for modeling dialogue consistency. We believe this is an interest- ing result", "meta": {"paper_id": "2012.13391v2", "chunk_index": 10, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_11", "text": "which are still present due to RCT’s construc- tion process.7 The fact that the utterance-based ap- proach has good transferability to the OOD human- bot test set indicates that injecting the correct in- ductive structure bias is beneﬁcial for modeling dialogue consistency. We believe this is an interest- ing result generally for research using Transform- ers, where there is currently a belief amongst some practitioners that they can just use a standard Trans- former and it will learn all the structure correctly on its own. In our setting that is not the case, and we provide a method that can rectify that failing. In general, there is still much room for improve- ment. The results in Table 4 also demonstrate that the modeling of dialogue consistency is a de- manding task. On the contradiction detection task, the best score achieved by the state-of-the-art pre- trained language models on DECODE (Test-Strict) is 80.86% and the best human-bot test score is 84.69%. Considering all the examples in the test sets are veriﬁed by at least 3 annotators, humans are able to swiftly identify such contradictions. This 7Overﬁtting on superﬁcial patterns is a typical issue and open problem in NLU modeling (Nie et al., 2020a). suggests there is a large ability gap between our best automatic detectors and humans. Closing this gap is an important challenge for the community. 5.2 Performance in an Interactive Setting The results discussed above evaluate models on constructed datasets with intentionally balanced labels. This facilitates the comparison between models following a NLU evaluation perspective. In practice, we would like to evaluate how well a model can detect contradicting utterances sampled naturally from interactive human-bot dialogue. To that end, we test our trained detection models on the raw interactive human-bot dialogue data8 hav- ing a total number of 764 dialogues consisting of 8,933 utterances. Since the contradiction task in naturally sampled dialogue can be extremely unbal- anced, the total number of contradicting utterances in the raw dialogue list is only 3819. We apply our contradiction detectors on every bot-generated utterance and calculate the precision, recall, and F1 on contradiction detection. Since the scores might be subjective to the threshold τ, we also evaluate the threshold-invariant Area Under the ROC Curve (AUC) (Bradley, 1997). As shown in Table 5, model precision on the task is not satisfactory (23.94 at best). However, the best model achieves acceptable scores on both Recall and AUC. This indicates its potential us- age for strict blocking of inconsistent utterances of a generative model (bot). The table also draws the same conclusion as Table 4 that the structured utterance-based RoBERTa model trained using DE- CODE data is the best method for contradiction detection, comparing to training on other NLI data or using an unstructured approach. In the follow- ing sections we thus use that best method as our detector for further experiments. Model vs. Human Judgement To further under- stand the detector predictions and how well they might align with human judgements, we conduct the following experiment. We ﬁrst divide all the utterances into two categories based on", "meta": {"paper_id": "2012.13391v2", "chunk_index": 11, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_12", "text": "ing sections we thus use that best method as our detector for further experiments. Model vs. Human Judgement To further under- stand the detector predictions and how well they might align with human judgements, we conduct the following experiment. We ﬁrst divide all the utterances into two categories based on whether they are generated by a human or a bot. Then, the bot-generated utterances that have been marked by annotators as contradicting utterances are cat- egorized into three sets based on the number of annotators that agree on the contradiction label. 8This is the same set of dialogues from which we con- structed the balanced human-bot test set. 9The majority baseline accuracy is 95.73%. Training Data Precision Recall F1 AUC Unstructured Approach All 15.89 60.11 25.14 80.47 All - DECODE 15.63 57.74 24.60 71.82 DECODE 17.05 50.13 25.45 73.40 Utterance-based Approach All 23.35 71.65 35.23 84.96 All - DECODE 17.17 68.50 27.46 80.09 DNLI 16.32 65.09 26.09 79.29 ANLI-R3 22.52 41.73 29.26 76.36 DECODE 23.94 74.28 36.21 87.16 Table 5: RoBERTa performance on all the bot- generated utterances from the raw interactive human- bot dialogue. The threshold τ for prediction is 0.5. Utterance-based (DECODE) Utterance-based (DNLI) Unstructured (DECODE) Fire Rate 5.5 17.9 14.0 22.9 29.4 21.7 44.3 44.8 31.7 46.3 48.9 39.7 74.3 65.1 50.1 Human Bot @1 @2 @3 Figure 4: The ﬁre rate of RoBERTa models with dif- ferent setups on utterances belonging to different cat- egories. “Human” and “Bot” stand for utterances by the human or the bot prospectively. “@N” indicates the category where N annotators agreed on the contra- diction label. The x-axis indicates different approaches and the text in parentheses denotes the training data. By design, the more annotators that agree on the contradiction label, the more plausible that it is a contradiction. We examine detector model ﬁre rate on the utterances in the 5 different categories and results are shown in Figure 4. The ﬁre rate of utterance-based RoBERTa trained on DECODE on human utterances is 5.5% contrasting to the 74.3% on 3-agreed contradicting utterances, whereas the ﬁre rates of unstructured RoBERTa on different cat- egories are more clustered together. This ﬁnding demonstrates that all the models can discriminate between utterances with a distinct nature, and the model predictions are aligned with human judg- ments. Moreover, the ﬁre rate of a strong discrimi- native detector could be a useful quantity to stratify utterances. 0.04 0.06 0.08 0.1 0.12 Human Identified Contradiction Rate 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 Avg. Contradiction Probability Figure 5: The comparison between the average contra- diction score by the detector (y-axis) and the human identiﬁed contradiction rate (x-axis) on the utterances by different bots, averaged by type of bot. Each point in the plot is a bot which has conversed with humans and produced at least 180 utterances (with some iden- tiﬁed as contradictions) in our interactive settings. The regression line shown yields a Pearson correlation co- efﬁcient of 0.81. Using DECODE as an Automatic Metric The results presented above indicate that the prediction of the detector can easily differentiate", "meta": {"paper_id": "2012.13391v2", "chunk_index": 12, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_13", "text": "with humans and produced at least 180 utterances (with some iden- tiﬁed as contradictions) in our interactive settings. The regression line shown yields a Pearson correlation co- efﬁcient of 0.81. Using DECODE as an Automatic Metric The results presented above indicate that the prediction of the detector can easily differentiate between the quality of utterances by humans and the utterances by bots. We further investigate whether it can dif- ferentiate the quality of the utterances by different bots and be used as an automatic metric checking generation consistency. We compare the average contradiction score of the detector with the contra- diction rate by human judgements on the utterances generated by different classes of model (bots). The bots are the same set of models described in subsec- tion 5.2 from which we collected our human-bot dialogue examples. The trend in Figure 5 reveals that the scores are positively correlated with human judgments, with a Pearson correlation coefﬁcient of 0.81. We would expect that improvement on the DECODE task will directly increase the correla- tion between the automatically produced detection score and human judgements, where use of such an automatic metric can ease the burden on laborious human evaluation of consistency. 5.3 Generation Re-ranking Given a contradiction detector, an obvious ques- tion other than using it as an automatic metric, is: can it be used to improve the consistency of di- alogue generation models? We consider a very simple way to do that in the state-of-the-art genera- tive model, BlenderBot (BST 2.7B) (Roller et al., 2020). During the decoding phase, for decod- ing methods that can output multiple hypotheses, we simply rerank the top scoring hypotheses us- Model + DECODE Human Decoding Strategy Contradict% Contradict% Standard generation Beam Search 38.1% 38.3% Top-k (k = 40) 29.0% 31.8% Sample-and-Rank 29.6% 29.0% DECODE Re-ranking Beam Search 22.7% 32.0% Top-k (k = 40) 1.1% 25.6% Table 6: Generation Re-ranking using DECODE vs. standard methods, reporting the contradiction % as ﬂagged by our contradiction detection classiﬁer (i.e., an automatic metric, “DECODE Contradict%”) in ad- dition to human judgments (“Human Contradict%”). ing the contradiction detection classiﬁer. We use our best performing classiﬁer, our utterance-based RoBERTa model with DECODE ﬁne-tuning, and consider three methods of decoding: beam search, top-k sampling (Fan et al., 2018) and sample-and- rank (Adiwardana et al., 2020), and compare the standard and DECODE-reranked decoding meth- ods to each other. For beam search we use the best found parameters from (Roller et al., 2020) which are beam size 10, minimum beam length 20 and beam blocking of 3-grams. For top-k we use k = 40. For Sample-and-Rank we use k=40 and 20 samples. We consider the same human-bot dia- logue logs as before, but only between Blenderbot BST 2.7B and humans, equally sampled between contradicting and non-contradicting utterances. Ta- ble 6 presents the results. Automatic metric using DECODE Using our same DECODE contradiction classiﬁer as the au- tomatic metric, as in Sec. 5.2. We observe that by re-ranking the beam of beam search (size 10) we can modestly improve the metric, but still 22.7% of the time", "meta": {"paper_id": "2012.13391v2", "chunk_index": 13, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_14", "text": "utterances. Ta- ble 6 presents the results. Automatic metric using DECODE Using our same DECODE contradiction classiﬁer as the au- tomatic metric, as in Sec. 5.2. We observe that by re-ranking the beam of beam search (size 10) we can modestly improve the metric, but still 22.7% of the time the detector ﬂags generations as con- tradictions. Upon observation of the outputs, this appears to be because the beam of beam decod- ing tends to be not diverse enough (Vijayakumar et al., 2016), and when the top scoring utterance is ﬂagged as contradicting, many of the other utter- ances in the beam are similar responses with slight rephrases, and are ﬂagged contradicting as well. Top-k sampling fares much better, where reranking in our test can very often ﬁnd at least one from the k = 40 samples that does not ﬂag the classiﬁer, leaving only a 1.1% contradiction ﬁring rate. We note we expect these numbers are over-optimisticly low because the metric itself is being used to search (re-rank) and evaluate in this case. Human Judgments The last column of Table 6 presents human judgments of the various model generations, judged using the same approach as before with three human veriﬁers, and reporting the percentage of contradictions. We observe sim- ilar results to the automatic metric ﬁndings: that DECODE re-ranking reduces the number of contra- dictions for both types of generation methods that we attempted to re-rank. 6 Conclusion We introduce the DialoguE COntradiction DEtec- tion task (DECODE) and a new conversational dataset containing both human-human and human- bot contradictory dialogues. Training models on DECODE achieves better performance than other existing NLI data by a large margin. We further pro- pose a structured utterance-based approach where each utterances are paired with other utterance be- fore being fed into Transformer NLI models to tackle the dialogue contradiction detection task. We show the superiority of such an approach when transferring to out-of-distribution dialogues com- pared to a standard unstructured approach repre- sentative of mainstream NLU modeling. This is a valuable property since intermediate in-domain data are often scarce when integrating NLU module into NLG systems. We further show that our best contradiction detector correlates with human judge- ments, and provide evidence for its usage in both automatic checking and improving the consistency of state-of-the-art generative chatbots. While this paper deeply studies the contradiction detection problem, we believe here we have only scratched the surface of the non-contradiction gen- eration problem, while obtaining promising ﬁrst results in that setting. Future work should address this further by studying and analysing the results of these techniques more deeply, as well as consid- ering other methods than simply rescoring during decoding. Going forward, we envision complemen- tary progress on both the modeling of NLU and NLG and the integration of the two. We hope our work could facilitate and provide guidelines for future work on incorporating NLU modeling into dialogue systems. References Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a", "meta": {"paper_id": "2012.13391v2", "chunk_index": 14, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_15", "text": "integration of the two. We hope our work could facilitate and provide guidelines for future work on incorporating NLU modeling into dialogue systems. References Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977. Jeesoo Bang, Hyungjong Noh, Yonghee Kim, and Gary Geunbae Lee. 2015. Example-based chat- oriented dialogue system with personalized long- term memory. In 2015 International Conference on Big Data and Smart Computing (BigComp), pages 238–243. IEEE. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings of the Inter- national AAAI Conference on Web and Social Media, volume 14, pages 830–839. Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large anno- tated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empiri- cal Methods in Natural Language Processing, pages 632–642, Lisbon, Portugal. Association for Compu- tational Linguistics. Andrew P Bradley. 1997. The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern recognition, 30(7):1145–1159. Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165. Chun-Yen Chen, Dian Yu, Weiming Wen, Yi Mang Yang, Jiaping Zhang, Mingyang Zhou, Kevin Jesse, Austin Chau, Antara Bhowmick, Shreenath Iyer, et al. 2018. Gunrock: Building a human-like social bot by leveraging large scale real user data. Alexa Prize Proceedings. Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019. Electra: Pre-training text encoders as discriminators rather than genera- tors. In International Conference on Learning Rep- resentations. Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The pascal recognising textual entailment challenge. In Machine Learning Challenges Work- shop, pages 177–190. Springer. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. 2020. The second conversational in- telligence challenge (convai2). In The NeurIPS’18 Competition, pages 187–208. Springer. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. In International Conference on Learning Representations. Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar Zaiane. 2019a. Evaluating coherence in dia- logue systems using entailment. In Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3806–3812, Minneapolis, Minnesota. Association for Computational Linguis- tics. Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. 2019b. Augmenting neural re- sponse generation with context-aware topical atten- tion. In Proceedings of", "meta": {"paper_id": "2012.13391v2", "chunk_index": 15, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_16", "text": "Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3806–3812, Minneapolis, Minnesota. Association for Computational Linguis- tics. Nouha Dziri, Ehsan Kamalloo, Kory Mathewson, and Osmar R Zaiane. 2019b. Augmenting neural re- sponse generation with context-aware topical atten- tion. In Proceedings of the First Workshop on NLP for Conversational AI, pages 18–31. Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi- erarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 889–898, Melbourne, Australia. Association for Computational Linguistics. Hao Fang, Hao Cheng, Elizabeth Clark, Ariel Holtz- man, Maarten Sap, Mari Ostendorf, Yejin Choi, and Noah A Smith. 2017. Sounding board–university of washington’s alexa prize submission. Alexa prize proceedings. Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel- son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models’ local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323, Online. Association for Computational Linguistics. Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an inves- tigation of annotator bias in natural language under- standing datasets. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing and the 9th International Joint Confer- ence on Natural Language Processing (EMNLP- IJCNLP), pages 1161–1166, Hong Kong, China. As- sociation for Computational Linguistics. Herbert P Grice. 1975. Logic and conversation. In Speech acts, pages 41–58. Brill. Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Trans- former architectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv preprint arXiv:1905.01969. Mike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 7871–7880, Online. Association for Computational Linguistics. Jiwei Li, Michel Galley, Chris Brockett, Georgios Sp- ithourakis, Jianfeng Gao, and Bill Dolan. 2016. A persona-based neural conversation model. In Pro- ceedings of the 54th Annual Meeting of the Associa- tion for Computational Linguistics (Volume 1: Long Papers), pages 994–1003, Berlin, Germany. Associ- ation for Computational Linguistics. Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, and Ja- son Weston. 2019a. Don’t say that! making incon- sistent dialogue unlikely with unlikelihood training. arXiv preprint arXiv:1911.03860. Margaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck, Y-Lan Boureau, Kyunghyun Cho, and Ja- son Weston. 2020. Don’t say that! making inconsis- tent dialogue unlikely with unlikelihood training. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4715– 4728, Online. Association for Computational Lin- guistics. Margaret Li, Jason Weston, and Stephen Roller. 2019b. Acute-eval: Improved dialogue evaluation with opti- mized questions", "meta": {"paper_id": "2012.13391v2", "chunk_index": 16, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_17", "text": "Don’t say that! making inconsis- tent dialogue unlikely with unlikelihood training. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4715– 4728, Online. Association for Computational Lin- guistics. Margaret Li, Jason Weston, and Stephen Roller. 2019b. Acute-eval: Improved dialogue evaluation with opti- mized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dia- logue systems. In Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 285–294, Prague, Czech Re- public. Association for Computational Linguistics. Gary Marcus. 2018. Deep learning: A critical ap- praisal. arXiv preprint arXiv:1801.00631. Alexander H Miller, Will Feng, Adam Fisch, Jiasen Lu, Dhruv Batra, Antoine Bordes, Devi Parikh, and Ja- son Weston. 2017. Parlai: A dialog research soft- ware platform. arXiv preprint arXiv:1705.06476. Clifford Nass and Youngme Moon. 2000. Machines and mindlessness: Social responses to computers. Journal of social issues, 56(1):81–103. Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2020a. Ad- versarial NLI: A new benchmark for natural lan- guage understanding. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 4885–4901, Online. Association for Computational Linguistics. Yixin Nie, Xiang Zhou, and Mohit Bansal. 2020b. What can we learn from collective human opinions on natural language inference data? In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9131–9143, Online. Association for Computational Linguistics. Qiao Qian, Minlie Huang, Haizhou Zhao, Jingfang Xu, and Xiaoyan Zhu. 2018. Assigning personal- ity/proﬁle to a chatting machine for coherent conver- sation generation. In Proceedings of the 27th Inter- national Joint Conference on Artiﬁcial Intelligence, pages 4279–4285. Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic open- domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguis- tics, pages 5370–5381, Florence, Italy. Association for Computational Linguistics. Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. Beyond accuracy: Be- havioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4902– 4912, Online. Association for Computational Lin- guistics. Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. 2020. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637. Abigail See, Stephen Roller, Douwe Kiela, and Ja- son Weston. 2019. What makes a good conver- sation? how controllable attributes affect human judgments. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1702–1723, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Iulian Vlad Serban, Alessandro", "meta": {"paper_id": "2012.13391v2", "chunk_index": 17, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_18", "text": "makes a good conver- sation? how controllable attributes affect human judgments. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1702–1723, Minneapolis, Minnesota. Associ- ation for Computational Linguistics. Iulian Vlad Serban, Alessandro Sordoni, Yoshua Ben- gio, Aaron C Courville, and Joelle Pineau. 2016. Building end-to-end dialogue systems using gener- ative hierarchical neural network models. In AAAI. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron C Courville, and Yoshua Bengio. 2017. A hierarchical latent variable encoder-decoder model for generating dia- logues. In AAAI. Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan Boureau. 2020a. Can you put it all together: Evaluating conversational agents’ ability to blend skills. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics, pages 2021–2030, Online. Association for Computational Linguistics. Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan Boureau. 2020b. Can you put it all together: Evaluating conversational agents’ ability to blend skills. arXiv preprint arXiv:2004.08449. Ashwin K Vijayakumar, Michael Cogswell, Ram- prasath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. 2016. Diverse beam search: Decoding diverse solutions from neural se- quence models. arXiv preprint arXiv:1610.02424. Di Wang, Nebojsa Jojic, Chris Brockett, and Eric Ny- berg. 2017. Steering output style and topic in neu- ral response generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Lan- guage Processing, pages 2140–2150, Copenhagen, Denmark. Association for Computational Linguis- tics. Sean Welleck, Jason Weston, Arthur Szlam, and Kyunghyun Cho. 2019. Dialogue natural language inference. In Proceedings of the 57th Annual Meet- ing of the Association for Computational Linguistics, pages 3731–3741, Florence, Italy. Association for Computational Linguistics. Jason Weston, Emily Dinan, and Alexander H Miller. 2018. Retrieve and reﬁne: Improved sequence generation models for dialogue. arXiv preprint arXiv:1808.04776. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen- tence understanding through inference. In Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin- guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112–1122, New Orleans, Louisiana. Association for Computational Linguis- tics. S Worsnick. 2018. Mitsuku wins loebner prize 2018. Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Per- sonalizing dialogue agents: I have a dog, do you have pets too? In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2204– 2213, Melbourne, Australia. Association for Com- putational Linguistics. Wei-Nan Zhang, Qingfu Zhu, Yifa Wang, Yanyan Zhao, and Ting Liu. 2019a. Neural personalized response generation as domain adaptation. World Wide Web, 22(4):1427–1446. Yizhe Zhang, Xiang Gao, Sungjin Lee, Chris Brockett, Michel Galley, Jianfeng Gao, and Bill Dolan. 2019b. Consistent dialogue generation with self-supervised feature learning. arXiv preprint arXiv:1903.05759. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Large- scale generative pre-training for conversational re- sponse generation. In", "meta": {"paper_id": "2012.13391v2", "chunk_index": 18, "filename": "2012.13391v2.pdf", "chunk_length": 512}}
{"id": "2012.13391v2_chunk_19", "text": "Michel Galley, Jianfeng Gao, and Bill Dolan. 2019b. Consistent dialogue generation with self-supervised feature learning. arXiv preprint arXiv:1903.05759. Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT : Large- scale generative pre-training for conversational re- sponse generation. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270– 278, Online. Association for Computational Linguis- tics. Li Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum. 2020. The design and implementation of xiaoice, an empathetic social chatbot. Computational Linguis- tics, 46(1):53–93. # of Veriﬁers Agreed Count Ratio (%) 0 484 7.67% 1 497 7.87% 2 1,211 19.18% 3 6,214 65.28% Table 7: Veriﬁcation Statistics. The ﬁrst column indi- cates the number of veriﬁers that agreed upon the given contradictions. A Veriﬁcation Statistics For a subset of the contradicting dialogues in DECODE we asked three veriﬁers to determine whether the original writer indeed created a con- tradiction example. Table 7 shows the veriﬁcation statistics. Note that we only use examples on which all three veriﬁers agreed for DECODE (dev) and DECODE (Test).", "meta": {"paper_id": "2012.13391v2", "chunk_index": 19, "filename": "2012.13391v2.pdf", "chunk_length": 188}}
{"id": "2012.13872v1_chunk_0", "text": "My Teacher Thinks The World Is Flat! Interpreting Automatic Essay Scoring Mechanism Swapnil Parekh⋆⋆1, Yaman Kumar Singla⋆⋆1,2,3, Changyou Chen3, Junyi Jessy Li4, and Rajiv Ratn Shah1 1 IIIT-Delhi swapnilbp100@gmail.com, {yamank,rajivratn}@iiitd.ac.in 2 Adobe ykumar@adobe.com 3 State University of New York at Buﬀalo changyou@buffalo.edu 4 University of Texas at Austin jessy@austin.utexas.edu Abstract. Signiﬁcant progress has been made in deep-learning based Automatic Essay Scoring (AES) systems in the past two decades. How- ever, little research has been put to understand and interpret the black- box nature of these deep-learning based scoring models. Recent work shows that automated scoring systems are prone to even common-sense adversarial samples. Their lack of natural language understanding capa- bility raises questions on the models being actively used by millions of candidates for life-changing decisions. With scoring being a highly multi- modal task, it becomes imperative for scoring models to be validated and tested on all these modalities. We utilize recent advances in interpretabil- ity to ﬁnd the extent to which features such as coherence, content and relevance are important for automated scoring mechanisms and why they are susceptible to adversarial samples. We ﬁnd that the systems tested consider essays not as a piece of prose having the characteristics of natu- ral ﬂow of speech and grammatical structure, but as ‘word-soups’ where a few words are much more important than the other words. Remov- ing the context surrounding those few important words causes the prose to lose the ﬂow of speech and grammar, however has little impact on the predicted score. We also ﬁnd that since the models are not seman- tically grounded with world-knowledge and common sense, adding false facts such as “the world is ﬂat” actually increases the score instead of decreasing it. Keywords: Automatic Essay Scoring · Interpretability in AI · Adver- sarial Deep Learning. ⋆⋆Equal Contribution arXiv:2012.13872v1 [cs.CL] 27 Dec 2020 2 S. Parekh et al 1 Introduction Automatic Essay Scoring (AES) systems help to alleviate workload of teach- ers and save time and costs associated with grading. On an average, a British teacher spends 5 hours in a calendar week scoring exams and assignments [12,10]. This ﬁgure is even higher for developing and low-resource countries where the teacher to student ratio is dismal. While on one hand, autograding systems eﬀectively reduce this burden, allowing more working hours for teaching ac- tivities, on the other, there have been many complaints against these systems for not scoring the way they are supposed to [6,23,7,13,18]. For instance, with the recently released Utah Automatic Scoring system, students scored lower by writing question-relevant keywords but higher by including unrelated words and sentences [6,23]. Similarly, it has been a common complaint that AES systems focus unjustiﬁably on obscure and diﬃcult vocabulary [17]. The concerns are further alleviated by the fact that the scores awarded by these systems are used in life-changing decisions ranging from college and job applications to visa ap- provals. Traditionally, autograding systems are built using manually crafted features used with machine learning based models. Lately, these systems have been shift- ing to deep learning based models. However, very few", "meta": {"paper_id": "2012.13872v1", "chunk_index": 0, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_1", "text": "scores awarded by these systems are used in life-changing decisions ranging from college and job applications to visa ap- provals. Traditionally, autograding systems are built using manually crafted features used with machine learning based models. Lately, these systems have been shift- ing to deep learning based models. However, very few research systems have tried to address the problems of robustness and interpretability, which plague deep learning based black-box models. Simply measuring test set performance may mean that the model is right for the wrong reasons. Hence, much research is required in order to understand the scoring algorithms used by AES models and to validate them on linguistic and testing criteria. Motivated by the previous studies on testing automatic scoring systems [29,20,22], which show that AES models are vulnerable to atypical inputs, our aim is to gain some intuitions behind how models score a human written sample. For instance, these studies show that automatic scoring systems score high on construct-irrelevant inputs like speeches and false facts [22], gibberish text [17], repeated paragraphs and canned responses [20], etc but do not show why do the models award high scores in these cases. While designing these tests, authors theorize the discrepancy observed in essay scores given by humans and mod- els and attribute it to various reasons [16,20,22]. Reasons such as presence of prompt-relevant keywords, length of essay, repetition, transitional and canned phrases, etc. are blamed responsible for the observed discrepancy. Therefore, in this paper, we make the following contributions. Firstly, using integrated gradients [24], we ﬁnd and visualize the most important words for scoring an essay, i.e., those words which are the most responsible for the score of that essay. Through this, we try to infer the scoring mechanism of black-box deep learning AES models. We ﬁnd that on an average by including just 31% of words from an essay, one can reproduce the original score given by the SkipFlow model (a model which measures neural coherence) [26] within a range of 1. The corresponding ﬁgure for Memory Network based scoring [30] (the SOTA model) is 51%. We also ﬁnd that the models pay attention to words but not the context in which they occur. Even when the context of the words is removed and only the Interpreting Automatic Essay Scoring Mechanism 3 top-attributed words are retained, the attribution of those words (and hence the models’ scores) are little impacted. Further, apart from the context, the order of occurrence of top-attributed words also does not have a signiﬁcant impact on the scores produced. We call this phenomenon as word-soup-scoring, where words like ingredients of a soup can occur in any order to produce the same net eﬀect. Similarly, just like a soup is made of stock (water) and solid pieces (vegetables, meat, etc), essays have a stock made of a mass of unimportant words and a few solid pieces of the most important words. In [22], the authors showed that essay scoring models are overstable, i.e., even after changing one of every ﬁve words of an essay, the scores do not change much. Thus,", "meta": {"paper_id": "2012.13872v1", "chunk_index": 1, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_2", "text": "have a stock made of a mass of unimportant words and a few solid pieces of the most important words. In [22], the authors showed that essay scoring models are overstable, i.e., even after changing one of every ﬁve words of an essay, the scores do not change much. Thus, secondly we extend their work by addressing why the models are overstable. Thirdly, with the insights we get from attributions, we are able to improve upon the perturbations provided by [22]. For instance, for memory- networks automatic scoring model [30], we delete 20% words from essays without signiﬁcantly changing score (<1%) whereas [22] observed that deleting similar number of words randomly resulted in a decrease of 20% scores. Fourth, in contrast to earlier linguistic studies which claim to ﬁnd that auto-scorers favor obscure vocabulary over common words, we ﬁnd that diﬃcult and low-frequency words like legerdemain, and propinquity are in-fact scored negatively. Fifth, we release all our code5, dataset and tools for public use with the hope that it will spur testing and validation of autoscoring models since they have a huge impact on the lives of millions of candidates (including the authors’) every year. 2 Background 2.1 Task and Dataset We use the widely cited ASAP-AES [1] dataset which comes from Kaggle Au- tomated Student Assessment Prize (ASAP) competition sponsored by Hewlett Packard Foundation for the evaluation of automatic essay scoring systems. The ASAP-AES dataset has been used for automatically scoring essay responses by many research studies [25,5,26,30]. It is one of the largest publicly available datasets. The relevant statistics for ASAP-AES are listed in Table 1. The questions covered by the dataset are from many diﬀerent areas such as Sciences and English. The responses were written by high school students and were subsequently double-scored. We test the following two models in this work: SkipFlow [26] and Memory Augmented Neural Network (MANN ) [30]. These models are trained with an objective of minimising the mean squared error of score diﬀerences between an expert rater and the model. The performance is measure using Quadratic Weighted Kappa (QWK) metric. QWK indicates the agreement between a model’s and the expert human rater’s scores. The individual models are brieﬂy explained below: 5 The code for all the experiments is given at https://github.com/midas-research/ interpreting-AES-Integrated-Gradients 4 S. Parekh et al SkipFlow: SkipFlow [26] treats the essay scoring task as a regression model, and utilizes Glove embeddings for representing the tokens. The authors mention that SkipFlow captures coherence, ﬂow and semantic relatedness over time, which they call as the neural coherence features. They also say that essays being long sequences are diﬃcult for a model to capture. For this reason, SkipFlow involves access to intermediate states. By doing this, it shows an impressive agreement kappa of 0.764. We take the SkipFlow’s embedding layer to compute the IGs for the regression output, which is then scaled to the original scale as a prediction. We replicated the model using the oﬃcial tensorﬂow implementation. Further details can be explored in the original paper. MANN: Memory Augmented Neural Network (MANN) [30] use", "meta": {"paper_id": "2012.13872v1", "chunk_index": 2, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_3", "text": "take the SkipFlow’s embedding layer to compute the IGs for the regression output, which is then scaled to the original scale as a prediction. We replicated the model using the oﬃcial tensorﬂow implementation. Further details can be explored in the original paper. MANN: Memory Augmented Neural Network (MANN) [30] use memory-networks for automatic scoring to select some responses for each grade. These responses are stored in the memory and then used for scoring ungraded responses. The memory component helps to characterize the various score levels similar to what a rubric does. They too show an agreement score of 0.78 QWK and beat the previous state-of-the-art models. We replicated the model using the oﬃcial ten- sorﬂow implementation. Further details can be explored in the original paper. Prompt Number 1 2 3 4 5 6 7 8 #Responses 1783 1800 1726 1772 1805 1800 1569 723 Score Range 2-12 1-6 0-3 0-3 0-4 0-4 0-30 0-60 #Avg words per response 350 350 150 150 150 150 250 650 #Avg sentences per response 23 20 6 4 7 8 12 35 Type Argumentative Argumentative RC RC RC RC Narrative Narrative Table 1. Overview of the ASAP AES Dataset used for evaluation of AS systems. (RC = Reading Comprehension). Table adapted from [22] 2.2 Attribution Mechanism The task of attributing the score, F(x) given by an AES model F, on an input essay x, can be formally deﬁned as producing attributions a1, .., an corresponding to the words w1, .., wn contained in the essay x. The attributions produced are such that Sum(a1, .., an) = F(x) (Proposition 1 in [24]) , i.e. net attributions of all words (Sum(a1, .., an)) equal the assigned score (F(x)). In a way, if F is a regression based model, a1, .., an can be thought of as the scores of each word of that essay, which sum to produce the ﬁnal score, F(x). We use a path-based attribution method, Integrated Gradients (IGs) [24] for getting these attributions for each of the trained models, F. Formally, IGs employ the following method to ﬁnd blame assignments: Deﬁnition 1 (Integrated Gradients). Given an input x and a baseline b (deﬁned as an input containing absence of cause for the output of a model; also Interpreting Automatic Essay Scoring Mechanism 5 called neutral input [21,24]), the integrated gradient along the ith dimension is deﬁned as follows. IntegratedGradsi(x, b) = (xi −bi) × Z 1 α=0 ∂F(b + α × (x −b)) ∂xi dα (where ∂F (x) ∂xi represents the gradient of F along the ith dimension of input x). We choose the baseline as empty input (all 0s) for essay scoring models since an empty essay should get a score of 0 as per the scoring rubrics. It is the neutral input which models the absence of cause of any score, thus getting a zero score. Since we want to see the eﬀect of only words on score, any additional inputs (such as memory in MANN [30]) of the baseline b is set to be that of x. We choose IGs over other", "meta": {"paper_id": "2012.13872v1", "chunk_index": 3, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_4", "text": "models the absence of cause of any score, thus getting a zero score. Since we want to see the eﬀect of only words on score, any additional inputs (such as memory in MANN [30]) of the baseline b is set to be that of x. We choose IGs over other explainability techniques since they have many desirable properties which make them useful for this task. For instance, the attributions sum to the score of an essay (Sum(a1, .., an) = F(x)), they are implementation invariant, do not require any model to be retrained and are readily implementable. Previous literature such as [14] also use Integrated Gradients for explaining the undersen- sitivity of factoid based question-answer (QA) models. Other mechanisms like attention attribute only to a speciﬁc input-output path (though multiple can ex- ist) [24], thus is not a good choice 6. For instance, for an LSTM based attention model, there are more than one path for the input to inﬂuence the output, like recurrent state, memory cell, etc. [24]. Hence, attention applied over one input- output path captures the attributions from that path only whereas we want to capture the attributions irrespective of which path it arises from. 3 Experiments and Results 3.1 Attribution on original samples: We take the original human-written essays from the ASAP-AES dataset [1] and do a word-level attribution of scores. Figure 1 shows the attributions of both the models for an essay sample from Prompt 2. We observe that SkipFlow does not attribute any word after the ﬁrst few lines (ﬁrst 30% essay-content) of the essay. Words present in the last lines do not get any or very low attribution values. It is also observed that if a word is negatively attributed at a certain po- sition in an essay sample, it is then commonly negatively attributed in its other occurrences as well. For instance, books, magazines were negatively attributed in all its occurrences while materials, censored were positively attributed and library was not attributed at all. We could not ﬁnd any patterns in the direction of attribution. Table 2 lists the top-positive, top-negative attributed words and the mostly unattributed words for both the models. For MANN, we observe that 6 We ensure that IGs are within the acceptable error margin of <5%, where error is calculated by the property that the attributions’ sum should be equal to the diﬀerence between the probabilities of the input and the baseline. IG parameters: Number of Repetitions = 20-50, Internal Batch Size = 20-50 6 S. Parekh et al attributions are spread over the complete length of the essay. We also notice that the attributions are stronger for construct-irrelevant words like, is, was, a, as, about, I, my, she, etc. and lesser for construct-relevant words like, appointment, listening, reading, exercise, coach, etc. Here, the same word changes its attribu- tion sign when present in diﬀerent length of essays but in an essay, it shows the same sign overall. Through this experiment, we ﬁnd that SkipFlow scores based only the ﬁrst 3-4 lines of an essay while eﬀectively ignoring what is", "meta": {"paper_id": "2012.13872v1", "chunk_index": 4, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_5", "text": "coach, etc. Here, the same word changes its attribu- tion sign when present in diﬀerent length of essays but in an essay, it shows the same sign overall. Through this experiment, we ﬁnd that SkipFlow scores based only the ﬁrst 3-4 lines of an essay while eﬀectively ignoring what is said later. Although MANN is better in this terms and takes into account the full essay, but the top attributed words show that this focus is misled and stopwords are much more important than construct-relevant words. Fig. 1. Attributions for SkipFlow and MANN respectively of an essay sample for Prompt 2. Prompt 2 asks candidates to write an essay to a newspaper reﬂecting their vies on censorship in libraries and express their views if they believe that materials, such as books, etc., should be removed from the shelves if they are found oﬀensive. This candidate scores a 3 out of 6 in this essay. 3.2 Iteratively Deleting Unimportant Words: For this test, we take the original samples and iteratively delete the least at- tributed words. Through this, we note the dependence of each word on an essay’s score. Figure 2 presents the results for iterative removal of least attributed words for SkipFlow and MANN. As can be seen from the ﬁgure, the relative QWK stays within 90% of the original even if one of every four words was removed from an essay in reverse order of their attribution values. This happens for both MANN and SkipFlow. While Figure 1 showed that MANN paid attention to the full length of the response, yet removing the bottom attributed words does not seem to aﬀect the scores much. Table 3 notes the statistics for this test. We have used the same metrics as used in [22]. It is to be noted that the words removed are not Interpreting Automatic Essay Scoring Mechanism 7 Model Positively Attributed Words MANN to, of, are, ,, you, do, ’, children SKIPFLOW of, oﬀensive, movies, censorship, is, our, material Model Negatively Attributed Words MANN i, shelf, the, shelves, libraries, music, a SKIPFLOW i, the, to, in, that, do, a, or, be Model Mostly Unattributed Words MANN i, you, the, think, oﬀensive, from, my SKIPFLOW it, be, but, their, from, dont, one, what Table 2. Top positive, negative and un-attributed words for SkipFlow and MANN model for Prompt 2. contiguous but interspersed across sentences, therefore deleting the unattributed words does not produce grammatically correct response (see Fig. 3), yet is able to get a similar score thus defeating the whole purpose of testing and feedback. From the table, we also ﬁnd that deleting 20% of the total words in the reverse order of attribution, results in a minor decrease of approximately 1 point for only 25% samples. From further experimentation, we found that for SkipFlow, approximately 31% words on an average are required for getting the original score within a range of 1. For memory networks, we get the corresponding ﬁgure as 51.5%. Both the ﬁgure and results show that there is a point after which the score ﬂattens out, i.e.,", "meta": {"paper_id": "2012.13872v1", "chunk_index": 5, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_6", "text": "found that for SkipFlow, approximately 31% words on an average are required for getting the original score within a range of 1. For memory networks, we get the corresponding ﬁgure as 51.5%. Both the ﬁgure and results show that there is a point after which the score ﬂattens out, i.e., it does not change in that region either by adding or removing words. This is odd since adding or removing a word from a sentence alters its meaning and grammaticality entirely, yet the models do not seem to get aﬀected by either. They decide their scores only on the basis of 30-50% words. % µpos µneg Npos Nneg σ SkipFlow 0 0 0 0 0 0 20 0.02 0.97 0.31 18 2 60 0.06 7.4 1.2 63 12 80 0.07 22 1.5 83 28 MANN 0 0 0 0 0 0 20 0.01 1 0.32 32 1.87 60 0 8 0 94.55 8 80 0 15 0 94.55 16 Table 3. Statistics for iterative removal of least attributed words on Prompt 7. Legend [22]: {%: % words removed from a response, µpos: Mean diﬀerence of posi- tively impacted samples (as % of score range), µneg: Mean diﬀerence of negatively impacted samples (as % of score range), Npos: Percentage of positively impacted sam- ples, Nneg: Percentage of negatively impacted samples, σ: Standard deviation of the diﬀerence (as % of score range)} 8 S. Parekh et al 3.3 Iteratively Adding High Attribution Words For this test, we take the original samples and iteratively keep on adding the top attributed words to an empty essay (with context words appearing only if it occurs in the top attributed words). Through this, we note the dependence of each of the top words on an essay’s score. Although a reader might think that the experiment of iteratively deleting unimportant words (Sec 3.2) is a conjugate of this test but we would like to note that it is not the case. With addition or removal of each word in an essay, the score and the word-attributions of that essay change. Therefore, adding high attribution words is not a conjugate of removing low attribution words from an essay sample. However, the results show that all the models tested, consider only a subset of words as important while scoring. Fig. 2. Variation of QWK with iterative removal and addition of response words. The ﬁrst set presents the results for iterative removal of least attributed words for SkipFlow and MANN respectively. The second set presents the iterative addition of the most attributed words. The y-axis notes the relative QWK with respect to the original QWK of the models and the x-axis represents iterative removal (and addition) of response words sorted according to their attributions in decreasing (and increasing) order. These results are obtained on Prompt 7, similar results were obtained for all the prompts tested. We see (Figure 2) that with only 20-40% of the most important words, both the models are able to achieve 85% of their original Kappa scores. This is sur- prising since an essay consisting of 40% (most-attributed)", "meta": {"paper_id": "2012.13872v1", "chunk_index": 6, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_7", "text": "obtained on Prompt 7, similar results were obtained for all the prompts tested. We see (Figure 2) that with only 20-40% of the most important words, both the models are able to achieve 85% of their original Kappa scores. This is sur- prising since an essay consisting of 40% (most-attributed) words creates a ‘word- soup’. This word-soup is incoherent, grammatically, lexically and semantically incorrect. An example of such a ‘word-soup’ using only 40% words is given in Interpreting Automatic Essay Scoring Mechanism 9 the Figure 3. This property of word-soup violates the famous Firthian view of linguistics where a word is known by the company it keeps and hence a word without a company is meaningless. Table 4 shows the results for this test. From the table, we observe that by adding 80% of all words in the order of attribution, instead of decreasing scores, increases it by 8% and 1.1% for 80% and 30% sam- ples for SkipFlow and MANN respectively. We also found that by adding 45% words to an empty essay for SkipFlow and 52% words for MANN, gives back the original score of that essay within a range of 1. Fig. 3. Word-Soups containing 40% of (top-attributed) words for SkipFlow and MANN model. The word-soup scores 21 and 18 out of 30. The original essay was scored 20, 18 by SkipFlow and MANN respectively. % µpos µneg Npos Nneg σ SkipFlow 80 8 0.59 80.1 9.2 10.9 60 6 1.68 61 23 10.2 40 5.8 1.79 59 24.2 10.3 20 5.74 1.81 58.4 24.6 10.24 0 61 0 0 100 62 MANN 80 1.1 0.09 31 2.88 2 60 0.37 1.4 9.2 39.1 2.6 40 0.07 5.8 2.24 88.4 6.5 20 0.02 13.7 0.6 94.55 14.5 0 0 20 0 94.5 22.39 Table 4. Statistics for iterative addition of the most-attributed words on Prompt 7. Legend [22]: {%: % words added to form a response, µpos: Mean diﬀerence of pos- itively impacted samples (as % of score range), µneg: Mean diﬀerence of negatively impacted samples (as % of score range), Npos: Percentage of positively impacted sam- ples, Nneg: Percentage of negatively impacted samples, σ: Standard deviation of the diﬀerence (as % of score range)} 10 S. Parekh et al 3.4 Sentence and Word Shuﬄe Coherence and organization are important features for scoring measure which measure unity of diﬀerent ideas in an essay and determine its cohesiveness in the narrative [2,28,10]. To check the dependence of AES models on coherence, [22] shuﬄed the order of sentences randomly and note the change in score between the original and modiﬁed essay. We take 100 essays from each prompt of the ShuffleSent test case of [22] for this. Figure 4 presents the attributions for an essay sample. Curiously, we ﬁnd that the word-attributions do not change much (<0.002 %) with sentence shuﬄe. The attributions are mostly dependent on word identities rather than their position and context for both the models. In addition, we ﬁnd that shuﬄing results in 10% and 2% score diﬀerence for SkipFlow and MANN respectively. This is surprising since", "meta": {"paper_id": "2012.13872v1", "chunk_index": 7, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_8", "text": "word-attributions do not change much (<0.002 %) with sentence shuﬄe. The attributions are mostly dependent on word identities rather than their position and context for both the models. In addition, we ﬁnd that shuﬄing results in 10% and 2% score diﬀerence for SkipFlow and MANN respectively. This is surprising since change in the order of ideas in a paragraph completely destroys the meaning of a prose but the models are not able to detect the change in position of occurrence of ideas. SkipFlow is more sensitive than MANN since as shown by Fig. 1, SkipFlow primarily pays attention to the ﬁrst 30% of essay content and if there is a change in those 30% words, the scores change. Fig. 4. Attributions for SkipFlow and MANN respectively of an essay sample where all the sentences have been randomly shuﬄed. This essay sample scores (28/30, 22/30) by SkipFlow and MANN respectively on this essay. The original essay (without the added lie) also scored (28/30) and (22/30) respectively. We also tried out shuﬄing all the words of an essay. The results obtained are similar to sentence shuﬄing. The diﬀerence in score obtained was close to 7% for SkipFlow and close to 1% for MANN. The diﬀerence in attributions given to Interpreting Automatic Essay Scoring Mechanism 11 each word was -0.45% for SkipFlow and unnoticeable for MANN. The average number of words which changed their attribution was 2 for SkipFlow and 0 for MANN. These results further show that autoscorers majorly take word as units for scoring. SkipFlow additionally shows a minor preference for words occurring in the ﬁrst 30% positions. 3.5 Modiﬁcation in Lexicon Several previous research studies have highlighted the importance vocabulary plays in scoring and how AES models may be only scoring vocabulary [17,16,8,20,22]. To verify their claims, we take an approach similar to [22] and replace the top and bottom 10% attributed words with similar words7. Table 5 shows the results for this test. It can be noted that after replacing all the top and bottom 10% attributed words with their corresponding similar words results in less than 5% diﬀerence in scores for both the models. Additionally, this type of perturbation results in changing approximately 4% (20% of top and bottom 20% attributed words) top and bottom attributed words of each essay. These results imply that networks are surprisingly not perturbed by modifying even the most attributed words and produce equivalent results with other similarly-placed words. In ad- dition, change of a word by a similar word although changes the meaning and form of a sentence, yet the models do not recognize that change by showing no change in their scores 8. This implies that the replacement of solid parts within a soup with equivalent ingredients does not alter the nature of word-soup. Result SkipFlow MANN Average diﬀerence in score 4.8% 2.4% % of top-20% attributed words which change attribution 18.3% 9.5% % of bottom-20% attributed words which change attribution 27.6% 26.0% Table 5. Statistics obtained after replacing the top and bottom 10% attributed words of each essay with their synonyms. 3.6", "meta": {"paper_id": "2012.13872v1", "chunk_index": 8, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_9", "text": "word-soup. Result SkipFlow MANN Average diﬀerence in score 4.8% 2.4% % of top-20% attributed words which change attribution 18.3% 9.5% % of bottom-20% attributed words which change attribution 27.6% 26.0% Table 5. Statistics obtained after replacing the top and bottom 10% attributed words of each essay with their synonyms. 3.6 Knowledge of Factuality Factuality is an important feature in scoring essays [28]. While a human expert can readily catch a lie, it is diﬃcult for a machine to do so. We randomly sample 100 sample essays of each prompt from the AddLies test case of [22]. For con- structing these samples, [22] used various online databases and appended the 7 sampled from Glove with distance calculated using Euclidean distance metric [15] 8 For example, consider the replacement of word ‘agility’ with its synonym ‘cleverness’ in the sentence ‘This exercise requires agility.’ does not produce sentence with the same meaning. 12 S. Parekh et al false information at various positions in the essay. These statements not only introduce false facts in the essay but also perturb its coherence. Most of the NLP models use some form of language embeddings trained on large corpora such as Wikipedia. A teacher who is responsible for teaching, scoring and feedback of a student must have knowledge of common facts such as, ‘Sun rises in the East’, ‘Apple is a fruit’ and ‘The world is not ﬂat’. However, Figure 5 shows that scoring models which are responsible for making important career-decisions like college, job and visa eligibility of candidates do not have the capability to check even these commonly-known facts. The models tested attribute positive scores to lies like the world is ﬂat. We also found that if a lie is added at the beginning, for both the models, approximately 70% of words from the added lie appear in the top 20% of the attributed words of that essay. When the lie is added at the end, the numbers drop to 30%. Fig. 5. Attributions for SkipFlow and MANN respectively of an essay sample where a false fact has been introduced at the beginning. This essay sample scores (25/30, 18/30) by SkipFlow and MANN respectively on this essay. The original essay (without the added lie) scored (24/30) and (18/30) respectively. 3.7 Babel Generated Samples For this case, we use B.S. Essay Language Generator (BABEL generator) [17,18] to generate atypical English samples. These samples are essentially semantic garbage with perfect spellings and diﬃcult and obscure vocabulary. We take 15 samples from the tool using keywords sampled from the essays. Figure 6 shows the SkipFlow and MANN attributions for a BABEL generated essay. We ob- served a diﬀerent pattern of scoring in SkipFlow and MANN. In a stark contrast with [17] and the commonly held notion that writing obscure and diﬃcult to use words fetch more marks, MANN attributed non-frequently used words such as forbearance, legerdemain, and propinquity negatively while common words such as establishment, celebration, and demonstration were positively scored. We also note that neither of the two models have any knowledge of grammar. While on one hand, common", "meta": {"paper_id": "2012.13872v1", "chunk_index": 9, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_10", "text": "diﬃcult to use words fetch more marks, MANN attributed non-frequently used words such as forbearance, legerdemain, and propinquity negatively while common words such as establishment, celebration, and demonstration were positively scored. We also note that neither of the two models have any knowledge of grammar. While on one hand, common used and grammatically correct phrases such as ‘as Interpreting Automatic Essay Scoring Mechanism 13 well as’, ‘all of the’ have unequal attributions on words, grammatically incorrect phrases such as, ‘as well a will be a on’ and ‘of in my of’ have words which have both positive and negative attributions. Similarly, models are not checking for logic. Phrases like, ‘house is both innumerable and consummate’ get an overall positive score. Historically incorrect phrases like, ‘according to the professor of philosophy Oscar Wilde’, get a positive attribution. Fig. 6. Attributions for SkipFlow and MANN respectively of a BABEL essay sample for Prompt 7. Prompt 7 asks candidates to write a story on patience. This essay sample gets scored (22/30, 18/30) by SkipFlow and MANN respectively. 4 Related Work Although automatic-scoring has seen much in the recent years [10,26,30,5,25,11], the validation and testing of the models developed has not seen much work in the machine learning ﬁeld. This section brieﬂy covers the research work from the testing and linguistics area. Powers et al [20] in 2002, asked 27 specialists and non-specialists to write essays that could produce signiﬁcant deviation with respect to scores from ETS’s e-rater. The winner entry (the one which pro- duced maximum deviation) repeated the same paragraph 37 times. The study concluded that repetition, prompt-related keywords make the scores given by AES unreliable. Perelman and colleagues [17] made a software that takes in ﬁve keywords and produces a semantic garbage written in a diﬃcult and obscure language. They tested it out with the ETS’s system and produced high scores, thus concluding that essay writing system learn to recognize obscure language with diﬃcult and non-meaningful words and phrases like, ‘fundamental drone of humanity’, ‘auguring commencements, torpor of library’ and ‘personal dis- enfranchisement for the exposition we accumulate conjectures’ 9. In [16], the 9 Generated by giving the keywords, ‘Library’, ‘Delhi’ and ‘College’ respectively. 14 S. Parekh et al author analyzes some college board exams and some other tests he gave in his class to conclude that length is the major predictor of essay score and that essay scoring systems must be “counting words to claim state-of-the-art”. Similarly, in [8], the authors design a website which generates essays for getting them graded by Pearson’s Intelligent Essay Assessor (IEA). Some of the excerpts from the essay are, “An essay on aphasia: Man’s greatest achievement” and it gets top scores from the program. The author concludes that perhaps the scoring engine is learning to score on the basis of sentence length and variety, the presence of semantic chains and diction. ETS researchers in [4] design ‘shell’ language and test out its eﬀect on their scoring system. The research study deﬁnes shell language as the generalized language often used to provide organizational framework in academic argumen-", "meta": {"paper_id": "2012.13872v1", "chunk_index": 10, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_11", "text": "on the basis of sentence length and variety, the presence of semantic chains and diction. ETS researchers in [4] design ‘shell’ language and test out its eﬀect on their scoring system. The research study deﬁnes shell language as the generalized language often used to provide organizational framework in academic argumen- tation. The language does not contain any construct-relevant material. After some experimentation, they conclude that the GRE scorer system is adequately trained to handle shell. In [9], the authors use three strategies to score responses: by length, by inclusion of question words, and by the usage of generic aca- demic English. Similarly, in [3], the authors tried out lexical substitutions as a construct-irrelevant response strategy and found that the E-rater is not suf- ﬁciently perturbed by these modiﬁcations. Finally, in [22], the authors tried four strategies to adversarially modify the input to essay scoring models - Add, Delete, Modify and Generate. They used other textual inputs like song and speech databases, true and false sources of information and jumbling up of sentences to test out the various features that an AES system should score [28]. They do not present what part of input gets scored while grading an essay. All of those analyses rely on showing a positive correlation between their perturbation technique and increase or decrease in scores. In addition, the researchers in [19] also note that opinions on this subject among the various research studies vary considerably and are often contradictory to one another. 5 Conclusion and Future Work Automatic Scoring is one of the ﬁrst tasks that were tried to be automated using Artiﬁcial Intelligence. The eﬀorts began in 1960s [27] with systems trying to grade using a few features extracted from essays. In the last two decades, eﬀorts have been shifting to building automated neural-network based systems which bypass the feature-engineering step and score directly using a black-box model. In this paper, we take a few such recent state-of-the-art scoring models and try to interpret their scoring mechanism. We test the models on various features considered important for scoring such as coherence, factuality, content, relevance, suﬃciency, logic, etc. We ﬁnd that the models don’t see an essay as a unitary piece of coherent text but as a word-soup with a few words making up the main ingredients of the soup and the rest just forming the sap. The removal or addition of the sap does not carry much importance and the order of occurrence of main ingredients has little impact on the ﬁnal product (score of Interpreting Automatic Essay Scoring Mechanism 15 essay). We call this word-soup-scoring. We also perform tests modifying both the main words and the remaining mass to note the change in scoring mechanism. Here we analysed the scoring algorithm using word-level attribution. Al- though we tried to infer phrase and paragraph-level attribution by looking at phrases and sum of attributions, but this approach is diﬀerent from a sentential or paragraphic analysis. Future studies should look into these linguistic con- structs as well. On a diﬀerent note, extensive work needs to be done on each", "meta": {"paper_id": "2012.13872v1", "chunk_index": 11, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_12", "text": "we tried to infer phrase and paragraph-level attribution by looking at phrases and sum of attributions, but this approach is diﬀerent from a sentential or paragraphic analysis. Future studies should look into these linguistic con- structs as well. On a diﬀerent note, extensive work needs to be done on each feature important for scoring a written sample. With millions of candidates each year relying on automatically scored tests for life-changing decisions like col- lege, job opportunities and, visa, it becomes imperative for language and testing community to validate their models and show performance metrics beyond just accuracy and kappa numbers. References 1. ASAP-AES: The hewlett foundation: Automated essay scoring develop an auto- mated scoring algorithm for student-written essays. https://www.kaggle.com/c/ asap-aes/ (2012) 2. Barzilay, R., Lapata, M.: Modeling local coherence: An entity-based approach. Computational Linguistics 34(1), 1–34 (2008) 3. Bejar, I.I., Flor, M., Futagi, Y., Ramineni, C.: On the vulnerability of automated scoring to construct-irrelevant response strategies (cirs): An illustration. Assessing Writing 22, 48–59 (2014) 4. Bejar, I.I., VanWinkle, W., Madnani, N., Lewis, W., Steier, M.: Length of textual response as a construct-irrelevant response strategy: The case of shell language. ETS Research Report Series 2013(1), i–39 (2013) 5. EASE, E.: Ease (enhanced ai scoring engine) is a library that allows for machine learning based classiﬁcation of textual content. this is useful for tasks such as scoring student essays. https://github.com/edx/ease (2013) 6. Feathers, T.: Flawed algorithms are grading millions of stu- dents’ essays. https://www.vice.com/en/article/pa7dj9/ flawed-algorithms-are-grading-millions-of-students-essays (2019) 7. Greene, P.: Automated essay scoring remains an empty dream. https://www.forbes.com/sites/petergreene/2018/07/02/ automated-essay-scoring-remains-an-empty-dream/?sh=da976a574b91 (2018) 8. Hesse, D.D.: 2005 cccc chair’s address: Who owns writing? College Composition and Communication 57(2), 335–57 (2005) 9. Higgins, D., Heilman, M.: Managing what we can measure: Quantifying the sus- ceptibility of automated scoring systems to gaming behavior. Educational Mea- surement: Issues and Practice 33(3), 36–46 (2014) 10. Kumar, Y., Aggarwal, S., Mahata, D., Shah, R.R., Kumaraguru, P., Zimmermann, R.: Get it scored using autosas—an automated system for scoring short answers. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence. vol. 33, pp. 9662–9669 (2019) 11. Liu, J., Xu, Y., Zhu, Y.: Automated essay scoring based on two-stage learning. arXiv preprint arXiv:1901.07744 (2019) 12. Micklewright, J., Jerrim, J., Vignoles, A., Jenkins, A., Allen, R., Ilie, S., Bellarbre, E., Barrera, F., Hein, C.: Teachers in england’s secondary schools: Evidence from talis 2013 (2014) 16 S. Parekh et al 13. Mid-Day: What?! students write song lyrics and abuses in exam answer sheet. https://www.mid-day.com/articles/ national-news-west-bengal-students-write-film-song-lyrics-abuses-in-exam-answer-sheet/ 18210196 (2017) 14. Mudrakarta, P.K., Taly, A., Sundararajan, M., Dhamdhere, K.: Did the model understand the question? arXiv preprint arXiv:1805.05492 (2018) 15. Pennington, J., Socher, R., Manning, C.: GloVe: Global vectors for word repre- sentation. In: Proceedings of the 2014 Conference on Empirical Methods in Nat- ural Language Processing (EMNLP). pp. 1532–1543. Association for Computa- tional Linguistics, Doha, Qatar (Oct 2014). https://doi.org/10.3115/v1/D14-1162, https://www.aclweb.org/anthology/D14-1162 16. Perelman, L.: When “the state of the art” is counting words. Assessing Writing 21, 104–111 (2014) 17. Perelman, L., Sobel, L., Beckman, M., Jiang, D.: Basic automatic b.s. essay lan- guage generator (babel). https://babel-generator.herokuapp.com/ (2014) 18. Perelman, L., Sobel,", "meta": {"paper_id": "2012.13872v1", "chunk_index": 12, "filename": "2012.13872v1.pdf", "chunk_length": 512}}
{"id": "2012.13872v1_chunk_13", "text": "for Computa- tional Linguistics, Doha, Qatar (Oct 2014). https://doi.org/10.3115/v1/D14-1162, https://www.aclweb.org/anthology/D14-1162 16. Perelman, L.: When “the state of the art” is counting words. Assessing Writing 21, 104–111 (2014) 17. Perelman, L., Sobel, L., Beckman, M., Jiang, D.: Basic automatic b.s. essay lan- guage generator (babel). https://babel-generator.herokuapp.com/ (2014) 18. Perelman, L., Sobel, L., Beckman, M., Jiang, D.: Basic automatic b.s. essay language generator (babel) by les perelman, ph.d. http://lesperelman.com/ writing-assessment-robo-grading/babel-generator/ (2014) 19. Powers, D.E., Burstein, J.C., Chodorow, M., Fowles, M.E., Kukich, K.: Stumping e-rater: Challenging the validity of automated essay scoring. ETS Research Report Series 2001(1), i–44 (2001) 20. Powers, D.E., Burstein, J.C., Chodorow, M., Fowles, M.E., Kukich, K.: Stumping e-rater: challenging the validity of automated essay scoring. Computers in Human Behavior 18(2), 103–134 (2002) 21. Shrikumar, A., Greenside, P., Shcherbina, A., Kundaje, A.: Not just a black box: Learning important features through propagating activation diﬀerences. arXiv preprint arXiv:1605.01713 (2016) 22. Singla, Y.K., Bhatia, M., Kabra, A., Li, J.J., Jin, D., Shah, R.R.: Calling out bluﬀ: Attacking the robustness of automatic scoring systems with simple adversarial testing. arXiv preprint arXiv:2007.06796 (2020) 23. Smith, T.: More states opting to ’robo-grade’ student es- says by computer. https://www.npr.org/2018/06/30/624373367/ more-states-opting-to-robo-grade-student-essays-by-computer (2018) 24. Sundararajan, M., Taly, A., Yan, Q.: Axiomatic attribution for deep networks. arXiv preprint arXiv:1703.01365 (2017) 25. Taghipour, K., Ng, H.T.: A neural approach to automated essay scoring. In: Pro- ceedings of the 2016 conference on empirical methods in natural language process- ing. pp. 1882–1891 (2016) 26. Tay, Y., Phan, M.C., Tuan, L.A., Hui, S.C.: Skipﬂow: Incorporating neural co- herence features for end-to-end automatic text scoring. In: Thirty-Second AAAI Conference on Artiﬁcial Intelligence (2018) 27. Whitlock, J.W.: Automatic data processing in education. Macmillan (1964) 28. Yan, D., Rupp, A.A., Foltz, P.W.: Handbook of automated scoring: Theory into practice. CRC Press (2020) 29. Yoon, S.Y., Cahill, A., Loukina, A., Zechner, K., Riordan, B., Madnani, N.: Atyp- ical inputs in educational applications. In: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers). pp. 60–67 (2018) 30. Zhao, S., Zhang, Y., Xiong, X., Botelho, A., Heﬀernan, N.: A memory-augmented neural model for automated grading. In: Proceedings of the Fourth (2017) ACM Conference on Learning@ Scale. pp. 189–192 (2017)", "meta": {"paper_id": "2012.13872v1", "chunk_index": 13, "filename": "2012.13872v1.pdf", "chunk_length": 374}}
{"id": "2012.13980v1_chunk_0", "text": "1 Measuring University Impact: Wikipedia approach Tatiana Kozitsina (Babkina)1,2*, Viacheslav Goiko3, Roman Palkin3, Valentin Khomutenko3, Yulia Mundrievskaya3, Maria Sukhareva4, Isak Froumin5, and Mikhail Myagkov3,5,6 1Moscow Institute of Physics and Technology (State University), Department of Control and Applied Mathematics, 9 Institutskiy per., Dolgoprudny, Moscow Region, 141701, Russian Federation. 2Federal Research Center Computer Science and Control, Russian Academy of Sciences, Moscow, 119333, Russian Federation 3Tomsk State University, Laboratory of Big Data in Social Sciences, 36 Lenin Ave., Tomsk, 634050, Russian Federation. 4Moscow State University, Public administration faculty, 27(4) Lomonosov Pr., Moscow, 119991, Russian Federation. 5Institute of Education, National Research University Higher School of Economics, 16 Potpovskiy Pereulok, building 10, Moscow, 101000, Russian Federation. 6Department of Political Science, University of Oregon, Oregon, 97403, United States. * Corresponding author (e-mail: babkinats@yandex.ru) Abstract The impact of Universities on the social, economic and political landscape is one of the key directions in contemporary educational evaluation. In this paper, we discuss the new methodological technique that evaluates the impact of university based on popularity (number of page-views) of their alumni’s pages on Wikipedia. It allows revealing the alumni popularity dynamics and tracking its state. Preliminary analysis shows that the number of page-views is higher for the contemporary persons that prove the perspectives of this approach. Then, universities were ranked based on the methodology and compared to the famous international university rankings ARWU and QS based only on alumni scales: for the top 10 universities, there 2 is an intersection of two universities (Columbia University, Stanford University). The correlation coefficients between different university rankings are provided in the paper. Finally, the ranking based on the alumni popularity was compared with the ranking of universities based on the popularity of their webpages on Wikipedia: there is a strong connection between these indicators. Keywords: University impact, Wikipedia, Universities rankings, Alumni, Data mining. Introduction As a compulsory component of the progress engine, university is an important organization, which affects many strata of a society. University is not only about education, but also about social connections, career mobility, scientific discoveries and achievements, student movements, and alumni unions. So, firstly, university is defined through the people: students, teachers, scientists, graduates, etc. Secondly, it can be noticed, that a university is a forge of thoughts; however, humanity has not yet managed to comprehend the measurement of thinking. And while there is no doubt about university’s significance, there appears a question of how to evaluate university’s influence, which seems to be a big and ambitious goal (Agasisti 2017; Khazragui and Hudson 2014; Witte and López-Torres 2017). By this moment, the directions of university’s activities can be divided into three components: education, science, and communication with the society – university's “third mission” (Laredo 2007; Lukman et al. 2010; Montesinos et al. 2008; Scott 2006). The influence of a university may be measured through the first two components by assessing the quality of admission, students’ grades, quantity of articles and citations, number of awards won, grants received (Brooks 2005). But the role of the university’s contribution to society is a crucial issue which is common debates among researchers (Fumasoli", "meta": {"paper_id": "2012.13980v1", "chunk_index": 0, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_1", "text": "university may be measured through the first two components by assessing the quality of admission, students’ grades, quantity of articles and citations, number of awards won, grants received (Brooks 2005). But the role of the university’s contribution to society is a crucial issue which is common debates among researchers (Fumasoli 2016; Kosmützky and Ewen 2016). University’s impact on society can be assessed through the success of graduates, joint projects of the university with external companies, transfer of knowledge to society, usage of university’s research by society, and interaction between university and the region (Olcay and Bulu 2017; 3 Sánchez-Barrioluengo 2014; Secundo et al. 2017; Uyarra 2010). Such measurements are not entirely obvious and require the development of new methods. In relation to this, the growing rates of life digitalization may become the key to discover university’s impact on society. The impact of a modern university largely depends on its presence on the online information sites. Digital data from Internet resources can be used to form new metrics for evaluating university activities. In recent years, altimetric has emerged in the scientific community - non-traditional ways of evaluating scientific activities based on digital traces - mentions in social media, count of downloads, views, etc. (Altmetric 2019; Chavda and Patel 2016; Nariani 2017). By analogy, this way we can approach the task of evaluating universities. For example, the influence of a university directly depends on the influence and success of its graduates. However, data on the success of the graduate can be obtained from Wikipedia, social networks (Kempe et al. 2005; Kozitsin 2020; Kozitsin et al. 2020) and other information online sites. In this paper, we propose to use the data on graduates from a multilingual, web-based, free encyclopedia Wikipedia (Wikipedia 2019) to analyze their impact on society. We suggest using the number of page-views of the graduates' webpages on Wikipedia as a metrics for universities comparison. Alumnus is the results of the universities’ production activities. Their career success represents the quality of the educational programs and the university’s system (Vermeulen and Schmidt 2008). Graduates involuntarily become the face of the university and in some cases a potential source of income (Scillitoe 2013). Commonly, only famous graduates get into consideration for the research purposes: people winning Nobel Prizes and Fields Medals (Shanghai Ranking 2017), or graduates who stay in science, or higher education (Top Universities 2019). At the same time, the significance of the university is standing through the influence of the graduates. For that purpose, we should learn how to locate all the important for society graduates and how to estimate their influence. Traditional studies of the alumni use the survey method, for example, questionnaires sent to the graduates and employers (Top Universities 2019). The main focus of such studies is the 4 work satisfaction (Laschinger 2012), meeting expectations (Nicolescu & Paun 2009), the level of salary (Lee and Wilbur 1985), employer’s estimation (Cole and Thompson 2002), competencies and skills obtained from higher education (Hernández-March et al. 2009). Cabrera et al. (2005) show that from the 1930s to 1970s researchers studied the satisfaction from", "meta": {"paper_id": "2012.13980v1", "chunk_index": 1, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_2", "text": "satisfaction (Laschinger 2012), meeting expectations (Nicolescu & Paun 2009), the level of salary (Lee and Wilbur 1985), employer’s estimation (Cole and Thompson 2002), competencies and skills obtained from higher education (Hernández-March et al. 2009). Cabrera et al. (2005) show that from the 1930s to 1970s researchers studied the satisfaction from work, a connection between education, and the employment field. After the 1980s the focus of studies shifted to the competencies received at the university, and also alumni engagement in the university life. It was found (Ng et al. 2005; Seibert et al. 1999) which psychological characteristics are connected with career success. Additionally, career success relates positively to the internships during the education processes (Gault et al. 2000). Also, the research focused on the period of time which graduate spent to find the job (Quarmby et al. 1999). Despite the fact that the survey method stays actual and useful, scientists started to pay attention to the data from the Web to study alumni (Ranking Web of Universities 2019). In the paper (Gonçalves et al. 2014) the information from the business social network LinkedIn (LinkedIn 2019) was gathered. The uniqueness of this approach is that the users of this site post there a lot of information about their education, work experience, expectations — the important characteristics of the alumni investigation. Papoutsoglou et al. (2017) proposed a similar method. They created the algorithm to information crawling from the Stack Overflow Job Advertisements (Stack Overflow 2019). The separate way of the alumni research is to study the social connections (Nann et al. 2010; Rubens et al. 2011). The main conclusion here is that the wide social network facilitates the career success. Besides the scientific literature, we studied the methods of the international university rankings. We found that alumni success takes a little role in forming the rankings: QS (Employer Reputation — 10%) (Top Universities 2019), Times Higher Education (0%) (The World University Rankings 2019), ARWU (Alumni of an institution winning Nobel Prizes and Fields Medals — 10%) (Shanghai Ranking 2017). The remaining indicators in the rankings refer to science and students. The general issue here is that searching for information about alumni is hard 5 to conduct automatically. And the way to extract the information about alumni from Wikipedia solve this problem with automatization. It should be noticed that all persons must go through the “Wikipedia Notability for people” (Wikipedia: Notability (people) 2019) to have the own webpage on Wikipedia. Thus, we can find on Wikipedia only those people who made some significant impact. According to the Wikipedia rules, a person should be \"remarkable\" or \"significant, interesting, or unusual enough to deserve attention or to be recorded\" (Wikipedia: Notability (people) 2019). Being famous or popular is not the primary criteria for dedicating personal page. A person must be mentioned on the other sources (independent from this person), or must have a well-known and significant award or honor, or must have some historical importance, or must be the scholars with famous and important ideas, etc. Moreover, the relationship with the famous person is not enough to get", "meta": {"paper_id": "2012.13980v1", "chunk_index": 2, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_3", "text": "must be mentioned on the other sources (independent from this person), or must have a well-known and significant award or honor, or must have some historical importance, or must be the scholars with famous and important ideas, etc. Moreover, the relationship with the famous person is not enough to get a personal page on Wikipedia. Finding information about university graduates on Wikipedia allows not only to cover the most important graduates, but also to understand how important these graduates are to society. The number of page-views of the graduates' webpages shows the popularity, familiarity, and interest. The research goal of the paper is to discuss the possibility to evaluate the university impact based on the importance of its graduates to society and to compare the result with the well-known world university rankings. The methodological goal of the research is to demonstrate the method of collecting information on alumni through the number of page-views of their webpages on the free encyclopedia Wikipedia. For this purpose, we created a database containing 265,709 graduates from 464 inter-national universities. For every alumnus we found their name in English and national language, university (alma mater), birth year, link to the webpages in English and national languages, the total sum of views from webpages in English and national languages. Final sample contains graduates born from the X century to the XX century. The results of the investigation of the dataset are proposed for consideration and compared with the existing university rankings. The perspective of using the methods is discussed. 6 Methods Firstly, we defined the list of top 464 universities. Then we created the database which contained all the webpages of the chosen universities on Wikipedia (in English and national language). Finally, we identified all the webpages devoted to people who were graduated from the chosen university (in English and national language). After that, we found the total amount of views of these found webpages devoted to graduates and universities for 2017 year. The full procedure is described in Appendix. The final dataset includes: 265,709 graduates from 464 Universities and the sum of views of their Wikipedia pages; 464 Universities and the sum of views of their Wikipedia pages. The following parameters were taken into consideration for the current research: • The sum of graduates from the particular University; • The birth year of the graduates from the particular University; • The sum of views of the graduate’s Wikipedia page; • The sum of views of the university’s Wikipedia page; • The total sum of the all graduate’s webpage views for the particular University. Thus, current results are based on the statistics about number of webpage-views of the alumni and universities on Wikipedia. For comparison, we observed three popular universities rankings for 2017 year respectively: QS World University Rankings (later - QS) (Top Universities 2017), QS Graduate Employability Rankings (QS Graduate Employability Rankings 2017), Academic Ranking of World Universities (later - ARWU) (Shanghai Ranking 2017). These rankings were chosen because they took into consideration data about alumni. Further, we provide a review of these rankings. ARWU is formed", "meta": {"paper_id": "2012.13980v1", "chunk_index": 3, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_4", "text": "Rankings (later - QS) (Top Universities 2017), QS Graduate Employability Rankings (QS Graduate Employability Rankings 2017), Academic Ranking of World Universities (later - ARWU) (Shanghai Ranking 2017). These rankings were chosen because they took into consideration data about alumni. Further, we provide a review of these rankings. ARWU is formed on the basis of 6 indicators: alumni of an institution winning Nobel Prizes and Fields Medals, staff of an institution winning Nobel Prizes and Fields Medals, highly cited researchers in 21 broad subject categories, papers published in Nature and Science, papers indexed 7 in Science Citation Index-expanded and Social Science Citation Index, per capita academic performance of an institution. Alumni indicator is based on those who obtained bachelor's, master's or doctoral degrees from the university. The weight for particular alumnus is depended on the year of graduating: the biggest (100%) for 2001-2010, the lowest (10%) for 1911-1920. Thus, this indicator is cumulative and considering historical persons. QS forms its rankings, considering such indicators as research activities, teaching, number of foreign students, etc. To be included in the ranking, a university should offer bachelor and post- diploma programs in at least two wide spheres. For example, business and law QS offers a huge variety of rankings, they can be separated by location, program, subjects etc. We are going to concentrate on two of them: QS World University Rankings and QS Graduate Employability Rankings. The first uses the following 6 metrics in its methodological framework: Academic Reputation (40%), Employer Reputation (10%), Faculty/Student Ratio (20%), Citations per faculty (20%), International Faculty Ratio (5%), International Student Ratio (5%). Graduates play an active role in forming the Employer Reputation coefficient, as it shows whether they are prepared good enough for labor market competition. The coefficient is based on the QS Employer Survey, which includes answers of 40,000 responses about identification of institutions from which they source the most competent, innovative, effective graduates. The second one takes into account employer-student connections, alumni outcomes, employer reputation, partnerships with employers and graduate employment rate. QS Graduate Employability Rankings provides information about graduate employability outcomes and perspectives, which is achieved by comparison of different universities. The following indicators were used to form the ranking: Employer Reputation (30%), Partnerships with Employers (25%), Alumni outcomes (20%), Employers’ presence on campus (20%), Graduate employment rate (20%). Alumni outcomes are totally based on the success of individuals, as it helps to find out which universities supply labor market with the most wealthy and prosperous students. Graduate employment rate considers the 8 success of students, measuring the number of alumni, who have found a job during the first 12 months after their graduation. Thus, in the paper we observe the related to alumni scales and the final universities rankings for the 2017 year from QS, QS graduate, ARWU and compare them with the ranking based on the Wikipedia. Results 3.1 Descriptive statistics First of all, we prepared descriptive statistics on alumni page-views using public API Wikipedia. We received information about 464 universities and 265,709 alumni. The full dataset contains data about alumni since the X century.", "meta": {"paper_id": "2012.13980v1", "chunk_index": 4, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_5", "text": "graduate, ARWU and compare them with the ranking based on the Wikipedia. Results 3.1 Descriptive statistics First of all, we prepared descriptive statistics on alumni page-views using public API Wikipedia. We received information about 464 universities and 265,709 alumni. The full dataset contains data about alumni since the X century. Despite the fact that the full dataset about them was gathered, it was decided to concentrate on the XX century in order to trace the alumni influence on universities and on modern society as a whole. As a consequence, additional parameters were entered – “born after 1947” and “has more than 999 views” – specifically to clarify the quality of the modern educational programs (Table 1). Table 1 Descriptive statistics of the data about alumni on Wikipedia: number of alumni and number of universities in the dataset depending on the period of their years of birth. Dataset Number of alumni Number of universities Full dataset 265,709 464 1900-2000 190,885 463 1948-2000 105,112 460 1948-2000 & >999 views 88,242 460 We have made a calculation of mean, median and standard deviation of the page-views numbers (Table 2). According to the results, the gap between the most popular alumni and ranking outsiders increases because of the narrowing of the alumni birth range that enhances the correlation 9 of page-views sum and the page-views standard deviation. It provides evidence that more and more graduates born after 1900 or after 1947 have been introduced on Wikipedia. This conclusion provides wider coverage of alumni in contrast to counting of alumni as Nobel laureates & Fields Medalists, for instance, and gives a more realistic view on alumnus as the results of the universities’ production activities. Table 2 Descriptive statistics of the data about alumni on Wikipedia: the characteristics of the page-views number depending on the period of their years of birth. Dataset Mean page-views Median page- views Standard deviation page- views Full dataset 21,146 2,133 162,485 1900-2000 25,301 2,472 180,412 1948-2000 31,778 2,983 208,063 1948-2000 & >999 views 37,728 4,025 226,596 Moreover, contemporary graduates attract more page-views than older university mates regardless of their recognition (Fig. 1). 10 Fig. 1. The number of page-views dependence on the year of the graduate birth. Aggregating the obtain dataset by universities, one can able to construct the ranking based on the sum of page-views of alumni. Further, we will provide the comparison between Wikipedia ranking with the famous world universities ranking. But now we illustrate the correlation between rankings which were created for different periods of year of alumni birth. We divided years of birth on five categories: born anytime, born in XX century, born after 1947, 1965, 1980 (Table 3). Table 3 Correlation matrix between rankings based on Wikipedia alumni page-views for different periods of year of alumni birth. born any time born in XX century born after 1947 born after 1965 born after 1980 born any time 1.00 born in XX century 0.99 1.00 born after 1947 0.95 0.97 1.00 11 born after 1965 0.89 0.92 0.96 1.00 born after 1980 0.81 0.84 0.88 0.93 1.00 We can notice", "meta": {"paper_id": "2012.13980v1", "chunk_index": 5, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_6", "text": "any time born in XX century born after 1947 born after 1965 born after 1980 born any time 1.00 born in XX century 0.99 1.00 born after 1947 0.95 0.97 1.00 11 born after 1965 0.89 0.92 0.96 1.00 born after 1980 0.81 0.84 0.88 0.93 1.00 We can notice from Table 3 that ranking is changed with increasing of year of alumni birth. However, all correlation coefficients are positive and high. 3.2 Top-10 graduates based on the popularity on Wikipedia The world news and events indicate the seasonal popularity of the alumni. For example, Table 4 represents the top-10 graduates in the dataset, Table 5 represents the top-10 graduates that were born after 1947. Some of them have been a long time famous, but the others are famous due to events in the observed time. Table 4 Top-10 graduates based on the number of page-views on Wikipedia University Name Birth year Views Northwestern University Meghan Markle 1981 30,430,581 University of Cambridge Stephen Hawking 1942 19,183,278 University of Pennsylvania Elon Musk 1971 15,791,090 University of Cambridge Charles, Prince of Wales 1948 12,944,420 Saint Petersburg State University Vladimir Putin 1952 11,426,497 University of Edinburgh Charles Darwin 1809 10,225,649 ETH Zurich – Swiss Federal Institute of Technology Zurich Albert Einstein 1879 9,206,148 University of Miami Sylvester Stallone 1946 8,444,153 Columbia University Barack Obama 1961 8,418,653 University of St Andrews Prince William, Duke of Cambridge 1982 8,331,379 12 Table 5 Top-10 graduates that were born after 1947 based on the number of page-views on Wikipedia University Name Birth year Views Northwestern University Meghan Markle 1981 30,430,581 University of Pennsylvania Elon Musk 1971 15,791,090 University of Cambridge Charles, Prince of Wales 1948 12,944,420 Saint Petersburg State University Vladimir Putin 1952 11,4264,97 Columbia University Barack Obama 1961 8,418,653 University of St Andrews Prince William, Duke of Cambridge 1982 8,331,379 Princeton University Jeff Bezos 1964 7,661,172 Reed College Steve Jobs 1955 6,899,464 University of St Andrews Catherine, Duchess of Cambridge 1982 6,336,213 Harvard University Bill Gates 1955 5,880,135 3.3 University rankings Using described method, the universities ranking based on the alumni from Wikipedia was created and its results were compared with some famous universities rankings: QS, QS graduate, and ARWU (Table 6). These universities rankings were chosen because they contain the scales connected with alumni (see Methods). Table 6 Top-10 universities based on different rankings and metrics Wikipedia: full dataset Wikipedia: 1948- 2000 & >999 views ARWU QS QS Graduate 13 Harvard University New York University Harvard University Massachusetts Institute of Technology (MIT) Stanford University New York University Harvard University University of Cambridge Stanford University University of Oxford Columbia University University of California, Los Angeles Massachusetts Institute of Technology (MIT) Harvard University The University of Tokyo University of California, Los Angeles Columbia University University of California, Berkeley University of Cambridge Lomonosov Moscow State University Yale University University of Southern California Columbia University University of Oxford Massachusetts Institute of Technology (MIT) University of Southern California Waseda University University of Chicago Imperial College London University of Cambridge Waseda University Stanford University Princeton University National University of Singapore (NUS) University of", "meta": {"paper_id": "2012.13980v1", "chunk_index": 6, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_7", "text": "of Cambridge Lomonosov Moscow State University Yale University University of Southern California Columbia University University of Oxford Massachusetts Institute of Technology (MIT) University of Southern California Waseda University University of Chicago Imperial College London University of Cambridge Waseda University Stanford University Princeton University National University of Singapore (NUS) University of California, Los Angeles (UCLA) University of Tokyo Yale University University of Oxford London School of Economics and Political Science (LSE) University of Pennsylvania Stanford University Keio University California Institute of Technology CentraleSupélec Columbia University Keio University Northwestern University Ecole Normale Superieure - Paris London Business School Peking University 14 The different contents of the top 10 universities rankings are explained by various parameters included in the assessment of universities where alumni are only one of the criteria. However, the following universities are intersected in the rankings: • Columbia University, • Stanford University, The emergence of these universities in all mentioned rankings indicates not only the high level of the scientific research criteria, but also their importance for society provided by their graduates. The difference between full dataset Wikipedia ranking and the one after 1947 shows that the universities rarely maintain the tendency of graduating the most influential people. Some of them had the famous alumni in the past; the other has the popular alumni nowadays. Also, we detect the distinction between Wikipedia ranking and the international rankings (QS, ARWU) (Table 6): e.g. in the top 10 universities based on the Wikipedia ranking we observe three Japanese universities which are not represented in the top 10 universities according to famous rankings. This effect can be explained through the national differences in the Web consumption. Moreover, rankings' correlation was calculated on the base of only those scales, which were connected with alumni (Table 7). The biggest correlation was found between different modifications of the ranking, formed from Wikipedia dataset. The smallest correlation appeared to be between QS and ARWU. It can also be noticed, that the Wikipedia ranking correlates with ARWU more than with QS, however, the correlation coefficient between Wikipedia ranking and QS graduate is close to the correlation coefficient between Wikipedia ranking and ARWU. It may be partly explained by the similarity of these rankings: both take into account famous graduates, who are honored, and, thus, have got a scientific reward (ARWU) or a page in Wikipedia. Table 7 Correlation between rankings, based on the scales connected with alumni 15 Wikipedia: full dataset Wikipedia: 1900-2000 Wikipedia: 1948-2000 & >999 views ARWU QS QS Graduate Wikipedia: full dataset 1.00 Wikipedia: 1900-2000 0.99 1.00 Wikipedia: 1948-2000 & >999 views 0.94 0.97 1.00 ARWU 0.52 0.49 0.42 1.00 QS 0.35 0.35 0.36 0.28 1.00 QS Graduate 0.48 0.46 0.42 0.38 0.63 1.00 3.4 Webpages of universities on Wikipedia. To compare the popularity of the universities’ webpages on Wikipedia with the popularity of their alumni there, we have made the two separate rankings (Table 8). The correlation of the number of the university page-views and the university graduates’ views is equal to 0,83. This indicates that the popularity of graduates reflects the popularity of the university.", "meta": {"paper_id": "2012.13980v1", "chunk_index": 7, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_8", "text": "webpages on Wikipedia with the popularity of their alumni there, we have made the two separate rankings (Table 8). The correlation of the number of the university page-views and the university graduates’ views is equal to 0,83. This indicates that the popularity of graduates reflects the popularity of the university. Unfortunately, we cannot prove now that the alumni popularity leads to the increase in number of page-views of the universities webpages, but logically Wikipedia users follow the link on university from the alumni webpages. Table 8 Top 10 universities based on the alumni and universities popularity on Wikipedia. Wikipedia: full dataset Wikipedia: 1948-2000 & >999 views Wikipedia: webpages of universities Harvard University New York University Harvard University New York University Harvard University University of Oxford Columbia University University of California, Los Angeles Stanford University 16 University of California, Los Angeles Columbia University Columbia University Yale University University of Southern California Massachusetts Institute of Technology University of Southern California Waseda University Yale University Waseda University Stanford University University of Cambridge University of Tokyo Yale University Princeton University Stanford University Keio University University of California, Berkeley Keio University Northwestern University University of Pennsylvania Limitations The method has the following limitations that should be taken into consideration. For instance, there is a close dependence on the influence of certain current events or news. However, the method allows observing the dynamics of the alumni popularity. Next, it is necessary to take into consideration the year of the university’s foundation, the target set of the students, the specifics of the university, the alumni role in society (e.g. president, singer or scientist). Finally, the proposed algorithm can be improved to increase the accuracy of the dataset. Conclusion In this study, we discuss the possibility to evaluate the university contribution to society. We focus on investigating the university impact based on popularity (number of page-views) of their alumni’s pages on Wikipedia. This method is also aimed to provide wider coverage of alumni in research. It deals with the information crawling from a web-based free encyclopedia Wikipedia. We used data from Wikipedia: all alumni from 464 top universities were found and the number of page-views (of their webpages on Wikipedia) was calculated. We propose to evaluate the influence of the alumni on the society through the popularity of their webpages on Wikipedia. The 17 perspective of this approach is proved by the higher number of the contemporary persons page- views in comparison to great people of the past. This method reduces the limitation of the current universities ranking in graduate research: to take into consideration alumni who influence the whole society, not only in business or science. The method allows to define the contribution in society in past or to track every-year changes in popularity based on the alumni page-views. For potential students at universities, it may be interesting to know the success of recent graduates. For example, QS provides some metrics to evaluate graduates efficiently at work that is highly important during the period of choosing between universities (QS Graduate Employability Rankings 2019). However, the popularity of alumni on", "meta": {"paper_id": "2012.13980v1", "chunk_index": 8, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_9", "text": "For potential students at universities, it may be interesting to know the success of recent graduates. For example, QS provides some metrics to evaluate graduates efficiently at work that is highly important during the period of choosing between universities (QS Graduate Employability Rankings 2019). However, the popularity of alumni on Wikipedia as well as the possibility to get acquainted with the history of their success give less abstract information rather than percentiles in employee’s rankings. In this paper, we have created the universities ranking based on the alumni and compared it with the top three universities rankings which are included alumni analyses in the scales: QS, ARWU, and QS Graduate Employability Rankings. The results of this comparison illustrate a connection between these indicators that support the adequacy of the Wikipedia approach. However, we suggest considering our method as an addition to the already created methodology. Finally, we compared the alumni ranking with the ranking based on the popularity of the universities’ webpages on Wikipedia: there is a strong connection between them (0,83). So, we observed the connection between alumni and universities. We assume that the popularity of graduates reflects the popularity of the university. Although the method is not completely free from limitations now, it has been used quite successfully in this study. At the same time, we will be working on the improvement of the method. Additional information about alumnus achievements in particular of activities spheres can help to deepen the research. Addition to the existing metrics in the university rankings, the contribution of the alumni to the society can make the studies more powerful, precise and complete. Further investigation will shed light on the open question of the university impact. 18 Funding: This work was supported by The Tomsk State University competitiveness improvement program and by the Basic Research Program of the National Research University Higher School of Economics. References Agasisti, T. (2017). Management of Higher Education Institutions and the Evaluation of their Efficiency and Performance. Tertiary Education and Management, 23(3), 187–190. doi:10.1080/13583883.2017.1336250 Altmetric (2019). What are Altmetrics? https://www.altmetric.com/about-altmetrics/what-are- altmetrics/ Accessed 26 April 2020. Brooks, R. (2005). Measuring university quality. The Review of Higher Education, 29(1), 1–21. Cabrera, A. F., Weerts, D. J., & Zulick, B. J. (2005). Making an impact with alumni surveys. New Directions for Institutional Research, 2005(126), 5–17. Chavda, J., & Patel, A. (2016). Measuring research impact: bibliometrics, social media, altmetrics, and the BJGP. Br J Gen Pract, 66(642), e59–e61. Cole, L., & Thompson, G. (2002). Satisfaction of agri-business employers with college graduates they have hired. NACTA Journal, 34–39. Fumasoli, T. (2016). The Roles of the University in Society. Higher Education Quarterly, 70(2), 106–107. Gault, J., Redington, J., & Schlager, T. (2000). Undergraduate business internships and career success: are they related? Journal of marketing education, 22(1), 45–53. Gonçalves, G. R., Ferreira, A. A., de Assis, G. T., & Tavares, A. I. (2014). Gathering alumni information from a web social network. In 2014 9th Latin American Web Congress (pp. 100–108). IEEE. Hernández-March, J., Del Peso, M. M., & Leguey, S. (2009). Graduates’ skills and higher education: The employers’", "meta": {"paper_id": "2012.13980v1", "chunk_index": 9, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_10", "text": "Gonçalves, G. R., Ferreira, A. A., de Assis, G. T., & Tavares, A. I. (2014). Gathering alumni information from a web social network. In 2014 9th Latin American Web Congress (pp. 100–108). IEEE. Hernández-March, J., Del Peso, M. M., & Leguey, S. (2009). Graduates’ skills and higher education: The employers’ perspective. Tertiary education and management, 15(1), 1–16. Kempe, D., Kleinberg, J., & Tardos, É. (2005). Influential nodes in a diffusion model for social 19 networks. In International Colloquium on Automata, Languages, and Programming (pp. 1127-1138). Springer, Berlin, Heidelberg. Khazragui, H., & Hudson, J. (2014). Measuring the benefits of university research: Impact and the REF in the UK. Research Evaluation, 24(1), 51–62. Kosmützky, A., & Ewen, A. (2016). Global, national and local? The multilayered spatial ties of universities to society. In RE-BECOMING UNIVERSITIES? (pp. 223–245). Springer. Kozitsin, I. V. (2020). Formal models of opinion formation and their application to real data: evidence from online social networks. The Journal of Mathematical Sociology, 1-28. Kozitsin, I. V., & Chkhartishvili, A. G. (2020, September). Users’ Activity in Online Social Networks and the Formation of Echo Chambers. In 2020 13th International Conference\" Management of large-scale system development\"(MLSD) (pp. 1-5). IEEE. Laredo, P. (2007). Revisiting the third mission of universities: toward a renewed categorization of university activities? Higher education policy, 20(4), 441–456. Laschinger, H. K. S. (2012). Job and career satisfaction and turnover intentions of newly graduated nurses. Journal of nursing management, 20(4), 472–484. Lee, R., & Wilbur, E. R. (1985). Age, education, job tenure, salary, job characteristics, and job satisfaction: A multivariate analysis. Human Relations, 38(8), 781–791. Linkedin (2019). www.linkedin.com Accessed 26 April 2020. Lukman, R., Krajnc, D., & Glavič, P. (2010). University ranking using research, educational and environmental indicators. Journal of Cleaner Production, 18(7), 619–628. Montesinos, P., Carot, J. M., Martinez, J., & Mora, F. (2008). Third mission ranking for world class universities: Beyond teaching and research. Higher education in Europe, 33(2–3), 259–271. Nann, S., Krauss, J. S., Schober, M., Gloor, P. A., Fischbach, K., & Führes, H. (2010). The power of alumni networks-success of startup companies correlates with online social network structure of its founders. 20 https://dspace.mit.edu/bitstream/handle/1721.1/66586/CCI-2010-001.pdf?...1 Accessed 26 April 2020. Nariani, R. (2017). Supplementing Traditional Ways of Measuring Scholarly Impact: The Altmetrics Way. https://yorkspace.library.yorku.ca/xmlui/bitstream/handle/10315/33652/SupplementingTrad itionalWays_RN.pdf?sequence=1 Accessed 26 April 2020. Ng, T. W. H., Eby, L. T., Sorensen, K. L., & Feldman, D. C. (2005). Predictors of objective and subjective career success: A meta‐analysis. Personnel psychology, 58(2), 367–408. Nicolescu, L., & Paun, C. (2009). Relating Higher Education with the Labour Market: Graduates’ expectations and employers’ requirements. Tertiary education and management, 15(1), 17–33. Olcay, G. A., & Bulu, M. (2017). Is measuring the knowledge creation of universities possible?: A review of university rankings. Technological Forecasting and Social Change, 123, 153– 160. Papoutsoglou, M., Mittas, N., & Angelis, L. (2017). Mining People Analytics from StackOverflow Job Advertisements. In 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 108–115). IEEE. QS Graduate Employability Rankings (2017). https://www.topuniversities.com/university- rankings/employability-rankings/2017 Accessed 26 April 2020. QS Graduate Employability Rankings (2019). https://www.topuniversities.com/university- rankings/employability-rankings/2019 Accessed 26 April 2020. Quarmby, K. L.,", "meta": {"paper_id": "2012.13980v1", "chunk_index": 10, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_11", "text": "Angelis, L. (2017). Mining People Analytics from StackOverflow Job Advertisements. In 2017 43rd Euromicro Conference on Software Engineering and Advanced Applications (SEAA) (pp. 108–115). IEEE. QS Graduate Employability Rankings (2017). https://www.topuniversities.com/university- rankings/employability-rankings/2017 Accessed 26 April 2020. QS Graduate Employability Rankings (2019). https://www.topuniversities.com/university- rankings/employability-rankings/2019 Accessed 26 April 2020. Quarmby, K. L., Willett, P., & Wood, F. E. (1999). Follow-up study of graduates from the MSc Information Management programme at the University of Sheffield. Journal of information science, 25(2), 147–155. Ranking Web of Universities (2019). http://www.webometrics.info/en/Europe Accessed 26 April 2020. 21 Rubens, N., Russell, M., Perez, R., Huhtamäki, J., Still, K., Kaplan, D., & Okamoto, T. (2011). Alumni network analysis. In 2011 IEEE Global Engineering Education Conference (EDUCON) (pp. 606–611). IEEE. Sánchez-Barrioluengo, M. (2014). Articulating the ‘three-missions’ in Spanish universities. Research Policy, 43(10), 1760–1773. Scillitoe, J. L. (2013). The role of alumni attachment in the university technology transfer process. In 2013 Proceedings of PICMET’13: Technology Management in the IT-Driven Services (PICMET) (pp. 2397–2406). IEEE. Scott, J. C. (2006). The mission of the university: Medieval to postmodern transformations. The journal of higher education, 77(1), 1–39. Secundo, G., Perez, S. E., Martinaitis, Ž., & Leitner, K. H. (2017). An Intellectual Capital framework to measure universities’ third mission activities. Technological Forecasting and Social Change, 123, 229–239. Seibert, S. E., Crant, J. M., & Kraimer, M. L. (1999). Proactive personality and career success. Journal of applied psychology, 84(3), 416. Shanghai Ranking (2017). http://www.shanghairanking.com/ARWU2017.html Accessed 26 April 2020. Stack Overflow (2019). https://www.stackoverflowbusiness.com/talent/platform/source/job- listings Accessed 26 April 2020. The World University Rankings (2019). https://www.timeshighereducation.com/ Accessed 26 April 2020. Top Universities (2017). https://www.topuniversities.com/university-rankings/world-university- rankings/2016 Accessed 26 April 2020. Uyarra, E. (2010). Conceptualizing the regional roles of universities, implications and contradictions. European Planning Studies, 18(8), 1227–1246. Vermeulen, L., & Schmidt, H. G. (2008). Learning environment, learning process, academic 22 outcomes and career success of university graduates. Studies in Higher Education, 33(4), 431–451. doi:10.1080/03075070802211810 Wikipedia (2019). https://en.wikipedia.org/wiki/Wikipedia Accessed 26 April 2020. Wikipedia:Notability (people) (2019). https://en.wikipedia.org/wiki/Wikipedia:Notability_(people) Accessed 26 April 2020. Witte, K. De, & López-Torres, L. (2017). Efficiency in education: a review of literature and a way forward. Journal of the Operational Research Society, 68(4), 339–363. 23 Appendix The description of the method to crawl information about alumni and universities from a web- based free encyclopedia Wikipedia. Firstly, we created the database which contained all the webpages of the chosen universities on Wikipedia where all the versions of the redirect pages (to the universities) in English and national languages were mentioned. Secondly, we identified all the webpages devoted to people. Thus, a complete copy of all Wikipedia wikis (later — dump) (in the form of XML text) was obtained (relevant for September 1, 2018, in 18 languages). Using the list of universities, we programmatically composed the table which includes university’s Id; the name of the webpage on Wikipedia; language domain (ru, en, fr, etc) with the all direct pages and the webpages in English and/or national languages. For the dump we performed the following algorithm: • If the file has not been completed the following independent webpage inside the dump is read. • In", "meta": {"paper_id": "2012.13980v1", "chunk_index": 11, "filename": "2012.13980v1.pdf", "chunk_length": 512}}
{"id": "2012.13980v1_chunk_12", "text": "the webpage on Wikipedia; language domain (ru, en, fr, etc) with the all direct pages and the webpages in English and/or national languages. For the dump we performed the following algorithm: • If the file has not been completed the following independent webpage inside the dump is read. • In the webpage, depending on the language used, the keywords from the special dictionary are found. For example, the following phrases are included in the dictionary: “born”, “Category: 1980 births”. • If the markers of a person’s page are found, then a birth year is found (the first four numbers at the beginning of the page, no further than in 1000 words from the start). If the birth year is not found in the first 1000 words (because of the traditional article structure on Wikipedia), it stays empty. • The page is saved in the independent file .xml with the name “page_Id_birthday”. We performed the following algorithm for all files from the previous step: • If the file has not been completed the next sentence is read (all words until a full stop). The entire link to the other Wiki-pages is saved in the database. 24 • In the obtained sentence the trigger-words from the definite set are searched. Under a definite set we mean the words indicating the connection with the university (“graduated”, “alumni”, “received degree”, etc. (case-insensitive)). Searched words and collocations were prepared by translators on the basis of manual sampling from the webpages of graduates in 18 languages. • If such words are found, then for every link in the obtained database a match with the universities’ names is searched. • If the match is found then in a final file the following row is added: “University’s Id”, “University's name”, “link to the webpage of graduates”, “birth year”, “language domain”. • The next file is opened. For each record with the language domain different from “en”, the existence of the webpage in the English domain for this person is checked with help of public API Wikipedia. If the webpage exists so the link to this page and the language domain (“en”) are added to the record. To calculate the number of views the obtained table was read by rows. The webpages in English and national languages were extracted. Using public API Wikipedia, we calculated the statistics of the total amount of views for the last year. Values for the webpage in English and national language were summarized and added to the table. To find statistics of page-views of the universities’ webpages on Wikipedia, the following method was performed. Using public API Wikipedia, we calculated the statistics of the total amount of views for the last (2017) year — the sum of page-views in English and national languages. Names of all universities were translated into national languages. These algorithms were launched and implemented on September 1st, 2018. The estimated accuracy was found. A selective manual check of the results was carried out (the final dataset contained 6-8% of mistakes).", "meta": {"paper_id": "2012.13980v1", "chunk_index": 12, "filename": "2012.13980v1.pdf", "chunk_length": 500}}
{"id": "2012.13980v1_chunk_13", "text": "universities were translated into national languages. These algorithms were launched and implemented on September 1st, 2018. The estimated accuracy was found. A selective manual check of the results was carried out (the final dataset contained 6-8% of mistakes).", "meta": {"paper_id": "2012.13980v1", "chunk_index": 13, "filename": "2012.13980v1.pdf", "chunk_length": 38}}
{"id": "2012.14005v1_chunk_0", "text": "Neural document expansion for ad-hoc information retrieval Cheng Tang ⋆⋆⋆, Andrew Arnold † Amazon Abstract. Recently, Nogueira et al. [2019] proposed a new approach to document expansion based on a neural Seq2Seq model, showing signif- icant improvement on short text retrieval task. However, this approach needs a large amount of in-domain training data. In this paper, we show that this neural document expansion approach can be eﬀectively adapted to standard IR tasks, where labels are scarce and many long documents are present. 1 Introduction In traditional ad-hoc retrieval, queries and documents are represented by variants of bag-of-words representations. This leads to the so called vocabulary mismatch problem: when a query contains words that do not exactly match words in a relevant document, the search engine may fail to retrieve this document. Query expansion and document expansion, the methods of adding additional terms to the original query or document, are two popular solution to alleviate the vocabulary mismatch problem. Document expansion has been shown to be particularly eﬀective for short text retrieval and language-model based retrieval [Tao et al., 2006, Efron et al., 2012]. Most of the existing works in document expansion are unsupervised: using information from the corpus to augment document representation, e.g., retrieval based [Efron et al., 2012] and clustering based [Liu and Croft, 2004, Tao et al., 2006], or using external information to augment document representation [Agirre et al., 2010, Sherman and Efron, 2017]. Recently, Nogueira et al. [2019] proposed a new approach to document ex- pansion, which is based on a popular generative sequence-to-sequence model (Seq2Seq) in NLP, transformers [Wolf et al., 2020]. It leverages supervision to train the model to predict expansion terms conditional on each document. The paper has shown signiﬁcant improvement on passage (short document) datasets, when trained in-domain. In this paper, we follow this line of supervised neu- ral document expansion approach and explore its performance on standard IR benchmarking dataset. Our main contributions are: 1. Adapting the method to unlabeled datasets by exploring transfer learning and weak-supervision ap- proaches. 2. Adapting the method to traditional IR datasets, where a large num- ber of long documents are present. ⋆⋆⋆tcheng@amazon.com † anarnld@amazon.com arXiv:2012.14005v1 [cs.IR] 27 Dec 2020 document counts Fig. 1: Left: document lengths distribution in Robust04; Right: distribution of relevant passage (answer-containing passage) indices in MSMarco-documents 2 Document retrieval with Seq2Seq models In this section, we discuss our approach to document expansion. Train a passage level document expansion model We follow the approach of Nogueira et al. [2019]: we use data of format (p, q), where q is a question and p is a passage relevant to it. A Seq2Seq model is trained so that conditional on input p, it learns to generate the ground-truth question q. The dataset we found most eﬀective for training the document expansion model is the MSMarco passage-ranking dataset [Bajaj et al., 2018]. Domain adaptation Since most IR datasets are short of annotated queries, it is important to explore ways to adapt the document expansion model to out- of-domain unlabeled datasets. We explored two simple approaches: zero-shot transfer and weak", "meta": {"paper_id": "2012.14005v1", "chunk_index": 0, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_1", "text": "the document expansion model is the MSMarco passage-ranking dataset [Bajaj et al., 2018]. Domain adaptation Since most IR datasets are short of annotated queries, it is important to explore ways to adapt the document expansion model to out- of-domain unlabeled datasets. We explored two simple approaches: zero-shot transfer and weak supervision (retrieval-based pseudo annotation). In zero-shot transfer, we directly apply a model trained from one domain to another; in retrieval-based pseudo annotation, we issue out-of-domain queries to a target corpus and treat the top-k returned documents as “relevant”. Adaptation to long documents Due to the memory overhead of transformer- based models, the maximal input length of this family of models are constrained to be short. We trained our expansion models on passage-level datasets. However, during inference time, a standard IR dataset is usually a mixture of short and long documents, and expanding on an arbitrarily capped portion of a document may yield sub-optimal performance. Figure 1-Left shows the document lengths distribution of Robust04 dataset. We see that most of the documents are around 1000-tokens long, which has exceeded the typical input length transformer based models can take [Hofst¨atter et al., 2020]. To deal with this problem, we explored three strategies (see experiment results at Table 3: “Long document generation methods” section): 1. concatenation of paragraph generation (CONCAT in Table 3): Split each long document into overlapping passages, respecting natural sentence boundaries (with sliding window size to be around 60 tokens, since this is roughly the median size of passages in MSMarco dataset, which we used for training) and run expansion inference on each passage. All generated expansions are directly appended to the original documents. 2. ﬁrst k sentences (FIRST-K in Table 3): Run expansion model on the ﬁrst k whole sentences of each document. This strategy is based on our analysis of MSMarco datasets. From MSMarco-QA dataset, we can obtain (q, p, a) triplets so that we know passage p contains an answer for question q. We treat this as a signal of question-passage relevance. Then we can trace these passages back to the document they belong to using the MSMarco- documents dataset. Comparing “Capped passage generation methods” to “Long document generation methods” in Table 3, we see that the “Long doc- ument generation methods”, and in particular, the CONCAT and FIRST-K generation methods are more eﬀective. 3. passage-importance indicator (PI in Table 3): Before seeing any ques- tions, we predict which passage will be queried using a unsupervised pre- dictor [Bendersky and Kurland, 2008], and generate expansion exclusively based on this passage. In our experiments, this method do not yield good performance, likely due to the low quality of the passage-importance predic- tor. 3 Experiments To train the document expansion models, we selected datasets that are typically used for passage-ranking and question-answering tasks, i.e., MSMarco passage- ranking [Bajaj et al., 2018], Natural Questions [Kwiatkowski et al., 2019], and SQuAD [Rajpurkar et al., 2018]. In addition, we also used a TREC collection short-document dataset, microblogs [Lin et al., 2014]. To evaluate the perfor- mance of document expansion method on information retrieval, we", "meta": {"paper_id": "2012.14005v1", "chunk_index": 1, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_2", "text": "question-answering tasks, i.e., MSMarco passage- ranking [Bajaj et al., 2018], Natural Questions [Kwiatkowski et al., 2019], and SQuAD [Rajpurkar et al., 2018]. In addition, we also used a TREC collection short-document dataset, microblogs [Lin et al., 2014]. To evaluate the perfor- mance of document expansion method on information retrieval, we additionally add the standard IR benchmarking dataset Robust04 [Voorhees, 2004]. Our base- line retrieval methods, BM25 and Query Likelihood (QL), use implementation from the open-source Anserini toolkit [Yang et al., 2017]; for Seq2Seq models, we experimented with transformers (OpenNMT implementation [Klein et al., 2017]) and pre-trained BART (Huggingface Pytorch implementation [Wolf et al., 2020]). 3.1 Retrieval performance on passage datasets From Table 1, we see that retrieval-based weak supervision, while showing good performance as training signal for neural retrieval models [Dehghani et al., 2017], does not yield good document expansion models. Instead, the BART-based zero- shot transfer model has competitive performance to in-domain trained-from- scratch models. Once decided on the zero-shot transfer learning approach, we tried several ﬁne-tuning strategies with the BART model (Table 2), drawing inspiration from [Yilmaz et al., 2019]. We found that ﬁne-tuning pre-trained BART with MSMarco-passages dataset and with a mixed MSMarco-passages and microblogs dataset produces the best document expansion models. Although less eﬀective, our experiment suggests that other passage-level datasets such as Natural Questions and SQuAD, can also be used as sources to train document expansion models. DE models MSMarco-passages Natural Questions SQuAD-v2 Trec-DL Trec-DL (DEV) (DEV) R@100 MAP R@100 MAP R@100 MAP BM25 0.4531 0.3773 0.7619 0.3616 0.9737 0.7997 (0.7619) (0.3616) In-domain trained 0.4545 0.3872 0.8671 0.4450 0.9754 0.7915 transformer (0.7127) (0.2203) Weakly supervised transformer NA NA 0.7649 0.3608 0.9717 0.7913 Zero-shot transfer (transformer) NA NA 0.7773 0.3879 0.9764 0.8056 (ﬁne-tuning 0.5297 0.465 0.8302 0.4501 0.9787 0.8121 BART) (0.7949) (0.2674) Table 1: In-domain trained and weakly-supervised document expansion model; for MSMarco-passages, we have two test sets: DEV and Trec-DL [Craswell et al., 2020] DE models Natural Questions SQuAD-v2 Robust04 R@100 MAP R@100 MAP R@100 MAP BM25 0.7619 0.3616 0.9737 0.7997 0.4152 0.2553 MSMarco-passages 0.8302 0.4501 0.9787 0.8121 0.4229 0.2620 MSMarco-passage →microblogs 0.7931 0.4 0.9757 0.7962 0.4206 0.2533 MSMarco-passages + microblogs 0.8239 0.4437 0.9787 0.8133 0.4212 0.2630 Natural Questions 0.9031 0.5271 0.9782 0.8099 0.4190 0.2626 SQuAD-v2 0.8173 0.4228 0.9798 0.8156 0.4218 0.2616 Table 2: Zero-shot transfer of passage-level DE model 3.2 Retrieval performance on Robust04 To test the performance of passage-level trained document expansion model on standard IR corpus, we ﬁxed the passage-level model to be “MSMaroc-passage + microblog” trained. Then we explored three expansion generation methods as described in Section 2. The result of applying passage-level trained document expansion model to Robust04 can be found in Table 3. Traditional IR baselines In addition to tf-idf based BM25, we applied doc- ument expansion to two popular language-model based retrieval methods: query- likelihood with Dirichlet Smooth (QLD) and query-likelihood with Jelinek-Mercer smoothing (QLJM). We found that document expansion has good performance with QLD. We speculate that this is because the way our current document ex- pansion model works is similar to the Dirichlet smoothing model, which models the", "meta": {"paper_id": "2012.14005v1", "chunk_index": 2, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_3", "text": "based retrieval methods: query- likelihood with Dirichlet Smooth (QLD) and query-likelihood with Jelinek-Mercer smoothing (QLJM). We found that document expansion has good performance with QLD. We speculate that this is because the way our current document ex- pansion model works is similar to the Dirichlet smoothing model, which models the process of adding unseen words to the original documents by drawing from a background distribution based on the entire corpus. Here, the document expan- sion model in addition samples words from the query distribution (conditional on each document). Since document expansion does not signiﬁcantly improve QLJM, we did not include it in the rest of our experiments. Capped passage generation vs Long document generation Comparing “Capped passage generation methods” to “Long document generation meth- ods”, we see that for both BM25 and QLD, the two long document expansion methods, CONCAT and FIRST-K, have better performance. The fact the CON- CAT has the best performance suggests that even if most queries are targeting the head portion of a document (Figure 1-Right), using information from the entire document may still be beneﬁcial to expansion generation. Performance with pseudo-relevance feedback and BERT re-ranker Since document expansion is one of several techniques that can improve the document retrieval performance, we also want to understand how it works when combined with other techniques. In our experiments, we ﬁrst explored combining the best performing document expansion model with RM3 [Abdul-jaleel et al., 2004], a popular query expansion method based on pseudo-relevance feedback. While RM3 alone signiﬁcantly improves the baseline retrieval performance, DE** 1 can still add an additional boost on top. We want to point out that comparing to RM3, which requires two rounds of retrieval at query time, document expansion model is applied oﬄine and does not add additional computational overhead at run time. BM25 with document and query expansion makes a ﬁrst-stage ranker in the ranking pipeline. In our last experiment, we test its end-to-end perfor- mance when combined with a second-stage neural ranker, BERT [Nogueira and Cho, 2019]. To evaluate the end-to-end result, we used metrics R@k and P@k for small k, mimicking what the ranking system presents to a user (the top-k ranked documents). Our experiment results indicate that our document expan- sion models are complementary to query expansion as a ﬁrst-stage ranker and can improve the end-to-end ranking performance when combined with a second- stage ranker. 1 DE** indicates concatenation of paragraph generation with MSMarco+microblog trained passage model. Experiment category Methods Robust04 R@100 MAP Traditional BM25 0.4152 0.2553 IR baselines QLD 0.4157 0.2502 QLJM 0.3995 0.2287 Capped passage BM25+passage-DE* 0.4212 0.2630 generation methods QLD+passage-DE* 0.4270 0.2620 QLJM+passage-DE* 0.4058 0.2350 Long document BM25+DE* (CONCAT) 0.4283 0.2631 generation methods BM25+DE* (FIRST-K) 0.4226 0.2625 BM25+DE* (PI) 0.4212 0.2588 QLD+DE* (CONCAT) 0.4290 0.2615 QLD+DE* (FIRST-K) 0.4272 0.2625 QLD+DE* (PI) 0.4259 0.2577 DE+pseudo-relevance feedback BM25+RM3 0.4517 0.2941 BM25+RM3+DE** 0.4641 0.3035 DE + BERT reranker End-to-end metrics R@10 P@10 P@5 BM25 + BERT 0.2771 0.3731 0.4137 BM25 + RM3 + BERT 0.2608 0.3803 0.4048 BM25 + DE** + BERT 0.2824 0.3767 0.4161 BM25 + RM3 + DE** + BERT", "meta": {"paper_id": "2012.14005v1", "chunk_index": 3, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_4", "text": "QLD+DE* (PI) 0.4259 0.2577 DE+pseudo-relevance feedback BM25+RM3 0.4517 0.2941 BM25+RM3+DE** 0.4641 0.3035 DE + BERT reranker End-to-end metrics R@10 P@10 P@5 BM25 + BERT 0.2771 0.3731 0.4137 BM25 + RM3 + BERT 0.2608 0.3803 0.4048 BM25 + DE** + BERT 0.2824 0.3767 0.4161 BM25 + RM3 + DE** + BERT 0.2726 0.3944 0.4241 Table 3: Robust04 experiments (DE*: MSMarco-passage+microblog trained pas- sage model; DE**: concatenation of paragraph generation, CONCAT, with DE*) 4 Conclusion We showed that a document expansion model trained on passage-level datasets of (question, relevant passage) pairs can be directly applied to out-of-domain IR datasets to improve retrieval performance. We explored simple approaches to adapt the model to standard IR dataset (Robust04) where a large number of long documents are present, and we showed that adapting the passage-level trained model to long documents further improves retrieval performance. How- ever, our current simple adaptations to long documents do not signiﬁcantly im- prove the model performance (see Table 3, Long document generation methods). We cannot conclude whether this is due to the nature of the relevant passage distribution over long documents (i.e., they tend to be the ﬁrst few passages of any document, according to Figure 1-Right). Hence, it may be worth exploring model architectures that allow longer input sequences, for example, by switching to sparse attention layers [Beltagy et al., 2020]. Bibliography Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. Document expansion by query prediction, 2019. Tao Tao, Xuanhui Wang, Qiaozhu Mei, and ChengXiang Zhai. Language model information retrieval with document expansion. In Proceedings of the Main Conference on Human Language Technology Conference of the North Ameri- can Chapter of the Association of Computational Linguistics, HLT-NAACL ’06, page 407–414, USA, 2006. Association for Computational Linguistics. https://doi.org/10.3115/1220835.1220887. URL https://doi.org/10.3115/ 1220835.1220887. Miles Efron, Peter Organisciak, and Katrina Fenlon. Improving retrieval of short texts through document expansion. In Proceedings of the 35th International ACM SIGIR Conference on Research and Develop- ment in Information Retrieval, SIGIR ’12, page 911–920, New York, NY, USA, 2012. Association for Computing Machinery. ISBN 9781450314725. https://doi.org/10.1145/2348283.2348405. URL https://doi.org/10.1145/ 2348283.2348405. Xiaoyong Liu and W. Bruce Croft. Cluster-based retrieval using language models. In Proceedings of the 27th Annual International ACM SIGIR Con- ference on Research and Development in Information Retrieval, SIGIR ’04, page 186–193, New York, NY, USA, 2004. Association for Computing Ma- chinery. ISBN 1581138814. https://doi.org/10.1145/1008992.1009026. URL https://doi.org/10.1145/1008992.1009026. Eneko Agirre, Xabier Arregi, and Arantxa Otegi. Document expansion based on wordnet for robust ir. In COLING, 2010. Garrick Sherman and Miles Efron. Document expansion using external col- lections. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’17, page 1045–1048, New York, NY, USA, 2017. Association for Computing Machin- ery. ISBN 9781450350228. https://doi.org/10.1145/3077136.3080716. URL https://doi.org/10.1145/3077136.3080716. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement De- langue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State- of-the-art natural language processing, 2020.", "meta": {"paper_id": "2012.14005v1", "chunk_index": 4, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_5", "text": "Clement De- langue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State- of-the-art natural language processing, 2020. Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang. Ms marco: A human generated machine reading comprehension dataset, 2018. Sebastian Hofst¨atter, Hamed Zamani, Bhaskar Mitra, Nick Craswell, and Al- lan Hanbury. Local self-attention over long text for eﬃcient document re- trieval. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’20, page 2021–2024, New York, NY, USA, 2020. Association for Computing Machin- ery. ISBN 9781450380164. https://doi.org/10.1145/3397271.3401224. URL https://doi.org/10.1145/3397271.3401224. Michael Bendersky and Oren Kurland. Utilizing passage-based language models for document retrieval. In Proceedings of the IR Research, 30th European Conference on Advances in Information Retrieval, ECIR’08, page 162–174, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 3540786457. Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kel- cey, Jacob Devlin, Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics, 2019. Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784–789, Melbourne, Australia, July 2018. Association for Computational Linguistics. https://doi.org/10.18653/v1/P18-2124. URL https://www.aclweb.org/anthology/P18-2124. J. Lin, Y. Wang, M. Efron, and Garrick Sherman. Overview of the trec-2014 microblog track. In TREC, 2014. E. Voorhees. Overview of the trec 2004 robust track. 2004. Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Enabling the use of lucene for information retrieval research. In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’17, page 1253–1256, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450350228. https://doi.org/10.1145/3077136.3080721. URL https://doi.org/10.1145/ 3077136.3080721. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. OpenNMT: Open-source toolkit for neural machine translation. In Proceedings of ACL 2017, System Demonstrations, pages 67–72, Vancouver, Canada, July 2017. Association for Computational Linguistics. URL https://www.aclweb. org/anthology/P17-4012. Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, and W. Bruce Croft. Neural ranking models with weak supervision. In Pro- ceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’17, page 65–74, New York, NY, USA, 2017. Association for Computing Machinery. ISBN 9781450350228. https://doi.org/10.1145/3077136.3080832. URL https://doi.org/10.1145/ 3077136.3080832. Zeynep Akkalyoncu Yilmaz, W. Yang, Haotian Zhang, and Jimmy Lin. Cross- domain modeling of sentence-level evidence for document retrieval. In EMNLP/IJCNLP, 2019. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the trec 2019 deep learning track, 2020. Nasreen Abdul-jaleel, James Allan, W. Bruce Croft, O Diaz, Leah Larkey, Xi- aoyan", "meta": {"paper_id": "2012.14005v1", "chunk_index": 5, "filename": "2012.14005v1.pdf", "chunk_length": 512}}
{"id": "2012.14005v1_chunk_6", "text": "Zhang, and Jimmy Lin. Cross- domain modeling of sentence-level evidence for document retrieval. In EMNLP/IJCNLP, 2019. Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. Overview of the trec 2019 deep learning track, 2020. Nasreen Abdul-jaleel, James Allan, W. Bruce Croft, O Diaz, Leah Larkey, Xi- aoyan Li, Mark D. Smucker, and Courtney Wade. Umass at trec 2004: Novelty and hard. In In Proceedings of TREC-13, 2004. Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with bert. arXiv preprint arXiv:1901.04085, 2019. Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long- document transformer, 2020.", "meta": {"paper_id": "2012.14005v1", "chunk_index": 6, "filename": "2012.14005v1.pdf", "chunk_length": 97}}
{"id": "2012.14312v1_chunk_0", "text": "Panarchy: ripples of a boundary concept Juan Rocha1,2,3, Linda Luvuno4, Jesse Rieb5, Erin Crockett6, Katja Malmborg1, Michael Schoon7, Garry Peterson1 1Stockholm Resilience Centre, Stockholm University, Kräftriket 2B, 10691 Stockholm 2Future Earth, The Swedish Royal Academy of Sciences, Lilla Frescativägen 4A, 104 05 Stockholm 3South American Institute for Resilience and Sustainability Studies, Maldonado, Uruguay 4Center for Complex Systems in Transition, Stellenbosch University, South Africa 5Department of Geography, McGill University, 805 rue Sherbrooke ouest, Montréal (Québec) H3A 0B9, Canada 6Department of Natural Resource Sciences, McGill University, 21111 Lakeshore Road, Sainte-11 Anne-de-Bellevue, QC, H9X 3V9, Canada 7School of Sustainability, Arizona State University, Tempe, Arizona, USA How do social-ecological systems change over time? In 2002 Holling and colleagues proposed the concept of Panarchy, which presented social-ecological systems as an interacting set of adaptive cycles, each of which is produced by the dynamic tensions between novelty and eﬃciency at multiple scales. Initially introduced as a conceptual framework and set of metaphors, panarchy has gained the attention of scholars across many disciplines and its ideas continue to inspire further conceptual developments. Almost twenty years after this concept was introduced we review how it has been used, tested, extended and revised. We do this by combining qualitative methods and machine learning. Document analysis was used to code panarchy features that are commonly used in the scientiﬁc literature (N = 42), a qualitative analysis that was complemented with topic modeling of 2177 documents. We ﬁnd that the adaptive cycle is the feature of panarchy that has attracted the most attention. Challenges remain in empirically grounding the metaphor, but recent theoretical and empirical work oﬀer some avenues for future research. Introduction Almost two decades ago, the edited book “Panarchy: Understanding Transformations in Human and Natural Systems” (Gunderson and Holling 2002), presented a synthetic perspective of how a group of leading social-ecological researchers associated with the Resilience Alliance understood change in social-ecological systems. The concept of Panarchy was a key focus on this inﬂuential book. In the late 1990s and early 2000s, the Resilience Alliance was a productive, innovative highly collaborative group of environmental scientists that was focussed on bridging social and natural sciences as well as theory and practice. They were very focussed on addressing social-ecological problems by combining insights from the social and natural sciences, as well as the arts and humanities (Parker and Hackett 2012). Panarchy encapsulates a set of concepts that have inspired the work of environmental scientists and practitioners for almost two decades. It was a central concept produced by the Resilience Alliance, an international group of inﬂuential academics focused on the resilience of social-ecological systems in theory and practice (Parker and Hackett 2012). The Panarchy concept builds on Holling’s adaptive cycle (C. Holling 1986) by extending the idea across spatial and temporal scales. 1 arXiv:2012.14312v1 [cs.CL] 28 Dec 2020 Panarchy remains a boundary object that has inspired research topics, enabled collaborations and nurtured new scientiﬁc frameworks. The ideas put forward have been applied in ﬁeld studies, archaeology, mathematical models, participatory work, and scenario development. Panarchy has inspired resilience assessments and guided decision making. In this article", "meta": {"paper_id": "2012.14312v1", "chunk_index": 0, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_1", "text": "28 Dec 2020 Panarchy remains a boundary object that has inspired research topics, enabled collaborations and nurtured new scientiﬁc frameworks. The ideas put forward have been applied in ﬁeld studies, archaeology, mathematical models, participatory work, and scenario development. Panarchy has inspired resilience assessments and guided decision making. In this article we pay tribute to the book by studying how the concepts and metaphors proposed have been further developed in the academic literature. We also document criticism of the concepts and identify key research frontiers. Panarchy proposes that it is useful to conceptualize systems in terms of interacting adaptive cycles. The adaptive cycle was an idea ﬁrst proposed by Holling based upon his experience working and studying managed ecosystems (C. Holling 1986). It was meant to be a conceptual tool that focussed attention on processes of destruction and reorganization, which had been neglected in comparison to those of growth and conservation. Considering these processes provides a more complex may to understand system dynamics. An adaptive cycle alternates between long periods of system aggregation, connection, and accumula- tion, and shorter periods of disruption and reorganization. The adaptive cycle exhibits two major phases. The ﬁrst, often referred to as the front-loop, from r to K, is the slow, incremental phase of growth and accumulation. The second, referred to as the back-loop, from Omega to Alpha, is the rapid phase of reorganization leading to renewal. A system going into the back-loop can either remerge in a similar form, or may re-emerge as a new type of system with new boundaries and key components. Resilience researchers proposed that tensions between demands for organization and eﬃciency versus demands for novelty and diversity drive adaptive cycle type dynamics in many diﬀerent types of complex systems (C. S. Holling, Gunderson, and Peterson 2002). Panarchy is not a theory of what it is, but a metaphor of what might be (Gunderson and Holling 2002). It is not a predictive tool, but aims to understand adaptive change. The adaptive cycle is hypothesized to exist within a three-dimensional space deﬁned by three properties: potential, connectivity, and resilience (Gunderson and Holling 2002). Potential refers to the capital available capital to the system, for example nutrients and carbon captured by a forest, or human capital - skills and knowledge accumulated to run the economy. Connectivity is a proxy of the structure of the system; it is the network of interactions and strength between its elements. Resilience is the capacity of any system to absorb disturbance and reorganize while undergoing change so as to retain essentially the same identity, as determined by its function, structure, and feedbacks (Folke 2016). Panarchy posits that systems are organized in nested hierarchies across space and time, where each level of the hierarchy is a subsystem that can be in a diﬀerent phase of the adaptive cycle. These phases are exploitation, conservation, release, and reorganization, the latter characterized by events of creative destruction (Fig. 1). They are inspired by mathematical models used in economics and ecology, but broadly describe patterns of growth, collapse, and recovery that are common to populations, ecological", "meta": {"paper_id": "2012.14312v1", "chunk_index": 1, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_2", "text": "diﬀerent phase of the adaptive cycle. These phases are exploitation, conservation, release, and reorganization, the latter characterized by events of creative destruction (Fig. 1). They are inspired by mathematical models used in economics and ecology, but broadly describe patterns of growth, collapse, and recovery that are common to populations, ecological communities, markets, or political organizations. While each subsystem in the hierarchy can be at a diﬀerent phase of the cycle, these subsystems can inﬂuence each other through cross-scale interactions called “revolt” and “remember.” The three-dimensional space has corners with attractors that can cause the adaptive cycle to stagnate: poverty and rigidity traps (Gunderson and Holling 2002). Poverty traps are described in panarchy as maladaptive states where potential, connectivity and resilience are low. Poverty traps are a series of feedback mechanisms that reinforce impoverished states (Bowles, Durlauf, and Hoﬀ2006), limiting the system’s capacity to innovate and increase potential. The opposite corner, where potential, connectivity and resilience are high, is another maladaptive space called a rigidity trap. In that corner there is little space for experimentation and innovation. Examples include systems where 2 Figure 1: Panarchy is an heuristic of nested adaptive cycles that serves to represent a variety of systems and environmental problems. Adaptive cycles (A) at diﬀerent scales of the hierarchy (B) can be connected through remember and revolt cross-scale interactions. ecological resilience has been expensively replaced by artiﬁcial processes to maintain the system such as levees, ﬂood barriers, or chemical control of pests (C. S. Holling and Meﬀe 1996). Panarchy oﬀers a rich conceptual framework for understanding environmental problems. While inspired by a few mathematical constructs (e.g. cycles, traps, scaling laws), it is general enough to invite scholars from multiple disciplinary backgrounds, ontologies and epistemologies, to collaborate around research questions and applied problems. As such, it is useful as a boundary object and can be used empirically or metaphorically. The book itself presents a series of case studies where geographers, economists, political scientists, and ecologists have demonstrated the utility of the framework to their area of research. However, the extent to which these concepts have gained empirical support beyond the metaphor, or how they have evolved over time, remain open questions. For example, the adaptive cycle is a useful metaphor to look back at history and organize events and periods around phases, but it remains challenging to identify the phase of a system in real time, or to use panarchy to project future trajectories. In other words, panarchy is useful for retrospective studies, but its applications may be more limited for prospective ones. Are we at a stage where we can start distilling theory that enables case study comparisons, deriving functional forms, or draw theoretical expectations and predictions? Here we review the academic literature of the last two decades to trace how these concepts have evolved, in what type of problems they have been applied and found useful, and ﬁnally what remains as the key frontiers of research. Methods To answer these questions we combined an automated literature review based on topic modeling (Blei 2012; Griﬃths and Steyvers 2004) with human-coded", "meta": {"paper_id": "2012.14312v1", "chunk_index": 2, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_3", "text": "how these concepts have evolved, in what type of problems they have been applied and found useful, and ﬁnally what remains as the key frontiers of research. Methods To answer these questions we combined an automated literature review based on topic modeling (Blei 2012; Griﬃths and Steyvers 2004) with human-coded document analysis (Bryman 2008). All data and code to replicate the analysis are available at: https://github.com/juanrocha/panarchy and a public online repository (10.6084/m9.figshare.13490919). Data: We used the Web of Science, Scopus, and GoogleScholar to survey academic literature that has 3 used or referenced works that trace back to Panarchy (Gunderson and Holling 2002). We extracted complete records from the Scopus database that matched the search for “(panarchy OR adaptive cycle)” (N = 595), or the search “(panarchy OR adaptive cycle) AND resilience” (N = 278). The data was combined with all papers (N = 1923) that cited the inaugural paper that introduced the book to the academic community (C. S. Holling 2001). Records with missing abstracts were dropped (N = 191), and records with missing year were set to 2020 given that they are accepted manuscripts with digital object identiﬁer (doi) scheduled to be published later in 2020 or 2021. To prepare the data for topic modeling, we constructed a document term matrix with documents in rows and words in columns. Here our unit of analysis for the document are the abstracts retrieved, and the matrix contains the count of words per document. We removed stop words (e.g. “the,” “a”) and digits from the matrix, as well as a list of words that were over represented in our data and are common in the scientiﬁc literature but are unrelated to the papers’ topics (e.g. “paper,” “study,” “aim”). Topic models: are an unsupervised statistical technique to reduce the dimensionality of a corpus of data (typically but not necessarily text) into topics (Blei 2012). Here a topic is a latent variable that ranks words with high probability of appearing together within the same document. Documents in turn can be described by the probability distribution of a particular set of topics. Since they are (posterior) probabilities, the sum of the probability of all words for any given topic should be one, and the sum of the probability of all topics for any given document should also be one. An iterative process or algorithm is what allows the model to learn the ranking of words that best explain topics, and the ranking of topics that best explain documents. The underlying statistical technique for this machine learning approach is called Latent Dirichlet Allocation (LDA) (Blei, Ng, and Jordan 2003). It allocates probabilities to latent variables (topics) based on the distribution of words in text data, assuming a multivariate continuous (Dirichlet) distribution. We compared three LDA algorithms: correlated topic models (CTM), variational expectation maximization (VEM), and Gibbs sampling (Gibbs), by assessing their performance against their log-likelihood estimation, entropy and perplexity (Grün and Hornik 2011). Entropy is a measure of order or disorder of a system; in the context of topic models it measures how evenly the topic distribution is", "meta": {"paper_id": "2012.14312v1", "chunk_index": 3, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_4", "text": "models (CTM), variational expectation maximization (VEM), and Gibbs sampling (Gibbs), by assessing their performance against their log-likelihood estimation, entropy and perplexity (Grün and Hornik 2011). Entropy is a measure of order or disorder of a system; in the context of topic models it measures how evenly the topic distribution is spread. Perplexity measures the uncertainty of predicting a single word, so if the model performance were the same as random, perplexity would approximate the vocabulary size (16 728 words). These metrics enable us to choose which algorithm best ﬁts our data, what is the optimal number of topics to ﬁt, and how to avoid overﬁtting. Document analysis: We complemented our unsupervised approach with document analysis (Bryman 2008) by coding an additional set of categorical variables for a sample of highly cited papers (N = 41). We annotated qualitative aspects such as use of the adaptive cycle, identiﬁcation of its phases, whether the paper is conceptual, modeling or an empirical study. When empirical, we record the temporal and spatial scales of the case study. We also identiﬁed what aspects of panarchy were most used in the papers. For example whether there is emphasis on cross-scale interactions, or poverty and rigidity traps. We used text annotations to capture potential criticisms as well as the methods used. The code book is available in the supplementary material (archived in 10.6084/m9.figshare.13490919). Results Panarchy, the book published in 2002, has been cited over 7200 times according to Google Scholar. The scientiﬁc paper that introduced the book to the scientiﬁc community (C. S. Holling 2001) has 4 received 1715 citations in the Web of Science, and 1923 in Scopus. Roughly half of the citations have come from environmental (28%) and social sciences (22%). Computer science (2.1%) and arts and humanities (2.6%) have been the least represented. Figure 2: Algorithm and number of topic selection Gibbs sampling maximizes entropy and log-likelihood estimation, making it a suitable algorithm for our data (A). Increasing the number of topics (from 5-100) shows that α decreases, suggesting that despite the larger number of topics, a few of them suﬃce to describe most papers (B). Log-likelihood is maximized for 50 topics followed closely with 25. While 50 topics is marginally better, for visualization purposes we restrict our analysis to 25. Note that perplexity cannot be calculated for Gibbs sampling, hence the missing value in A and absence in B. Comparing algorithm performance revealed that Gibbs sampling was the best ﬁt for the data (Fig 2). As a rule of thumb, an ideal method should maximize entropy and the log-likelihood estimation while minimizing perplexity (Grün and Hornik 2011). Gibbs sampling maximised both entropy and likelihood with our data when compared to other alternatives. The second best was the variational-expectation maximisation algorithm (VEM) when α was not set constant. α is a hyper parameter that weights the evenness of topic distribution. A lower α than default values indicates that the documents can be described by rather fewer topics, or that its distribution is very uneven. In fact, we observed that increasing the number of topics from 5", "meta": {"paper_id": "2012.14312v1", "chunk_index": 4, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_5", "text": "set constant. α is a hyper parameter that weights the evenness of topic distribution. A lower α than default values indicates that the documents can be described by rather fewer topics, or that its distribution is very uneven. In fact, we observed that increasing the number of topics from 5 to 100 topics did increase entropy at expense of reducing α, meaning that despite the larger number of topics, the main content of a document is still captured by a few of them. The log-likelihood maximization stopped at around 25-50 topics. Thus, we restricted the rest of our analysis to 25 topics. A topic is a set of words that are ranked according to a probability that they represent an underlying content of the document (Fig 3). For example, the words “resilience,” “vulnerability,” “risk” and “disaster” have a high probability to capture the content of topic 25. Papers early in our time series 5 Figure 3: Panarchy topics over time The number of papers per year (A) with a maximum of 324 in 2019. The proportion of papers per year (B) and the relative proportion of topic content per year (C) do not show strong trends for the time window with most papers (gray area in A). Each topic is summarized in (D) by the top 10-words that best describe them according to the posterior probability of our model ﬁt. See FigS1 for a panel of time series per topics (2001-3) have high content largely dominated by topics 25 on resilience, 24 on the adaptive cycle, and 18 on adaptive governance and management. Towards the end of the time series, topic 3 on urban systems and 9 on local communities and knowledge have spikes up to 9% of the content of each year (2019-2020). For comparison, if all topics were equally represented in the content, they would have 4% in the corpus (grey line in Fig 3B). Despite ﬂuctuations, most topics showed a relatively constant level of interest over time (Fig 3, Fig S1). We did not observe strong trends, but some topics have gained a small amount of attention. For example, topic 1 on sustainability, or topic 10 on ecosystem services appeared consistently across time. In contrast, research on business innovation (topic 20), urban infrastructure (topic 3), or archaeology (topic 5) have gained attention in recent years. Topic 8 was an outlier, with a selection of papers ranking high on content related to network infrastructure and performance, possibly from engineering disciplines. It was the only topic with a set of papers that was clearly distinct from the rest of the collection, and showed a decreasing trend over time. 6 The human-coded document analysis revealed that the most common feature of panarchy in the literature was the adaptive cycle (81%, N = 42) followed by cross-scale interactions (Fig 4). Poverty and rigidity traps were less studied features in our sample, even when accounting for slightly diﬀerent terminology such as “lock-ins.” The bulk of the papers analyzed were conceptual papers, many without a method or discernible research question. Roughly half of the papers", "meta": {"paper_id": "2012.14312v1", "chunk_index": 5, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_6", "text": "42) followed by cross-scale interactions (Fig 4). Poverty and rigidity traps were less studied features in our sample, even when accounting for slightly diﬀerent terminology such as “lock-ins.” The bulk of the papers analyzed were conceptual papers, many without a method or discernible research question. Roughly half of the papers were conceptual or only used panarchy as a metaphor. Over 40% of papers in our sample went a step further and used panarchy as an empirical construct, for example by attempting to identify the phases of the adaptive cycle (61%). Out of the 22 empirical cases, 6 were at the time scale of centuries, 7 on decades, 6 on years, and 1 in weeks. Spatially, 4 were at a city scale, 17 regional, and 1 national. Empirical papers were dominated by qualitative methods (77%, N = 22) and were generally retrospective historical reviews. The high level of conceptualization but lack of empirically grounded hypotheses or theory testing came across as one of the major limitations. Nonetheless, several of the papers reviewed attempted to identify the adaptive cycle either through qualitative or quantitative methods. Identiﬁcation of adaptive cycles has found applications in a wide range of disciplines and research problems, from delimitation of periods in archaeology and anthropology (Redman and Kinzig 2003), to ﬁnancial crises in Europe (Castell and Schrenk 2020), or traﬃc jams in China (Zeng et al. 2020). Recent empirical tests of the adaptive cycle innovate on the types of data and methods that one can use. Information transfer methods based on entropy have been suggested to approximate relevant components of a system and the empirical proxies of the axis where the adaptive cycle unfolds: potential, connectedness, and resilience (Castell and Schrenk 2020). Percolation methods combined with big data have been shown to be useful to test hypotheses of regime shifts in urban systems (originally proposed in Panarchy (Gunderson and Holling 2002)), and derive the temporal and spatial scales at which the adaptive cycle emerges (Zeng et al. 2020). Figure 4: Qualitative results Document analysis was used to disentangle the diﬀerent panarchy features addressed by a sub-sample of papers (N = 42). Over half of the papers are conceptual work, and most empirical papers fall into qualitative methods. Discussion Given the strong dominance of conceptual papers, it is not surprising that one of the key frontiers of research is to empirically ground such conceptualizations in a way that one can test hypotheses 7 and advance theory beyond metaphors. Metaphors have played an important role as boundary objects enabling interdisciplinary dialogues. We observe it on the wide applications that panarchy has inspired, from anthropology to engineering. In the empirical realm, the adaptive cycle is the panarchy dimension that researchers engage the most with, but it is dominated by qualitative and retrospective studies. That means, we still lack the theory and methods to gather observations and be able to decide in which phase of the adaptive cycle a system currently is, or what is the probability of it transitioning to a new phase? The criticism of over conceptualization is not unique to the", "meta": {"paper_id": "2012.14312v1", "chunk_index": 6, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_7", "text": "That means, we still lack the theory and methods to gather observations and be able to decide in which phase of the adaptive cycle a system currently is, or what is the probability of it transitioning to a new phase? The criticism of over conceptualization is not unique to the literature engaging Panarchy ideas. A recent review of sustainability science mapped the diﬀerent schools of thought that the discipline has developed over the last decades (Clark and Harley 2020). A similar conclusion was reached, where too many conceptual frameworks have been developed, but fewer empirical attempts try to test the frameworks against data to falsify hypotheses. The review emphasizes the problem of measurement and observation (Clark and Harley 2020). In the context of panarchy, recent work has developed an information theory based approach that enables the identiﬁcation of adaptive cycles (Castell and Schrenk 2020). The authors identify phases of the adaptive cycle in the European ﬁnancial crises and in grassland ecosystems, but fail to ﬁnd support for the diﬀerence in speed between the forward and backward loops originally proposed in Panarchy. The panarchy dimensions that received less attention in our qualitative analysis are creative destruc- tion, rigidity and poverty traps. This may be at least partly due to the scope of our data: papers that have cited Panarchy or Holling’s (Gunderson and Holling 2002; C. S. Holling 2001). Concepts such as creative destruction and poverty traps precede Panarchy and therefore have been theorized and empirically grounded outside the panarchy stream of thinking. For example, the theory of poverty traps dates back to the 1950’s in economics, and has received both theoretical development (Bowles, Durlauf, and Hoﬀ2006) as well as empirical grounding (Banerjee et al. 2015; Banerjee and Duﬂo 2012) that has enabled researchers and governments to distinguish what kind of interventions are likely to reduce poverty. Here we encourage similar empirical eﬀorts in formalizing theory and empirical support for the adaptive cycles, their cross-scale interactions (remember and revolt), or how the hierarchical nested nature of complex systems enhance or erode resilience in social-ecological systems. This paper aimed to study the use of panarchy related concepts and their evolution since the publication of the book. Systematic literature reviews often suﬀer from the limitation of restricting sample size to a subset of what is readable on the time frame of a project – between dozens and maybe hundreds of papers. Qualitative analysis oﬀers rich insights into the papers but is limited by sample size. In our analysis, the qualitative insights could be biased towards a non-random selection of papers that aligned our research interests, or highly cited, review type of work. Topic modeling enabled us to complement the analysis to all papers reported in major scientiﬁc databases. It has the advantage of reproducibility and reduces sample bias, but oﬀers little insights about the dimensions of panarchy used, methods, or criticisms. Here we show that both methods combined are a powerful combination. Future studies could beneﬁt from including gray literature such as theses, books, non-governmental organizations reports, government agencies reports, or non-english literature. Previous studies", "meta": {"paper_id": "2012.14312v1", "chunk_index": 7, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_8", "text": "reduces sample bias, but oﬀers little insights about the dimensions of panarchy used, methods, or criticisms. Here we show that both methods combined are a powerful combination. Future studies could beneﬁt from including gray literature such as theses, books, non-governmental organizations reports, government agencies reports, or non-english literature. Previous studies have shown that the use of full text instead of abstracts can also oﬀer additional insights on the automated analyses, for example in attributing impacts of ecosystem services from regime shifts in social-ecological systems (Rocha and Wikström 2015). Replication attempts do however face the challenge of accessing full text when many of the papers are behind pay-walls. 8 Conclusion We reviewed the academic literature of the last two decades to trace how Panarchy related concepts have evolved, in what type of problems they have been applied and found useful, and what remains as the key frontiers of research. Despite a growing body of literature, no topic seems to dominate the academic production of scholars using these concepts. The feature most used is the adaptive cycle, and the problems where it is found most useful is in studies with a historical perspective, either in short time horizons such as natural resource management, urban development, or conservation; all the way to archeology and anthropology studies on the time horizon of centuries to millennia. Hierarchies in scale or maladaptive traps have received comparatively less attention. The frontiers of research point out to the problem of measurement and prediction. For example, how to observe and approximate resilience, connectedness and potential in the present; or how likely a current social-ecological system is to undergo a transition in the phases of the adaptive cycle, or a “memory” or “revolt” type of cross-scale interaction. While our survey of the literature oﬀers a few options to empirically ground panarchy concepts, we believe the problem of measurement and prediction are fertile ground for future research eﬀorts. Acknowledgements JCR acknowledges ﬁnancial support from Formas research grant (942-2015-731), the Stockholm Resilience Centre, and Stockholm University. References Banerjee, Abhijit, and Esther Duﬂo. 2012. Poor Economics. A Radical Rethinking of the Way to Fight Global Poverty. Hachette UK. Banerjee, Abhijit, Esther Duﬂo, Nathanael Goldberg, Dean Karlan, Robert Osei, William Parienté, Jeremy Shapiro, Bram Thuysbaert, and Christopher Udry. 2015. “A multifaceted program causes lasting progress for the very poor: Evidence from six countries.” Science 348 (6236): 1260799–99. Blei, David M. 2012. “Probabilistic topic models.” Commun ACM 55 (4): 77–84. Blei, David M, Andrew Y Ng, and Michael I Jordan. 2003. “Latent dirichlet allocation.” J. Mach. Learn. Res. 3: 993–1022. Bowles, Samuel, Steven N Durlauf, and Karla Hoﬀ. 2006. Poverty Traps. Princeton University Press. Bryman, A. 2008. Social Research Methods. 3 Ed. Oxford University Press. Castell, Wolfgang zu, and Hannah Schrenk. 2020. “Computing the adaptive cycle.” Scientiﬁc Reports 10 (1): 1. Clark, William C, and Alicia G Harley. 2020. “Sustainability Science: Toward a Synthesis.” Annual Review of Environment and Resources 45 (1): annurev-environ-012420-043621. Folke, Carl. 2016. “Resilience (Republished).” Ecology and Society 21 (4): art44. Griﬃths, Thomas L, and Mark Steyvers. 2004. “Finding scientiﬁc topics.” P Natl Acad", "meta": {"paper_id": "2012.14312v1", "chunk_index": 8, "filename": "2012.14312v1.pdf", "chunk_length": 512}}
{"id": "2012.14312v1_chunk_9", "text": "Reports 10 (1): 1. Clark, William C, and Alicia G Harley. 2020. “Sustainability Science: Toward a Synthesis.” Annual Review of Environment and Resources 45 (1): annurev-environ-012420-043621. Folke, Carl. 2016. “Resilience (Republished).” Ecology and Society 21 (4): art44. Griﬃths, Thomas L, and Mark Steyvers. 2004. “Finding scientiﬁc topics.” P Natl Acad Sci Usa 101 (suppl 1) (April): 5228–35. 9 Grün, Bettina, and Kurt Hornik. 2011. “topicmodels: An R package for ﬁtting topic models.” J Stat Softw 40 (13): 1–30. Gunderson, Lance H, and C S Holling. 2002. Panarchy. Understanding Transformations in Human and Natural Systems. Island Press. Holling, C S. 2001. “Understanding the Complexity of Economic, Ecological, and Social Systems.” Ecosystems 4 (5): 390–405. Holling, C S, Lance Gunderson, and G Peterson. 2002. “Sustainability and panarchies.” In Panarchy. Island Press. Holling, C S, and G Meﬀe. 1996. “Command and control and the pathology of natural resource management” 10 (2): 328–37. Holling, CS. 1986. “The resilience of terrestrial ecosystems: local surprise and global change.” In Sustainable Development of the Biosphere. Cambridge. Parker, John N, and Edward J Hackett. 2012. “Hot Spots and Hot Moments in Scientiﬁc Collabora- tions and Social Movements.” American Sociological Review 77 (1): 21–44. Redman, Charles L., and Ann P Kinzig. 2003. “Resilience of Past Landscapes: Resilience Theory, Society, and the Longue Durée.” Conservation Ecology 7 (1): art14. Rocha, Juan Carlos, and Robin Wikström. 2015. “Detecting potential impacts on ecosystem services related to ecological regime shifts a matter of wording.” In Regime Shifts in the Anthropocene, edited by Garry D Peterson and Reinette Oonsie Biggs. Stockholm: Stockholm Resilience Centre, Stockholm University. Zeng, Guanwen, Jianxi Gao, Louis Shekhtman, Shengmin Guo, Weifeng Lv, Jianjun Wu, Hao Liu, et al. 2020. “Multiple metastable network states in urban traﬃc” 117 (30): 17528–34. 10 Supplementary Material Figure S1: Proportion of papers per year per topic. 11", "meta": {"paper_id": "2012.14312v1", "chunk_index": 9, "filename": "2012.14312v1.pdf", "chunk_length": 302}}
