Abstract:We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. this https URL
Abstract:The "end-to-end" label for LLMs is a misnomer. In practice, they depend on a non-differentiable decoding process that requires laborious, hand-tuning of hyperparameters like temperature and top-p. This paper introduces AutoDeco, a novel architecture that enables truly "end-to-end" generation by learning to control its own decoding strategy. We augment the standard transformer with lightweight heads that, at each step, dynamically predict context-specific temperature and top-p values alongside the next-token logits. This approach transforms decoding into a parametric, token-level process, allowing the model to self-regulate its sampling strategy within a single forward pass.
Through extensive experiments on eight benchmarks, we demonstrate that AutoDeco not only significantly outperforms default decoding strategies but also achieves performance comparable to an oracle-tuned baseline derived from "hacking the test set"-a practical upper bound for any static method. Crucially, we uncover an emergent capability for instruction-based decoding control: the model learns to interpret natural language commands (e.g., "generate with low randomness") and adjusts its predicted temperature and top-p on a token-by-token basis, opening a new paradigm for steerable and interactive LLM decoding.
Abstract:We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.
We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.
To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.
Abstract:Large Language Models (LLMs) face significant inference latency challenges stemming from their autoregressive design and large size. To address this, speculative decoding emerges as a solution, enabling the simultaneous generation and validation of multiple tokens. While recent approaches like EAGLE-2 and EAGLE-3 improve speculative decoding using dynamic tree structures, they often neglect the impact of crucial system variables such as GPU devices and batch sizes.
Therefore, we introduce a new dynamic tree decoding approach called CAST that takes into account inference costs, including factors such as GPU configurations and batch sizes, to dynamically refine the tree structure. Through comprehensive experimentation across six diverse tasks and utilizing six distinct LLMs, our methodology demonstrates remarkable results, achieving speeds up to 5.2 times faster than conventional decoding methods. Moreover, it generally outperforms existing state-of-the-art techniques from 5% to 20%.
Abstract:Diacritics restoration in Hebrew is a fundamental task for ensuring accurate word pronunciation and disambiguating textual meaning. Despite the language's high degree of ambiguity when unvocalized, recent machine learning approaches have significantly advanced performance on this task.
In this work, we present DIVRIT, a novel system for Hebrew diacritization that frames the task as a zero-shot classification problem. Our approach operates at the word level, selecting the most appropriate diacritization pattern for each undiacritized word from a dynamically generated candidate set, conditioned on the surrounding textual context. A key innovation of DIVRIT is its use of a Hebrew Visual Language Model, which processes undiacritized text as an image, allowing diacritic information to be embedded directly within the input's vector representation.
Through a comprehensive evaluation across various configurations, we demonstrate that the system effectively performs diacritization without relying on complex, explicit linguistic analysis. Notably, in an ``oracle'' setting where the correct diacritized form is guaranteed to be among the provided candidates, DIVRIT achieves a high level of accuracy. Furthermore, strategic architectural enhancements and optimized training methodologies yield significant improvements in the system's overall generalization capabilities. These findings highlight the promising potential of visual representations for accurate and automated Hebrew diacritization.
Abstract:Purpose: The purpose of this study was to determine if an ensemble of multiple LLM agents could be used collectively to provide a more reliable assessment of a pixel-based AI triage tool than a single LLM.
Methods: 29,766 non-contrast CT head exams from fourteen hospitals were processed by a commercial intracranial hemorrhage (ICH) AI detection tool. Radiology reports were analyzed by an ensemble of eight open-source LLM models and a HIPAA compliant internal version of GPT-4o using a single multi-shot prompt that assessed for presence of ICH. 1,726 examples were manually reviewed. Performance characteristics of the eight open-source models and consensus were compared to GPT-4o. Three ideal consensus LLM ensembles were tested for rating the performance of the triage tool.
Results: The cohort consisted of 29,766 head CTs exam-report pairs. The highest AUC performance was achieved with llama3.3:70b and GPT-4o (AUC= 0.78). The average precision was highest for Llama3.3:70b and GPT-4o (AP=0.75 & 0.76). Llama3.3:70b had the highest F1 score (0.81) and recall (0.85), greater precision (0.78), specificity (0.72), and MCC (0.57). Using MCC (95% CI) the ideal combination of LLMs were: Full-9 Ensemble 0.571 (0.552-0.591), Top-3 Ensemble 0.558 (0.537-0.579), Consensus 0.556 (0.539-0.574), and GPT4o 0.522 (0.500-0.543). No statistically significant differences were observed between Top-3, Full-9, and Consensus (p > 0.05).
Conclusion: An ensemble of medium to large sized open-source LLMs provides a more consistent and reliable method to derive a ground truth retrospective evaluation of a clinical AI triage tool over a single LLM alone.
Abstract:With the increasing use of generative Artificial Intelligence (AI) methods to support science workflows, we are interested in the use of discourse-level information to find supporting evidence for AI generated scientific claims. A first step towards this objective is to examine the task of inferring discourse structure in scientific writing.
In this work, we present a preliminary investigation of pretrained language model (PLM) and Large Language Model (LLM) approaches for Discourse Relation Classification (DRC), focusing on scientific publications, an under-studied genre for this task. We examine how context can help with the DRC task, with our experiments showing that context, as defined by discourse structure, is generally helpful. We also present an analysis of which scientific discourse relation types might benefit most from context.
Abstract:Health-related misinformation is very prevalent and potentially harmful. It is difficult to identify, especially when claims distort or misinterpret scientific findings. We investigate the impact of synthetic data generation and lightweight fine-tuning techniques on the ability of large language models (LLMs) to recognize fallacious arguments using the MISSCI dataset and framework. In this work, we propose MisSynth, a pipeline that applies retrieval-augmented generation (RAG) to produce synthetic fallacy samples, which are then used to fine-tune an LLM model. Our results show substantial accuracy gains with fine-tuned models compared to vanilla baselines. For instance, the LLaMA 3.1 8B fine-tuned model achieved an over 35% F1-score absolute improvement on the MISSCI test split over its vanilla baseline. We demonstrate that introducing synthetic fallacy data to augment limited annotated resources can significantly enhance zero-shot LLM classification performance on real-world scientific misinformation tasks, even with limited computational resources. The code and synthetic dataset are available on this https URL.
Abstract:Large Language Models (LLMs) excel at general tasks but underperform in specialized domains like economics and psychology, which require deep, principled understanding. To address this, we introduce ACER (Automated Curriculum-Enhanced Regimen) that transforms generalist models into domain experts without sacrificing their broad capabilities. ACER first synthesizes a comprehensive, textbook-style curriculum by generating a table of contents for a subject and then creating question-answer (QA) pairs guided by Bloom's taxonomy. This ensures systematic topic coverage and progressively increasing difficulty. The resulting synthetic corpus is used for continual pretraining with an interleaved curriculum schedule, aligning learning across both content and cognitive dimensions.
Experiments with Llama 3.2 (1B and 3B) show significant gains in specialized MMLU subsets. In challenging domains like microeconomics, where baselines struggle, ACER boosts accuracy by 5 percentage points. Across all target domains, we observe a consistent macro-average improvement of 3 percentage points. Notably, ACER not only prevents catastrophic forgetting but also facilitates positive cross-domain knowledge transfer, improving performance on non-target domains by 0.7 points. Beyond MMLU, ACER enhances performance on knowledge-intensive benchmarks like ARC and GPQA by over 2 absolute points, while maintaining stable performance on general reasoning tasks. Our results demonstrate that ACER offers a scalable and effective recipe for closing critical domain gaps in LLMs.
Abstract:OpenAI's ChatGPT Atlas introduces new capabilities for web interaction, enabling the model to analyze webpages, process user intents, and execute cursor and keyboard inputs directly within the browser. While its capacity for information retrieval tasks has been demonstrated, its performance in dynamic, interactive environments remains less explored. In this study, we conduct an early evaluation of Atlas's web interaction capabilities using browser-based games as test scenarios, including Google's T-Rex Runner, Sudoku, Flappy Bird, and this http URL. We employ in-game performance scores as quantitative metrics to assess performance across different task types. Our results show that Atlas performs strongly in logical reasoning tasks like Sudoku, completing puzzles significantly faster than human baselines, but struggles substantially in real-time games requiring precise timing and motor control, often failing to progress beyond initial obstacles. These findings suggest that while Atlas demonstrates capable analytical processing, there remain notable limitations in dynamic web environments requiring real-time interaction. The website of our project can be found at this https URL.
Abstract:While Test-Time Scaling (TTS) has proven effective in improving the reasoning ability of large language models (LLMs), low diversity in model outputs often becomes a bottleneck; this is partly caused by the common "one problem, one solution" (1P1S) training practice, which provides a single canonical answer and can push models toward a narrow set of reasoning paths. To address this, we propose a "one problem, multiple solutions" (1PNS) training paradigm that exposes the model to a variety of valid reasoning trajectories and thus increases inference diversity. A core challenge for 1PNS is reliably measuring semantic differences between multi-step chains of thought, so we introduce Reasoning Path Divergence (RPD), a step-level metric that aligns and scores Long Chain-of-Thought solutions to capture differences in intermediate reasoning. Using RPD, we curate maximally diverse solution sets per problem and fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields more varied outputs and higher pass@k, with an average +2.80% gain in pass@16 over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that 1PNS further amplifies the effectiveness of TTS. Our code is available at this https URL .
Abstract:Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: this https URL
Title: Beyond Long Context: When Semantics Matter More than Tokens
Authors: Tarun Kumar Chawdhury, Jon D. Duke
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25816

=== Abstract ===
Title: A Survey on Efficient Large Language Model Training: From Data-centric Perspectives
Authors: Junyu Luo, Bohan Wu, Xiao Luo, Zhiping Xiao, Yiqiao Jin, Rong-Cheng Tu, Nan Yin, Yifan Wang, Jingyang Yuan, Wei Ju, Ming Zhang
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25817

=== Abstract ===

Abstract:Post-training of Large Language Models (LLMs) is crucial for unlocking their task generalization potential and domain-specific capabilities. However, the current LLM post-training paradigm faces significant data challenges, including the high costs of manual annotation and diminishing marginal returns on data scales. Therefore, achieving data-efficient post-training has become a key research question. In this paper, we present the first systematic survey of data-efficient LLM post-training from a data-centric perspective. We propose a taxonomy of data-efficient LLM post-training methods, covering data selection, data quality enhancement, synthetic data generation, data distillation and compression, and self-evolving data ecosystems. We summarize representative approaches in each category and outline future research directions. By examining the challenges in data-efficient LLM post-training, we highlight open problems and propose potential research avenues. We hope our work inspires further exploration into maximizing the potential of data utilization in large-scale model training. Paper List: this https URL
Title: Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation
Authors: Frederico Belcavello, Ely Matos, Arthur Lorenzi, Lisandra Bonoto, Lívia Ruiz, Luiz Fernando Pereira, Victor Herbst, Yulla Navarro, Helen de Andrade Abreu, Lívia Dutra, Tiago Timponi Torrent
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25904

=== Abstract ===
Title: RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline
Authors: André V. Duarte, Xuying li, Bin Zeng, Arlindo L. Oliveira, Lei Li, Zhuo Li
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25941

=== Abstract ===
Title: Revisiting Multilingual Data Mixtures in Language Model Pretraining
Authors: Negar Foroutan, Paul Teiletche, Ayush Kumar Tarun, Antoine Bosselut
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25947

=== Abstract ===
Title: Semantic Label Drift in Cross-Cultural Translation
Authors: Mohsinul Kabir, Tasnim Ahmed, Md Mezbaur Rahman, Polydoros Giannouris, Sophia Ananiadou
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25967

=== Abstract ===
Title: SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation
Authors: Sina Bagheri Nezhad, Yao Li, Ameeta Agrawal
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25975

=== Abstract ===
Title: NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium
Authors: Dinghong Song (1), Jierui Xu (2), Weichu Yang (2), Pengfei Su (1), Dong Li (1) ((1) University of California, Merced, (2) University of Wisconsin, Madison)
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25977

=== Abstract ===
Title: AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache
Authors: Dinghong Song (1), Yuan Feng (1), Yiwei Wang (1), Shangye Chen (1), Cyril Guyot (2), Filip Blagojevic (2), Hyeran Jeon (1), Pengfei Su (1), Dong Li (1) ((1) University of California, Merced, USA, (2) Western Digital Research, USA)
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25979

=== Abstract ===
Title: Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning
Authors: Yihe Deng, I-Hung Hsu, Jun Yan, Zifeng Wang, Rujun Han, Gufeng Zhang, Yanfei Chen, Wei Wang, Tomas Pfister, Chen-Yu Lee
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.25992

=== Abstract ===
Title: PORTool: Tool-Use LLM Training with Rewarded Tree
Authors: Feijie Wu, Weiwu Zhu, Yuxiang Zhang, Soumya Chatterjee, Jiarong Zhu, Fan Mo, Rodin Luo, Jing Gao
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.26020

=== Abstract ===
Title: Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs
Authors: HyoJung Han, Sweta Agrawal, Eleftheria Briakou
Date: [Submitted on 29 Oct 2025]
URL: https://arxiv.org/abs/2510.26024

=== Abstract ===
Title: Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings
Authors: Felipe Larios, Mariana Borras-Osorio, Yuqi Wu, Ana Gabriela Claros, David Toro-Tobon, Esteban Cabezas, Ricardo Loor-Torres, Maria Mateo Chavez, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Maria Lizarazo Jimenez, Ivan Mateo Alzamora, Misk Al Zahidy, Marcelo Montero, Ana Cristina Proano, Cristian Soto Jacome, Jungwei W. Fan, Oscar J. Ponce-Ponte, Megan E. Branda, Naykky Singh Ospina, Juan P. Brito
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26032

=== Abstract ===
Title: On the Influence of Discourse Relations in Persuasive Texts
Authors: Nawar Turk, Sevag Kaspar, Leila Kosseim
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26124

=== Abstract ===
Title: MossNet: Mixture of State-Space Experts is a Multi-Head Attention
Authors: Shikhar Tuli, James Seale Smith, Haris Jeelani, Chi-Heng Lin, Abhishek Patel, Vasili Ramanishka, Yen-Chang Hsu, Hongxia Jin
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26182

=== Abstract ===
Title: Similarity-Distance-Magnitude Language Models
Authors: Allen Schmaltz
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26183

=== Abstract ===
Title: RCScore: Quantifying Response Consistency in Large Language Models
Authors: Dongjun Jang, Youngchae Ahn, Hyopil Shin
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26193

=== Abstract ===
Title: Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation
Authors: Woojin Kim, Jaeyoung Do
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26200

=== Abstract ===
Title: What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data
Authors: Rajiv Movva, Smitha Milli, Sewon Min, Emma Pierson
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26202

=== Abstract ===
Title: Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning
Authors: Qi Luo, Xiaonan Li, Tingshuo Fan, Xinchi Chen, Xipeng Qiu
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26205

=== Abstract ===
Title: Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs
Authors: Takuma Sato, Seiya Kawano, Koichiro Yoshino
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26253

=== Abstract ===
Title: Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages
Authors: Mérilin Sousa Silva, Sina Ahmadi
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26254

=== Abstract ===
Title: Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual
Authors: Sukrit Sriratanawilai, Jhayahgrit Thongwat, Romrawin Chumpu, Patomporn Payoungkhamdee, Sarana Nutanong, Peerat Limkonchotiwat
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26271

=== Abstract ===
Title: Do LLMs Signal When They're Right? Evidence from Neuron Agreement
Authors: Kang Chen, Yaoning Wang, Kai Xiong, Zhuoka Feng, Wenhe Sun, Haotian Chen, Yixin Cao
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26277

=== Abstract ===
Title: Unravelling the Mechanisms of Manipulating Numbers in Language Models
Authors: Michal Štefánik, Timothee Mickus, Marek Kadlčík, Bertram Højer, Michal Spiegel, Raúl Vázquez, Aman Sinha, Josef Kuchař, Philipp Mondorf
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26285

=== Abstract ===
Title: SCRIBE: Structured Chain Reasoning for Interactive Behaviour Explanations using Tool Calling
Authors: Fares Fawzi, Vinitra Swamy, Dominik Glandorf, Tanya Nazaretsky, Tanja Käser
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26322

=== Abstract ===
Title: The Geometry of Dialogue: Graphing Language Models to Reveal Synergistic Teams for Multi-Agent Collaboration
Authors: Kotaro Furuya, Yuichi Kitagawa
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26352

=== Abstract ===
Title: OmniEduBench: A Comprehensive Chinese Benchmark for Evaluating Large Language Models in Education
Authors: Min Zhang, Hao Chen, Hao Chen, Wenqi Zhang, Didi Zhu, Xin Lin, Bo Jiang, Aimin Zhou, Fei Wu, Kun Kuang
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26422

=== Abstract ===
Title: 1+1>2: A Synergistic Sparse and Low-Rank Compression Method for Large Language Models
Authors: Zeliang Zong, Kai Zhang, Zheyang Li, Wenming Tan, Ye Ren, Yiyan Zhai, Jilin Hu
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26446

=== Abstract ===
Title: Bayesian Network Fusion of Large Language Models for Sentiment Analysis
Authors: Rasoul Amirzadeh, Dhananjay Thiruvady, Fatemeh Shiri
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26484

=== Abstract ===
Title: Inside CORE-KG: Evaluating Structured Prompting and Coreference Resolution for Knowledge Graphs
Authors: Dipak Meher, Carlotta Domeniconi
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26512

=== Abstract ===
Title: The Structure of Relation Decoding Linear Operators in Large Language Models
Authors: Miranda Anna Christ, Adrián Csiszárik, Gergely Becsó, Dániel Varga
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26543

=== Abstract ===
Title: InfoFlow: Reinforcing Search Agent Via Reward Density Optimization
Authors: Kun Luo, Hongjin Qian, Zheng Liu, Ziyi Xia, Shitao Xiao, Siqi Bao, Jun Zhao, Kang Liu
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26575

=== Abstract ===
Title: SlideAgent: Hierarchical Agentic Framework for Multi-Page Visual Document Understanding
Authors: Yiqiao Jin, Rachneet Kaur, Zhen Zeng, Sumitra Ganesh, Srijan Kumar
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26615

=== Abstract ===
Title: Encoder-Decoder or Decoder-Only? Revisiting Encoder-Decoder Large Language Model
Authors: Biao Zhang, Yong Cheng, Siamak Shakeri, Xinyi Wang, Min Ma, Orhan Firat
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26622

=== Abstract ===
Title: Evontree: Ontology Rule-Guided Self-Evolution of Large Language Models
Authors: Mingchen Tu, Zhiqiang Liu, Juan Li, Liangyurui Liu, Junjie Wang, Lei Liang, Wen Zhang
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26683

=== Abstract ===
Title: Kimi Linear: An Expressive, Efficient Attention Architecture
Authors: Kimi Team: Yu Zhang, Zongyu Lin, Xingcheng Yao, Jiaxi Hu, Fanqing Meng, Chengyin Liu, Xin Men, Songlin Yang, Zhiyuan Li, Wentao Li, Enzhe Lu, Weizhou Liu, Yanru Chen, Weixin Xu, Longhui Yu, Yejie Wang, Yu Fan, Longguang Zhong, Enming Yuan, Dehao Zhang, Yizhi Zhang, T.Y. Liu, Haiming Wang, Shengjun Fang, Weiran He, Shaowei Liu, Yiwei Li, Jianlin Su, Jiezhong Qiu, Bo Pang, Junjie Yan, Zhejun Jiang, Weixiao Huang, Bohong Yin, Jiacheng You, Chu Wei, Zhengtao Wang, Chao Hong, Yutian Chen, Guanduo Chen, Yucheng Wang, Huabin Zheng, Feng Wang, Yibo Liu, Mengnan Dong, Zheng Zhang, Siyuan Pan, Wenhao Wu, Yuhao Wu, Longyu Guan, Jiawen Tao, Guohong Fu, Xinran Xu, Yuzhi Wang, Guokun Lai, Yuxin Wu, Xinyu Zhou, Zhilin Yang, Yulun Du
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26692

=== Abstract ===

Abstract:We introduce Kimi Linear, a hybrid linear attention architecture that, for the first time, outperforms full attention under fair comparisons across various scenarios -- including short-context, long-context, and reinforcement learning (RL) scaling regimes. At its core lies Kimi Delta Attention (KDA), an expressive linear attention module that extends Gated DeltaNet with a finer-grained gating mechanism, enabling more effective use of limited finite-state RNN memory. Our bespoke chunkwise algorithm achieves high hardware efficiency through a specialized variant of the Diagonal-Plus-Low-Rank (DPLR) transition matrices, which substantially reduces computation compared to the general DPLR formulation while remaining more consistent with the classical delta rule.
We pretrain a Kimi Linear model with 3B activated parameters and 48B total parameters, based on a layerwise hybrid of KDA and Multi-Head Latent Attention (MLA). Our experiments show that with an identical training recipe, Kimi Linear outperforms full MLA with a sizeable margin across all evaluated tasks, while reducing KV cache usage by up to 75% and achieving up to 6 times decoding throughput for a 1M context. These results demonstrate that Kimi Linear can be a drop-in replacement for full attention architectures with superior performance and efficiency, including tasks with longer input and output lengths.
To support further research, we open-source the KDA kernel and vLLM implementations, and release the pre-trained and instruction-tuned model checkpoints.
Title: Value Drifts: Tracing Value Alignment During LLM Post-Training
Authors: Mehar Bhatia, Shravan Nayak, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Vered Shwartz, Siva Reddy
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26707

=== Abstract ===
Title: AMO-Bench: Large Language Models Still Struggle in High School Math Competitions
Authors: Shengnan An, Xunliang Cai, Xuezhi Cao, Xiaoyu Li, Yehao Lin, Junlin Liu, Xinxuan Lv, Dan Ma, Xuanlin Wang, Ziwen Wang, Shuang Zhou (Alphabetical order by last name)
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26768

=== Abstract ===

Abstract:We present AMO-Bench, an Advanced Mathematical reasoning benchmark with Olympiad level or even higher difficulty, comprising 50 human-crafted problems. Existing benchmarks have widely leveraged high school math competitions for evaluating mathematical reasoning capabilities of large language models (LLMs). However, many existing math competitions are becoming less effective for assessing top-tier LLMs due to performance saturation (e.g., AIME24/25). To address this, AMO-Bench introduces more rigorous challenges by ensuring all 50 problems are (1) cross-validated by experts to meet at least the International Mathematical Olympiad (IMO) difficulty standards, and (2) entirely original problems to prevent potential performance leakages from data memorization. Moreover, each problem in AMO-Bench requires only a final answer rather than a proof, enabling automatic and robust grading for evaluation. Experimental results across 26 LLMs on AMO-Bench show that even the best-performing model achieves only 52.4% accuracy on AMO-Bench, with most LLMs scoring below 40%. Beyond these poor performances, our further analysis reveals a promising scaling trend with increasing test-time compute on AMO-Bench. These results highlight the significant room for improving the mathematical reasoning in current LLMs. We release AMO-Bench to facilitate further research into advancing the reasoning abilities of language models. this https URL
Title: Gistify! Codebase-Level Understanding via Runtime Execution
Authors: Hyunji Lee, Minseon Kim, Chinmay Singh, Matheus Pereira, Atharv Sonwane, Isadora White, Elias Stengel-Eskin, Mohit Bansal, Zhengyan Shi, Alessandro Sordoni, Marc-Alexandre Côté, Xingdi Yuan, Lucas Caccia
Date: [Submitted on 30 Oct 2025]
URL: https://arxiv.org/abs/2510.26790

=== Abstract ===
